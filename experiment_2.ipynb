{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0091b416",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: torchviz in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (0.0.3)\n",
      "Requirement already satisfied: torch in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torchviz) (2.5.1)\n",
      "Requirement already satisfied: graphviz in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torchviz) (0.21)\n",
      "Requirement already satisfied: filelock in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (1.13.1)\n",
      "Requirement already satisfied: torchviz in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (0.0.3)\n",
      "Requirement already satisfied: torch in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torchviz) (2.5.1)\n",
      "Requirement already satisfied: graphviz in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torchviz) (0.21)\n",
      "Requirement already satisfied: filelock in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from torch->torchviz) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from jinja2->torch->torchviz) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages (from jinja2->torch->torchviz) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51f53d",
   "metadata": {},
   "source": [
    "Experiment # 2\n",
    "===============\n",
    "\n",
    "Generative transformer backbone (pretrained with tagalog websites) + classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e84136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedTokenizerFast,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import transformers\n",
    "\n",
    "try:\n",
    "    from torch.optim.lr_scheduler import LRScheduler  # type: ignore[attr-defined]\n",
    "except ImportError:  # pragma: no cover - fallback for older PyTorch versions\n",
    "    from torch.optim.lr_scheduler import _LRScheduler as LRScheduler  # type: ignore\n",
    "\n",
    "@dataclass\n",
    "class TextCorpus:\n",
    "    texts: List[str]\n",
    "    source: str\n",
    "\n",
    "class CharacterSequenceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: Sequence[str],\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        block_size: int,\n",
    "        stride: int,\n",
    "    ) -> None:\n",
    "        if block_size < 2:\n",
    "            raise ValueError(\"block_size must be at least 2\")\n",
    "        if stride < 1:\n",
    "            raise ValueError(\"stride must be at least 1\")\n",
    "\n",
    "        # Suppress token length warnings as we are intentionally encoding long sequences\n",
    "        verbosity = transformers.logging.get_verbosity()\n",
    "        transformers.logging.set_verbosity_error()\n",
    "\n",
    "        try:\n",
    "            token_sequence: List[int] = []\n",
    "            for text in texts:\n",
    "                text = str(text)\n",
    "                if not text.strip():\n",
    "                    continue\n",
    "                encoded = tokenizer.encode(text, add_special_tokens=True, truncation=False)\n",
    "                if not encoded:\n",
    "                    continue\n",
    "                token_sequence.extend(encoded)\n",
    "        finally:\n",
    "            transformers.logging.set_verbosity(verbosity)\n",
    "\n",
    "        if len(token_sequence) < block_size + 1:\n",
    "            raise ValueError(\n",
    "                \"Not enough tokens to create sequences. Try reducing block_size or increasing the number of samples.\"\n",
    "            )\n",
    "\n",
    "        self.tokens = torch.tensor(token_sequence, dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "        self.stride = stride\n",
    "        max_start = len(self.tokens) - (block_size + 1)\n",
    "        if max_start < 0:\n",
    "            raise ValueError(\"Not enough tokens to create input-target pairs for the requested block size.\")\n",
    "        starts = list(range(0, max_start + 1, stride))\n",
    "        if not starts or starts[-1] != max_start:\n",
    "            starts.append(max_start)\n",
    "        self.starts = starts\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.starts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        start = self.starts[idx]\n",
    "        x = self.tokens[start : start + self.block_size]\n",
    "        y = self.tokens[start + 1 : start + self.block_size + 1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9183b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73938e40",
   "metadata": {},
   "source": [
    "\n",
    "Experiment # 2: Train a generative model and then use that model as a backbone for the hate speech classifier\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "OUTPUT_DIR = \"models/tagalog_lm\"\n",
    "MAX_POSITION_EMBEDDINGS = 512\n",
    "EMBED_DIMENSION = 256\n",
    "BLOCK_SIZE = 256\n",
    "NUM_HEADS = 4\n",
    "\n",
    "TOKENIZER_VOCAB_SIZE = 32000\n",
    "MIN_FREQUENCY = 2\n",
    "STRIDE = 128\n",
    "VAL_RATIO = 0.1\n",
    "NUM_LAYERS = 6\n",
    "MAX_POSITION_EMBEDDINGS = 512\n",
    "DROPOUT = 0.1\n",
    "FF_MULTIPLIER = 4\n",
    "BATCH_SIZE = 32\n",
    "WARMUP_RATIO = 0.1\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 3e-4\n",
    "LR_SCHEDULER = \"cosine\"\n",
    "WEIGHT_DECAY = 1e-2\n",
    "GRADIENT_CLIPPING = 1.0\n",
    "LOG_INTERVAL = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b1d62",
   "metadata": {},
   "source": [
    "Step 1: Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe4acac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text cleaning function\n",
    "def _clean_texts(texts: Iterable[str]) -> List[str]:\n",
    "    return [str(text).strip() for text in texts if isinstance(text, str) and str(text).strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b97708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer_from_corpus(\n",
    "    texts: Sequence[str],\n",
    "    vocab_size: int,\n",
    "    min_frequency: int,\n",
    "    limit_alphabet: int,\n",
    "    lowercase: bool,\n",
    "    sample_size: int,\n",
    "    seed: int,\n",
    "    max_length: int,\n",
    ") -> Tuple[PreTrainedTokenizerBase, int]:\n",
    "    try:\n",
    "        from tokenizers import Tokenizer\n",
    "        from tokenizers.models import BPE\n",
    "        from tokenizers.normalizers import Lowercase, NFKC, Sequence as NormalizerSequence\n",
    "        from tokenizers.pre_tokenizers import ByteLevel\n",
    "        from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "        from tokenizers.processors import TemplateProcessing\n",
    "        from tokenizers.trainers import BpeTrainer\n",
    "    except ImportError as exc:  # pragma: no cover - optional dependency\n",
    "        raise RuntimeError(\n",
    "            \"The 'tokenizers' package is required to train a tokenizer from scratch. Install tokenizers>=0.13.\"\n",
    "        ) from exc\n",
    "\n",
    "    if not texts:\n",
    "        raise ValueError(\"Cannot train tokenizer without any text samples.\")\n",
    "\n",
    "    population = range(len(texts))\n",
    "    if sample_size > 0 and sample_size < len(texts):\n",
    "        rng = random.Random(seed)\n",
    "        selected_indices = sorted(rng.sample(population, sample_size))\n",
    "    else:\n",
    "        selected_indices = population\n",
    "\n",
    "    def iterator() -> Iterable[str]:\n",
    "        for idx in selected_indices:\n",
    "            text = texts[idx]\n",
    "            if text:\n",
    "                yield text\n",
    "\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "    normalizers = [NFKC()]\n",
    "    if lowercase:\n",
    "        normalizers.append(Lowercase())\n",
    "    tokenizer.normalizer = normalizers[0] if len(normalizers) == 1 else NormalizerSequence(normalizers)\n",
    "    tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=True)\n",
    "\n",
    "    trainer_kwargs = {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"min_frequency\": max(1, min_frequency),\n",
    "        \"special_tokens\": [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\"],\n",
    "        \"initial_alphabet\": ByteLevel.alphabet(),\n",
    "    }\n",
    "    if limit_alphabet > 0:\n",
    "        trainer_kwargs[\"limit_alphabet\"] = limit_alphabet\n",
    "\n",
    "    trainer = BpeTrainer(**trainer_kwargs)\n",
    "    tokenizer.train_from_iterator(iterator(), trainer=trainer, length=len(selected_indices))\n",
    "\n",
    "    if tokenizer.token_to_id(\"<s>\") is None or tokenizer.token_to_id(\"</s>\") is None:\n",
    "        raise ValueError(\"Tokenizer training failed to include required special tokens <s> and </s>.\")\n",
    "\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"<s> $A </s>\",\n",
    "        pair=\"<s> $A </s> </s> $B </s>\",\n",
    "        special_tokens=[\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "        ],\n",
    "    )\n",
    "    tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    fast_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\",\n",
    "    )\n",
    "    fast_tokenizer.model_max_length = max_length\n",
    "    fast_tokenizer.init_kwargs[\"model_max_length\"] = max_length\n",
    "    fast_tokenizer.padding_side = \"right\"\n",
    "    fast_tokenizer.truncation_side = \"right\"\n",
    "    return fast_tokenizer, len(selected_indices)\n",
    "\n",
    "\n",
    "def save_tokenizer_artifacts(\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    save_dir: Path,\n",
    "    metadata: Optional[Dict[str, object]] = None,\n",
    ") -> None:\n",
    "    if save_dir.exists():\n",
    "        shutil.rmtree(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tokenizer.save_pretrained(save_dir, legacy_format=False)\n",
    "    if metadata:\n",
    "        metadata_path = save_dir / \"training_metadata.json\"\n",
    "        metadata_path.write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39369290",
   "metadata": {},
   "source": [
    "Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f6d8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_COMBINED_CSV = Path(\"data/combined/processed/train.csv\")\n",
    "\n",
    "DEFAULT_CORPUS_CANDIDATES: Tuple[Path, ...] = (\n",
    "    DEFAULT_COMBINED_CSV,\n",
    "    Path(\"data/processed\"),\n",
    "    Path(\"data/tagalog_corpus/all_texts.txt\"),\n",
    "    Path(\"data/tagalog_corpus_test/all_texts.txt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d9345ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_vocab_size(tokenizer: PreTrainedTokenizerBase) -> int:\n",
    "    try:\n",
    "        return len(tokenizer)\n",
    "    except TypeError:\n",
    "        return len(tokenizer.get_vocab())\n",
    "    \n",
    "def create_datasets(\n",
    "    texts: Sequence[str],\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    block_size: int,\n",
    "    stride: int,\n",
    "    val_ratio: float,\n",
    "    seed: int,\n",
    ") -> Tuple[CharacterSequenceDataset, CharacterSequenceDataset]:\n",
    "    if not 0 < val_ratio < 1:\n",
    "        raise ValueError(\"val_ratio must be between 0 and 1\")\n",
    "\n",
    "    texts = list(texts)\n",
    "    random.Random(seed).shuffle(texts)\n",
    "    val_count = max(1, int(len(texts) * val_ratio))\n",
    "    if val_count >= len(texts):\n",
    "        val_count = min(len(texts) // 5, len(texts) - 1) or 1\n",
    "    train_texts = texts[:-val_count]\n",
    "    val_texts = texts[-val_count:]\n",
    "    if not train_texts:\n",
    "        raise ValueError(\"Training texts are empty after splitting. Reduce val_ratio or provide more data.\")\n",
    "\n",
    "    train_dataset = CharacterSequenceDataset(train_texts, tokenizer, block_size, stride)\n",
    "    val_dataset = CharacterSequenceDataset(val_texts, tokenizer, block_size, stride)\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def load_additional_combined_texts() -> Tuple[List[str], Optional[str]]:\n",
    "    csv_path = DEFAULT_COMBINED_CSV\n",
    "    if not csv_path.exists():\n",
    "        return [], None\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[0])\n",
    "    except Exception:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    if df.empty:\n",
    "        return [], None\n",
    "    first_col = df.columns[0]\n",
    "    texts = _clean_texts(df[first_col].tolist())\n",
    "    if not texts:\n",
    "        return [], None\n",
    "    return texts, str(csv_path)\n",
    "\n",
    "def resolve_default_corpus_file() -> Optional[Path]:\n",
    "    for candidate in DEFAULT_CORPUS_CANDIDATES:\n",
    "        candidate_path = candidate\n",
    "        if candidate_path.exists():\n",
    "            return candidate_path\n",
    "    return None\n",
    "\n",
    "def load_text_corpus(dataset_name: str, split: str = \"train\", text_column: str = \"text\") -> TextCorpus:\n",
    "    if dataset_name is None:\n",
    "        raise ValueError(\"dataset_name cannot be None\")\n",
    "\n",
    "    if isinstance(dataset_name, Path):\n",
    "        dataset_name = str(dataset_name)\n",
    "\n",
    "    all_texts = []\n",
    "    sources = []\n",
    "\n",
    "    # 1. Load from data/raw/tagalog (recursively find jsonl files)\n",
    "    raw_tagalog_path = Path(\"data/raw/tagalog\")\n",
    "    if raw_tagalog_path.exists() and raw_tagalog_path.is_dir():\n",
    "        jsonl_files = sorted(list(raw_tagalog_path.rglob(\"*.jsonl\")))\n",
    "        if jsonl_files:\n",
    "            print(f\"Found {len(jsonl_files)} JSONL files in {raw_tagalog_path}\")\n",
    "            try:\n",
    "                dataset = load_dataset(\"json\", data_files={split: [str(f) for f in jsonl_files]}, split=split)\n",
    "                if text_column in dataset.column_names:\n",
    "                    texts = _clean_texts(dataset[text_column])\n",
    "                    all_texts.extend(texts)\n",
    "                    sources.append(f\"raw_tagalog_jsonl({len(texts)})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading raw tagalog jsonl: {e}\")\n",
    "\n",
    "    # 2. Load from data/combined/processed/train.csv\n",
    "    combined_csv_path = Path(\"data/combined/processed/train.csv\")\n",
    "    if combined_csv_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(combined_csv_path)\n",
    "            if text_column in df.columns:\n",
    "                texts = _clean_texts(df[text_column].tolist())\n",
    "                all_texts.extend(texts)\n",
    "                sources.append(f\"combined_train_csv({len(texts)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading combined train csv: {e}\")\n",
    "\n",
    "    # 3. Fallback/Additional loading logic if dataset_name is provided and not covered above\n",
    "    # (This part preserves existing logic for other potential inputs)\n",
    "    if not all_texts and dataset_name:\n",
    "         if os.path.exists(dataset_name):\n",
    "            if os.path.isdir(dataset_name):\n",
    "                # Check for jsonl files recursively\n",
    "                jsonl_files = sorted(list(Path(dataset_name).rglob(\"*.jsonl\")))\n",
    "                if jsonl_files:\n",
    "                    print(f\"Found {len(jsonl_files)} JSONL files in {dataset_name}\")\n",
    "                    dataset = load_dataset(\"json\", data_files={split: [str(f) for f in jsonl_files]}, split=split)\n",
    "                else:\n",
    "                     dataset = load_dataset(dataset_name, split=split)\n",
    "            elif dataset_name.endswith('.txt'):\n",
    "                dataset = load_dataset(\"text\", data_files={split: dataset_name}, split=split)\n",
    "            elif dataset_name.endswith('.csv'):\n",
    "                dataset = load_dataset(\"csv\", data_files={split: dataset_name}, split=split)\n",
    "            elif dataset_name.endswith('.jsonl') or dataset_name.endswith('.json'):\n",
    "                 dataset = load_dataset(\"json\", data_files={split: dataset_name}, split=split)\n",
    "            else:\n",
    "                 dataset = load_dataset(dataset_name, split=split)\n",
    "            \n",
    "            if text_column in dataset.column_names:\n",
    "                texts = _clean_texts(dataset[text_column])\n",
    "                all_texts.extend(texts)\n",
    "                sources.append(f\"{dataset_name}({len(texts)})\")\n",
    "\n",
    "    if not all_texts:\n",
    "        raise ValueError(\"No text data found from specified sources.\")\n",
    "\n",
    "    print(f\"Loaded total {len(all_texts)} samples from sources: {', '.join(sources)}\")\n",
    "    \n",
    "    # Shuffle combined texts\n",
    "    random.shuffle(all_texts)\n",
    "\n",
    "    return TextCorpus(texts=all_texts, source=\"combined_sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42768987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 JSONL files in data/raw/tagalog\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffc272caa44472e93e4277408d95954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded total 29056 samples from sources: raw_tagalog_jsonl(10888), combined_train_csv(18168)\n",
      "Training new tokenizer from corpus...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finished tokenizer training | vocab size: 32000 | samples used: 29056\n",
      "\n",
      "Finished tokenizer training | vocab size: 32000 | samples used: 29056\n"
     ]
    }
   ],
   "source": [
    "default_corpus = resolve_default_corpus_file()\n",
    "corpus = load_text_corpus(default_corpus)\n",
    "tokenizer_sampled = len(corpus.texts)\n",
    "tokenizer_target_max_length = max(MAX_POSITION_EMBEDDINGS, BLOCK_SIZE + 1, BLOCK_SIZE * 4)\n",
    "\n",
    "print(\"Training new tokenizer from corpus...\")\n",
    "tokenizer, tokenizer_sampled = train_tokenizer_from_corpus(\n",
    "    texts=corpus.texts,\n",
    "    vocab_size= TOKENIZER_VOCAB_SIZE,\n",
    "    min_frequency=MIN_FREQUENCY,\n",
    "    limit_alphabet=1000,\n",
    "    lowercase=False,\n",
    "    sample_size=-1,\n",
    "    seed=SEED,\n",
    "    max_length=tokenizer_target_max_length,\n",
    ")\n",
    "tokenizer_identifier = \"custom-trained\"\n",
    "print(\n",
    "    \"Finished tokenizer training | vocab size: \"\n",
    "    f\"{tokenizer_vocab_size(tokenizer)} | samples used: {tokenizer_sampled}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73251f0a",
   "metadata": {},
   "source": [
    "Sanity check the tokenizer. Make test encode and decodes and make sure things make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "484d26ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Ang bilis ng takbo ng panahon.\n",
      "Encoded IDs: [1, 1086, 7603, 300, 12447, 300, 4480, 18, 2]\n",
      "Decoded (with special): <s>Ang bilis ng takbo ng panahon.</s>\n",
      "Decoded (clean): Ang bilis ng takbo ng panahon.\n",
      "Tokenizer sanity check passed!\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Ang bilis ng takbo ng panahon.\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded_with_special = tokenizer.decode(encoded)\n",
    "decoded_clean = tokenizer.decode(encoded, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Encoded IDs: {encoded}\")\n",
    "print(f\"Decoded (with special): {decoded_with_special}\")\n",
    "print(f\"Decoded (clean): {decoded_clean}\")\n",
    "\n",
    "assert test_text == decoded_clean, f\"Decoding failed: expected '{test_text}', got '{decoded_clean}'\"\n",
    "\n",
    "print(\"Tokenizer sanity check passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfdf01b",
   "metadata": {},
   "source": [
    "Ok let's prepare the dataset now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f663a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = create_datasets(\n",
    "    corpus.texts,\n",
    "    tokenizer,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    stride=STRIDE,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda7f4d",
   "metadata": {},
   "source": [
    "Test dataset sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b920707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Samples ---\n",
      "Sample 1:\n",
      "Input: <s>BREAKING: VCM Inside Novotel Cubao owned by Mar Roxas https://t.co/KPeHzLPSHe</s><s>@inquirerdotnet @SenGracePOE you should tell Binay camps. Walang magnanakaw sa LP lalo na ang Roxas-Robredo tandem vote Roxas-Robredo</s><s>Public warned of LTO text scam claiming traffic violations\n",
      "Home\n",
      "Trends + Spotlights\n",
      "Politics + Issues\n",
      "Celebrities\n",
      "Hobbies + Interests\n",
      "Rumor Cop\n",
      "Search\n",
      "Powered by\n",
      "Philstar.com\n",
      "Interaksyon\n",
      "Home\n",
      "Trends + Spotlights\n",
      "Politics + Issues\n",
      "Celebrities\n",
      "Hobbies + Interests\n",
      "Rumor Cop\n",
      "Share\n",
      "Facebook\n",
      "Twitter\n",
      "Screengrab of a text scam posted by TV5 Chief Data and Elections Analyst Edson Guido as posted on his X (fomerly Twitter) account on Nov. 3, 2025 (EdsonCGuido via X)\n",
      "SPONSORED ARTICLES\n",
      "How logistics industry experts plan to ‘move forward’ after COVID-19\n",
      "Reasons why Filipinos love Korean culture and products\n",
      "Why Batangas is the destination for budget-friendly family holidays\n",
      "A data and elections analyst has warned Filipinos about a scam message claiming to notify them of a supposed traffic violation..\n",
      "On Monday, November 3, TV5 Chief Data and Elections Analyst\n",
      "E\n",
      "Target: BREAKING: VCM Inside Novotel Cubao owned by Mar Roxas https://t.co/KPeHzLPSHe</s><s>@inquirerdotnet @SenGracePOE you should tell Binay camps. Walang magnanakaw sa LP lalo na ang Roxas-Robredo tandem vote Roxas-Robredo</s><s>Public warned of LTO text scam claiming traffic violations\n",
      "Home\n",
      "Trends + Spotlights\n",
      "Politics + Issues\n",
      "Celebrities\n",
      "Hobbies + Interests\n",
      "Rumor Cop\n",
      "Search\n",
      "Powered by\n",
      "Philstar.com\n",
      "Interaksyon\n",
      "Home\n",
      "Trends + Spotlights\n",
      "Politics + Issues\n",
      "Celebrities\n",
      "Hobbies + Interests\n",
      "Rumor Cop\n",
      "Share\n",
      "Facebook\n",
      "Twitter\n",
      "Screengrab of a text scam posted by TV5 Chief Data and Elections Analyst Edson Guido as posted on his X (fomerly Twitter) account on Nov. 3, 2025 (EdsonCGuido via X)\n",
      "SPONSORED ARTICLES\n",
      "How logistics industry experts plan to ‘move forward’ after COVID-19\n",
      "Reasons why Filipinos love Korean culture and products\n",
      "Why Batangas is the destination for budget-friendly family holidays\n",
      "A data and elections analyst has warned Filipinos about a scam message claiming to notify them of a supposed traffic violation..\n",
      "On Monday, November 3, TV5 Chief Data and Elections Analyst\n",
      "Eds\n",
      "--------------------\n",
      "Sample 2:\n",
      "Input: rab of a text scam posted by TV5 Chief Data and Elections Analyst Edson Guido as posted on his X (fomerly Twitter) account on Nov. 3, 2025 (EdsonCGuido via X)\n",
      "SPONSORED ARTICLES\n",
      "How logistics industry experts plan to ‘move forward’ after COVID-19\n",
      "Reasons why Filipinos love Korean culture and products\n",
      "Why Batangas is the destination for budget-friendly family holidays\n",
      "A data and elections analyst has warned Filipinos about a scam message claiming to notify them of a supposed traffic violation..\n",
      "On Monday, November 3, TV5 Chief Data and Elections Analyst\n",
      "Edson Guido\n",
      "shared a screengrab of a text message he received from a number impersonating the\n",
      "Land Transportation Office (LTO).\n",
      "The message falsely claimed that Guido had an “unsettled violation” related to the Republic Act 10913 also known as the\n",
      "Anti-Distracted Driving Act\n",
      ", alleging that he was caught “using” his mobile phone “while driving.”\n",
      "The scam message also said that Guido needed to pay a penalty fee of P1,000 “due within 24 hours.”\n",
      "“Failure to settle may result in additional penalties, license\n",
      "Target:  of a text scam posted by TV5 Chief Data and Elections Analyst Edson Guido as posted on his X (fomerly Twitter) account on Nov. 3, 2025 (EdsonCGuido via X)\n",
      "SPONSORED ARTICLES\n",
      "How logistics industry experts plan to ‘move forward’ after COVID-19\n",
      "Reasons why Filipinos love Korean culture and products\n",
      "Why Batangas is the destination for budget-friendly family holidays\n",
      "A data and elections analyst has warned Filipinos about a scam message claiming to notify them of a supposed traffic violation..\n",
      "On Monday, November 3, TV5 Chief Data and Elections Analyst\n",
      "Edson Guido\n",
      "shared a screengrab of a text message he received from a number impersonating the\n",
      "Land Transportation Office (LTO).\n",
      "The message falsely claimed that Guido had an “unsettled violation” related to the Republic Act 10913 also known as the\n",
      "Anti-Distracted Driving Act\n",
      ", alleging that he was caught “using” his mobile phone “while driving.”\n",
      "The scam message also said that Guido needed to pay a penalty fee of P1,000 “due within 24 hours.”\n",
      "“Failure to settle may result in additional penalties, license suspension\n",
      "--------------------\n",
      "Sample 3:\n",
      "Input: dson Guido\n",
      "shared a screengrab of a text message he received from a number impersonating the\n",
      "Land Transportation Office (LTO).\n",
      "The message falsely claimed that Guido had an “unsettled violation” related to the Republic Act 10913 also known as the\n",
      "Anti-Distracted Driving Act\n",
      ", alleging that he was caught “using” his mobile phone “while driving.”\n",
      "The scam message also said that Guido needed to pay a penalty fee of P1,000 “due within 24 hours.”\n",
      "“Failure to settle may result in additional penalties, license suspension, or alarm block,” it added.\n",
      "The message included a hyperlink that claimed to be the official website of the LTO.\n",
      "“SCAM ALERT! Guys, kung makatanggap kayo ng ganitong text na meron kayong traffic violation, HUWAG ninyong i-click ang link. SCAM ‘yan! Please share at huwag po tayong magpaloko,” Guido wrote on the X (formerly Twitter) platform.\n",
      "SCAM ALERT!\n",
      "Guys, kung makatanggap kayo ng ganitong text na meron kayong traffic violation, HUWAG ninyong i-click ang link. SCAM yan!\n",
      "Please share\n",
      "Target: on Guido\n",
      "shared a screengrab of a text message he received from a number impersonating the\n",
      "Land Transportation Office (LTO).\n",
      "The message falsely claimed that Guido had an “unsettled violation” related to the Republic Act 10913 also known as the\n",
      "Anti-Distracted Driving Act\n",
      ", alleging that he was caught “using” his mobile phone “while driving.”\n",
      "The scam message also said that Guido needed to pay a penalty fee of P1,000 “due within 24 hours.”\n",
      "“Failure to settle may result in additional penalties, license suspension, or alarm block,” it added.\n",
      "The message included a hyperlink that claimed to be the official website of the LTO.\n",
      "“SCAM ALERT! Guys, kung makatanggap kayo ng ganitong text na meron kayong traffic violation, HUWAG ninyong i-click ang link. SCAM ‘yan! Please share at huwag po tayong magpaloko,” Guido wrote on the X (formerly Twitter) platform.\n",
      "SCAM ALERT!\n",
      "Guys, kung makatanggap kayo ng ganitong text na meron kayong traffic violation, HUWAG ninyong i-click ang link. SCAM yan!\n",
      "Please share at\n",
      "--------------------\n",
      "Sample 4:\n",
      "Input:  suspension, or alarm block,” it added.\n",
      "The message included a hyperlink that claimed to be the official website of the LTO.\n",
      "“SCAM ALERT! Guys, kung makatanggap kayo ng ganitong text na meron kayong traffic violation, HUWAG ninyong i-click ang link. SCAM ‘yan! Please share at huwag po tayong magpaloko,” Guido wrote on the X (formerly Twitter) platform.\n",
      "SCAM ALERT!\n",
      "Guys, kung makatanggap kayo ng ganitong text na meron kayong traffic violation, HUWAG ninyong i-click ang link. SCAM yan!\n",
      "Please share at huwag po tayong magpaloko.\n",
      "pic.twitter.com/FBlerRGSg2\n",
      "— Edson C. Guido (@EdsonCGuido)\n",
      "November 3, 2025\n",
      "Guido’s post has so far garnered 67,700 views, over 800 likes, and more than 530 reposts.\n",
      "“Buti na lang nabasa ko ‘to. Just received one,” an\n",
      "online user\n",
      "commented.\n",
      "“Lto-gov-ph should make you block it already, and the .com made it even more [suspicious],” another\n",
      "wrote\n",
      "\n",
      "Target: , or alarm block,” it added.\n",
      "The message included a hyperlink that claimed to be the official website of the LTO.\n",
      "“SCAM ALERT! Guys, kung makatanggap kayo ng ganitong text na meron kayong traffic violation, HUWAG ninyong i-click ang link. SCAM ‘yan! Please share at huwag po tayong magpaloko,” Guido wrote on the X (formerly Twitter) platform.\n",
      "SCAM ALERT!\n",
      "Guys, kung makatanggap kayo ng ganitong text na meron kayong traffic violation, HUWAG ninyong i-click ang link. SCAM yan!\n",
      "Please share at huwag po tayong magpaloko.\n",
      "pic.twitter.com/FBlerRGSg2\n",
      "— Edson C. Guido (@EdsonCGuido)\n",
      "November 3, 2025\n",
      "Guido’s post has so far garnered 67,700 views, over 800 likes, and more than 530 reposts.\n",
      "“Buti na lang nabasa ko ‘to. Just received one,” an\n",
      "online user\n",
      "commented.\n",
      "“Lto-gov-ph should make you block it already, and the .com made it even more [suspicious],” another\n",
      "wrote\n",
      ".\n",
      "--------------------\n",
      "Sample 5:\n",
      "Input:  at huwag po tayong magpaloko.\n",
      "pic.twitter.com/FBlerRGSg2\n",
      "— Edson C. Guido (@EdsonCGuido)\n",
      "November 3, 2025\n",
      "Guido’s post has so far garnered 67,700 views, over 800 likes, and more than 530 reposts.\n",
      "“Buti na lang nabasa ko ‘to. Just received one,” an\n",
      "online user\n",
      "commented.\n",
      "“Lto-gov-ph should make you block it already, and the .com made it even more [suspicious],” another\n",
      "wrote\n",
      ".\n",
      "“Haha, nakatangap ako ng ganyang [message], eh wala naman akong sasakyan, like, duh... Grabe na talaga mga scammer ngayon,” a different\n",
      "Pinoy\n",
      "commented.\n",
      "The legitimate hyperlink should be\n",
      "https://portal.lto.gov.ph\n",
      "Meanwhile, text hijacking is a tactic used by scammers to make messages appear as though they come from official numbers, even when they contain red flags, such as poor grammar or overly casual language.\n",
      "The government has warned that scammers trick recipients by sending messages with malicious links designed to access their accounts\n",
      "Target:  huwag po tayong magpaloko.\n",
      "pic.twitter.com/FBlerRGSg2\n",
      "— Edson C. Guido (@EdsonCGuido)\n",
      "November 3, 2025\n",
      "Guido’s post has so far garnered 67,700 views, over 800 likes, and more than 530 reposts.\n",
      "“Buti na lang nabasa ko ‘to. Just received one,” an\n",
      "online user\n",
      "commented.\n",
      "“Lto-gov-ph should make you block it already, and the .com made it even more [suspicious],” another\n",
      "wrote\n",
      ".\n",
      "“Haha, nakatangap ako ng ganyang [message], eh wala naman akong sasakyan, like, duh... Grabe na talaga mga scammer ngayon,” a different\n",
      "Pinoy\n",
      "commented.\n",
      "The legitimate hyperlink should be\n",
      "https://portal.lto.gov.ph\n",
      "Meanwhile, text hijacking is a tactic used by scammers to make messages appear as though they come from official numbers, even when they contain red flags, such as poor grammar or overly casual language.\n",
      "The government has warned that scammers trick recipients by sending messages with malicious links designed to access their accounts or\n",
      "--------------------\n",
      "Sample 6:\n",
      "Input: .\n",
      "“Haha, nakatangap ako ng ganyang [message], eh wala naman akong sasakyan, like, duh... Grabe na talaga mga scammer ngayon,” a different\n",
      "Pinoy\n",
      "commented.\n",
      "The legitimate hyperlink should be\n",
      "https://portal.lto.gov.ph\n",
      "Meanwhile, text hijacking is a tactic used by scammers to make messages appear as though they come from official numbers, even when they contain red flags, such as poor grammar or overly casual language.\n",
      "The government has warned that scammers trick recipients by sending messages with malicious links designed to access their accounts or credit card information without authorization.\n",
      "According to a\n",
      "telecommunications firm\n",
      ", these fraudsters can deceive people by using fake cell towers to send text scams by using legitimate names of entities as the sender’s name.\n",
      "Additionally, the\n",
      "Metropolitan Manila Development Authority\n",
      "clarified that those with traffic violations under the No Contact Apprehension Policy (NCAP) should\n",
      "receive\n",
      "an email and a text message from “\n",
      "[email protected]\n",
      "” and “MMDA_NCAP,” respectively.\n",
      "Interaksyon\n",
      "RELATED ARTICLES\n",
      "MORE FROM AUTHOR\n",
      "Three-vehic\n",
      "Target: \n",
      "“Haha, nakatangap ako ng ganyang [message], eh wala naman akong sasakyan, like, duh... Grabe na talaga mga scammer ngayon,” a different\n",
      "Pinoy\n",
      "commented.\n",
      "The legitimate hyperlink should be\n",
      "https://portal.lto.gov.ph\n",
      "Meanwhile, text hijacking is a tactic used by scammers to make messages appear as though they come from official numbers, even when they contain red flags, such as poor grammar or overly casual language.\n",
      "The government has warned that scammers trick recipients by sending messages with malicious links designed to access their accounts or credit card information without authorization.\n",
      "According to a\n",
      "telecommunications firm\n",
      ", these fraudsters can deceive people by using fake cell towers to send text scams by using legitimate names of entities as the sender’s name.\n",
      "Additionally, the\n",
      "Metropolitan Manila Development Authority\n",
      "clarified that those with traffic violations under the No Contact Apprehension Policy (NCAP) should\n",
      "receive\n",
      "an email and a text message from “\n",
      "[email protected]\n",
      "” and “MMDA_NCAP,” respectively.\n",
      "Interaksyon\n",
      "RELATED ARTICLES\n",
      "MORE FROM AUTHOR\n",
      "Three-vehicle\n",
      "--------------------\n",
      "Sample 7:\n",
      "Input:  or credit card information without authorization.\n",
      "According to a\n",
      "telecommunications firm\n",
      ", these fraudsters can deceive people by using fake cell towers to send text scams by using legitimate names of entities as the sender’s name.\n",
      "Additionally, the\n",
      "Metropolitan Manila Development Authority\n",
      "clarified that those with traffic violations under the No Contact Apprehension Policy (NCAP) should\n",
      "receive\n",
      "an email and a text message from “\n",
      "[email protected]\n",
      "” and “MMDA_NCAP,” respectively.\n",
      "Interaksyon\n",
      "RELATED ARTICLES\n",
      "MORE FROM AUTHOR\n",
      "Three-vehicle crash on E. Rodriguez Sr. Avenue in QC raises calls for road safety training\n",
      "LTO suspends licenses of parents who allowed minors to drive in Lanao del Norte and Isabela\n",
      "Authorities urged to act after minor seen driving on public road\n",
      "SUV in Bacolod City attracts attention with American license plate\n",
      "E-bike earns flak for swerving in emergency vehicle’s path\n",
      "LTO urged to act after video shows driver speeding while filming himself\n",
      "LATEST\n",
      "From Gemini AI to ‘Squid Game’: What Filipinos Googled most in 2025\n",
      "Catholic devotee shows ‘miracle’ of how bible stayed intact from house fire\n",
      "Target:  credit card information without authorization.\n",
      "According to a\n",
      "telecommunications firm\n",
      ", these fraudsters can deceive people by using fake cell towers to send text scams by using legitimate names of entities as the sender’s name.\n",
      "Additionally, the\n",
      "Metropolitan Manila Development Authority\n",
      "clarified that those with traffic violations under the No Contact Apprehension Policy (NCAP) should\n",
      "receive\n",
      "an email and a text message from “\n",
      "[email protected]\n",
      "” and “MMDA_NCAP,” respectively.\n",
      "Interaksyon\n",
      "RELATED ARTICLES\n",
      "MORE FROM AUTHOR\n",
      "Three-vehicle crash on E. Rodriguez Sr. Avenue in QC raises calls for road safety training\n",
      "LTO suspends licenses of parents who allowed minors to drive in Lanao del Norte and Isabela\n",
      "Authorities urged to act after minor seen driving on public road\n",
      "SUV in Bacolod City attracts attention with American license plate\n",
      "E-bike earns flak for swerving in emergency vehicle’s path\n",
      "LTO urged to act after video shows driver speeding while filming himself\n",
      "LATEST\n",
      "From Gemini AI to ‘Squid Game’: What Filipinos Googled most in 2025\n",
      "Catholic devotee shows ‘miracle’ of how bible stayed intact from house fire\n",
      "\n",
      "--------------------\n",
      "Sample 8:\n",
      "Input: le crash on E. Rodriguez Sr. Avenue in QC raises calls for road safety training\n",
      "LTO suspends licenses of parents who allowed minors to drive in Lanao del Norte and Isabela\n",
      "Authorities urged to act after minor seen driving on public road\n",
      "SUV in Bacolod City attracts attention with American license plate\n",
      "E-bike earns flak for swerving in emergency vehicle’s path\n",
      "LTO urged to act after video shows driver speeding while filming himself\n",
      "LATEST\n",
      "From Gemini AI to ‘Squid Game’: What Filipinos Googled most in 2025\n",
      "Catholic devotee shows ‘miracle’ of how bible stayed intact from house fire\n",
      "Residents in Palawan help capture 14.7-foot crocodile\n",
      "UP Manila students join Oblation run to protest corruption\n",
      "Angelica Panganiban says she and Ellen Adarna can be friends amid tangled showbiz relationships\n",
      "Kidapawan bishop urges renewed devotion to Eucharist amid low Mass turnout\n",
      "Simbang gabi 2025: Schedule of Masses at Megaworld Malls across the Philippines\n",
      "About Us\n",
      "|\n",
      "Contact Us\n",
      "Unit 909 and 910, The Infinity Tower, 26th Street, Bonifacio Global City, Taguig City\n",
      "© 2025 Philstar Global Corporation.</s><s>Ngislo mo Binay! Puro ka Nognog!!!!!</s><s>Binay &gt; Roxas #justsaying</s><s>#PiliPinasDebates2016</s>\n",
      "Target:  crash on E. Rodriguez Sr. Avenue in QC raises calls for road safety training\n",
      "LTO suspends licenses of parents who allowed minors to drive in Lanao del Norte and Isabela\n",
      "Authorities urged to act after minor seen driving on public road\n",
      "SUV in Bacolod City attracts attention with American license plate\n",
      "E-bike earns flak for swerving in emergency vehicle’s path\n",
      "LTO urged to act after video shows driver speeding while filming himself\n",
      "LATEST\n",
      "From Gemini AI to ‘Squid Game’: What Filipinos Googled most in 2025\n",
      "Catholic devotee shows ‘miracle’ of how bible stayed intact from house fire\n",
      "Residents in Palawan help capture 14.7-foot crocodile\n",
      "UP Manila students join Oblation run to protest corruption\n",
      "Angelica Panganiban says she and Ellen Adarna can be friends amid tangled showbiz relationships\n",
      "Kidapawan bishop urges renewed devotion to Eucharist amid low Mass turnout\n",
      "Simbang gabi 2025: Schedule of Masses at Megaworld Malls across the Philippines\n",
      "About Us\n",
      "|\n",
      "Contact Us\n",
      "Unit 909 and 910, The Infinity Tower, 26th Street, Bonifacio Global City, Taguig City\n",
      "© 2025 Philstar Global Corporation.</s><s>Ngislo mo Binay! Puro ka Nognog!!!!!</s><s>Binay &gt; Roxas #justsaying</s><s>#PiliPinasDebates2016</s><s>\n",
      "--------------------\n",
      "Sample 9:\n",
      "Input: \n",
      "Residents in Palawan help capture 14.7-foot crocodile\n",
      "UP Manila students join Oblation run to protest corruption\n",
      "Angelica Panganiban says she and Ellen Adarna can be friends amid tangled showbiz relationships\n",
      "Kidapawan bishop urges renewed devotion to Eucharist amid low Mass turnout\n",
      "Simbang gabi 2025: Schedule of Masses at Megaworld Malls across the Philippines\n",
      "About Us\n",
      "|\n",
      "Contact Us\n",
      "Unit 909 and 910, The Infinity Tower, 26th Street, Bonifacio Global City, Taguig City\n",
      "© 2025 Philstar Global Corporation.</s><s>Ngislo mo Binay! Puro ka Nognog!!!!!</s><s>Binay &gt; Roxas #justsaying</s><s>#PiliPinasDebates2016</s><s>VP Binay’s party fears glitches meant for Comelec to suspend polls #Eleksyon2016 https://t.co/qwvWEaLDW0</s><s>YOU ARE SHOWING HOW DESPERATE YOU ARE MAR ROXAS, SHAME ON YOU. #AyawSaDILAW</s><s>IBON Birdtalk 2025 tackles poverty, ill-governance, and what can be done\n",
      "civic engagement\n",
      "IBON Birdtalk 2025 tackles poverty, ill-governance, and what can be done\n",
      "Jan 21, 2025 1:45 PM PHT\n",
      "Rappler.com\n",
      "SUMMARY\n",
      "This is AI generated summarization, which\n",
      "Target: Residents in Palawan help capture 14.7-foot crocodile\n",
      "UP Manila students join Oblation run to protest corruption\n",
      "Angelica Panganiban says she and Ellen Adarna can be friends amid tangled showbiz relationships\n",
      "Kidapawan bishop urges renewed devotion to Eucharist amid low Mass turnout\n",
      "Simbang gabi 2025: Schedule of Masses at Megaworld Malls across the Philippines\n",
      "About Us\n",
      "|\n",
      "Contact Us\n",
      "Unit 909 and 910, The Infinity Tower, 26th Street, Bonifacio Global City, Taguig City\n",
      "© 2025 Philstar Global Corporation.</s><s>Ngislo mo Binay! Puro ka Nognog!!!!!</s><s>Binay &gt; Roxas #justsaying</s><s>#PiliPinasDebates2016</s><s>VP Binay’s party fears glitches meant for Comelec to suspend polls #Eleksyon2016 https://t.co/qwvWEaLDW0</s><s>YOU ARE SHOWING HOW DESPERATE YOU ARE MAR ROXAS, SHAME ON YOU. #AyawSaDILAW</s><s>IBON Birdtalk 2025 tackles poverty, ill-governance, and what can be done\n",
      "civic engagement\n",
      "IBON Birdtalk 2025 tackles poverty, ill-governance, and what can be done\n",
      "Jan 21, 2025 1:45 PM PHT\n",
      "Rappler.com\n",
      "SUMMARY\n",
      "This is AI generated summarization, which may\n",
      "--------------------\n",
      "Sample 10:\n",
      "Input: <s>VP Binay’s party fears glitches meant for Comelec to suspend polls #Eleksyon2016 https://t.co/qwvWEaLDW0</s><s>YOU ARE SHOWING HOW DESPERATE YOU ARE MAR ROXAS, SHAME ON YOU. #AyawSaDILAW</s><s>IBON Birdtalk 2025 tackles poverty, ill-governance, and what can be done\n",
      "civic engagement\n",
      "IBON Birdtalk 2025 tackles poverty, ill-governance, and what can be done\n",
      "Jan 21, 2025 1:45 PM PHT\n",
      "Rappler.com\n",
      "SUMMARY\n",
      "This is AI generated summarization, which may have errors. For context, always refer to the full article.\n",
      "PRESS RELEASE: The IBON 2025 Yearstarter Birdtalk will be held on January 23, from 1 pm to 4 pm, at the University of the Philippines Diliman.\n",
      "This is a press release from IBON Foundation.\n",
      "In the run-up to the 2025 midterm elections, the Filipino people are confronted with greater economic challenges and political turmoil. The Marcos administration cites encouraging statistics and welfare interventions. It also counterposes itself to the previous Duterte government as a champion of human rights, democracy and sovereignty, and climate justice.\n",
      "\n",
      "Target: VP Binay’s party fears glitches meant for Comelec to suspend polls #Eleksyon2016 https://t.co/qwvWEaLDW0</s><s>YOU ARE SHOWING HOW DESPERATE YOU ARE MAR ROXAS, SHAME ON YOU. #AyawSaDILAW</s><s>IBON Birdtalk 2025 tackles poverty, ill-governance, and what can be done\n",
      "civic engagement\n",
      "IBON Birdtalk 2025 tackles poverty, ill-governance, and what can be done\n",
      "Jan 21, 2025 1:45 PM PHT\n",
      "Rappler.com\n",
      "SUMMARY\n",
      "This is AI generated summarization, which may have errors. For context, always refer to the full article.\n",
      "PRESS RELEASE: The IBON 2025 Yearstarter Birdtalk will be held on January 23, from 1 pm to 4 pm, at the University of the Philippines Diliman.\n",
      "This is a press release from IBON Foundation.\n",
      "In the run-up to the 2025 midterm elections, the Filipino people are confronted with greater economic challenges and political turmoil. The Marcos administration cites encouraging statistics and welfare interventions. It also counterposes itself to the previous Duterte government as a champion of human rights, democracy and sovereignty, and climate justice.\n",
      "Y\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample a few lines from the training and validation datasets\n",
    "print(\"--- Training Samples ---\")\n",
    "for i in range(10):\n",
    "    x, y = train_dataset[i]\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Input: {tokenizer.decode(x)}\")\n",
    "    print(f\"Target: {tokenizer.decode(y)}\")\n",
    "    print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d953f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validation Samples ---\n",
      "Sample 1:\n",
      "#Duterte2016</s><s>Health Emergency Management Bureau (HEMB)10 surveys ay Roxas by landslide. Nanalo si Binay. Madami pang pwede mangyari</s><s>@wawam voting for Poe is worst than voting for Binay. A disaster in the making. Thats my opinion.</s><s>Roxas' new ad is absurd: \"this poor woman's mother is going to die if you don't vote for Mar Roxas.\"\n",
      "Share this page\n",
      "Share in Chat\n",
      "Share article\n",
      "Facebook\n",
      "Facebook\n",
      "X (Twitter)\n",
      "Twitter\n",
      "Copy Link\n",
      "Copied\n",
      "Government\n",
      "Health Emergency Management Bureau (HEMB)\n",
      "Government Agency\n",
      "Acronym\n",
      "HEMB\n",
      "For comment or corrections, contact\n",
      "feedback@rappler.com\n",
      "Must Read\n",
      "Daily wRap\n",
      "Sara Duterte faces plunder complaint over confidential funds mess | The wRap\n",
      "4 hours ago\n",
      "Exam results\n",
      "RESULTS: November 2025 Dentists Licensure Examination\n",
      "4 hours ago\n",
      "SEA Games\n",
      "LIVE UPDATES: SEA Games 2025\n",
      "4 hours ago\n",
      "SEA Games\n",
      "John Cabang Tolentino sets new SEA Games record to highlight strong day for PH\n",
      "4 hours ago\n",
      "SEA Games\n",
      "John Ivan Cruz vaults to SEA Games gold after failed title defense in floor exercise\n",
      "5 hours ago\n",
      "#Duterte2016</s><s>Health Emergency Management Bureau (HEMB) surveys ay Roxas by landslide. Nanalo si Binay. Madami pang pwede mangyari</s><s>@wawam voting for Poe is worst than voting for Binay. A disaster in the making. Thats my opinion.</s><s>Roxas' new ad is absurd: \"this poor woman's mother is going to die if you don't vote for Mar Roxas.\"\n",
      "Share this page\n",
      "Share in Chat\n",
      "Share article\n",
      "Facebook\n",
      "Facebook\n",
      "X (Twitter)\n",
      "Twitter\n",
      "Copy Link\n",
      "Copied\n",
      "Government\n",
      "Health Emergency Management Bureau (HEMB)\n",
      "Government Agency\n",
      "Acronym\n",
      "HEMB\n",
      "For comment or corrections, contact\n",
      "feedback@rappler.com\n",
      "Must Read\n",
      "Daily wRap\n",
      "Sara Duterte faces plunder complaint over confidential funds mess | The wRap\n",
      "4 hours ago\n",
      "Exam results\n",
      "RESULTS: November 2025 Dentists Licensure Examination\n",
      "4 hours ago\n",
      "SEA Games\n",
      "LIVE UPDATES: SEA Games 2025\n",
      "4 hours ago\n",
      "SEA Games\n",
      "John Cabang Tolentino sets new SEA Games record to highlight strong day for PH\n",
      "4 hours ago\n",
      "SEA Games\n",
      "John Ivan Cruz vaults to SEA Games gold after failed title defense in floor exercise\n",
      "5 hours ago\n",
      "\n",
      "--------------------\n",
      "Sample 2:\n",
      "Input: \n",
      "Health Emergency Management Bureau (HEMB)\n",
      "Government Agency\n",
      "Acronym\n",
      "HEMB\n",
      "For comment or corrections, contact\n",
      "feedback@rappler.com\n",
      "Must Read\n",
      "Daily wRap\n",
      "Sara Duterte faces plunder complaint over confidential funds mess | The wRap\n",
      "4 hours ago\n",
      "Exam results\n",
      "RESULTS: November 2025 Dentists Licensure Examination\n",
      "4 hours ago\n",
      "SEA Games\n",
      "LIVE UPDATES: SEA Games 2025\n",
      "4 hours ago\n",
      "SEA Games\n",
      "John Cabang Tolentino sets new SEA Games record to highlight strong day for PH\n",
      "4 hours ago\n",
      "SEA Games\n",
      "John Ivan Cruz vaults to SEA Games gold after failed title defense in floor exercise\n",
      "5 hours ago\n",
      "Philippine News\n",
      "Rappler Recap: VP Sara faces plunder complaint over confidential funds misuse\n",
      "6 hours ago\n",
      "SEA Games\n",
      "Kayla Sanchez gets another SEA Games gold as PH posts 1-2 finish in women’s 100m freestyle\n",
      "Dec 12, 2025 8:27 PM PHT\n",
      "SEA Games\n",
      "Tachiana Mangin ends long Thai reign with SEA Games taekwondo gold\n",
      "Dec 12, 2025 6:31 PM PHT</s><s>Pa reserve daw ng seat si @PeejDasal sa VIP section please. She's looking forward to seeing Mar Roxas naman.  https://t.co/VlZSMc6tE3</s><s>[\n",
      "Target: Health Emergency Management Bureau (HEMB)\n",
      "Government Agency\n",
      "Acronym\n",
      "HEMB\n",
      "For comment or corrections, contact\n",
      "feedback@rappler.com\n",
      "Must Read\n",
      "Daily wRap\n",
      "Sara Duterte faces plunder complaint over confidential funds mess | The wRap\n",
      "4 hours ago\n",
      "Exam results\n",
      "RESULTS: November 2025 Dentists Licensure Examination\n",
      "4 hours ago\n",
      "SEA Games\n",
      "LIVE UPDATES: SEA Games 2025\n",
      "4 hours ago\n",
      "SEA Games\n",
      "John Cabang Tolentino sets new SEA Games record to highlight strong day for PH\n",
      "4 hours ago\n",
      "SEA Games\n",
      "John Ivan Cruz vaults to SEA Games gold after failed title defense in floor exercise\n",
      "5 hours ago\n",
      "Philippine News\n",
      "Rappler Recap: VP Sara faces plunder complaint over confidential funds misuse\n",
      "6 hours ago\n",
      "SEA Games\n",
      "Kayla Sanchez gets another SEA Games gold as PH posts 1-2 finish in women’s 100m freestyle\n",
      "Dec 12, 2025 8:27 PM PHT\n",
      "SEA Games\n",
      "Tachiana Mangin ends long Thai reign with SEA Games taekwondo gold\n",
      "Dec 12, 2025 6:31 PM PHT</s><s>Pa reserve daw ng seat si @PeejDasal sa VIP section please. She's looking forward to seeing Mar Roxas naman.  https://t.co/VlZSMc6tE3</s><s>[EDITORIAL\n",
      "--------------------\n",
      "Sample 3:\n",
      "Input: \n",
      "Philippine News\n",
      "Rappler Recap: VP Sara faces plunder complaint over confidential funds misuse\n",
      "6 hours ago\n",
      "SEA Games\n",
      "Kayla Sanchez gets another SEA Games gold as PH posts 1-2 finish in women’s 100m freestyle\n",
      "Dec 12, 2025 8:27 PM PHT\n",
      "SEA Games\n",
      "Tachiana Mangin ends long Thai reign with SEA Games taekwondo gold\n",
      "Dec 12, 2025 6:31 PM PHT</s><s>Pa reserve daw ng seat si @PeejDasal sa VIP section please. She's looking forward to seeing Mar Roxas naman.  https://t.co/VlZSMc6tE3</s><s>[EDITORIAL] When disunity is a stumbling block to change\n",
      "protests in the Philippines\n",
      "[EDITORIAL] When disunity is a stumbling block to change\n",
      "Dec 1, 2025 1:30 PM PHT\n",
      "Rappler.com\n",
      "SUMMARY\n",
      "This is AI generated summarization, which may have errors. For context, always refer to the full article.\n",
      "Nico Villarete\n",
      "INFO\n",
      "Saan ka nagpunta? Sa EDSA ba o sa Luneta? 'Yan, beshie/comrade ang problema.\n",
      "Katatapos lang ng pangalawang Trillion Peso March — bumabaha ang mga selfie, nakaaaliw na mensahe sa mga T-shirts,\n",
      "Target: Philippine News\n",
      "Rappler Recap: VP Sara faces plunder complaint over confidential funds misuse\n",
      "6 hours ago\n",
      "SEA Games\n",
      "Kayla Sanchez gets another SEA Games gold as PH posts 1-2 finish in women’s 100m freestyle\n",
      "Dec 12, 2025 8:27 PM PHT\n",
      "SEA Games\n",
      "Tachiana Mangin ends long Thai reign with SEA Games taekwondo gold\n",
      "Dec 12, 2025 6:31 PM PHT</s><s>Pa reserve daw ng seat si @PeejDasal sa VIP section please. She's looking forward to seeing Mar Roxas naman.  https://t.co/VlZSMc6tE3</s><s>[EDITORIAL] When disunity is a stumbling block to change\n",
      "protests in the Philippines\n",
      "[EDITORIAL] When disunity is a stumbling block to change\n",
      "Dec 1, 2025 1:30 PM PHT\n",
      "Rappler.com\n",
      "SUMMARY\n",
      "This is AI generated summarization, which may have errors. For context, always refer to the full article.\n",
      "Nico Villarete\n",
      "INFO\n",
      "Saan ka nagpunta? Sa EDSA ba o sa Luneta? 'Yan, beshie/comrade ang problema.\n",
      "Katatapos lang ng pangalawang Trillion Peso March — bumabaha ang mga selfie, nakaaaliw na mensahe sa mga T-shirts, at\n",
      "--------------------\n",
      "Sample 4:\n",
      "Input: EDITORIAL] When disunity is a stumbling block to change\n",
      "protests in the Philippines\n",
      "[EDITORIAL] When disunity is a stumbling block to change\n",
      "Dec 1, 2025 1:30 PM PHT\n",
      "Rappler.com\n",
      "SUMMARY\n",
      "This is AI generated summarization, which may have errors. For context, always refer to the full article.\n",
      "Nico Villarete\n",
      "INFO\n",
      "Saan ka nagpunta? Sa EDSA ba o sa Luneta? 'Yan, beshie/comrade ang problema.\n",
      "Katatapos lang ng pangalawang Trillion Peso March — bumabaha ang mga selfie, nakaaaliw na mensahe sa mga T-shirts, at creative na mga protest paraphernalia.\n",
      "Pero saan ka nagpunta? Sa EDSA ba o sa Luneta?\n",
      "‘Wag magtaka kung walang iisang mensaheng dumagundong na puwedeng umuga sa Malacañang. Hitik kasi sa mixed messaging ang mga rally — sa Luneta, “Marcos-Duterte resign” at “resign all” ang sintingkad ng pulang mga streamer. Sa EDSA naman, “ikulong,” “panagutin ang mga corrupt” at “accountability” ang maugong.\n",
      "Naganap ang mga pagkilos laban sa korupsiyon sa gitna ng isang malaking dilemma ng mga organizer: Kung manan\n",
      "Target: ] When disunity is a stumbling block to change\n",
      "protests in the Philippines\n",
      "[EDITORIAL] When disunity is a stumbling block to change\n",
      "Dec 1, 2025 1:30 PM PHT\n",
      "Rappler.com\n",
      "SUMMARY\n",
      "This is AI generated summarization, which may have errors. For context, always refer to the full article.\n",
      "Nico Villarete\n",
      "INFO\n",
      "Saan ka nagpunta? Sa EDSA ba o sa Luneta? 'Yan, beshie/comrade ang problema.\n",
      "Katatapos lang ng pangalawang Trillion Peso March — bumabaha ang mga selfie, nakaaaliw na mensahe sa mga T-shirts, at creative na mga protest paraphernalia.\n",
      "Pero saan ka nagpunta? Sa EDSA ba o sa Luneta?\n",
      "‘Wag magtaka kung walang iisang mensaheng dumagundong na puwedeng umuga sa Malacañang. Hitik kasi sa mixed messaging ang mga rally — sa Luneta, “Marcos-Duterte resign” at “resign all” ang sintingkad ng pulang mga streamer. Sa EDSA naman, “ikulong,” “panagutin ang mga corrupt” at “accountability” ang maugong.\n",
      "Naganap ang mga pagkilos laban sa korupsiyon sa gitna ng isang malaking dilemma ng mga organizer: Kung mananawagan\n",
      "--------------------\n",
      "Sample 5:\n",
      "Input:  at creative na mga protest paraphernalia.\n",
      "Pero saan ka nagpunta? Sa EDSA ba o sa Luneta?\n",
      "‘Wag magtaka kung walang iisang mensaheng dumagundong na puwedeng umuga sa Malacañang. Hitik kasi sa mixed messaging ang mga rally — sa Luneta, “Marcos-Duterte resign” at “resign all” ang sintingkad ng pulang mga streamer. Sa EDSA naman, “ikulong,” “panagutin ang mga corrupt” at “accountability” ang maugong.\n",
      "Naganap ang mga pagkilos laban sa korupsiyon sa gitna ng isang malaking dilemma ng mga organizer: Kung mananawagan ka ng pagbibitiw ng pinakamataas na pinuno ng bansa na si President Ferdinand Marcos Jr. dahil sa pagkakadawit niya umano sa plunder sampu ng mga kasangga niya, ‘di ba’t parang inannoint mo na rin si Bise Presidente Sara Duterte?\n",
      "Kaya siguro nag-evolve ang panawagang “Marcos-Duterte Resign.” Pero sa totoo lang, mismong mga nagpoprotesta, aminadong hindi magreresign ang dalawa. So...duh...ano ang punto ng panawagang hindi naman pala actionable?\n",
      "Kaya rin siguro, mas niyakap ng civil society ang “Panagutin\n",
      "Target:  creative na mga protest paraphernalia.\n",
      "Pero saan ka nagpunta? Sa EDSA ba o sa Luneta?\n",
      "‘Wag magtaka kung walang iisang mensaheng dumagundong na puwedeng umuga sa Malacañang. Hitik kasi sa mixed messaging ang mga rally — sa Luneta, “Marcos-Duterte resign” at “resign all” ang sintingkad ng pulang mga streamer. Sa EDSA naman, “ikulong,” “panagutin ang mga corrupt” at “accountability” ang maugong.\n",
      "Naganap ang mga pagkilos laban sa korupsiyon sa gitna ng isang malaking dilemma ng mga organizer: Kung mananawagan ka ng pagbibitiw ng pinakamataas na pinuno ng bansa na si President Ferdinand Marcos Jr. dahil sa pagkakadawit niya umano sa plunder sampu ng mga kasangga niya, ‘di ba’t parang inannoint mo na rin si Bise Presidente Sara Duterte?\n",
      "Kaya siguro nag-evolve ang panawagang “Marcos-Duterte Resign.” Pero sa totoo lang, mismong mga nagpoprotesta, aminadong hindi magreresign ang dalawa. So...duh...ano ang punto ng panawagang hindi naman pala actionable?\n",
      "Kaya rin siguro, mas niyakap ng civil society ang “Panagutin.”\n",
      "--------------------\n",
      "Sample 6:\n",
      "Input: awagan ka ng pagbibitiw ng pinakamataas na pinuno ng bansa na si President Ferdinand Marcos Jr. dahil sa pagkakadawit niya umano sa plunder sampu ng mga kasangga niya, ‘di ba’t parang inannoint mo na rin si Bise Presidente Sara Duterte?\n",
      "Kaya siguro nag-evolve ang panawagang “Marcos-Duterte Resign.” Pero sa totoo lang, mismong mga nagpoprotesta, aminadong hindi magreresign ang dalawa. So...duh...ano ang punto ng panawagang hindi naman pala actionable?\n",
      "Kaya rin siguro, mas niyakap ng civil society ang “Panagutin.” Tahasan pa ngang\n",
      "nireject ni Cardinal Ambo David\n",
      "ang panawagang “Marcos-Duterte resign.” Naka-align ito sa hard-nosed analysis ng mga political observers tulad ng mamamahayag na si\n",
      "Vergel Santos\n",
      "at political communications consultant\n",
      "Joey Salgado\n",
      ".\n",
      "More than anything, reminder ito na ang civil society at protest movement ay “utterly divided and caught in its old wars,” sabi nga ng isang batikang journalist sa Rappler.\n",
      "Panahon pa ito ng protest movement na umusbong matapos ma-assassinate si Ninoy Aquino: nanawagan ang\n",
      "Target:  ka ng pagbibitiw ng pinakamataas na pinuno ng bansa na si President Ferdinand Marcos Jr. dahil sa pagkakadawit niya umano sa plunder sampu ng mga kasangga niya, ‘di ba’t parang inannoint mo na rin si Bise Presidente Sara Duterte?\n",
      "Kaya siguro nag-evolve ang panawagang “Marcos-Duterte Resign.” Pero sa totoo lang, mismong mga nagpoprotesta, aminadong hindi magreresign ang dalawa. So...duh...ano ang punto ng panawagang hindi naman pala actionable?\n",
      "Kaya rin siguro, mas niyakap ng civil society ang “Panagutin.” Tahasan pa ngang\n",
      "nireject ni Cardinal Ambo David\n",
      "ang panawagang “Marcos-Duterte resign.” Naka-align ito sa hard-nosed analysis ng mga political observers tulad ng mamamahayag na si\n",
      "Vergel Santos\n",
      "at political communications consultant\n",
      "Joey Salgado\n",
      ".\n",
      "More than anything, reminder ito na ang civil society at protest movement ay “utterly divided and caught in its old wars,” sabi nga ng isang batikang journalist sa Rappler.\n",
      "Panahon pa ito ng protest movement na umusbong matapos ma-assassinate si Ninoy Aquino: nanawagan ang August\n",
      "--------------------\n",
      "Sample 7:\n",
      "Input: .” Tahasan pa ngang\n",
      "nireject ni Cardinal Ambo David\n",
      "ang panawagang “Marcos-Duterte resign.” Naka-align ito sa hard-nosed analysis ng mga political observers tulad ng mamamahayag na si\n",
      "Vergel Santos\n",
      "at political communications consultant\n",
      "Joey Salgado\n",
      ".\n",
      "More than anything, reminder ito na ang civil society at protest movement ay “utterly divided and caught in its old wars,” sabi nga ng isang batikang journalist sa Rappler.\n",
      "Panahon pa ito ng protest movement na umusbong matapos ma-assassinate si Ninoy Aquino: nanawagan ang August Twenty-One Movement ng “paglahok” sa snap presidential elections habang nanawagan ang National Democratic (ND) Movement ng “boycott.” Ano ang leksiyon ng kasaysayan? Sa terminong masasapul ng henerasyon ngayon, the moral of the lesson is: Choose engagement over being passive-aggressive.\n",
      "For the sake of argument, kung sakali mang may scenario na mapipilitang mag-resign si Marcos at Duterte, anong “transitory council” ang mabubuo sa harap ng hudikatura na compromised, ng Senado na factionalized, ng militar\n",
      "Target:  Tahasan pa ngang\n",
      "nireject ni Cardinal Ambo David\n",
      "ang panawagang “Marcos-Duterte resign.” Naka-align ito sa hard-nosed analysis ng mga political observers tulad ng mamamahayag na si\n",
      "Vergel Santos\n",
      "at political communications consultant\n",
      "Joey Salgado\n",
      ".\n",
      "More than anything, reminder ito na ang civil society at protest movement ay “utterly divided and caught in its old wars,” sabi nga ng isang batikang journalist sa Rappler.\n",
      "Panahon pa ito ng protest movement na umusbong matapos ma-assassinate si Ninoy Aquino: nanawagan ang August Twenty-One Movement ng “paglahok” sa snap presidential elections habang nanawagan ang National Democratic (ND) Movement ng “boycott.” Ano ang leksiyon ng kasaysayan? Sa terminong masasapul ng henerasyon ngayon, the moral of the lesson is: Choose engagement over being passive-aggressive.\n",
      "For the sake of argument, kung sakali mang may scenario na mapipilitang mag-resign si Marcos at Duterte, anong “transitory council” ang mabubuo sa harap ng hudikatura na compromised, ng Senado na factionalized, ng militar na\n",
      "--------------------\n",
      "Sample 8:\n",
      "Input:  August Twenty-One Movement ng “paglahok” sa snap presidential elections habang nanawagan ang National Democratic (ND) Movement ng “boycott.” Ano ang leksiyon ng kasaysayan? Sa terminong masasapul ng henerasyon ngayon, the moral of the lesson is: Choose engagement over being passive-aggressive.\n",
      "For the sake of argument, kung sakali mang may scenario na mapipilitang mag-resign si Marcos at Duterte, anong “transitory council” ang mabubuo sa harap ng hudikatura na compromised, ng Senado na factionalized, ng militar na irrational to all things “red,” at sa tubig at langis na dynamics ng civil society at ND movement?\n",
      "May mga panukala, tulad ng “hybrid” constitutional solution ni\n",
      "law professor at civil society leader Tony La Viña\n",
      "pero tadtad ng\n",
      "paghamon ang ganitong alternatibo\n",
      ".\n",
      "Ang laging challenge sa ating mga Pilipino — saan mang larangan, mapa-pulitika o mapa-kapitbahay o mapa-pamilya pa nga minsan — ang magkaisa. Lagi tayong watak-watak, sindami ng mga isla ng bansa natin. Kanya-kanya\n",
      "Target:  Twenty-One Movement ng “paglahok” sa snap presidential elections habang nanawagan ang National Democratic (ND) Movement ng “boycott.” Ano ang leksiyon ng kasaysayan? Sa terminong masasapul ng henerasyon ngayon, the moral of the lesson is: Choose engagement over being passive-aggressive.\n",
      "For the sake of argument, kung sakali mang may scenario na mapipilitang mag-resign si Marcos at Duterte, anong “transitory council” ang mabubuo sa harap ng hudikatura na compromised, ng Senado na factionalized, ng militar na irrational to all things “red,” at sa tubig at langis na dynamics ng civil society at ND movement?\n",
      "May mga panukala, tulad ng “hybrid” constitutional solution ni\n",
      "law professor at civil society leader Tony La Viña\n",
      "pero tadtad ng\n",
      "paghamon ang ganitong alternatibo\n",
      ".\n",
      "Ang laging challenge sa ating mga Pilipino — saan mang larangan, mapa-pulitika o mapa-kapitbahay o mapa-pamilya pa nga minsan — ang magkaisa. Lagi tayong watak-watak, sindami ng mga isla ng bansa natin. Kanya-kanya,\n",
      "--------------------\n",
      "Sample 9:\n",
      "Input:  na irrational to all things “red,” at sa tubig at langis na dynamics ng civil society at ND movement?\n",
      "May mga panukala, tulad ng “hybrid” constitutional solution ni\n",
      "law professor at civil society leader Tony La Viña\n",
      "pero tadtad ng\n",
      "paghamon ang ganitong alternatibo\n",
      ".\n",
      "Ang laging challenge sa ating mga Pilipino — saan mang larangan, mapa-pulitika o mapa-kapitbahay o mapa-pamilya pa nga minsan — ang magkaisa. Lagi tayong watak-watak, sindami ng mga isla ng bansa natin. Kanya-kanya, hirap magsanib puwersa, kahit noon pang panahon ng Kastila at Amerikano. Ganito na lang ba tayo lagi?\n",
      "Hinog ang panahon para sa pagbabago — pagbabago na maaaring radikal pero nakabatay sa Konstitusyon tulad ng panukala ni La Viña — sa harap ng nakasusukang katiwalian sa halos lahat ng aspeto ng gobyerno. Pero, may nakaamba bang mang-agaw ng momentum? Tila wala, sa harap ng pagkakawatak-watak ng mga puwersang may prinsipyo.\n",
      "Sa tinagal-tagal ng panahon nang pakikipagtunggali sa kapangyarihan, wala\n",
      "Target:  irrational to all things “red,” at sa tubig at langis na dynamics ng civil society at ND movement?\n",
      "May mga panukala, tulad ng “hybrid” constitutional solution ni\n",
      "law professor at civil society leader Tony La Viña\n",
      "pero tadtad ng\n",
      "paghamon ang ganitong alternatibo\n",
      ".\n",
      "Ang laging challenge sa ating mga Pilipino — saan mang larangan, mapa-pulitika o mapa-kapitbahay o mapa-pamilya pa nga minsan — ang magkaisa. Lagi tayong watak-watak, sindami ng mga isla ng bansa natin. Kanya-kanya, hirap magsanib puwersa, kahit noon pang panahon ng Kastila at Amerikano. Ganito na lang ba tayo lagi?\n",
      "Hinog ang panahon para sa pagbabago — pagbabago na maaaring radikal pero nakabatay sa Konstitusyon tulad ng panukala ni La Viña — sa harap ng nakasusukang katiwalian sa halos lahat ng aspeto ng gobyerno. Pero, may nakaamba bang mang-agaw ng momentum? Tila wala, sa harap ng pagkakawatak-watak ng mga puwersang may prinsipyo.\n",
      "Sa tinagal-tagal ng panahon nang pakikipagtunggali sa kapangyarihan, wala pa\n",
      "--------------------\n",
      "Sample 10:\n",
      "Input: , hirap magsanib puwersa, kahit noon pang panahon ng Kastila at Amerikano. Ganito na lang ba tayo lagi?\n",
      "Hinog ang panahon para sa pagbabago — pagbabago na maaaring radikal pero nakabatay sa Konstitusyon tulad ng panukala ni La Viña — sa harap ng nakasusukang katiwalian sa halos lahat ng aspeto ng gobyerno. Pero, may nakaamba bang mang-agaw ng momentum? Tila wala, sa harap ng pagkakawatak-watak ng mga puwersang may prinsipyo.\n",
      "Sa tinagal-tagal ng panahon nang pakikipagtunggali sa kapangyarihan, wala pa rin gagap sa tactical compromise and strategic unity ang mga mulat na puwersa.\n",
      "Tinatamaan na ang ekonomiya\n",
      ". Palubog na ang barko ng pamahalaan, at lahat tayo nanganganib mahila pailalim ng undercurrent.\n",
      "– Rappler.com\n",
      "Summarize this article with AI\n",
      "Share in Chat\n",
      "Share article\n",
      "Facebook\n",
      "X (Twitter)\n",
      "Copy Link\n",
      "Copied\n",
      "How does this make you feel?\n",
      "Loading\n",
      "Related Topics\n",
      "Cardinal Pablo Virgilio David\n",
      "Corruption in the Philippines\n",
      "flood control\n",
      "Cardinal Pablo Virgilio David\n",
      "Daily wRap\n",
      "House suspends Kiko Barzaga for disorderly behavior | The wRap\n",
      "\n",
      "Target:  hirap magsanib puwersa, kahit noon pang panahon ng Kastila at Amerikano. Ganito na lang ba tayo lagi?\n",
      "Hinog ang panahon para sa pagbabago — pagbabago na maaaring radikal pero nakabatay sa Konstitusyon tulad ng panukala ni La Viña — sa harap ng nakasusukang katiwalian sa halos lahat ng aspeto ng gobyerno. Pero, may nakaamba bang mang-agaw ng momentum? Tila wala, sa harap ng pagkakawatak-watak ng mga puwersang may prinsipyo.\n",
      "Sa tinagal-tagal ng panahon nang pakikipagtunggali sa kapangyarihan, wala pa rin gagap sa tactical compromise and strategic unity ang mga mulat na puwersa.\n",
      "Tinatamaan na ang ekonomiya\n",
      ". Palubog na ang barko ng pamahalaan, at lahat tayo nanganganib mahila pailalim ng undercurrent.\n",
      "– Rappler.com\n",
      "Summarize this article with AI\n",
      "Share in Chat\n",
      "Share article\n",
      "Facebook\n",
      "X (Twitter)\n",
      "Copy Link\n",
      "Copied\n",
      "How does this make you feel?\n",
      "Loading\n",
      "Related Topics\n",
      "Cardinal Pablo Virgilio David\n",
      "Corruption in the Philippines\n",
      "flood control\n",
      "Cardinal Pablo Virgilio David\n",
      "Daily wRap\n",
      "House suspends Kiko Barzaga for disorderly behavior | The wRap\n",
      "Share\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Validation Samples ---\")\n",
    "for i in range(10):\n",
    "    x, y = val_dataset[i]\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Input: {tokenizer.decode(x)}\")\n",
    "    print(f\"Target: {tokenizer.decode(y)}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6eaf36",
   "metadata": {},
   "source": [
    "Describe vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a76eda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared datasets | train samples: 141004 | val samples: 17339 | tokenizer vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "tokenizer_vocab = tokenizer_vocab_size(tokenizer)\n",
    "print(\n",
    "    f\"Prepared datasets | train samples: {len(train_dataset)} | \"\n",
    "    f\"val samples: {len(val_dataset)} | tokenizer vocab size: {tokenizer_vocab}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97711115",
   "metadata": {},
   "source": [
    "Model Training\n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ede8c",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59b0b7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from models.language_model import MiniTransformerLanguageModel\n",
    "\n",
    "model = MiniTransformerLanguageModel(\n",
    "    vocab_size=tokenizer_vocab,\n",
    "    embed_dim=EMBED_DIMENSION,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    dropout=DROPOUT,\n",
    "    ff_multiplier=FF_MULTIPLIER,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e2702",
   "metadata": {},
   "source": [
    "Show model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a53ba603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:\n",
      "MiniTransformerLanguageModel(\n",
      "  (token_embedding): Embedding(32000, 256)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lm_head): Linear(in_features=256, out_features=32000, bias=True)\n",
      "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "--------------------------------------------------\n",
      "Total Parameters: 21,286,144\n",
      "Trainable Parameters: 21,286,144\n",
      "Estimated Model Size (Params only): 81.20 MB\n",
      "--------------------------------------------------\n",
      "Generating compact model diagram...\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: model Pages: 1 -->\n",
       "<svg width=\"535pt\" height=\"4126pt\"\n",
       " viewBox=\"0.00 0.00 535.13 4126.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.72 0.72) rotate(0) translate(4 5717)\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-5717 738,-5717 738,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"8,-208 8,-5439 532,-5439 532,-208 8,-208\"/>\n",
       "<text text-anchor=\"middle\" x=\"65\" y=\"-5425.4\" font-family=\"Times,serif\" font-size=\"12.00\">TransformerEncoder</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_3</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"202,-4551 202,-5410 524,-5410 524,-4551 202,-4551\"/>\n",
       "<text text-anchor=\"middle\" x=\"273\" y=\"-5396.4\" font-family=\"Times,serif\" font-size=\"12.00\">TransformerEncoderLayer</text>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_4</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"190,-3684 190,-4543 512,-4543 512,-3684 190,-3684\"/>\n",
       "<text text-anchor=\"middle\" x=\"261\" y=\"-4529.4\" font-family=\"Times,serif\" font-size=\"12.00\">TransformerEncoderLayer</text>\n",
       "</g>\n",
       "<g id=\"clust4\" class=\"cluster\">\n",
       "<title>cluster_5</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"190,-2817 190,-3676 512,-3676 512,-2817 190,-2817\"/>\n",
       "<text text-anchor=\"middle\" x=\"261\" y=\"-3662.4\" font-family=\"Times,serif\" font-size=\"12.00\">TransformerEncoderLayer</text>\n",
       "</g>\n",
       "<g id=\"clust5\" class=\"cluster\">\n",
       "<title>cluster_6</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"190,-1950 190,-2809 512,-2809 512,-1950 190,-1950\"/>\n",
       "<text text-anchor=\"middle\" x=\"261\" y=\"-2795.4\" font-family=\"Times,serif\" font-size=\"12.00\">TransformerEncoderLayer</text>\n",
       "</g>\n",
       "<g id=\"clust6\" class=\"cluster\">\n",
       "<title>cluster_7</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"190,-1083 190,-1942 512,-1942 512,-1083 190,-1083\"/>\n",
       "<text text-anchor=\"middle\" x=\"261\" y=\"-1928.4\" font-family=\"Times,serif\" font-size=\"12.00\">TransformerEncoderLayer</text>\n",
       "</g>\n",
       "<g id=\"clust7\" class=\"cluster\">\n",
       "<title>cluster_8</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"190,-216 190,-1075 512,-1075 512,-216 190,-216\"/>\n",
       "<text text-anchor=\"middle\" x=\"261\" y=\"-1061.4\" font-family=\"Times,serif\" font-size=\"12.00\">TransformerEncoderLayer</text>\n",
       "</g>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"515,-5713 391,-5713 391,-5681 515,-5681 515,-5713\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-5681 391,-5713 461,-5713 461,-5681 391,-5681\"/>\n",
       "<text text-anchor=\"start\" x=\"396\" y=\"-5700\" font-family=\"Linux libertine\" font-size=\"10.00\">input&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"407.5\" y=\"-5689\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"461,-5681 461,-5713 515,-5713 515,-5681 461,-5681\"/>\n",
       "<text text-anchor=\"start\" x=\"466\" y=\"-5694.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256)</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"550,-5645 356,-5645 356,-5603 550,-5603 550,-5645\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"356,-5603 356,-5645 421,-5645 421,-5603 356,-5603\"/>\n",
       "<text text-anchor=\"start\" x=\"361\" y=\"-5627\" font-family=\"Linux libertine\" font-size=\"10.00\">Embedding</text>\n",
       "<text text-anchor=\"start\" x=\"370\" y=\"-5616\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"421,-5624 421,-5645 469,-5645 469,-5624 421,-5624\"/>\n",
       "<text text-anchor=\"start\" x=\"431\" y=\"-5632\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"469,-5624 469,-5645 550,-5645 550,-5624 469,-5624\"/>\n",
       "<text text-anchor=\"start\" x=\"486\" y=\"-5632\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"421,-5603 421,-5624 469,-5624 469,-5603 421,-5603\"/>\n",
       "<text text-anchor=\"start\" x=\"426\" y=\"-5611\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"469,-5603 469,-5624 550,-5624 550,-5603 469,-5603\"/>\n",
       "<text text-anchor=\"start\" x=\"474\" y=\"-5611\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M453,-5680.94C453,-5673.45 453,-5664.12 453,-5655.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"456.5,-5655.16 453,-5645.16 449.5,-5655.16 456.5,-5655.16\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"541,-5567 365,-5567 365,-5525 541,-5525 541,-5567\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"365,-5525 365,-5567 412,-5567 412,-5525 365,-5525\"/>\n",
       "<text text-anchor=\"start\" x=\"379\" y=\"-5549\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"370\" y=\"-5538\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"412,-5546 412,-5567 460,-5567 460,-5546 412,-5546\"/>\n",
       "<text text-anchor=\"start\" x=\"422\" y=\"-5554\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"460,-5546 460,-5567 541,-5567 541,-5546 460,-5546\"/>\n",
       "<text text-anchor=\"start\" x=\"465\" y=\"-5554\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"412,-5525 412,-5546 460,-5546 460,-5525 412,-5525\"/>\n",
       "<text text-anchor=\"start\" x=\"417\" y=\"-5533\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"460,-5525 460,-5546 541,-5546 541,-5525 460,-5525\"/>\n",
       "<text text-anchor=\"start\" x=\"465\" y=\"-5533\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M453,-5602.63C453,-5594.82 453,-5585.73 453,-5577.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"456.5,-5577.16 453,-5567.16 449.5,-5577.16 456.5,-5577.16\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"542.5,-5489 363.5,-5489 363.5,-5447 542.5,-5447 542.5,-5489\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"364,-5447 364,-5489 414,-5489 414,-5447 364,-5447\"/>\n",
       "<text text-anchor=\"start\" x=\"369\" y=\"-5471\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"370.5\" y=\"-5460\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"414,-5468 414,-5489 462,-5489 462,-5468 414,-5468\"/>\n",
       "<text text-anchor=\"start\" x=\"424\" y=\"-5476\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"462,-5468 462,-5489 543,-5489 543,-5468 462,-5468\"/>\n",
       "<text text-anchor=\"start\" x=\"467\" y=\"-5476\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"414,-5447 414,-5468 462,-5468 462,-5447 414,-5447\"/>\n",
       "<text text-anchor=\"start\" x=\"419\" y=\"-5455\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"462,-5447 462,-5468 543,-5468 543,-5447 462,-5447\"/>\n",
       "<text text-anchor=\"start\" x=\"467\" y=\"-5455\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M453,-5524.63C453,-5516.82 453,-5507.73 453,-5499.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"456.5,-5499.16 453,-5489.16 449.5,-5499.16 456.5,-5499.16\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"488,-5381 294,-5381 294,-5339 488,-5339 488,-5381\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"294,-5339 294,-5381 359,-5381 359,-5339 294,-5339\"/>\n",
       "<text text-anchor=\"start\" x=\"299\" y=\"-5363\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"308\" y=\"-5352\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"359,-5360 359,-5381 407,-5381 407,-5360 359,-5360\"/>\n",
       "<text text-anchor=\"start\" x=\"369\" y=\"-5368\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"407,-5360 407,-5381 488,-5381 488,-5360 407,-5360\"/>\n",
       "<text text-anchor=\"start\" x=\"412\" y=\"-5368\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"359,-5339 359,-5360 407,-5360 407,-5339 359,-5339\"/>\n",
       "<text text-anchor=\"start\" x=\"364\" y=\"-5347\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"407,-5339 407,-5360 488,-5360 488,-5339 407,-5339\"/>\n",
       "<text text-anchor=\"start\" x=\"412\" y=\"-5347\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;8 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>3&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M441.05,-5446.56C431.64,-5430.48 418.41,-5407.86 407.91,-5389.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"410.78,-5387.88 402.71,-5381.02 404.73,-5391.41 410.78,-5387.88\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"502,-5147 308,-5147 308,-5105 502,-5105 502,-5147\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"308,-5105 308,-5147 355,-5147 355,-5105 308,-5105\"/>\n",
       "<text text-anchor=\"start\" x=\"322\" y=\"-5129\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"313\" y=\"-5118\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"355,-5126 355,-5147 403,-5147 403,-5126 355,-5126\"/>\n",
       "<text text-anchor=\"start\" x=\"365\" y=\"-5134\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"403,-5126 403,-5147 502,-5147 502,-5126 403,-5126\"/>\n",
       "<text text-anchor=\"start\" x=\"408\" y=\"-5134\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"355,-5105 355,-5126 403,-5126 403,-5105 355,-5105\"/>\n",
       "<text text-anchor=\"start\" x=\"360\" y=\"-5113\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"403,-5105 403,-5126 502,-5126 502,-5105 403,-5105\"/>\n",
       "<text text-anchor=\"start\" x=\"417\" y=\"-5113\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;11 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>3&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M490.13,-5446.83C492.7,-5444.44 495.05,-5441.83 497,-5439 542.45,-5373.07 547.17,-5337.96 525,-5261 513.67,-5221.66 503.36,-5213.45 476,-5183 466.29,-5172.19 454.32,-5161.88 443,-5153.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"445,-5150.25 434.91,-5147.03 440.79,-5155.84 445,-5150.25\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"730,-5381 572,-5381 572,-5339 730,-5339 730,-5381\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"572,-5339 572,-5381 619,-5381 619,-5339 572,-5339\"/>\n",
       "<text text-anchor=\"start\" x=\"587\" y=\"-5363\" font-family=\"Linux libertine\" font-size=\"10.00\">triu</text>\n",
       "<text text-anchor=\"start\" x=\"577\" y=\"-5352\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"619,-5360 619,-5381 667,-5381 667,-5360 619,-5360\"/>\n",
       "<text text-anchor=\"start\" x=\"629\" y=\"-5368\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"667,-5360 667,-5381 730,-5381 730,-5360 667,-5360\"/>\n",
       "<text text-anchor=\"start\" x=\"672\" y=\"-5368\" font-family=\"Linux libertine\" font-size=\"10.00\">(256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"619,-5339 619,-5360 667,-5360 667,-5339 619,-5339\"/>\n",
       "<text text-anchor=\"start\" x=\"624\" y=\"-5347\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"667,-5339 667,-5360 730,-5360 730,-5339 667,-5339\"/>\n",
       "<text text-anchor=\"start\" x=\"672\" y=\"-5347\" font-family=\"Linux libertine\" font-size=\"10.00\">(256, 256) </text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"192,-5303 16,-5303 16,-5261 192,-5261 192,-5303\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"16,-5261 16,-5303 63,-5303 63,-5261 16,-5261\"/>\n",
       "<text text-anchor=\"start\" x=\"22.5\" y=\"-5285\" font-family=\"Linux libertine\" font-size=\"10.00\">__eq__</text>\n",
       "<text text-anchor=\"start\" x=\"21\" y=\"-5274\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"63,-5282 63,-5303 111,-5303 111,-5282 63,-5282\"/>\n",
       "<text text-anchor=\"start\" x=\"73\" y=\"-5290\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"111,-5282 111,-5303 192,-5303 192,-5282 111,-5282\"/>\n",
       "<text text-anchor=\"start\" x=\"116\" y=\"-5290\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"63,-5261 63,-5282 111,-5282 111,-5261 63,-5261\"/>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-5269\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"111,-5261 111,-5282 192,-5282 192,-5261 111,-5261\"/>\n",
       "<text text-anchor=\"start\" x=\"125\" y=\"-5269\" font-family=\"Linux libertine\" font-size=\"10.00\">(256, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;6 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M571.69,-5345.48C557.18,-5343.18 542.15,-5340.92 528,-5339 385.09,-5319.6 345.44,-5326.07 202.06,-5303.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"202.44,-5300.12 192.01,-5302.01 201.34,-5307.03 202.44,-5300.12\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"516.5,-5303 209.5,-5303 209.5,-5261 516.5,-5261 516.5,-5303\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"210,-5261 210,-5303 314,-5303 314,-5261 210,-5261\"/>\n",
       "<text text-anchor=\"start\" x=\"215\" y=\"-5285\" font-family=\"Linux libertine\" font-size=\"10.00\">MultiheadAttention</text>\n",
       "<text text-anchor=\"start\" x=\"243.5\" y=\"-5274\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"314,-5282 314,-5303 362,-5303 362,-5282 314,-5282\"/>\n",
       "<text text-anchor=\"start\" x=\"324\" y=\"-5290\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"362,-5282 362,-5303 517,-5303 517,-5282 362,-5282\"/>\n",
       "<text text-anchor=\"start\" x=\"367\" y=\"-5290\" font-family=\"Linux libertine\" font-size=\"10.00\">3 x (32, 256, 256), (256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"314,-5261 314,-5282 362,-5282 362,-5261 314,-5261\"/>\n",
       "<text text-anchor=\"start\" x=\"319\" y=\"-5269\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"362,-5261 362,-5282 517,-5282 517,-5261 362,-5261\"/>\n",
       "<text text-anchor=\"start\" x=\"404\" y=\"-5269\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>4&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M574.95,-5338.93C536.16,-5328.69 488.82,-5316.2 448.73,-5305.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"449.53,-5302.22 438.97,-5303.05 447.75,-5308.98 449.53,-5302.22\"/>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>20</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"504.5,-4436 197.5,-4436 197.5,-4394 504.5,-4394 504.5,-4436\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"198,-4394 198,-4436 302,-4436 302,-4394 198,-4394\"/>\n",
       "<text text-anchor=\"start\" x=\"203\" y=\"-4418\" font-family=\"Linux libertine\" font-size=\"10.00\">MultiheadAttention</text>\n",
       "<text text-anchor=\"start\" x=\"231.5\" y=\"-4407\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-4415 302,-4436 350,-4436 350,-4415 302,-4415\"/>\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-4423\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-4415 350,-4436 505,-4436 505,-4415 350,-4415\"/>\n",
       "<text text-anchor=\"start\" x=\"355\" y=\"-4423\" font-family=\"Linux libertine\" font-size=\"10.00\">3 x (32, 256, 256), (256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-4394 302,-4415 350,-4415 350,-4394 302,-4394\"/>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-4402\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-4394 350,-4415 505,-4415 505,-4394 350,-4394\"/>\n",
       "<text text-anchor=\"start\" x=\"392\" y=\"-4402\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;20 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>4&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M631.32,-5338.97C622.41,-5328.91 612.46,-5316.09 606,-5303 586.15,-5262.79 582,-5249.84 582,-5205 582,-5205 582,-5205 582,-4579 582,-4505.9 503.82,-4462.69 438.43,-4439.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"439.48,-4435.99 428.88,-4436.03 437.19,-4442.6 439.48,-4435.99\"/>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>31</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"504.5,-3569 197.5,-3569 197.5,-3527 504.5,-3527 504.5,-3569\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"198,-3527 198,-3569 302,-3569 302,-3527 198,-3527\"/>\n",
       "<text text-anchor=\"start\" x=\"203\" y=\"-3551\" font-family=\"Linux libertine\" font-size=\"10.00\">MultiheadAttention</text>\n",
       "<text text-anchor=\"start\" x=\"231.5\" y=\"-3540\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-3548 302,-3569 350,-3569 350,-3548 302,-3548\"/>\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-3556\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-3548 350,-3569 505,-3569 505,-3548 350,-3548\"/>\n",
       "<text text-anchor=\"start\" x=\"355\" y=\"-3556\" font-family=\"Linux libertine\" font-size=\"10.00\">3 x (32, 256, 256), (256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-3527 302,-3548 350,-3548 350,-3527 302,-3527\"/>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-3535\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-3527 350,-3548 505,-3548 505,-3527 350,-3527\"/>\n",
       "<text text-anchor=\"start\" x=\"392\" y=\"-3535\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;31 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>4&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M644.27,-5338.83C635.16,-5309.66 620,-5253.82 620,-5205 620,-5205 620,-5205 620,-3712 620,-3635.44 539.31,-3593.85 466.32,-3571.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"467.12,-3568.47 456.54,-3569.05 465.17,-3575.2 467.12,-3568.47\"/>\n",
       "</g>\n",
       "<!-- 42 -->\n",
       "<g id=\"node43\" class=\"node\">\n",
       "<title>42</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"504.5,-2702 197.5,-2702 197.5,-2660 504.5,-2660 504.5,-2702\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"198,-2660 198,-2702 302,-2702 302,-2660 198,-2660\"/>\n",
       "<text text-anchor=\"start\" x=\"203\" y=\"-2684\" font-family=\"Linux libertine\" font-size=\"10.00\">MultiheadAttention</text>\n",
       "<text text-anchor=\"start\" x=\"231.5\" y=\"-2673\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-2681 302,-2702 350,-2702 350,-2681 302,-2681\"/>\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-2689\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-2681 350,-2702 505,-2702 505,-2681 350,-2681\"/>\n",
       "<text text-anchor=\"start\" x=\"355\" y=\"-2689\" font-family=\"Linux libertine\" font-size=\"10.00\">3 x (32, 256, 256), (256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-2660 302,-2681 350,-2681 350,-2660 302,-2660\"/>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-2668\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-2660 350,-2681 505,-2681 505,-2660 350,-2660\"/>\n",
       "<text text-anchor=\"start\" x=\"392\" y=\"-2668\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;42 -->\n",
       "<g id=\"edge51\" class=\"edge\">\n",
       "<title>4&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M652.52,-5338.59C654.58,-5309.11 658,-5252.91 658,-5205 658,-5205 658,-5205 658,-2845 658,-2766.73 579.19,-2725.93 501.44,-2704.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"502.03,-2701.26 491.47,-2702.11 500.26,-2708.03 502.03,-2701.26\"/>\n",
       "</g>\n",
       "<!-- 53 -->\n",
       "<g id=\"node54\" class=\"node\">\n",
       "<title>53</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"504.5,-1835 197.5,-1835 197.5,-1793 504.5,-1793 504.5,-1835\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"198,-1793 198,-1835 302,-1835 302,-1793 198,-1793\"/>\n",
       "<text text-anchor=\"start\" x=\"203\" y=\"-1817\" font-family=\"Linux libertine\" font-size=\"10.00\">MultiheadAttention</text>\n",
       "<text text-anchor=\"start\" x=\"231.5\" y=\"-1806\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-1814 302,-1835 350,-1835 350,-1814 302,-1814\"/>\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-1822\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-1814 350,-1835 505,-1835 505,-1814 350,-1814\"/>\n",
       "<text text-anchor=\"start\" x=\"355\" y=\"-1822\" font-family=\"Linux libertine\" font-size=\"10.00\">3 x (32, 256, 256), (256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-1793 302,-1814 350,-1814 350,-1793 302,-1793\"/>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-1801\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-1793 350,-1814 505,-1814 505,-1793 350,-1793\"/>\n",
       "<text text-anchor=\"start\" x=\"392\" y=\"-1801\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;53 -->\n",
       "<g id=\"edge65\" class=\"edge\">\n",
       "<title>4&#45;&gt;53</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M660.99,-5338.63C674.24,-5309.67 696,-5254.58 696,-5205 696,-5205 696,-5205 696,-1978 696,-1890.28 603.7,-1849.7 514.59,-1830.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"515.08,-1827.49 504.58,-1828.94 513.69,-1834.35 515.08,-1827.49\"/>\n",
       "</g>\n",
       "<!-- 64 -->\n",
       "<g id=\"node65\" class=\"node\">\n",
       "<title>64</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"504.5,-968 197.5,-968 197.5,-926 504.5,-926 504.5,-968\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"198,-926 198,-968 302,-968 302,-926 198,-926\"/>\n",
       "<text text-anchor=\"start\" x=\"203\" y=\"-950\" font-family=\"Linux libertine\" font-size=\"10.00\">MultiheadAttention</text>\n",
       "<text text-anchor=\"start\" x=\"231.5\" y=\"-939\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-947 302,-968 350,-968 350,-947 302,-947\"/>\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-955\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-947 350,-968 505,-968 505,-947 350,-947\"/>\n",
       "<text text-anchor=\"start\" x=\"355\" y=\"-955\" font-family=\"Linux libertine\" font-size=\"10.00\">3 x (32, 256, 256), (256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-926 302,-947 350,-947 350,-926 302,-926\"/>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-934\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-926 350,-947 505,-947 505,-926 350,-926\"/>\n",
       "<text text-anchor=\"start\" x=\"392\" y=\"-934\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;64 -->\n",
       "<g id=\"edge79\" class=\"edge\">\n",
       "<title>4&#45;&gt;64</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M670.25,-5338.94C694.77,-5311.2 734,-5258.45 734,-5205 734,-5205 734,-5205 734,-1111 734,-1009.49 618.69,-970.3 514.61,-955.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"514.95,-952.12 504.57,-954.26 514.02,-959.06 514.95,-952.12\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"183,-5381 25,-5381 25,-5339 183,-5339 183,-5381\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"25,-5339 25,-5381 72,-5381 72,-5339 25,-5339\"/>\n",
       "<text text-anchor=\"start\" x=\"40\" y=\"-5363\" font-family=\"Linux libertine\" font-size=\"10.00\">triu</text>\n",
       "<text text-anchor=\"start\" x=\"30\" y=\"-5352\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"72,-5360 72,-5381 120,-5381 120,-5360 72,-5360\"/>\n",
       "<text text-anchor=\"start\" x=\"82\" y=\"-5368\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"120,-5360 120,-5381 183,-5381 183,-5360 120,-5360\"/>\n",
       "<text text-anchor=\"start\" x=\"125\" y=\"-5368\" font-family=\"Linux libertine\" font-size=\"10.00\">(256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"72,-5339 72,-5360 120,-5360 120,-5339 72,-5339\"/>\n",
       "<text text-anchor=\"start\" x=\"77\" y=\"-5347\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"120,-5339 120,-5360 183,-5360 183,-5339 120,-5339\"/>\n",
       "<text text-anchor=\"start\" x=\"125\" y=\"-5347\" font-family=\"Linux libertine\" font-size=\"10.00\">(256, 256) </text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M104,-5338.63C104,-5330.82 104,-5321.73 104,-5313.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"107.5,-5313.16 104,-5303.16 100.5,-5313.16 107.5,-5313.16\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"183,-5225 25,-5225 25,-5183 183,-5183 183,-5225\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"25,-5183 25,-5225 72,-5225 72,-5183 25,-5183\"/>\n",
       "<text text-anchor=\"start\" x=\"42\" y=\"-5207\" font-family=\"Linux libertine\" font-size=\"10.00\">all</text>\n",
       "<text text-anchor=\"start\" x=\"30\" y=\"-5196\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"72,-5204 72,-5225 120,-5225 120,-5204 72,-5204\"/>\n",
       "<text text-anchor=\"start\" x=\"82\" y=\"-5212\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"120,-5204 120,-5225 183,-5225 183,-5204 120,-5204\"/>\n",
       "<text text-anchor=\"start\" x=\"125\" y=\"-5212\" font-family=\"Linux libertine\" font-size=\"10.00\">(256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"72,-5183 72,-5204 120,-5204 120,-5183 72,-5183\"/>\n",
       "<text text-anchor=\"start\" x=\"77\" y=\"-5191\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"120,-5183 120,-5204 183,-5204 183,-5183 120,-5183\"/>\n",
       "<text text-anchor=\"start\" x=\"146\" y=\"-5191\" font-family=\"Linux libertine\" font-size=\"10.00\">() </text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M104,-5260.63C104,-5252.82 104,-5243.73 104,-5235.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"107.5,-5235.16 104,-5225.16 100.5,-5235.16 107.5,-5235.16\"/>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M383.5,-5338.63C380.55,-5330.65 377.12,-5321.33 373.91,-5312.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"377.17,-5311.33 370.43,-5303.16 370.6,-5313.75 377.17,-5311.33\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"466.5,-5225 287.5,-5225 287.5,-5183 466.5,-5183 466.5,-5225\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"288,-5183 288,-5225 338,-5225 338,-5183 288,-5183\"/>\n",
       "<text text-anchor=\"start\" x=\"293\" y=\"-5207\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"294.5\" y=\"-5196\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"338,-5204 338,-5225 386,-5225 386,-5204 338,-5204\"/>\n",
       "<text text-anchor=\"start\" x=\"348\" y=\"-5212\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"386,-5204 386,-5225 467,-5225 467,-5204 386,-5204\"/>\n",
       "<text text-anchor=\"start\" x=\"391\" y=\"-5212\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"338,-5183 338,-5204 386,-5204 386,-5183 338,-5183\"/>\n",
       "<text text-anchor=\"start\" x=\"343\" y=\"-5191\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"386,-5183 386,-5204 467,-5204 467,-5183 386,-5183\"/>\n",
       "<text text-anchor=\"start\" x=\"391\" y=\"-5191\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M366.75,-5260.63C368.19,-5252.82 369.87,-5243.73 371.44,-5235.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"374.92,-5235.63 373.29,-5225.16 368.03,-5234.36 374.92,-5235.63\"/>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M384.5,-5182.63C387.45,-5174.65 390.88,-5165.33 394.09,-5156.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"397.4,-5157.75 397.57,-5147.16 390.83,-5155.33 397.4,-5157.75\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"446,-5069 252,-5069 252,-5027 446,-5027 446,-5069\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"252,-5027 252,-5069 317,-5069 317,-5027 252,-5027\"/>\n",
       "<text text-anchor=\"start\" x=\"257\" y=\"-5051\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"266\" y=\"-5040\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"317,-5048 317,-5069 365,-5069 365,-5048 317,-5048\"/>\n",
       "<text text-anchor=\"start\" x=\"327\" y=\"-5056\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"365,-5048 365,-5069 446,-5069 446,-5048 365,-5048\"/>\n",
       "<text text-anchor=\"start\" x=\"370\" y=\"-5056\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"317,-5027 317,-5048 365,-5048 365,-5027 317,-5027\"/>\n",
       "<text text-anchor=\"start\" x=\"322\" y=\"-5035\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"365,-5027 365,-5048 446,-5048 446,-5027 365,-5027\"/>\n",
       "<text text-anchor=\"start\" x=\"370\" y=\"-5035\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M389.99,-5104.63C383.79,-5096.22 376.5,-5086.32 369.8,-5077.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"372.6,-5075.13 363.85,-5069.16 366.97,-5079.29 372.6,-5075.13\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"496,-4601 302,-4601 302,-4559 496,-4559 496,-4601\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-4559 302,-4601 349,-4601 349,-4559 302,-4559\"/>\n",
       "<text text-anchor=\"start\" x=\"316\" y=\"-4583\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-4572\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"349,-4580 349,-4601 397,-4601 397,-4580 349,-4580\"/>\n",
       "<text text-anchor=\"start\" x=\"359\" y=\"-4588\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"397,-4580 397,-4601 496,-4601 496,-4580 397,-4580\"/>\n",
       "<text text-anchor=\"start\" x=\"402\" y=\"-4588\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"349,-4559 349,-4580 397,-4580 397,-4559 349,-4559\"/>\n",
       "<text text-anchor=\"start\" x=\"354\" y=\"-4567\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"397,-4559 397,-4580 496,-4580 496,-4559 397,-4559\"/>\n",
       "<text text-anchor=\"start\" x=\"411\" y=\"-4567\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;18 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>11&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M428.55,-5104.97C438.43,-5095.19 449.01,-5082.6 455,-5069 472.66,-5028.94 465,-5014.78 465,-4971 465,-4971 465,-4971 465,-4735 465,-4691.17 472.83,-4676.58 454,-4637 448.93,-4626.34 440.99,-4616.48 432.71,-4608.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"435.06,-4605.51 425.41,-4601.14 430.23,-4610.57 435.06,-4605.51\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"441,-4991 259,-4991 259,-4949 441,-4949 441,-4991\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"259,-4949 259,-4991 306,-4991 306,-4949 259,-4949\"/>\n",
       "<text text-anchor=\"start\" x=\"267\" y=\"-4973\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"264\" y=\"-4962\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"306,-4970 306,-4991 354,-4991 354,-4970 306,-4970\"/>\n",
       "<text text-anchor=\"start\" x=\"316\" y=\"-4978\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"354,-4970 354,-4991 441,-4991 441,-4970 354,-4970\"/>\n",
       "<text text-anchor=\"start\" x=\"362\" y=\"-4978\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"306,-4949 306,-4970 354,-4970 354,-4949 306,-4949\"/>\n",
       "<text text-anchor=\"start\" x=\"311\" y=\"-4957\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"354,-4949 354,-4970 441,-4970 441,-4949 354,-4949\"/>\n",
       "<text text-anchor=\"start\" x=\"359\" y=\"-4957\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M349.27,-5026.63C349.37,-5018.82 349.49,-5009.73 349.6,-5001.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"353.1,-5001.2 349.73,-4991.16 346.1,-5001.11 353.1,-5001.2\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"443,-4913 261,-4913 261,-4871 443,-4871 443,-4913\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"261,-4871 261,-4913 308,-4913 308,-4871 261,-4871\"/>\n",
       "<text text-anchor=\"start\" x=\"273.5\" y=\"-4895\" font-family=\"Linux libertine\" font-size=\"10.00\">gelu</text>\n",
       "<text text-anchor=\"start\" x=\"266\" y=\"-4884\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"308,-4892 308,-4913 356,-4913 356,-4892 308,-4892\"/>\n",
       "<text text-anchor=\"start\" x=\"318\" y=\"-4900\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"356,-4892 356,-4913 443,-4913 443,-4892 356,-4892\"/>\n",
       "<text text-anchor=\"start\" x=\"361\" y=\"-4900\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"308,-4871 308,-4892 356,-4892 356,-4871 308,-4871\"/>\n",
       "<text text-anchor=\"start\" x=\"313\" y=\"-4879\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"356,-4871 356,-4892 443,-4892 443,-4871 356,-4871\"/>\n",
       "<text text-anchor=\"start\" x=\"361\" y=\"-4879\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M350.54,-4948.63C350.74,-4940.82 350.98,-4931.73 351.21,-4923.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"354.71,-4923.25 351.47,-4913.16 347.71,-4923.06 354.71,-4923.25\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"445.5,-4835 260.5,-4835 260.5,-4793 445.5,-4793 445.5,-4835\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"261,-4793 261,-4835 311,-4835 311,-4793 261,-4793\"/>\n",
       "<text text-anchor=\"start\" x=\"266\" y=\"-4817\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"267.5\" y=\"-4806\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"311,-4814 311,-4835 359,-4835 359,-4814 311,-4814\"/>\n",
       "<text text-anchor=\"start\" x=\"321\" y=\"-4822\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"359,-4814 359,-4835 446,-4835 446,-4814 359,-4814\"/>\n",
       "<text text-anchor=\"start\" x=\"364\" y=\"-4822\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"311,-4793 311,-4814 359,-4814 359,-4793 311,-4793\"/>\n",
       "<text text-anchor=\"start\" x=\"316\" y=\"-4801\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"359,-4793 359,-4814 446,-4814 446,-4793 359,-4793\"/>\n",
       "<text text-anchor=\"start\" x=\"364\" y=\"-4801\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M352.27,-4870.63C352.37,-4862.82 352.49,-4853.73 352.6,-4845.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"356.1,-4845.2 352.73,-4835.16 349.1,-4845.11 356.1,-4845.2\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"445,-4757 263,-4757 263,-4715 445,-4715 445,-4757\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"263,-4715 263,-4757 310,-4757 310,-4715 263,-4715\"/>\n",
       "<text text-anchor=\"start\" x=\"271\" y=\"-4739\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"268\" y=\"-4728\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"310,-4736 310,-4757 358,-4757 358,-4736 310,-4736\"/>\n",
       "<text text-anchor=\"start\" x=\"320\" y=\"-4744\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"358,-4736 358,-4757 445,-4757 445,-4736 358,-4736\"/>\n",
       "<text text-anchor=\"start\" x=\"363\" y=\"-4744\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"310,-4715 310,-4736 358,-4736 358,-4715 310,-4715\"/>\n",
       "<text text-anchor=\"start\" x=\"315\" y=\"-4723\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"358,-4715 358,-4736 445,-4736 445,-4715 358,-4715\"/>\n",
       "<text text-anchor=\"start\" x=\"366\" y=\"-4723\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M353.27,-4792.63C353.37,-4784.82 353.49,-4775.73 353.6,-4767.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"357.1,-4767.2 353.73,-4757.16 350.1,-4767.11 357.1,-4767.2\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"444.5,-4679 265.5,-4679 265.5,-4637 444.5,-4637 444.5,-4679\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"266,-4637 266,-4679 316,-4679 316,-4637 266,-4637\"/>\n",
       "<text text-anchor=\"start\" x=\"271\" y=\"-4661\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"272.5\" y=\"-4650\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"316,-4658 316,-4679 364,-4679 364,-4658 316,-4658\"/>\n",
       "<text text-anchor=\"start\" x=\"326\" y=\"-4666\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"364,-4658 364,-4679 445,-4679 445,-4658 364,-4658\"/>\n",
       "<text text-anchor=\"start\" x=\"369\" y=\"-4666\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"316,-4637 316,-4658 364,-4658 364,-4637 316,-4637\"/>\n",
       "<text text-anchor=\"start\" x=\"321\" y=\"-4645\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"364,-4637 364,-4658 445,-4658 445,-4637 364,-4637\"/>\n",
       "<text text-anchor=\"start\" x=\"369\" y=\"-4645\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M354.27,-4714.63C354.37,-4706.82 354.49,-4697.73 354.6,-4689.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"358.1,-4689.2 354.73,-4679.16 351.1,-4689.11 358.1,-4689.2\"/>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M366.79,-4636.63C371.51,-4628.48 377.04,-4618.92 382.18,-4610.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"385.35,-4611.57 387.33,-4601.16 379.29,-4608.06 385.35,-4611.57\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>19</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"476,-4514 282,-4514 282,-4472 476,-4472 476,-4514\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"282,-4472 282,-4514 347,-4514 347,-4472 282,-4472\"/>\n",
       "<text text-anchor=\"start\" x=\"287\" y=\"-4496\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"296\" y=\"-4485\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"347,-4493 347,-4514 395,-4514 395,-4493 347,-4493\"/>\n",
       "<text text-anchor=\"start\" x=\"357\" y=\"-4501\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"395,-4493 395,-4514 476,-4514 476,-4493 395,-4493\"/>\n",
       "<text text-anchor=\"start\" x=\"400\" y=\"-4501\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"347,-4472 347,-4493 395,-4493 395,-4472 347,-4472\"/>\n",
       "<text text-anchor=\"start\" x=\"352\" y=\"-4480\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"395,-4472 395,-4493 476,-4493 476,-4472 395,-4472\"/>\n",
       "<text text-anchor=\"start\" x=\"400\" y=\"-4480\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;19 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>18&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M394.26,-4558.86C391.84,-4548.56 388.84,-4535.81 386.14,-4524.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"389.47,-4523.23 383.78,-4514.29 382.66,-4524.83 389.47,-4523.23\"/>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>22</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"478,-4280 284,-4280 284,-4238 478,-4238 478,-4280\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"284,-4238 284,-4280 331,-4280 331,-4238 284,-4238\"/>\n",
       "<text text-anchor=\"start\" x=\"298\" y=\"-4262\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"289\" y=\"-4251\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"331,-4259 331,-4280 379,-4280 379,-4259 331,-4259\"/>\n",
       "<text text-anchor=\"start\" x=\"341\" y=\"-4267\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"379,-4259 379,-4280 478,-4280 478,-4259 379,-4259\"/>\n",
       "<text text-anchor=\"start\" x=\"384\" y=\"-4267\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"331,-4238 331,-4259 379,-4259 379,-4238 331,-4238\"/>\n",
       "<text text-anchor=\"start\" x=\"336\" y=\"-4246\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"379,-4238 379,-4259 478,-4259 478,-4238 379,-4238\"/>\n",
       "<text text-anchor=\"start\" x=\"393\" y=\"-4246\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;22 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>18&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M465.28,-4558.96C472.66,-4554.61 479.48,-4549.35 485,-4543 529.18,-4492.13 532.24,-4458.58 513,-4394 501.17,-4354.3 490.91,-4345.66 462,-4316 451.14,-4304.86 437.77,-4294.56 425.04,-4285.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"426.65,-4282.78 416.37,-4280.21 422.8,-4288.63 426.65,-4282.78\"/>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;20 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>19&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M371.5,-4471.63C368.55,-4463.65 365.12,-4454.33 361.91,-4445.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"365.17,-4444.33 358.43,-4436.16 358.6,-4446.75 365.17,-4444.33\"/>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>21</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"452.5,-4358 273.5,-4358 273.5,-4316 452.5,-4316 452.5,-4358\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"274,-4316 274,-4358 324,-4358 324,-4316 274,-4316\"/>\n",
       "<text text-anchor=\"start\" x=\"279\" y=\"-4340\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"280.5\" y=\"-4329\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"324,-4337 324,-4358 372,-4358 372,-4337 324,-4337\"/>\n",
       "<text text-anchor=\"start\" x=\"334\" y=\"-4345\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"372,-4337 372,-4358 453,-4358 453,-4337 372,-4337\"/>\n",
       "<text text-anchor=\"start\" x=\"377\" y=\"-4345\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"324,-4316 324,-4337 372,-4337 372,-4316 324,-4316\"/>\n",
       "<text text-anchor=\"start\" x=\"329\" y=\"-4324\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"372,-4316 372,-4337 453,-4337 453,-4316 372,-4316\"/>\n",
       "<text text-anchor=\"start\" x=\"377\" y=\"-4324\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 20&#45;&gt;21 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>20&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M354.22,-4393.63C355.45,-4385.82 356.89,-4376.73 358.23,-4368.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"361.71,-4368.58 359.82,-4358.16 354.8,-4367.49 361.71,-4368.58\"/>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;22 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>21&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M367.82,-4315.63C369.69,-4307.73 371.88,-4298.53 373.92,-4289.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"377.33,-4290.7 376.23,-4280.16 370.51,-4289.08 377.33,-4290.7\"/>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>23</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"467,-4202 273,-4202 273,-4160 467,-4160 467,-4202\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"273,-4160 273,-4202 338,-4202 338,-4160 273,-4160\"/>\n",
       "<text text-anchor=\"start\" x=\"278\" y=\"-4184\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"287\" y=\"-4173\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"338,-4181 338,-4202 386,-4202 386,-4181 338,-4181\"/>\n",
       "<text text-anchor=\"start\" x=\"348\" y=\"-4189\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"386,-4181 386,-4202 467,-4202 467,-4181 386,-4181\"/>\n",
       "<text text-anchor=\"start\" x=\"391\" y=\"-4189\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"338,-4160 338,-4181 386,-4181 386,-4160 338,-4160\"/>\n",
       "<text text-anchor=\"start\" x=\"343\" y=\"-4168\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"386,-4160 386,-4181 467,-4181 467,-4160 386,-4160\"/>\n",
       "<text text-anchor=\"start\" x=\"391\" y=\"-4168\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;23 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>22&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M378.05,-4237.63C376.92,-4229.82 375.6,-4220.73 374.37,-4212.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"377.81,-4211.55 372.92,-4202.16 370.89,-4212.56 377.81,-4211.55\"/>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"492,-3734 298,-3734 298,-3692 492,-3692 492,-3734\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"298,-3692 298,-3734 345,-3734 345,-3692 298,-3692\"/>\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-3716\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"303\" y=\"-3705\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"345,-3713 345,-3734 393,-3734 393,-3713 345,-3713\"/>\n",
       "<text text-anchor=\"start\" x=\"355\" y=\"-3721\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"393,-3713 393,-3734 492,-3734 492,-3713 393,-3713\"/>\n",
       "<text text-anchor=\"start\" x=\"398\" y=\"-3721\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"345,-3692 345,-3713 393,-3713 393,-3692 345,-3692\"/>\n",
       "<text text-anchor=\"start\" x=\"350\" y=\"-3700\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"393,-3692 393,-3713 492,-3713 492,-3692 393,-3692\"/>\n",
       "<text text-anchor=\"start\" x=\"407\" y=\"-3700\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;29 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>22&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M434.32,-4237.85C450.24,-4229.22 466.08,-4217.47 476,-4202 499.62,-4165.14 486,-4147.78 486,-4104 486,-4104 486,-4104 486,-3868 486,-3824.17 497.38,-3807.68 475,-3770 467.82,-3757.92 456.91,-3747.79 445.43,-3739.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"447.28,-3736.64 437.01,-3734.01 443.4,-3742.47 447.28,-3736.64\"/>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>24</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"461,-4124 279,-4124 279,-4082 461,-4082 461,-4124\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"279,-4082 279,-4124 326,-4124 326,-4082 279,-4082\"/>\n",
       "<text text-anchor=\"start\" x=\"287\" y=\"-4106\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"284\" y=\"-4095\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"326,-4103 326,-4124 374,-4124 374,-4103 326,-4103\"/>\n",
       "<text text-anchor=\"start\" x=\"336\" y=\"-4111\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"374,-4103 374,-4124 461,-4124 461,-4103 374,-4103\"/>\n",
       "<text text-anchor=\"start\" x=\"382\" y=\"-4111\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"326,-4082 326,-4103 374,-4103 374,-4082 326,-4082\"/>\n",
       "<text text-anchor=\"start\" x=\"331\" y=\"-4090\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"374,-4082 374,-4103 461,-4103 461,-4082 374,-4082\"/>\n",
       "<text text-anchor=\"start\" x=\"379\" y=\"-4090\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;24 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>23&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M370,-4159.63C370,-4151.82 370,-4142.73 370,-4134.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"373.5,-4134.16 370,-4124.16 366.5,-4134.16 373.5,-4134.16\"/>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>25</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"463,-4046 281,-4046 281,-4004 463,-4004 463,-4046\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"281,-4004 281,-4046 328,-4046 328,-4004 281,-4004\"/>\n",
       "<text text-anchor=\"start\" x=\"293.5\" y=\"-4028\" font-family=\"Linux libertine\" font-size=\"10.00\">gelu</text>\n",
       "<text text-anchor=\"start\" x=\"286\" y=\"-4017\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"328,-4025 328,-4046 376,-4046 376,-4025 328,-4025\"/>\n",
       "<text text-anchor=\"start\" x=\"338\" y=\"-4033\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"376,-4025 376,-4046 463,-4046 463,-4025 376,-4025\"/>\n",
       "<text text-anchor=\"start\" x=\"381\" y=\"-4033\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"328,-4004 328,-4025 376,-4025 376,-4004 328,-4004\"/>\n",
       "<text text-anchor=\"start\" x=\"333\" y=\"-4012\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"376,-4004 376,-4025 463,-4025 463,-4004 376,-4004\"/>\n",
       "<text text-anchor=\"start\" x=\"381\" y=\"-4012\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;25 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>24&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M370.54,-4081.63C370.74,-4073.82 370.98,-4064.73 371.21,-4056.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"374.71,-4056.25 371.47,-4046.16 367.71,-4056.06 374.71,-4056.25\"/>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"465.5,-3968 280.5,-3968 280.5,-3926 465.5,-3926 465.5,-3968\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"281,-3926 281,-3968 331,-3968 331,-3926 281,-3926\"/>\n",
       "<text text-anchor=\"start\" x=\"286\" y=\"-3950\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"287.5\" y=\"-3939\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"331,-3947 331,-3968 379,-3968 379,-3947 331,-3947\"/>\n",
       "<text text-anchor=\"start\" x=\"341\" y=\"-3955\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"379,-3947 379,-3968 466,-3968 466,-3947 379,-3947\"/>\n",
       "<text text-anchor=\"start\" x=\"384\" y=\"-3955\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"331,-3926 331,-3947 379,-3947 379,-3926 331,-3926\"/>\n",
       "<text text-anchor=\"start\" x=\"336\" y=\"-3934\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"379,-3926 379,-3947 466,-3947 466,-3926 379,-3926\"/>\n",
       "<text text-anchor=\"start\" x=\"384\" y=\"-3934\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;26 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>25&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M372.27,-4003.63C372.37,-3995.82 372.49,-3986.73 372.6,-3978.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"376.1,-3978.2 372.73,-3968.16 369.1,-3978.11 376.1,-3978.2\"/>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"465,-3890 283,-3890 283,-3848 465,-3848 465,-3890\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"283,-3848 283,-3890 330,-3890 330,-3848 283,-3848\"/>\n",
       "<text text-anchor=\"start\" x=\"291\" y=\"-3872\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"288\" y=\"-3861\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"330,-3869 330,-3890 378,-3890 378,-3869 330,-3869\"/>\n",
       "<text text-anchor=\"start\" x=\"340\" y=\"-3877\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"378,-3869 378,-3890 465,-3890 465,-3869 378,-3869\"/>\n",
       "<text text-anchor=\"start\" x=\"383\" y=\"-3877\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"330,-3848 330,-3869 378,-3869 378,-3848 330,-3848\"/>\n",
       "<text text-anchor=\"start\" x=\"335\" y=\"-3856\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"378,-3848 378,-3869 465,-3869 465,-3848 378,-3848\"/>\n",
       "<text text-anchor=\"start\" x=\"386\" y=\"-3856\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;27 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>26&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M373.27,-3925.63C373.37,-3917.82 373.49,-3908.73 373.6,-3900.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"377.1,-3900.2 373.73,-3890.16 370.1,-3900.11 377.1,-3900.2\"/>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>28</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"465.5,-3812 286.5,-3812 286.5,-3770 465.5,-3770 465.5,-3812\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"287,-3770 287,-3812 337,-3812 337,-3770 287,-3770\"/>\n",
       "<text text-anchor=\"start\" x=\"292\" y=\"-3794\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"293.5\" y=\"-3783\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"337,-3791 337,-3812 385,-3812 385,-3791 337,-3791\"/>\n",
       "<text text-anchor=\"start\" x=\"347\" y=\"-3799\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"385,-3791 385,-3812 466,-3812 466,-3791 385,-3791\"/>\n",
       "<text text-anchor=\"start\" x=\"390\" y=\"-3799\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"337,-3770 337,-3791 385,-3791 385,-3770 337,-3770\"/>\n",
       "<text text-anchor=\"start\" x=\"342\" y=\"-3778\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"385,-3770 385,-3791 466,-3791 466,-3770 385,-3770\"/>\n",
       "<text text-anchor=\"start\" x=\"390\" y=\"-3778\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 27&#45;&gt;28 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>27&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M374.54,-3847.63C374.74,-3839.82 374.98,-3830.73 375.21,-3822.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"378.71,-3822.25 375.47,-3812.16 371.71,-3822.06 378.71,-3822.25\"/>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;29 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>28&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M381.09,-3769.63C383.07,-3761.73 385.37,-3752.53 387.52,-3743.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"390.93,-3744.71 389.96,-3734.16 384.14,-3743.01 390.93,-3744.71\"/>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>30</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"472,-3647 278,-3647 278,-3605 472,-3605 472,-3647\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"278,-3605 278,-3647 343,-3647 343,-3605 278,-3605\"/>\n",
       "<text text-anchor=\"start\" x=\"283\" y=\"-3629\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"292\" y=\"-3618\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-3626 343,-3647 391,-3647 391,-3626 343,-3626\"/>\n",
       "<text text-anchor=\"start\" x=\"353\" y=\"-3634\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-3626 391,-3647 472,-3647 472,-3626 391,-3626\"/>\n",
       "<text text-anchor=\"start\" x=\"396\" y=\"-3634\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-3605 343,-3626 391,-3626 391,-3605 343,-3605\"/>\n",
       "<text text-anchor=\"start\" x=\"348\" y=\"-3613\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-3605 391,-3626 472,-3626 472,-3605 391,-3605\"/>\n",
       "<text text-anchor=\"start\" x=\"396\" y=\"-3613\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 29&#45;&gt;30 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>29&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M390.26,-3691.86C387.84,-3681.56 384.84,-3668.81 382.14,-3657.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"385.47,-3656.23 379.78,-3647.29 378.66,-3657.83 385.47,-3656.23\"/>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>33</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"490,-3413 296,-3413 296,-3371 490,-3371 490,-3413\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"296,-3371 296,-3413 343,-3413 343,-3371 296,-3371\"/>\n",
       "<text text-anchor=\"start\" x=\"310\" y=\"-3395\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"301\" y=\"-3384\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-3392 343,-3413 391,-3413 391,-3392 343,-3392\"/>\n",
       "<text text-anchor=\"start\" x=\"353\" y=\"-3400\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-3392 391,-3413 490,-3413 490,-3392 391,-3392\"/>\n",
       "<text text-anchor=\"start\" x=\"396\" y=\"-3400\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-3371 343,-3392 391,-3392 391,-3371 343,-3371\"/>\n",
       "<text text-anchor=\"start\" x=\"348\" y=\"-3379\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-3371 391,-3392 490,-3392 490,-3371 391,-3371\"/>\n",
       "<text text-anchor=\"start\" x=\"405\" y=\"-3379\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 29&#45;&gt;33 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>29&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M461.07,-3691.77C468.48,-3687.46 475.37,-3682.26 481,-3676 526.29,-3625.64 531.75,-3592.09 513,-3527 501.67,-3487.66 491.36,-3479.45 464,-3449 454.29,-3438.19 442.32,-3427.88 431,-3419.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"433,-3416.25 422.91,-3413.03 428.79,-3421.84 433,-3416.25\"/>\n",
       "</g>\n",
       "<!-- 30&#45;&gt;31 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>30&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M368.57,-3604.63C366.07,-3596.73 363.17,-3587.53 360.44,-3578.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"363.72,-3577.64 357.37,-3569.16 357.04,-3579.75 363.72,-3577.64\"/>\n",
       "</g>\n",
       "<!-- 32 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>32</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"454.5,-3491 275.5,-3491 275.5,-3449 454.5,-3449 454.5,-3491\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"276,-3449 276,-3491 326,-3491 326,-3449 276,-3449\"/>\n",
       "<text text-anchor=\"start\" x=\"281\" y=\"-3473\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"282.5\" y=\"-3462\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"326,-3470 326,-3491 374,-3491 374,-3470 326,-3470\"/>\n",
       "<text text-anchor=\"start\" x=\"336\" y=\"-3478\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"374,-3470 374,-3491 455,-3491 455,-3470 374,-3470\"/>\n",
       "<text text-anchor=\"start\" x=\"379\" y=\"-3478\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"326,-3449 326,-3470 374,-3470 374,-3449 326,-3449\"/>\n",
       "<text text-anchor=\"start\" x=\"331\" y=\"-3457\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"374,-3449 374,-3470 455,-3470 455,-3449 374,-3449\"/>\n",
       "<text text-anchor=\"start\" x=\"379\" y=\"-3457\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;32 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>31&#45;&gt;32</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M354.75,-3526.63C356.19,-3518.82 357.87,-3509.73 359.44,-3501.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"362.92,-3501.63 361.29,-3491.16 356.03,-3500.36 362.92,-3501.63\"/>\n",
       "</g>\n",
       "<!-- 32&#45;&gt;33 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>32&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M372.5,-3448.63C375.45,-3440.65 378.88,-3431.33 382.09,-3422.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"385.4,-3423.75 385.57,-3413.16 378.83,-3421.33 385.4,-3423.75\"/>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>34</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"438,-3335 244,-3335 244,-3293 438,-3293 438,-3335\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"244,-3293 244,-3335 309,-3335 309,-3293 244,-3293\"/>\n",
       "<text text-anchor=\"start\" x=\"249\" y=\"-3317\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"258\" y=\"-3306\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"309,-3314 309,-3335 357,-3335 357,-3314 309,-3314\"/>\n",
       "<text text-anchor=\"start\" x=\"319\" y=\"-3322\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"357,-3314 357,-3335 438,-3335 438,-3314 357,-3314\"/>\n",
       "<text text-anchor=\"start\" x=\"362\" y=\"-3322\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"309,-3293 309,-3314 357,-3314 357,-3293 309,-3293\"/>\n",
       "<text text-anchor=\"start\" x=\"314\" y=\"-3301\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"357,-3293 357,-3314 438,-3314 438,-3293 357,-3293\"/>\n",
       "<text text-anchor=\"start\" x=\"362\" y=\"-3301\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 33&#45;&gt;34 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>33&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M379.06,-3370.63C373.37,-3362.3 366.67,-3352.52 360.5,-3343.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"363.33,-3341.44 354.79,-3335.16 357.55,-3345.39 363.33,-3341.44\"/>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node41\" class=\"node\">\n",
       "<title>40</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"490,-2867 296,-2867 296,-2825 490,-2825 490,-2867\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"296,-2825 296,-2867 343,-2867 343,-2825 296,-2825\"/>\n",
       "<text text-anchor=\"start\" x=\"310\" y=\"-2849\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"301\" y=\"-2838\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-2846 343,-2867 391,-2867 391,-2846 343,-2846\"/>\n",
       "<text text-anchor=\"start\" x=\"353\" y=\"-2854\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-2846 391,-2867 490,-2867 490,-2846 391,-2846\"/>\n",
       "<text text-anchor=\"start\" x=\"396\" y=\"-2854\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-2825 343,-2846 391,-2846 391,-2825 343,-2825\"/>\n",
       "<text text-anchor=\"start\" x=\"348\" y=\"-2833\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-2825 391,-2846 490,-2846 490,-2825 391,-2825\"/>\n",
       "<text text-anchor=\"start\" x=\"405\" y=\"-2833\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 33&#45;&gt;40 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>33&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M418.99,-3370.8C429.5,-3361.16 440.63,-3348.72 447,-3335 465.43,-3295.29 457,-3280.78 457,-3237 457,-3237 457,-3237 457,-3001 457,-2957.17 464.46,-2942.75 446,-2903 441.12,-2892.5 433.49,-2882.7 425.52,-2874.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"427.74,-2871.6 418.17,-2867.06 422.82,-2876.58 427.74,-2871.6\"/>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>35</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"433,-3257 251,-3257 251,-3215 433,-3215 433,-3257\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"251,-3215 251,-3257 298,-3257 298,-3215 251,-3215\"/>\n",
       "<text text-anchor=\"start\" x=\"259\" y=\"-3239\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"256\" y=\"-3228\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"298,-3236 298,-3257 346,-3257 346,-3236 298,-3236\"/>\n",
       "<text text-anchor=\"start\" x=\"308\" y=\"-3244\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"346,-3236 346,-3257 433,-3257 433,-3236 346,-3236\"/>\n",
       "<text text-anchor=\"start\" x=\"354\" y=\"-3244\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"298,-3215 298,-3236 346,-3236 346,-3215 298,-3215\"/>\n",
       "<text text-anchor=\"start\" x=\"303\" y=\"-3223\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"346,-3215 346,-3236 433,-3236 433,-3215 346,-3215\"/>\n",
       "<text text-anchor=\"start\" x=\"351\" y=\"-3223\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;35 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>34&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M341.27,-3292.63C341.37,-3284.82 341.49,-3275.73 341.6,-3267.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"345.1,-3267.2 341.73,-3257.16 338.1,-3267.11 345.1,-3267.2\"/>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>36</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"434,-3179 252,-3179 252,-3137 434,-3137 434,-3179\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"252,-3137 252,-3179 299,-3179 299,-3137 252,-3137\"/>\n",
       "<text text-anchor=\"start\" x=\"264.5\" y=\"-3161\" font-family=\"Linux libertine\" font-size=\"10.00\">gelu</text>\n",
       "<text text-anchor=\"start\" x=\"257\" y=\"-3150\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"299,-3158 299,-3179 347,-3179 347,-3158 299,-3158\"/>\n",
       "<text text-anchor=\"start\" x=\"309\" y=\"-3166\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"347,-3158 347,-3179 434,-3179 434,-3158 347,-3158\"/>\n",
       "<text text-anchor=\"start\" x=\"352\" y=\"-3166\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"299,-3137 299,-3158 347,-3158 347,-3137 299,-3137\"/>\n",
       "<text text-anchor=\"start\" x=\"304\" y=\"-3145\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"347,-3137 347,-3158 434,-3158 434,-3137 347,-3137\"/>\n",
       "<text text-anchor=\"start\" x=\"352\" y=\"-3145\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 35&#45;&gt;36 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>35&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M342.27,-3214.63C342.37,-3206.82 342.49,-3197.73 342.6,-3189.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"346.1,-3189.2 342.73,-3179.16 339.1,-3189.11 346.1,-3189.2\"/>\n",
       "</g>\n",
       "<!-- 37 -->\n",
       "<g id=\"node38\" class=\"node\">\n",
       "<title>37</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"437.5,-3101 252.5,-3101 252.5,-3059 437.5,-3059 437.5,-3101\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"253,-3059 253,-3101 303,-3101 303,-3059 253,-3059\"/>\n",
       "<text text-anchor=\"start\" x=\"258\" y=\"-3083\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"259.5\" y=\"-3072\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"303,-3080 303,-3101 351,-3101 351,-3080 303,-3080\"/>\n",
       "<text text-anchor=\"start\" x=\"313\" y=\"-3088\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"351,-3080 351,-3101 438,-3101 438,-3080 351,-3080\"/>\n",
       "<text text-anchor=\"start\" x=\"356\" y=\"-3088\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"303,-3059 303,-3080 351,-3080 351,-3059 303,-3059\"/>\n",
       "<text text-anchor=\"start\" x=\"308\" y=\"-3067\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"351,-3059 351,-3080 438,-3080 438,-3059 351,-3059\"/>\n",
       "<text text-anchor=\"start\" x=\"356\" y=\"-3067\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;37 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>36&#45;&gt;37</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M343.54,-3136.63C343.74,-3128.82 343.98,-3119.73 344.21,-3111.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"347.71,-3111.25 344.47,-3101.16 340.71,-3111.06 347.71,-3111.25\"/>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node39\" class=\"node\">\n",
       "<title>38</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"437,-3023 255,-3023 255,-2981 437,-2981 437,-3023\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"255,-2981 255,-3023 302,-3023 302,-2981 255,-2981\"/>\n",
       "<text text-anchor=\"start\" x=\"263\" y=\"-3005\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"260\" y=\"-2994\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-3002 302,-3023 350,-3023 350,-3002 302,-3002\"/>\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-3010\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-3002 350,-3023 437,-3023 437,-3002 350,-3002\"/>\n",
       "<text text-anchor=\"start\" x=\"355\" y=\"-3010\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-2981 302,-3002 350,-3002 350,-2981 302,-2981\"/>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-2989\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-2981 350,-3002 437,-3002 437,-2981 350,-2981\"/>\n",
       "<text text-anchor=\"start\" x=\"358\" y=\"-2989\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 37&#45;&gt;38 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>37&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M345.27,-3058.63C345.37,-3050.82 345.49,-3041.73 345.6,-3033.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.1,-3033.2 345.73,-3023.16 342.1,-3033.11 349.1,-3033.2\"/>\n",
       "</g>\n",
       "<!-- 39 -->\n",
       "<g id=\"node40\" class=\"node\">\n",
       "<title>39</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"436.5,-2945 257.5,-2945 257.5,-2903 436.5,-2903 436.5,-2945\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"258,-2903 258,-2945 308,-2945 308,-2903 258,-2903\"/>\n",
       "<text text-anchor=\"start\" x=\"263\" y=\"-2927\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"264.5\" y=\"-2916\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"308,-2924 308,-2945 356,-2945 356,-2924 308,-2924\"/>\n",
       "<text text-anchor=\"start\" x=\"318\" y=\"-2932\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"356,-2924 356,-2945 437,-2945 437,-2924 356,-2924\"/>\n",
       "<text text-anchor=\"start\" x=\"361\" y=\"-2932\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"308,-2903 308,-2924 356,-2924 356,-2903 308,-2903\"/>\n",
       "<text text-anchor=\"start\" x=\"313\" y=\"-2911\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"356,-2903 356,-2924 437,-2924 437,-2903 356,-2903\"/>\n",
       "<text text-anchor=\"start\" x=\"361\" y=\"-2911\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;39 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>38&#45;&gt;39</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346.27,-2980.63C346.37,-2972.82 346.49,-2963.73 346.6,-2955.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"350.1,-2955.2 346.73,-2945.16 343.1,-2955.11 350.1,-2955.2\"/>\n",
       "</g>\n",
       "<!-- 39&#45;&gt;40 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>39&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M359.33,-2902.63C364.32,-2894.39 370.17,-2884.72 375.58,-2875.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"378.61,-2877.53 380.8,-2867.16 372.63,-2873.9 378.61,-2877.53\"/>\n",
       "</g>\n",
       "<!-- 41 -->\n",
       "<g id=\"node42\" class=\"node\">\n",
       "<title>41</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"476,-2780 282,-2780 282,-2738 476,-2738 476,-2780\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"282,-2738 282,-2780 347,-2780 347,-2738 282,-2738\"/>\n",
       "<text text-anchor=\"start\" x=\"287\" y=\"-2762\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"296\" y=\"-2751\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"347,-2759 347,-2780 395,-2780 395,-2759 347,-2759\"/>\n",
       "<text text-anchor=\"start\" x=\"357\" y=\"-2767\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"395,-2759 395,-2780 476,-2780 476,-2759 395,-2759\"/>\n",
       "<text text-anchor=\"start\" x=\"400\" y=\"-2767\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"347,-2738 347,-2759 395,-2759 395,-2738 347,-2738\"/>\n",
       "<text text-anchor=\"start\" x=\"352\" y=\"-2746\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"395,-2738 395,-2759 476,-2759 476,-2738 395,-2738\"/>\n",
       "<text text-anchor=\"start\" x=\"400\" y=\"-2746\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;41 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>40&#45;&gt;41</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M389.68,-2824.86C387.99,-2814.56 385.89,-2801.81 384,-2790.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"387.42,-2789.59 382.34,-2780.29 380.51,-2790.73 387.42,-2789.59\"/>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node45\" class=\"node\">\n",
       "<title>44</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"490,-2546 296,-2546 296,-2504 490,-2504 490,-2546\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"296,-2504 296,-2546 343,-2546 343,-2504 296,-2504\"/>\n",
       "<text text-anchor=\"start\" x=\"310\" y=\"-2528\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"301\" y=\"-2517\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-2525 343,-2546 391,-2546 391,-2525 343,-2525\"/>\n",
       "<text text-anchor=\"start\" x=\"353\" y=\"-2533\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-2525 391,-2546 490,-2546 490,-2525 391,-2525\"/>\n",
       "<text text-anchor=\"start\" x=\"396\" y=\"-2533\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-2504 343,-2525 391,-2525 391,-2504 343,-2504\"/>\n",
       "<text text-anchor=\"start\" x=\"348\" y=\"-2512\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-2504 391,-2525 490,-2525 490,-2504 391,-2504\"/>\n",
       "<text text-anchor=\"start\" x=\"405\" y=\"-2512\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;44 -->\n",
       "<g id=\"edge50\" class=\"edge\">\n",
       "<title>40&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M465.22,-2824.77C472.6,-2820.48 479.42,-2815.28 485,-2809 529.78,-2758.65 531.65,-2724.75 513,-2660 501.67,-2620.66 491.36,-2612.45 464,-2582 454.29,-2571.19 442.32,-2560.88 431,-2552.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"433,-2549.25 422.91,-2546.03 428.79,-2554.84 433,-2549.25\"/>\n",
       "</g>\n",
       "<!-- 41&#45;&gt;42 -->\n",
       "<g id=\"edge52\" class=\"edge\">\n",
       "<title>41&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M371.5,-2737.63C368.55,-2729.65 365.12,-2720.33 361.91,-2711.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"365.17,-2710.33 358.43,-2702.16 358.6,-2712.75 365.17,-2710.33\"/>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node44\" class=\"node\">\n",
       "<title>43</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"454.5,-2624 275.5,-2624 275.5,-2582 454.5,-2582 454.5,-2624\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"276,-2582 276,-2624 326,-2624 326,-2582 276,-2582\"/>\n",
       "<text text-anchor=\"start\" x=\"281\" y=\"-2606\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"282.5\" y=\"-2595\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"326,-2603 326,-2624 374,-2624 374,-2603 326,-2603\"/>\n",
       "<text text-anchor=\"start\" x=\"336\" y=\"-2611\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"374,-2603 374,-2624 455,-2624 455,-2603 374,-2603\"/>\n",
       "<text text-anchor=\"start\" x=\"379\" y=\"-2611\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"326,-2582 326,-2603 374,-2603 374,-2582 326,-2582\"/>\n",
       "<text text-anchor=\"start\" x=\"331\" y=\"-2590\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"374,-2582 374,-2603 455,-2603 455,-2582 374,-2582\"/>\n",
       "<text text-anchor=\"start\" x=\"379\" y=\"-2590\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 42&#45;&gt;43 -->\n",
       "<g id=\"edge53\" class=\"edge\">\n",
       "<title>42&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M354.75,-2659.63C356.19,-2651.82 357.87,-2642.73 359.44,-2634.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"362.92,-2634.63 361.29,-2624.16 356.03,-2633.36 362.92,-2634.63\"/>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;44 -->\n",
       "<g id=\"edge54\" class=\"edge\">\n",
       "<title>43&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M372.5,-2581.63C375.45,-2573.65 378.88,-2564.33 382.09,-2555.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"385.4,-2556.75 385.57,-2546.16 378.83,-2554.33 385.4,-2556.75\"/>\n",
       "</g>\n",
       "<!-- 45 -->\n",
       "<g id=\"node46\" class=\"node\">\n",
       "<title>45</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"433,-2468 239,-2468 239,-2426 433,-2426 433,-2468\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"239,-2426 239,-2468 304,-2468 304,-2426 239,-2426\"/>\n",
       "<text text-anchor=\"start\" x=\"244\" y=\"-2450\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"253\" y=\"-2439\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"304,-2447 304,-2468 352,-2468 352,-2447 304,-2447\"/>\n",
       "<text text-anchor=\"start\" x=\"314\" y=\"-2455\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"352,-2447 352,-2468 433,-2468 433,-2447 352,-2447\"/>\n",
       "<text text-anchor=\"start\" x=\"357\" y=\"-2455\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"304,-2426 304,-2447 352,-2447 352,-2426 304,-2426\"/>\n",
       "<text text-anchor=\"start\" x=\"309\" y=\"-2434\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"352,-2426 352,-2447 433,-2447 433,-2426 352,-2426\"/>\n",
       "<text text-anchor=\"start\" x=\"357\" y=\"-2434\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;45 -->\n",
       "<g id=\"edge55\" class=\"edge\">\n",
       "<title>44&#45;&gt;45</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M377.73,-2503.63C371.41,-2495.22 363.99,-2485.32 357.17,-2476.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"359.92,-2474.06 351.12,-2468.16 354.32,-2478.26 359.92,-2474.06\"/>\n",
       "</g>\n",
       "<!-- 51 -->\n",
       "<g id=\"node52\" class=\"node\">\n",
       "<title>51</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"479,-2000 285,-2000 285,-1958 479,-1958 479,-2000\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"285,-1958 285,-2000 332,-2000 332,-1958 285,-1958\"/>\n",
       "<text text-anchor=\"start\" x=\"299\" y=\"-1982\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"290\" y=\"-1971\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"332,-1979 332,-2000 380,-2000 380,-1979 332,-1979\"/>\n",
       "<text text-anchor=\"start\" x=\"342\" y=\"-1987\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"380,-1979 380,-2000 479,-2000 479,-1979 380,-1979\"/>\n",
       "<text text-anchor=\"start\" x=\"385\" y=\"-1987\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"332,-1958 332,-1979 380,-1979 380,-1958 332,-1958\"/>\n",
       "<text text-anchor=\"start\" x=\"337\" y=\"-1966\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"380,-1958 380,-1979 479,-1979 479,-1958 380,-1958\"/>\n",
       "<text text-anchor=\"start\" x=\"394\" y=\"-1966\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;51 -->\n",
       "<g id=\"edge56\" class=\"edge\">\n",
       "<title>44&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M416.08,-2503.89C425.77,-2494.09 436.13,-2481.51 442,-2468 459.46,-2427.85 452,-2413.78 452,-2370 452,-2370 452,-2370 452,-2134 452,-2090.17 460.52,-2075.24 441,-2036 435.55,-2025.04 427.02,-2015.07 418.12,-2006.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"420.42,-2004.06 410.62,-2000.04 415.78,-2009.29 420.42,-2004.06\"/>\n",
       "</g>\n",
       "<!-- 46 -->\n",
       "<g id=\"node47\" class=\"node\">\n",
       "<title>46</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"428,-2390 246,-2390 246,-2348 428,-2348 428,-2390\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"246,-2348 246,-2390 293,-2390 293,-2348 246,-2348\"/>\n",
       "<text text-anchor=\"start\" x=\"254\" y=\"-2372\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"251\" y=\"-2361\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"293,-2369 293,-2390 341,-2390 341,-2369 293,-2369\"/>\n",
       "<text text-anchor=\"start\" x=\"303\" y=\"-2377\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"341,-2369 341,-2390 428,-2390 428,-2369 341,-2369\"/>\n",
       "<text text-anchor=\"start\" x=\"349\" y=\"-2377\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"293,-2348 293,-2369 341,-2369 341,-2348 293,-2348\"/>\n",
       "<text text-anchor=\"start\" x=\"298\" y=\"-2356\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"341,-2348 341,-2369 428,-2369 428,-2348 341,-2348\"/>\n",
       "<text text-anchor=\"start\" x=\"346\" y=\"-2356\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;46 -->\n",
       "<g id=\"edge57\" class=\"edge\">\n",
       "<title>45&#45;&gt;46</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.27,-2425.63C336.37,-2417.82 336.49,-2408.73 336.6,-2400.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"340.1,-2400.2 336.73,-2390.16 333.1,-2400.11 340.1,-2400.2\"/>\n",
       "</g>\n",
       "<!-- 47 -->\n",
       "<g id=\"node48\" class=\"node\">\n",
       "<title>47</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"429,-2312 247,-2312 247,-2270 429,-2270 429,-2312\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"247,-2270 247,-2312 294,-2312 294,-2270 247,-2270\"/>\n",
       "<text text-anchor=\"start\" x=\"259.5\" y=\"-2294\" font-family=\"Linux libertine\" font-size=\"10.00\">gelu</text>\n",
       "<text text-anchor=\"start\" x=\"252\" y=\"-2283\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"294,-2291 294,-2312 342,-2312 342,-2291 294,-2291\"/>\n",
       "<text text-anchor=\"start\" x=\"304\" y=\"-2299\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"342,-2291 342,-2312 429,-2312 429,-2291 342,-2291\"/>\n",
       "<text text-anchor=\"start\" x=\"347\" y=\"-2299\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"294,-2270 294,-2291 342,-2291 342,-2270 294,-2270\"/>\n",
       "<text text-anchor=\"start\" x=\"299\" y=\"-2278\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"342,-2270 342,-2291 429,-2291 429,-2270 342,-2270\"/>\n",
       "<text text-anchor=\"start\" x=\"347\" y=\"-2278\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 46&#45;&gt;47 -->\n",
       "<g id=\"edge58\" class=\"edge\">\n",
       "<title>46&#45;&gt;47</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M337.27,-2347.63C337.37,-2339.82 337.49,-2330.73 337.6,-2322.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"341.1,-2322.2 337.73,-2312.16 334.1,-2322.11 341.1,-2322.2\"/>\n",
       "</g>\n",
       "<!-- 48 -->\n",
       "<g id=\"node49\" class=\"node\">\n",
       "<title>48</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"432.5,-2234 247.5,-2234 247.5,-2192 432.5,-2192 432.5,-2234\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"248,-2192 248,-2234 298,-2234 298,-2192 248,-2192\"/>\n",
       "<text text-anchor=\"start\" x=\"253\" y=\"-2216\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"254.5\" y=\"-2205\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"298,-2213 298,-2234 346,-2234 346,-2213 298,-2213\"/>\n",
       "<text text-anchor=\"start\" x=\"308\" y=\"-2221\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"346,-2213 346,-2234 433,-2234 433,-2213 346,-2213\"/>\n",
       "<text text-anchor=\"start\" x=\"351\" y=\"-2221\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"298,-2192 298,-2213 346,-2213 346,-2192 298,-2192\"/>\n",
       "<text text-anchor=\"start\" x=\"303\" y=\"-2200\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"346,-2192 346,-2213 433,-2213 433,-2192 346,-2192\"/>\n",
       "<text text-anchor=\"start\" x=\"351\" y=\"-2200\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 47&#45;&gt;48 -->\n",
       "<g id=\"edge59\" class=\"edge\">\n",
       "<title>47&#45;&gt;48</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M338.54,-2269.63C338.74,-2261.82 338.98,-2252.73 339.21,-2244.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"342.71,-2244.25 339.47,-2234.16 335.71,-2244.06 342.71,-2244.25\"/>\n",
       "</g>\n",
       "<!-- 49 -->\n",
       "<g id=\"node50\" class=\"node\">\n",
       "<title>49</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"432,-2156 250,-2156 250,-2114 432,-2114 432,-2156\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"250,-2114 250,-2156 297,-2156 297,-2114 250,-2114\"/>\n",
       "<text text-anchor=\"start\" x=\"258\" y=\"-2138\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"255\" y=\"-2127\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"297,-2135 297,-2156 345,-2156 345,-2135 297,-2135\"/>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-2143\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"345,-2135 345,-2156 432,-2156 432,-2135 345,-2135\"/>\n",
       "<text text-anchor=\"start\" x=\"350\" y=\"-2143\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"297,-2114 297,-2135 345,-2135 345,-2114 297,-2114\"/>\n",
       "<text text-anchor=\"start\" x=\"302\" y=\"-2122\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"345,-2114 345,-2135 432,-2135 432,-2114 345,-2114\"/>\n",
       "<text text-anchor=\"start\" x=\"353\" y=\"-2122\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;49 -->\n",
       "<g id=\"edge60\" class=\"edge\">\n",
       "<title>48&#45;&gt;49</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M340.27,-2191.63C340.37,-2183.82 340.49,-2174.73 340.6,-2166.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"344.1,-2166.2 340.73,-2156.16 337.1,-2166.11 344.1,-2166.2\"/>\n",
       "</g>\n",
       "<!-- 50 -->\n",
       "<g id=\"node51\" class=\"node\">\n",
       "<title>50</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"431.5,-2078 252.5,-2078 252.5,-2036 431.5,-2036 431.5,-2078\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"253,-2036 253,-2078 303,-2078 303,-2036 253,-2036\"/>\n",
       "<text text-anchor=\"start\" x=\"258\" y=\"-2060\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"259.5\" y=\"-2049\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"303,-2057 303,-2078 351,-2078 351,-2057 303,-2057\"/>\n",
       "<text text-anchor=\"start\" x=\"313\" y=\"-2065\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"351,-2057 351,-2078 432,-2078 432,-2057 351,-2057\"/>\n",
       "<text text-anchor=\"start\" x=\"356\" y=\"-2065\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"303,-2036 303,-2057 351,-2057 351,-2036 303,-2036\"/>\n",
       "<text text-anchor=\"start\" x=\"308\" y=\"-2044\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"351,-2036 351,-2057 432,-2057 432,-2036 351,-2036\"/>\n",
       "<text text-anchor=\"start\" x=\"356\" y=\"-2044\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 49&#45;&gt;50 -->\n",
       "<g id=\"edge61\" class=\"edge\">\n",
       "<title>49&#45;&gt;50</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M341.27,-2113.63C341.37,-2105.82 341.49,-2096.73 341.6,-2088.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"345.1,-2088.2 341.73,-2078.16 338.1,-2088.11 345.1,-2088.2\"/>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;51 -->\n",
       "<g id=\"edge62\" class=\"edge\">\n",
       "<title>50&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M352.72,-2035.63C357.01,-2027.48 362.04,-2017.92 366.71,-2009.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"369.83,-2010.64 371.39,-2000.16 363.64,-2007.38 369.83,-2010.64\"/>\n",
       "</g>\n",
       "<!-- 52 -->\n",
       "<g id=\"node53\" class=\"node\">\n",
       "<title>52</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"469,-1913 275,-1913 275,-1871 469,-1871 469,-1913\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"275,-1871 275,-1913 340,-1913 340,-1871 275,-1871\"/>\n",
       "<text text-anchor=\"start\" x=\"280\" y=\"-1895\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"289\" y=\"-1884\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"340,-1892 340,-1913 388,-1913 388,-1892 340,-1892\"/>\n",
       "<text text-anchor=\"start\" x=\"350\" y=\"-1900\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"388,-1892 388,-1913 469,-1913 469,-1892 388,-1892\"/>\n",
       "<text text-anchor=\"start\" x=\"393\" y=\"-1900\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"340,-1871 340,-1892 388,-1892 388,-1871 340,-1871\"/>\n",
       "<text text-anchor=\"start\" x=\"345\" y=\"-1879\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"388,-1871 388,-1892 469,-1892 469,-1871 388,-1871\"/>\n",
       "<text text-anchor=\"start\" x=\"393\" y=\"-1879\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 51&#45;&gt;52 -->\n",
       "<g id=\"edge63\" class=\"edge\">\n",
       "<title>51&#45;&gt;52</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M379.63,-1957.86C378.42,-1947.56 376.92,-1934.81 375.57,-1923.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"379.03,-1922.82 374.39,-1913.29 372.08,-1923.63 379.03,-1922.82\"/>\n",
       "</g>\n",
       "<!-- 55 -->\n",
       "<g id=\"node56\" class=\"node\">\n",
       "<title>55</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"494,-1679 300,-1679 300,-1637 494,-1637 494,-1679\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"300,-1637 300,-1679 347,-1679 347,-1637 300,-1637\"/>\n",
       "<text text-anchor=\"start\" x=\"314\" y=\"-1661\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"305\" y=\"-1650\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"347,-1658 347,-1679 395,-1679 395,-1658 347,-1658\"/>\n",
       "<text text-anchor=\"start\" x=\"357\" y=\"-1666\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"395,-1658 395,-1679 494,-1679 494,-1658 395,-1658\"/>\n",
       "<text text-anchor=\"start\" x=\"400\" y=\"-1666\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"347,-1637 347,-1658 395,-1658 395,-1637 347,-1637\"/>\n",
       "<text text-anchor=\"start\" x=\"352\" y=\"-1645\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"395,-1637 395,-1658 494,-1658 494,-1637 395,-1637\"/>\n",
       "<text text-anchor=\"start\" x=\"409\" y=\"-1645\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 51&#45;&gt;55 -->\n",
       "<g id=\"edge64\" class=\"edge\">\n",
       "<title>51&#45;&gt;55</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M456.96,-1957.8C464.72,-1953.5 471.97,-1948.3 478,-1942 512.62,-1905.87 505.12,-1884.41 513,-1835 521.85,-1779.48 509.42,-1757.82 473,-1715 463.52,-1703.85 451.4,-1693.66 439.64,-1685.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"441.38,-1682.07 431.18,-1679.21 437.37,-1687.81 441.38,-1682.07\"/>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;53 -->\n",
       "<g id=\"edge66\" class=\"edge\">\n",
       "<title>52&#45;&gt;53</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M366.37,-1870.63C364.19,-1862.73 361.65,-1853.53 359.26,-1844.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"362.61,-1843.87 356.57,-1835.16 355.86,-1845.73 362.61,-1843.87\"/>\n",
       "</g>\n",
       "<!-- 54 -->\n",
       "<g id=\"node55\" class=\"node\">\n",
       "<title>54</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"463.5,-1757 284.5,-1757 284.5,-1715 463.5,-1715 463.5,-1757\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"285,-1715 285,-1757 335,-1757 335,-1715 285,-1715\"/>\n",
       "<text text-anchor=\"start\" x=\"290\" y=\"-1739\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"291.5\" y=\"-1728\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"335,-1736 335,-1757 383,-1757 383,-1736 335,-1736\"/>\n",
       "<text text-anchor=\"start\" x=\"345\" y=\"-1744\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"383,-1736 383,-1757 464,-1757 464,-1736 383,-1736\"/>\n",
       "<text text-anchor=\"start\" x=\"388\" y=\"-1744\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"335,-1715 335,-1736 383,-1736 383,-1715 335,-1715\"/>\n",
       "<text text-anchor=\"start\" x=\"340\" y=\"-1723\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"383,-1715 383,-1736 464,-1736 464,-1715 383,-1715\"/>\n",
       "<text text-anchor=\"start\" x=\"388\" y=\"-1723\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 53&#45;&gt;54 -->\n",
       "<g id=\"edge67\" class=\"edge\">\n",
       "<title>53&#45;&gt;54</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M357.16,-1792.63C359.55,-1784.73 362.34,-1775.53 364.95,-1766.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"368.35,-1767.74 367.9,-1757.16 361.65,-1765.72 368.35,-1767.74\"/>\n",
       "</g>\n",
       "<!-- 54&#45;&gt;55 -->\n",
       "<g id=\"edge68\" class=\"edge\">\n",
       "<title>54&#45;&gt;55</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M380.16,-1714.63C382.55,-1706.73 385.34,-1697.53 387.95,-1688.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"391.35,-1689.74 390.9,-1679.16 384.65,-1687.72 391.35,-1689.74\"/>\n",
       "</g>\n",
       "<!-- 56 -->\n",
       "<g id=\"node57\" class=\"node\">\n",
       "<title>56</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"438,-1601 244,-1601 244,-1559 438,-1559 438,-1601\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"244,-1559 244,-1601 309,-1601 309,-1559 244,-1559\"/>\n",
       "<text text-anchor=\"start\" x=\"249\" y=\"-1583\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"258\" y=\"-1572\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"309,-1580 309,-1601 357,-1601 357,-1580 309,-1580\"/>\n",
       "<text text-anchor=\"start\" x=\"319\" y=\"-1588\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"357,-1580 357,-1601 438,-1601 438,-1580 357,-1580\"/>\n",
       "<text text-anchor=\"start\" x=\"362\" y=\"-1588\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"309,-1559 309,-1580 357,-1580 357,-1559 309,-1559\"/>\n",
       "<text text-anchor=\"start\" x=\"314\" y=\"-1567\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"357,-1559 357,-1580 438,-1580 438,-1559 357,-1559\"/>\n",
       "<text text-anchor=\"start\" x=\"362\" y=\"-1567\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 55&#45;&gt;56 -->\n",
       "<g id=\"edge69\" class=\"edge\">\n",
       "<title>55&#45;&gt;56</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M381.99,-1636.63C375.79,-1628.22 368.5,-1618.32 361.8,-1609.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"364.6,-1607.13 355.85,-1601.16 358.97,-1611.29 364.6,-1607.13\"/>\n",
       "</g>\n",
       "<!-- 62 -->\n",
       "<g id=\"node63\" class=\"node\">\n",
       "<title>62</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"490,-1133 296,-1133 296,-1091 490,-1091 490,-1133\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"296,-1091 296,-1133 343,-1133 343,-1091 296,-1091\"/>\n",
       "<text text-anchor=\"start\" x=\"310\" y=\"-1115\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"301\" y=\"-1104\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-1112 343,-1133 391,-1133 391,-1112 343,-1112\"/>\n",
       "<text text-anchor=\"start\" x=\"353\" y=\"-1120\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-1112 391,-1133 490,-1133 490,-1112 391,-1112\"/>\n",
       "<text text-anchor=\"start\" x=\"396\" y=\"-1120\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-1091 343,-1112 391,-1112 391,-1091 343,-1091\"/>\n",
       "<text text-anchor=\"start\" x=\"348\" y=\"-1099\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-1091 391,-1112 490,-1112 490,-1091 391,-1091\"/>\n",
       "<text text-anchor=\"start\" x=\"405\" y=\"-1099\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 55&#45;&gt;62 -->\n",
       "<g id=\"edge70\" class=\"edge\">\n",
       "<title>55&#45;&gt;62</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M420.55,-1636.97C430.43,-1627.19 441.01,-1614.6 447,-1601 464.66,-1560.94 457,-1546.78 457,-1503 457,-1503 457,-1503 457,-1267 457,-1223.17 464.46,-1208.75 446,-1169 441.12,-1158.5 433.49,-1148.7 425.52,-1140.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"427.74,-1137.6 418.17,-1133.06 422.82,-1142.58 427.74,-1137.6\"/>\n",
       "</g>\n",
       "<!-- 57 -->\n",
       "<g id=\"node58\" class=\"node\">\n",
       "<title>57</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"433,-1523 251,-1523 251,-1481 433,-1481 433,-1523\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"251,-1481 251,-1523 298,-1523 298,-1481 251,-1481\"/>\n",
       "<text text-anchor=\"start\" x=\"259\" y=\"-1505\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"256\" y=\"-1494\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"298,-1502 298,-1523 346,-1523 346,-1502 298,-1502\"/>\n",
       "<text text-anchor=\"start\" x=\"308\" y=\"-1510\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"346,-1502 346,-1523 433,-1523 433,-1502 346,-1502\"/>\n",
       "<text text-anchor=\"start\" x=\"354\" y=\"-1510\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"298,-1481 298,-1502 346,-1502 346,-1481 298,-1481\"/>\n",
       "<text text-anchor=\"start\" x=\"303\" y=\"-1489\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"346,-1481 346,-1502 433,-1502 433,-1481 346,-1481\"/>\n",
       "<text text-anchor=\"start\" x=\"351\" y=\"-1489\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 56&#45;&gt;57 -->\n",
       "<g id=\"edge71\" class=\"edge\">\n",
       "<title>56&#45;&gt;57</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M341.27,-1558.63C341.37,-1550.82 341.49,-1541.73 341.6,-1533.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"345.1,-1533.2 341.73,-1523.16 338.1,-1533.11 345.1,-1533.2\"/>\n",
       "</g>\n",
       "<!-- 58 -->\n",
       "<g id=\"node59\" class=\"node\">\n",
       "<title>58</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"435,-1445 253,-1445 253,-1403 435,-1403 435,-1445\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"253,-1403 253,-1445 300,-1445 300,-1403 253,-1403\"/>\n",
       "<text text-anchor=\"start\" x=\"265.5\" y=\"-1427\" font-family=\"Linux libertine\" font-size=\"10.00\">gelu</text>\n",
       "<text text-anchor=\"start\" x=\"258\" y=\"-1416\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"300,-1424 300,-1445 348,-1445 348,-1424 300,-1424\"/>\n",
       "<text text-anchor=\"start\" x=\"310\" y=\"-1432\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"348,-1424 348,-1445 435,-1445 435,-1424 348,-1424\"/>\n",
       "<text text-anchor=\"start\" x=\"353\" y=\"-1432\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"300,-1403 300,-1424 348,-1424 348,-1403 300,-1403\"/>\n",
       "<text text-anchor=\"start\" x=\"305\" y=\"-1411\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"348,-1403 348,-1424 435,-1424 435,-1403 348,-1403\"/>\n",
       "<text text-anchor=\"start\" x=\"353\" y=\"-1411\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 57&#45;&gt;58 -->\n",
       "<g id=\"edge72\" class=\"edge\">\n",
       "<title>57&#45;&gt;58</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M342.54,-1480.63C342.74,-1472.82 342.98,-1463.73 343.21,-1455.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"346.71,-1455.25 343.47,-1445.16 339.71,-1455.06 346.71,-1455.25\"/>\n",
       "</g>\n",
       "<!-- 59 -->\n",
       "<g id=\"node60\" class=\"node\">\n",
       "<title>59</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"437.5,-1367 252.5,-1367 252.5,-1325 437.5,-1325 437.5,-1367\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"253,-1325 253,-1367 303,-1367 303,-1325 253,-1325\"/>\n",
       "<text text-anchor=\"start\" x=\"258\" y=\"-1349\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"259.5\" y=\"-1338\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"303,-1346 303,-1367 351,-1367 351,-1346 303,-1346\"/>\n",
       "<text text-anchor=\"start\" x=\"313\" y=\"-1354\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"351,-1346 351,-1367 438,-1367 438,-1346 351,-1346\"/>\n",
       "<text text-anchor=\"start\" x=\"356\" y=\"-1354\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"303,-1325 303,-1346 351,-1346 351,-1325 303,-1325\"/>\n",
       "<text text-anchor=\"start\" x=\"308\" y=\"-1333\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"351,-1325 351,-1346 438,-1346 438,-1325 351,-1325\"/>\n",
       "<text text-anchor=\"start\" x=\"356\" y=\"-1333\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 58&#45;&gt;59 -->\n",
       "<g id=\"edge73\" class=\"edge\">\n",
       "<title>58&#45;&gt;59</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M344.27,-1402.63C344.37,-1394.82 344.49,-1385.73 344.6,-1377.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"348.1,-1377.2 344.73,-1367.16 341.1,-1377.11 348.1,-1377.2\"/>\n",
       "</g>\n",
       "<!-- 60 -->\n",
       "<g id=\"node61\" class=\"node\">\n",
       "<title>60</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"437,-1289 255,-1289 255,-1247 437,-1247 437,-1289\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"255,-1247 255,-1289 302,-1289 302,-1247 255,-1247\"/>\n",
       "<text text-anchor=\"start\" x=\"263\" y=\"-1271\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"260\" y=\"-1260\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-1268 302,-1289 350,-1289 350,-1268 302,-1268\"/>\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-1276\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-1268 350,-1289 437,-1289 437,-1268 350,-1268\"/>\n",
       "<text text-anchor=\"start\" x=\"355\" y=\"-1276\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-1247 302,-1268 350,-1268 350,-1247 302,-1247\"/>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-1255\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-1247 350,-1268 437,-1268 437,-1247 350,-1247\"/>\n",
       "<text text-anchor=\"start\" x=\"358\" y=\"-1255\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 59&#45;&gt;60 -->\n",
       "<g id=\"edge74\" class=\"edge\">\n",
       "<title>59&#45;&gt;60</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M345.27,-1324.63C345.37,-1316.82 345.49,-1307.73 345.6,-1299.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.1,-1299.2 345.73,-1289.16 342.1,-1299.11 349.1,-1299.2\"/>\n",
       "</g>\n",
       "<!-- 61 -->\n",
       "<g id=\"node62\" class=\"node\">\n",
       "<title>61</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"436.5,-1211 257.5,-1211 257.5,-1169 436.5,-1169 436.5,-1211\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"258,-1169 258,-1211 308,-1211 308,-1169 258,-1169\"/>\n",
       "<text text-anchor=\"start\" x=\"263\" y=\"-1193\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"264.5\" y=\"-1182\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"308,-1190 308,-1211 356,-1211 356,-1190 308,-1190\"/>\n",
       "<text text-anchor=\"start\" x=\"318\" y=\"-1198\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"356,-1190 356,-1211 437,-1211 437,-1190 356,-1190\"/>\n",
       "<text text-anchor=\"start\" x=\"361\" y=\"-1198\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"308,-1169 308,-1190 356,-1190 356,-1169 308,-1169\"/>\n",
       "<text text-anchor=\"start\" x=\"313\" y=\"-1177\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"356,-1169 356,-1190 437,-1190 437,-1169 356,-1169\"/>\n",
       "<text text-anchor=\"start\" x=\"361\" y=\"-1177\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 60&#45;&gt;61 -->\n",
       "<g id=\"edge75\" class=\"edge\">\n",
       "<title>60&#45;&gt;61</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346.27,-1246.63C346.37,-1238.82 346.49,-1229.73 346.6,-1221.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"350.1,-1221.2 346.73,-1211.16 343.1,-1221.11 350.1,-1221.2\"/>\n",
       "</g>\n",
       "<!-- 61&#45;&gt;62 -->\n",
       "<g id=\"edge76\" class=\"edge\">\n",
       "<title>61&#45;&gt;62</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M359.33,-1168.63C364.32,-1160.39 370.17,-1150.72 375.58,-1141.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"378.61,-1143.53 380.8,-1133.16 372.63,-1139.9 378.61,-1143.53\"/>\n",
       "</g>\n",
       "<!-- 63 -->\n",
       "<g id=\"node64\" class=\"node\">\n",
       "<title>63</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"462,-1046 268,-1046 268,-1004 462,-1004 462,-1046\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"268,-1004 268,-1046 333,-1046 333,-1004 268,-1004\"/>\n",
       "<text text-anchor=\"start\" x=\"273\" y=\"-1028\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"282\" y=\"-1017\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"333,-1025 333,-1046 381,-1046 381,-1025 333,-1025\"/>\n",
       "<text text-anchor=\"start\" x=\"343\" y=\"-1033\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-1025 381,-1046 462,-1046 462,-1025 381,-1025\"/>\n",
       "<text text-anchor=\"start\" x=\"386\" y=\"-1033\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"333,-1004 333,-1025 381,-1025 381,-1004 333,-1004\"/>\n",
       "<text text-anchor=\"start\" x=\"338\" y=\"-1012\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-1004 381,-1025 462,-1025 462,-1004 381,-1004\"/>\n",
       "<text text-anchor=\"start\" x=\"386\" y=\"-1012\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 62&#45;&gt;63 -->\n",
       "<g id=\"edge77\" class=\"edge\">\n",
       "<title>62&#45;&gt;63</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M386.36,-1090.86C382.94,-1080.45 378.69,-1067.55 374.88,-1056\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"378.14,-1054.7 371.69,-1046.29 371.49,-1056.89 378.14,-1054.7\"/>\n",
       "</g>\n",
       "<!-- 66 -->\n",
       "<g id=\"node67\" class=\"node\">\n",
       "<title>66</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"490,-812 296,-812 296,-770 490,-770 490,-812\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"296,-770 296,-812 343,-812 343,-770 296,-770\"/>\n",
       "<text text-anchor=\"start\" x=\"310\" y=\"-794\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"301\" y=\"-783\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-791 343,-812 391,-812 391,-791 343,-791\"/>\n",
       "<text text-anchor=\"start\" x=\"353\" y=\"-799\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-791 391,-812 490,-812 490,-791 391,-791\"/>\n",
       "<text text-anchor=\"start\" x=\"396\" y=\"-799\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-770 343,-791 391,-791 391,-770 343,-770\"/>\n",
       "<text text-anchor=\"start\" x=\"348\" y=\"-778\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-770 391,-791 490,-791 490,-770 391,-770\"/>\n",
       "<text text-anchor=\"start\" x=\"405\" y=\"-778\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 62&#45;&gt;66 -->\n",
       "<g id=\"edge78\" class=\"edge\">\n",
       "<title>62&#45;&gt;66</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M450.15,-1091C457.81,-1086.58 465.06,-1081.29 471,-1075 518.21,-1024.95 532.04,-992.11 513,-926 501.67,-886.66 491.36,-878.45 464,-848 454.29,-837.19 442.32,-826.88 431,-818.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"433,-815.25 422.91,-812.03 428.79,-820.84 433,-815.25\"/>\n",
       "</g>\n",
       "<!-- 63&#45;&gt;64 -->\n",
       "<g id=\"edge80\" class=\"edge\">\n",
       "<title>63&#45;&gt;64</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M361.25,-1003.63C359.81,-995.82 358.13,-986.73 356.56,-978.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"359.97,-977.36 354.71,-968.16 353.08,-978.63 359.97,-977.36\"/>\n",
       "</g>\n",
       "<!-- 65 -->\n",
       "<g id=\"node66\" class=\"node\">\n",
       "<title>65</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"454.5,-890 275.5,-890 275.5,-848 454.5,-848 454.5,-890\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"276,-848 276,-890 326,-890 326,-848 276,-848\"/>\n",
       "<text text-anchor=\"start\" x=\"281\" y=\"-872\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"282.5\" y=\"-861\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"326,-869 326,-890 374,-890 374,-869 326,-869\"/>\n",
       "<text text-anchor=\"start\" x=\"336\" y=\"-877\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"374,-869 374,-890 455,-890 455,-869 374,-869\"/>\n",
       "<text text-anchor=\"start\" x=\"379\" y=\"-877\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"326,-848 326,-869 374,-869 374,-848 326,-848\"/>\n",
       "<text text-anchor=\"start\" x=\"331\" y=\"-856\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"374,-848 374,-869 455,-869 455,-848 374,-848\"/>\n",
       "<text text-anchor=\"start\" x=\"379\" y=\"-856\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 64&#45;&gt;65 -->\n",
       "<g id=\"edge81\" class=\"edge\">\n",
       "<title>64&#45;&gt;65</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M354.75,-925.63C356.19,-917.82 357.87,-908.73 359.44,-900.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"362.92,-900.63 361.29,-890.16 356.03,-899.36 362.92,-900.63\"/>\n",
       "</g>\n",
       "<!-- 65&#45;&gt;66 -->\n",
       "<g id=\"edge82\" class=\"edge\">\n",
       "<title>65&#45;&gt;66</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M372.5,-847.63C375.45,-839.65 378.88,-830.33 382.09,-821.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"385.4,-822.75 385.57,-812.16 378.83,-820.33 385.4,-822.75\"/>\n",
       "</g>\n",
       "<!-- 67 -->\n",
       "<g id=\"node68\" class=\"node\">\n",
       "<title>67</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"438,-734 244,-734 244,-692 438,-692 438,-734\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"244,-692 244,-734 309,-734 309,-692 244,-692\"/>\n",
       "<text text-anchor=\"start\" x=\"249\" y=\"-716\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"258\" y=\"-705\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"309,-713 309,-734 357,-734 357,-713 309,-713\"/>\n",
       "<text text-anchor=\"start\" x=\"319\" y=\"-721\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"357,-713 357,-734 438,-734 438,-713 357,-713\"/>\n",
       "<text text-anchor=\"start\" x=\"362\" y=\"-721\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"309,-692 309,-713 357,-713 357,-692 309,-692\"/>\n",
       "<text text-anchor=\"start\" x=\"314\" y=\"-700\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"357,-692 357,-713 438,-713 438,-692 357,-692\"/>\n",
       "<text text-anchor=\"start\" x=\"362\" y=\"-700\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 66&#45;&gt;67 -->\n",
       "<g id=\"edge83\" class=\"edge\">\n",
       "<title>66&#45;&gt;67</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M379.06,-769.63C373.37,-761.3 366.67,-751.52 360.5,-742.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"363.33,-740.44 354.79,-734.16 357.55,-744.39 363.33,-740.44\"/>\n",
       "</g>\n",
       "<!-- 73 -->\n",
       "<g id=\"node74\" class=\"node\">\n",
       "<title>73</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"473,-266 279,-266 279,-224 473,-224 473,-266\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"279,-224 279,-266 326,-266 326,-224 279,-224\"/>\n",
       "<text text-anchor=\"start\" x=\"293\" y=\"-248\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"284\" y=\"-237\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"326,-245 326,-266 374,-266 374,-245 326,-245\"/>\n",
       "<text text-anchor=\"start\" x=\"336\" y=\"-253\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"374,-245 374,-266 473,-266 473,-245 374,-245\"/>\n",
       "<text text-anchor=\"start\" x=\"379\" y=\"-253\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"326,-224 326,-245 374,-245 374,-224 326,-224\"/>\n",
       "<text text-anchor=\"start\" x=\"331\" y=\"-232\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"374,-224 374,-245 473,-245 473,-224 374,-224\"/>\n",
       "<text text-anchor=\"start\" x=\"388\" y=\"-232\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 66&#45;&gt;73 -->\n",
       "<g id=\"edge84\" class=\"edge\">\n",
       "<title>66&#45;&gt;73</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M418.99,-769.8C429.5,-760.16 440.63,-747.72 447,-734 465.43,-694.29 457,-679.78 457,-636 457,-636 457,-636 457,-400 457,-356.17 467.16,-340.38 446,-302 439.66,-290.49 429.88,-280.49 419.63,-272.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"421.46,-269.23 411.36,-266.01 417.24,-274.82 421.46,-269.23\"/>\n",
       "</g>\n",
       "<!-- 68 -->\n",
       "<g id=\"node69\" class=\"node\">\n",
       "<title>68</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"433,-656 251,-656 251,-614 433,-614 433,-656\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"251,-614 251,-656 298,-656 298,-614 251,-614\"/>\n",
       "<text text-anchor=\"start\" x=\"259\" y=\"-638\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"256\" y=\"-627\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"298,-635 298,-656 346,-656 346,-635 298,-635\"/>\n",
       "<text text-anchor=\"start\" x=\"308\" y=\"-643\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"346,-635 346,-656 433,-656 433,-635 346,-635\"/>\n",
       "<text text-anchor=\"start\" x=\"354\" y=\"-643\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"298,-614 298,-635 346,-635 346,-614 298,-614\"/>\n",
       "<text text-anchor=\"start\" x=\"303\" y=\"-622\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"346,-614 346,-635 433,-635 433,-614 346,-614\"/>\n",
       "<text text-anchor=\"start\" x=\"351\" y=\"-622\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 67&#45;&gt;68 -->\n",
       "<g id=\"edge85\" class=\"edge\">\n",
       "<title>67&#45;&gt;68</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M341.27,-691.63C341.37,-683.82 341.49,-674.73 341.6,-666.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"345.1,-666.2 341.73,-656.16 338.1,-666.11 345.1,-666.2\"/>\n",
       "</g>\n",
       "<!-- 69 -->\n",
       "<g id=\"node70\" class=\"node\">\n",
       "<title>69</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"434,-578 252,-578 252,-536 434,-536 434,-578\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"252,-536 252,-578 299,-578 299,-536 252,-536\"/>\n",
       "<text text-anchor=\"start\" x=\"264.5\" y=\"-560\" font-family=\"Linux libertine\" font-size=\"10.00\">gelu</text>\n",
       "<text text-anchor=\"start\" x=\"257\" y=\"-549\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"299,-557 299,-578 347,-578 347,-557 299,-557\"/>\n",
       "<text text-anchor=\"start\" x=\"309\" y=\"-565\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"347,-557 347,-578 434,-578 434,-557 347,-557\"/>\n",
       "<text text-anchor=\"start\" x=\"352\" y=\"-565\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"299,-536 299,-557 347,-557 347,-536 299,-536\"/>\n",
       "<text text-anchor=\"start\" x=\"304\" y=\"-544\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"347,-536 347,-557 434,-557 434,-536 347,-536\"/>\n",
       "<text text-anchor=\"start\" x=\"352\" y=\"-544\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 68&#45;&gt;69 -->\n",
       "<g id=\"edge86\" class=\"edge\">\n",
       "<title>68&#45;&gt;69</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M342.27,-613.63C342.37,-605.82 342.49,-596.73 342.6,-588.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"346.1,-588.2 342.73,-578.16 339.1,-588.11 346.1,-588.2\"/>\n",
       "</g>\n",
       "<!-- 70 -->\n",
       "<g id=\"node71\" class=\"node\">\n",
       "<title>70</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"437.5,-500 252.5,-500 252.5,-458 437.5,-458 437.5,-500\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"253,-458 253,-500 303,-500 303,-458 253,-458\"/>\n",
       "<text text-anchor=\"start\" x=\"258\" y=\"-482\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"259.5\" y=\"-471\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"303,-479 303,-500 351,-500 351,-479 303,-479\"/>\n",
       "<text text-anchor=\"start\" x=\"313\" y=\"-487\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"351,-479 351,-500 438,-500 438,-479 351,-479\"/>\n",
       "<text text-anchor=\"start\" x=\"356\" y=\"-487\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"303,-458 303,-479 351,-479 351,-458 303,-458\"/>\n",
       "<text text-anchor=\"start\" x=\"308\" y=\"-466\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"351,-458 351,-479 438,-479 438,-458 351,-458\"/>\n",
       "<text text-anchor=\"start\" x=\"356\" y=\"-466\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "</g>\n",
       "<!-- 69&#45;&gt;70 -->\n",
       "<g id=\"edge87\" class=\"edge\">\n",
       "<title>69&#45;&gt;70</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M343.54,-535.63C343.74,-527.82 343.98,-518.73 344.21,-510.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"347.71,-510.25 344.47,-500.16 340.71,-510.06 347.71,-510.25\"/>\n",
       "</g>\n",
       "<!-- 71 -->\n",
       "<g id=\"node72\" class=\"node\">\n",
       "<title>71</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"437,-422 255,-422 255,-380 437,-380 437,-422\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"255,-380 255,-422 302,-422 302,-380 255,-380\"/>\n",
       "<text text-anchor=\"start\" x=\"263\" y=\"-404\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"260\" y=\"-393\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-401 302,-422 350,-422 350,-401 302,-401\"/>\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-409\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-401 350,-422 437,-422 437,-401 350,-401\"/>\n",
       "<text text-anchor=\"start\" x=\"355\" y=\"-409\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 1024) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-380 302,-401 350,-401 350,-380 302,-380\"/>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-388\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-380 350,-401 437,-401 437,-380 350,-380\"/>\n",
       "<text text-anchor=\"start\" x=\"358\" y=\"-388\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 70&#45;&gt;71 -->\n",
       "<g id=\"edge88\" class=\"edge\">\n",
       "<title>70&#45;&gt;71</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M345.27,-457.63C345.37,-449.82 345.49,-440.73 345.6,-432.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.1,-432.2 345.73,-422.16 342.1,-432.11 349.1,-432.2\"/>\n",
       "</g>\n",
       "<!-- 72 -->\n",
       "<g id=\"node73\" class=\"node\">\n",
       "<title>72</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"436.5,-344 257.5,-344 257.5,-302 436.5,-302 436.5,-344\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"258,-302 258,-344 308,-344 308,-302 258,-302\"/>\n",
       "<text text-anchor=\"start\" x=\"263\" y=\"-326\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"264.5\" y=\"-315\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"308,-323 308,-344 356,-344 356,-323 308,-323\"/>\n",
       "<text text-anchor=\"start\" x=\"318\" y=\"-331\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"356,-323 356,-344 437,-344 437,-323 356,-323\"/>\n",
       "<text text-anchor=\"start\" x=\"361\" y=\"-331\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"308,-302 308,-323 356,-323 356,-302 308,-302\"/>\n",
       "<text text-anchor=\"start\" x=\"313\" y=\"-310\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"356,-302 356,-323 437,-323 437,-302 356,-302\"/>\n",
       "<text text-anchor=\"start\" x=\"361\" y=\"-310\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 71&#45;&gt;72 -->\n",
       "<g id=\"edge89\" class=\"edge\">\n",
       "<title>71&#45;&gt;72</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346.27,-379.63C346.37,-371.82 346.49,-362.73 346.6,-354.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"350.1,-354.2 346.73,-344.16 343.1,-354.11 350.1,-354.2\"/>\n",
       "</g>\n",
       "<!-- 72&#45;&gt;73 -->\n",
       "<g id=\"edge90\" class=\"edge\">\n",
       "<title>72&#45;&gt;73</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M354.77,-301.63C357.82,-293.65 361.38,-284.33 364.7,-275.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"368.01,-276.75 368.31,-266.16 361.47,-274.25 368.01,-276.75\"/>\n",
       "</g>\n",
       "<!-- 74 -->\n",
       "<g id=\"node75\" class=\"node\">\n",
       "<title>74</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"473,-188 279,-188 279,-146 473,-146 473,-188\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"279,-146 279,-188 344,-188 344,-146 279,-146\"/>\n",
       "<text text-anchor=\"start\" x=\"284\" y=\"-170\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"293\" y=\"-159\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"344,-167 344,-188 392,-188 392,-167 344,-167\"/>\n",
       "<text text-anchor=\"start\" x=\"354\" y=\"-175\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"392,-167 392,-188 473,-188 473,-167 392,-167\"/>\n",
       "<text text-anchor=\"start\" x=\"397\" y=\"-175\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"344,-146 344,-167 392,-167 392,-146 344,-146\"/>\n",
       "<text text-anchor=\"start\" x=\"349\" y=\"-154\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"392,-146 392,-167 473,-167 473,-146 392,-146\"/>\n",
       "<text text-anchor=\"start\" x=\"397\" y=\"-154\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "</g>\n",
       "<!-- 73&#45;&gt;74 -->\n",
       "<g id=\"edge91\" class=\"edge\">\n",
       "<title>73&#45;&gt;74</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M376,-223.63C376,-215.82 376,-206.73 376,-198.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"379.5,-198.16 376,-188.16 372.5,-198.16 379.5,-198.16\"/>\n",
       "</g>\n",
       "<!-- 75 -->\n",
       "<g id=\"node76\" class=\"node\">\n",
       "<title>75</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"470,-110 282,-110 282,-68 470,-68 470,-110\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"282,-68 282,-110 329,-110 329,-68 282,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"290\" y=\"-92\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"287\" y=\"-81\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"329,-89 329,-110 377,-110 377,-89 329,-89\"/>\n",
       "<text text-anchor=\"start\" x=\"339\" y=\"-97\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"377,-89 377,-110 470,-110 470,-89 377,-89\"/>\n",
       "<text text-anchor=\"start\" x=\"388\" y=\"-97\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"329,-68 329,-89 377,-89 377,-68 329,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"334\" y=\"-76\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"377,-68 377,-89 470,-89 470,-68 377,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"382\" y=\"-76\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 32000) </text>\n",
       "</g>\n",
       "<!-- 74&#45;&gt;75 -->\n",
       "<g id=\"edge92\" class=\"edge\">\n",
       "<title>74&#45;&gt;75</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M376,-145.63C376,-137.82 376,-128.73 376,-120.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"379.5,-120.16 376,-110.16 372.5,-120.16 379.5,-120.16\"/>\n",
       "</g>\n",
       "<!-- 76 -->\n",
       "<g id=\"node77\" class=\"node\">\n",
       "<title>76</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"459.5,-32 292.5,-32 292.5,0 459.5,0 459.5,-32\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"293,0 293,-32 370,-32 370,0 293,0\"/>\n",
       "<text text-anchor=\"start\" x=\"298\" y=\"-19\" font-family=\"Linux libertine\" font-size=\"10.00\">output&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"313\" y=\"-8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"370,0 370,-32 460,-32 460,0 370,0\"/>\n",
       "<text text-anchor=\"start\" x=\"375\" y=\"-13.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 256, 32000)</text>\n",
       "</g>\n",
       "<!-- 75&#45;&gt;76 -->\n",
       "<g id=\"edge93\" class=\"edge\">\n",
       "<title>75&#45;&gt;76</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M376,-67.84C376,-59.89 376,-50.66 376,-42.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"379.5,-42.24 376,-32.24 372.5,-42.24 379.5,-42.24\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x79f030b52f30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install torchview for compact visualization\n",
    "try:\n",
    "    import torchview\n",
    "except ImportError:\n",
    "    %pip install torchview\n",
    "    import torchview\n",
    "\n",
    "from torchview import draw_graph\n",
    "\n",
    "# 1. Print Model Statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model Structure:\\n{model}\\n\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "# Estimate model size in MB (assuming float32 - 4 bytes per param)\n",
    "param_size = total_params * 4 / (1024 * 1024)\n",
    "print(f\"Estimated Model Size (Params only): {param_size:.2f} MB\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2. Generate Diagram using torchview\n",
    "print(\"Generating compact model diagram...\")\n",
    "\n",
    "# Create dummy input with correct type (Long) for the embedding layer\n",
    "dummy_input = torch.randint(0, tokenizer_vocab, (BATCH_SIZE, BLOCK_SIZE), dtype=torch.long).to(device)\n",
    "\n",
    "# draw_graph creates a high-level graph of modules\n",
    "# depth=3 is usually good for Transformers to see layers but not internal ops\n",
    "model_graph = draw_graph(\n",
    "    model, \n",
    "    input_data=dummy_input, # Pass actual data to ensure correct type\n",
    "    device=device,\n",
    "    expand_nested=True,\n",
    "    depth=3,\n",
    "    save_graph=False\n",
    ")\n",
    "\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c59e8",
   "metadata": {},
   "source": [
    "Setup training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbc87319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler setup\n",
    "\n",
    "def build_scheduler(\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler_name: str,\n",
    "    total_steps: int,\n",
    "    warmup_ratio: float,\n",
    ") -> Optional[LRScheduler]:\n",
    "    if scheduler_name == \"none\" or total_steps <= 0:\n",
    "        return None\n",
    "\n",
    "    if not 0.0 <= warmup_ratio <= 1.0:\n",
    "        raise ValueError(\"warmup_ratio must be between 0 and 1\")\n",
    "\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    warmup_steps = max(0, min(warmup_steps, total_steps))\n",
    "\n",
    "    if scheduler_name == \"linear\":\n",
    "        return get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "    if scheduler_name == \"cosine\":\n",
    "        return get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "\n",
    "    raise ValueError(f\"Unsupported lr scheduler: {scheduler_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "facf1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scaler = torch.amp.GradScaler(device=\"cuda\", enabled=True)\n",
    "\n",
    "total_train_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "# Warmup + decay scheduling is a robust default for compact language models.\n",
    "scheduler = build_scheduler(\n",
    "    optimizer=optimizer,\n",
    "    scheduler_name=LR_SCHEDULER,\n",
    "    total_steps=total_train_steps,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bdcb61",
   "metadata": {},
   "source": [
    "Training helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc1d9149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: MiniTransformerLanguageModel,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    scaler: Optional[torch.cuda.amp.GradScaler],\n",
    "    grad_clip: float,\n",
    "    scheduler: Optional[LRScheduler],\n",
    "    log_interval: Optional[int] = None,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    for step, (inputs, targets) in enumerate(loader, start=1):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=scaler is not None):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits.view(-1, model.vocab_size), targets.reshape(-1))\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        loss_value = loss.detach().float().item()\n",
    "        step_tokens = targets.numel()\n",
    "        total_loss += loss_value * step_tokens\n",
    "        total_tokens += step_tokens\n",
    "\n",
    "        if log_interval and step % log_interval == 0:\n",
    "            running_avg = total_loss / max(total_tokens, 1)\n",
    "            running_ppl = math.exp(min(running_avg, 20.0))\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(\n",
    "                f\"  Step {step}/{len(loader)} - Loss: {running_avg:.4f} | PPL: {running_ppl:.2f} | LR: {current_lr:.6e}\"\n",
    "            )\n",
    "\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    perplexity = math.exp(min(avg_loss, 20.0))\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a33225",
   "metadata": {},
   "source": [
    "Perplexity based evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28963c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: MiniTransformerLanguageModel,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits.view(-1, model.vocab_size), targets.reshape(-1))\n",
    "        loss_value = loss.detach().float().item()\n",
    "        total_loss += loss_value * targets.numel()\n",
    "        total_tokens += targets.numel()\n",
    "    if total_tokens == 0:\n",
    "        return float(\"inf\"), float(\"inf\")\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(min(avg_loss, 20.0))\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed478ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 100/4407 - Loss: 10.3843 | PPL: 32348.29 | LR: 6.807352e-07\n",
      "  Step 200/4407 - Loss: 10.3621 | PPL: 31637.92 | LR: 1.361470e-06\n",
      "  Step 200/4407 - Loss: 10.3621 | PPL: 31637.92 | LR: 1.361470e-06\n",
      "  Step 300/4407 - Loss: 10.3274 | PPL: 30559.07 | LR: 2.042206e-06\n",
      "  Step 300/4407 - Loss: 10.3274 | PPL: 30559.07 | LR: 2.042206e-06\n",
      "  Step 400/4407 - Loss: 10.2870 | PPL: 29349.14 | LR: 2.722941e-06\n",
      "  Step 400/4407 - Loss: 10.2870 | PPL: 29349.14 | LR: 2.722941e-06\n",
      "  Step 500/4407 - Loss: 10.2437 | PPL: 28104.69 | LR: 3.403676e-06\n",
      "  Step 500/4407 - Loss: 10.2437 | PPL: 28104.69 | LR: 3.403676e-06\n",
      "  Step 600/4407 - Loss: 10.1986 | PPL: 26865.58 | LR: 4.084411e-06\n",
      "  Step 600/4407 - Loss: 10.1986 | PPL: 26865.58 | LR: 4.084411e-06\n",
      "  Step 700/4407 - Loss: 10.1512 | PPL: 25620.86 | LR: 4.765146e-06\n",
      "  Step 700/4407 - Loss: 10.1512 | PPL: 25620.86 | LR: 4.765146e-06\n",
      "  Step 800/4407 - Loss: 10.1014 | PPL: 24376.58 | LR: 5.445882e-06\n",
      "  Step 800/4407 - Loss: 10.1014 | PPL: 24376.58 | LR: 5.445882e-06\n",
      "  Step 900/4407 - Loss: 10.0488 | PPL: 23128.13 | LR: 6.126617e-06\n",
      "  Step 900/4407 - Loss: 10.0488 | PPL: 23128.13 | LR: 6.126617e-06\n",
      "  Step 1000/4407 - Loss: 9.9927 | PPL: 21867.29 | LR: 6.807352e-06\n",
      "  Step 1000/4407 - Loss: 9.9927 | PPL: 21867.29 | LR: 6.807352e-06\n",
      "  Step 1100/4407 - Loss: 9.9340 | PPL: 20620.12 | LR: 7.488087e-06\n",
      "  Step 1100/4407 - Loss: 9.9340 | PPL: 20620.12 | LR: 7.488087e-06\n",
      "  Step 1200/4407 - Loss: 9.8715 | PPL: 19370.88 | LR: 8.168822e-06\n",
      "  Step 1200/4407 - Loss: 9.8715 | PPL: 19370.88 | LR: 8.168822e-06\n",
      "  Step 1300/4407 - Loss: 9.8063 | PPL: 18148.45 | LR: 8.849558e-06\n",
      "  Step 1300/4407 - Loss: 9.8063 | PPL: 18148.45 | LR: 8.849558e-06\n",
      "  Step 1400/4407 - Loss: 9.7388 | PPL: 16962.91 | LR: 9.530293e-06\n",
      "  Step 1400/4407 - Loss: 9.7388 | PPL: 16962.91 | LR: 9.530293e-06\n",
      "  Step 1500/4407 - Loss: 9.6703 | PPL: 15840.22 | LR: 1.021103e-05\n",
      "  Step 1500/4407 - Loss: 9.6703 | PPL: 15840.22 | LR: 1.021103e-05\n",
      "  Step 1600/4407 - Loss: 9.5998 | PPL: 14761.88 | LR: 1.089176e-05\n",
      "  Step 1600/4407 - Loss: 9.5998 | PPL: 14761.88 | LR: 1.089176e-05\n",
      "  Step 1700/4407 - Loss: 9.5291 | PPL: 13753.67 | LR: 1.157250e-05\n",
      "  Step 1700/4407 - Loss: 9.5291 | PPL: 13753.67 | LR: 1.157250e-05\n",
      "  Step 1800/4407 - Loss: 9.4578 | PPL: 12807.86 | LR: 1.225323e-05\n",
      "  Step 1800/4407 - Loss: 9.4578 | PPL: 12807.86 | LR: 1.225323e-05\n",
      "  Step 1900/4407 - Loss: 9.3873 | PPL: 11935.25 | LR: 1.293397e-05\n",
      "  Step 1900/4407 - Loss: 9.3873 | PPL: 11935.25 | LR: 1.293397e-05\n",
      "  Step 2000/4407 - Loss: 9.3172 | PPL: 11128.14 | LR: 1.361470e-05\n",
      "  Step 2000/4407 - Loss: 9.3172 | PPL: 11128.14 | LR: 1.361470e-05\n",
      "  Step 2100/4407 - Loss: 9.2483 | PPL: 10386.38 | LR: 1.429544e-05\n",
      "  Step 2100/4407 - Loss: 9.2483 | PPL: 10386.38 | LR: 1.429544e-05\n",
      "  Step 2200/4407 - Loss: 9.1806 | PPL: 9706.93 | LR: 1.497617e-05\n",
      "  Step 2200/4407 - Loss: 9.1806 | PPL: 9706.93 | LR: 1.497617e-05\n",
      "  Step 2300/4407 - Loss: 9.1151 | PPL: 9091.53 | LR: 1.565691e-05\n",
      "  Step 2300/4407 - Loss: 9.1151 | PPL: 9091.53 | LR: 1.565691e-05\n",
      "  Step 2400/4407 - Loss: 9.0502 | PPL: 8520.34 | LR: 1.633764e-05\n",
      "  Step 2400/4407 - Loss: 9.0502 | PPL: 8520.34 | LR: 1.633764e-05\n",
      "  Step 2500/4407 - Loss: 8.9864 | PPL: 7993.37 | LR: 1.701838e-05\n",
      "  Step 2500/4407 - Loss: 8.9864 | PPL: 7993.37 | LR: 1.701838e-05\n",
      "  Step 2600/4407 - Loss: 8.9230 | PPL: 7502.82 | LR: 1.769912e-05\n",
      "  Step 2600/4407 - Loss: 8.9230 | PPL: 7502.82 | LR: 1.769912e-05\n",
      "  Step 2700/4407 - Loss: 8.8600 | PPL: 7044.39 | LR: 1.837985e-05\n",
      "  Step 2700/4407 - Loss: 8.8600 | PPL: 7044.39 | LR: 1.837985e-05\n",
      "  Step 2800/4407 - Loss: 8.7983 | PPL: 6623.01 | LR: 1.906059e-05\n",
      "  Step 2800/4407 - Loss: 8.7983 | PPL: 6623.01 | LR: 1.906059e-05\n",
      "  Step 2900/4407 - Loss: 8.7372 | PPL: 6230.57 | LR: 1.974132e-05\n",
      "  Step 2900/4407 - Loss: 8.7372 | PPL: 6230.57 | LR: 1.974132e-05\n",
      "  Step 3000/4407 - Loss: 8.6772 | PPL: 5867.41 | LR: 2.042206e-05\n",
      "  Step 3000/4407 - Loss: 8.6772 | PPL: 5867.41 | LR: 2.042206e-05\n",
      "  Step 3100/4407 - Loss: 8.6194 | PPL: 5537.85 | LR: 2.110279e-05\n",
      "  Step 3100/4407 - Loss: 8.6194 | PPL: 5537.85 | LR: 2.110279e-05\n",
      "  Step 3200/4407 - Loss: 8.5624 | PPL: 5231.27 | LR: 2.178353e-05\n",
      "  Step 3200/4407 - Loss: 8.5624 | PPL: 5231.27 | LR: 2.178353e-05\n",
      "  Step 3300/4407 - Loss: 8.5075 | PPL: 4951.92 | LR: 2.246426e-05\n",
      "  Step 3300/4407 - Loss: 8.5075 | PPL: 4951.92 | LR: 2.246426e-05\n",
      "  Step 3400/4407 - Loss: 8.4528 | PPL: 4688.33 | LR: 2.314500e-05\n",
      "  Step 3400/4407 - Loss: 8.4528 | PPL: 4688.33 | LR: 2.314500e-05\n",
      "  Step 3500/4407 - Loss: 8.3998 | PPL: 4446.38 | LR: 2.382573e-05\n",
      "  Step 3500/4407 - Loss: 8.3998 | PPL: 4446.38 | LR: 2.382573e-05\n",
      "  Step 3600/4407 - Loss: 8.3482 | PPL: 4222.70 | LR: 2.450647e-05\n",
      "  Step 3600/4407 - Loss: 8.3482 | PPL: 4222.70 | LR: 2.450647e-05\n",
      "  Step 3700/4407 - Loss: 8.2976 | PPL: 4014.18 | LR: 2.518720e-05\n",
      "  Step 3700/4407 - Loss: 8.2976 | PPL: 4014.18 | LR: 2.518720e-05\n",
      "  Step 3800/4407 - Loss: 8.2484 | PPL: 3821.68 | LR: 2.586794e-05\n",
      "  Step 3800/4407 - Loss: 8.2484 | PPL: 3821.68 | LR: 2.586794e-05\n",
      "  Step 3900/4407 - Loss: 8.1999 | PPL: 3640.67 | LR: 2.654867e-05\n",
      "  Step 3900/4407 - Loss: 8.1999 | PPL: 3640.67 | LR: 2.654867e-05\n",
      "  Step 4000/4407 - Loss: 8.1535 | PPL: 3475.56 | LR: 2.722941e-05\n",
      "  Step 4000/4407 - Loss: 8.1535 | PPL: 3475.56 | LR: 2.722941e-05\n",
      "  Step 4100/4407 - Loss: 8.1064 | PPL: 3315.68 | LR: 2.791014e-05\n",
      "  Step 4100/4407 - Loss: 8.1064 | PPL: 3315.68 | LR: 2.791014e-05\n",
      "  Step 4200/4407 - Loss: 8.0606 | PPL: 3167.15 | LR: 2.859088e-05\n",
      "  Step 4200/4407 - Loss: 8.0606 | PPL: 3167.15 | LR: 2.859088e-05\n",
      "  Step 4300/4407 - Loss: 8.0161 | PPL: 3029.21 | LR: 2.927161e-05\n",
      "  Step 4300/4407 - Loss: 8.0161 | PPL: 3029.21 | LR: 2.927161e-05\n",
      "  Step 4400/4407 - Loss: 7.9723 | PPL: 2899.61 | LR: 2.995235e-05\n",
      "  Step 4400/4407 - Loss: 7.9723 | PPL: 2899.61 | LR: 2.995235e-05\n",
      "Epoch 1/100 | Train Loss: 7.9696 | Train PPL: 2891.71 | Val Loss: 6.0571 | Val PPL: 427.13\n",
      "Epoch 1/100 | Train Loss: 7.9696 | Train PPL: 2891.71 | Val Loss: 6.0571 | Val PPL: 427.13\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 2891.71\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 2891.71\n",
      "  Step 100/4407 - Loss: 6.0332 | PPL: 417.06 | LR: 3.068074e-05\n",
      "  Step 100/4407 - Loss: 6.0332 | PPL: 417.06 | LR: 3.068074e-05\n",
      "  Step 200/4407 - Loss: 6.0121 | PPL: 408.35 | LR: 3.136147e-05\n",
      "  Step 200/4407 - Loss: 6.0121 | PPL: 408.35 | LR: 3.136147e-05\n",
      "  Step 300/4407 - Loss: 5.9852 | PPL: 397.48 | LR: 3.204221e-05\n",
      "  Step 300/4407 - Loss: 5.9852 | PPL: 397.48 | LR: 3.204221e-05\n",
      "  Step 400/4407 - Loss: 5.9616 | PPL: 388.23 | LR: 3.272294e-05\n",
      "  Step 400/4407 - Loss: 5.9616 | PPL: 388.23 | LR: 3.272294e-05\n",
      "  Step 500/4407 - Loss: 5.9402 | PPL: 380.03 | LR: 3.340368e-05\n",
      "  Step 500/4407 - Loss: 5.9402 | PPL: 380.03 | LR: 3.340368e-05\n",
      "  Step 600/4407 - Loss: 5.9117 | PPL: 369.34 | LR: 3.408441e-05\n",
      "  Step 600/4407 - Loss: 5.9117 | PPL: 369.34 | LR: 3.408441e-05\n",
      "  Step 700/4407 - Loss: 5.8879 | PPL: 360.65 | LR: 3.476515e-05\n",
      "  Step 700/4407 - Loss: 5.8879 | PPL: 360.65 | LR: 3.476515e-05\n",
      "  Step 800/4407 - Loss: 5.8628 | PPL: 351.71 | LR: 3.544588e-05\n",
      "  Step 800/4407 - Loss: 5.8628 | PPL: 351.71 | LR: 3.544588e-05\n",
      "  Step 900/4407 - Loss: 5.8417 | PPL: 344.35 | LR: 3.612662e-05\n",
      "  Step 900/4407 - Loss: 5.8417 | PPL: 344.35 | LR: 3.612662e-05\n",
      "  Step 1000/4407 - Loss: 5.8182 | PPL: 336.35 | LR: 3.680735e-05\n",
      "  Step 1000/4407 - Loss: 5.8182 | PPL: 336.35 | LR: 3.680735e-05\n",
      "  Step 1100/4407 - Loss: 5.7951 | PPL: 328.67 | LR: 3.748809e-05\n",
      "  Step 1100/4407 - Loss: 5.7951 | PPL: 328.67 | LR: 3.748809e-05\n",
      "  Step 1200/4407 - Loss: 5.7724 | PPL: 321.30 | LR: 3.816882e-05\n",
      "  Step 1200/4407 - Loss: 5.7724 | PPL: 321.30 | LR: 3.816882e-05\n",
      "  Step 1300/4407 - Loss: 5.7485 | PPL: 313.73 | LR: 3.884956e-05\n",
      "  Step 1300/4407 - Loss: 5.7485 | PPL: 313.73 | LR: 3.884956e-05\n",
      "  Step 1400/4407 - Loss: 5.7236 | PPL: 306.00 | LR: 3.953029e-05\n",
      "  Step 1400/4407 - Loss: 5.7236 | PPL: 306.00 | LR: 3.953029e-05\n",
      "  Step 1500/4407 - Loss: 5.6998 | PPL: 298.81 | LR: 4.021103e-05\n",
      "  Step 1500/4407 - Loss: 5.6998 | PPL: 298.81 | LR: 4.021103e-05\n",
      "  Step 1600/4407 - Loss: 5.6766 | PPL: 291.96 | LR: 4.089176e-05\n",
      "  Step 1600/4407 - Loss: 5.6766 | PPL: 291.96 | LR: 4.089176e-05\n",
      "  Step 1700/4407 - Loss: 5.6556 | PPL: 285.89 | LR: 4.157250e-05\n",
      "  Step 1700/4407 - Loss: 5.6556 | PPL: 285.89 | LR: 4.157250e-05\n",
      "  Step 1800/4407 - Loss: 5.6347 | PPL: 279.96 | LR: 4.225323e-05\n",
      "  Step 1800/4407 - Loss: 5.6347 | PPL: 279.96 | LR: 4.225323e-05\n",
      "  Step 1900/4407 - Loss: 5.6125 | PPL: 273.84 | LR: 4.293397e-05\n",
      "  Step 1900/4407 - Loss: 5.6125 | PPL: 273.84 | LR: 4.293397e-05\n",
      "  Step 2000/4407 - Loss: 5.5905 | PPL: 267.87 | LR: 4.361470e-05\n",
      "  Step 2000/4407 - Loss: 5.5905 | PPL: 267.87 | LR: 4.361470e-05\n",
      "  Step 2100/4407 - Loss: 5.5700 | PPL: 262.44 | LR: 4.429544e-05\n",
      "  Step 2100/4407 - Loss: 5.5700 | PPL: 262.44 | LR: 4.429544e-05\n",
      "  Step 2200/4407 - Loss: 5.5469 | PPL: 256.44 | LR: 4.497617e-05\n",
      "  Step 2200/4407 - Loss: 5.5469 | PPL: 256.44 | LR: 4.497617e-05\n",
      "  Step 2300/4407 - Loss: 5.5263 | PPL: 251.20 | LR: 4.565691e-05\n",
      "  Step 2300/4407 - Loss: 5.5263 | PPL: 251.20 | LR: 4.565691e-05\n",
      "  Step 2400/4407 - Loss: 5.5061 | PPL: 246.19 | LR: 4.633764e-05\n",
      "  Step 2400/4407 - Loss: 5.5061 | PPL: 246.19 | LR: 4.633764e-05\n",
      "  Step 2500/4407 - Loss: 5.4846 | PPL: 240.96 | LR: 4.701838e-05\n",
      "  Step 2500/4407 - Loss: 5.4846 | PPL: 240.96 | LR: 4.701838e-05\n",
      "  Step 2600/4407 - Loss: 5.4642 | PPL: 236.09 | LR: 4.769912e-05\n",
      "  Step 2600/4407 - Loss: 5.4642 | PPL: 236.09 | LR: 4.769912e-05\n",
      "  Step 2700/4407 - Loss: 5.4457 | PPL: 231.75 | LR: 4.837985e-05\n",
      "  Step 2700/4407 - Loss: 5.4457 | PPL: 231.75 | LR: 4.837985e-05\n",
      "  Step 2800/4407 - Loss: 5.4269 | PPL: 227.45 | LR: 4.906059e-05\n",
      "  Step 2800/4407 - Loss: 5.4269 | PPL: 227.45 | LR: 4.906059e-05\n",
      "  Step 2900/4407 - Loss: 5.4078 | PPL: 223.14 | LR: 4.974132e-05\n",
      "  Step 2900/4407 - Loss: 5.4078 | PPL: 223.14 | LR: 4.974132e-05\n",
      "  Step 3000/4407 - Loss: 5.3901 | PPL: 219.23 | LR: 5.042206e-05\n",
      "  Step 3000/4407 - Loss: 5.3901 | PPL: 219.23 | LR: 5.042206e-05\n",
      "  Step 3100/4407 - Loss: 5.3736 | PPL: 215.63 | LR: 5.110279e-05\n",
      "  Step 3100/4407 - Loss: 5.3736 | PPL: 215.63 | LR: 5.110279e-05\n",
      "  Step 3200/4407 - Loss: 5.3548 | PPL: 211.62 | LR: 5.178353e-05\n",
      "  Step 3200/4407 - Loss: 5.3548 | PPL: 211.62 | LR: 5.178353e-05\n",
      "  Step 3300/4407 - Loss: 5.3374 | PPL: 207.97 | LR: 5.246426e-05\n",
      "  Step 3300/4407 - Loss: 5.3374 | PPL: 207.97 | LR: 5.246426e-05\n",
      "  Step 3400/4407 - Loss: 5.3193 | PPL: 204.23 | LR: 5.314500e-05\n",
      "  Step 3400/4407 - Loss: 5.3193 | PPL: 204.23 | LR: 5.314500e-05\n",
      "  Step 3500/4407 - Loss: 5.3009 | PPL: 200.52 | LR: 5.382573e-05\n",
      "  Step 3500/4407 - Loss: 5.3009 | PPL: 200.52 | LR: 5.382573e-05\n",
      "  Step 3600/4407 - Loss: 5.2833 | PPL: 197.02 | LR: 5.450647e-05\n",
      "  Step 3600/4407 - Loss: 5.2833 | PPL: 197.02 | LR: 5.450647e-05\n",
      "  Step 3700/4407 - Loss: 5.2661 | PPL: 193.65 | LR: 5.518720e-05\n",
      "  Step 3700/4407 - Loss: 5.2661 | PPL: 193.65 | LR: 5.518720e-05\n",
      "  Step 3800/4407 - Loss: 5.2496 | PPL: 190.48 | LR: 5.586794e-05\n",
      "  Step 3800/4407 - Loss: 5.2496 | PPL: 190.48 | LR: 5.586794e-05\n",
      "  Step 3900/4407 - Loss: 5.2329 | PPL: 187.33 | LR: 5.654867e-05\n",
      "  Step 3900/4407 - Loss: 5.2329 | PPL: 187.33 | LR: 5.654867e-05\n",
      "  Step 4000/4407 - Loss: 5.2171 | PPL: 184.40 | LR: 5.722941e-05\n",
      "  Step 4000/4407 - Loss: 5.2171 | PPL: 184.40 | LR: 5.722941e-05\n",
      "  Step 4100/4407 - Loss: 5.2007 | PPL: 181.40 | LR: 5.791014e-05\n",
      "  Step 4100/4407 - Loss: 5.2007 | PPL: 181.40 | LR: 5.791014e-05\n",
      "  Step 4200/4407 - Loss: 5.1831 | PPL: 178.24 | LR: 5.859088e-05\n",
      "  Step 4200/4407 - Loss: 5.1831 | PPL: 178.24 | LR: 5.859088e-05\n",
      "  Step 4300/4407 - Loss: 5.1684 | PPL: 175.63 | LR: 5.927161e-05\n",
      "  Step 4300/4407 - Loss: 5.1684 | PPL: 175.63 | LR: 5.927161e-05\n",
      "  Step 4400/4407 - Loss: 5.1533 | PPL: 173.00 | LR: 5.995235e-05\n",
      "  Step 4400/4407 - Loss: 5.1533 | PPL: 173.00 | LR: 5.995235e-05\n",
      "Epoch 2/100 | Train Loss: 5.1524 | Train PPL: 172.84 | Val Loss: 4.5085 | Val PPL: 90.78\n",
      "Epoch 2/100 | Train Loss: 5.1524 | Train PPL: 172.84 | Val Loss: 4.5085 | Val PPL: 90.78\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 172.84\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 172.84\n",
      "  Step 100/4407 - Loss: 4.3934 | PPL: 80.92 | LR: 6.068074e-05\n",
      "  Step 100/4407 - Loss: 4.3934 | PPL: 80.92 | LR: 6.068074e-05\n",
      "  Step 200/4407 - Loss: 4.4148 | PPL: 82.67 | LR: 6.136147e-05\n",
      "  Step 200/4407 - Loss: 4.4148 | PPL: 82.67 | LR: 6.136147e-05\n",
      "  Step 300/4407 - Loss: 4.3977 | PPL: 81.27 | LR: 6.204221e-05\n",
      "  Step 300/4407 - Loss: 4.3977 | PPL: 81.27 | LR: 6.204221e-05\n",
      "  Step 400/4407 - Loss: 4.3840 | PPL: 80.16 | LR: 6.272294e-05\n",
      "  Step 400/4407 - Loss: 4.3840 | PPL: 80.16 | LR: 6.272294e-05\n",
      "  Step 500/4407 - Loss: 4.3841 | PPL: 80.17 | LR: 6.340368e-05\n",
      "  Step 500/4407 - Loss: 4.3841 | PPL: 80.17 | LR: 6.340368e-05\n",
      "  Step 600/4407 - Loss: 4.3819 | PPL: 79.99 | LR: 6.408441e-05\n",
      "  Step 600/4407 - Loss: 4.3819 | PPL: 79.99 | LR: 6.408441e-05\n",
      "  Step 700/4407 - Loss: 4.3639 | PPL: 78.56 | LR: 6.476515e-05\n",
      "  Step 700/4407 - Loss: 4.3639 | PPL: 78.56 | LR: 6.476515e-05\n",
      "  Step 800/4407 - Loss: 4.3505 | PPL: 77.51 | LR: 6.544588e-05\n",
      "  Step 800/4407 - Loss: 4.3505 | PPL: 77.51 | LR: 6.544588e-05\n",
      "  Step 900/4407 - Loss: 4.3391 | PPL: 76.64 | LR: 6.612662e-05\n",
      "  Step 900/4407 - Loss: 4.3391 | PPL: 76.64 | LR: 6.612662e-05\n",
      "  Step 1000/4407 - Loss: 4.3303 | PPL: 75.97 | LR: 6.680735e-05\n",
      "  Step 1000/4407 - Loss: 4.3303 | PPL: 75.97 | LR: 6.680735e-05\n",
      "  Step 1100/4407 - Loss: 4.3173 | PPL: 74.99 | LR: 6.748809e-05\n",
      "  Step 1100/4407 - Loss: 4.3173 | PPL: 74.99 | LR: 6.748809e-05\n",
      "  Step 1200/4407 - Loss: 4.3049 | PPL: 74.06 | LR: 6.816882e-05\n",
      "  Step 1200/4407 - Loss: 4.3049 | PPL: 74.06 | LR: 6.816882e-05\n",
      "  Step 1300/4407 - Loss: 4.2969 | PPL: 73.47 | LR: 6.884956e-05\n",
      "  Step 1300/4407 - Loss: 4.2969 | PPL: 73.47 | LR: 6.884956e-05\n",
      "  Step 1400/4407 - Loss: 4.2863 | PPL: 72.70 | LR: 6.953029e-05\n",
      "  Step 1400/4407 - Loss: 4.2863 | PPL: 72.70 | LR: 6.953029e-05\n",
      "  Step 1500/4407 - Loss: 4.2787 | PPL: 72.14 | LR: 7.021103e-05\n",
      "  Step 1500/4407 - Loss: 4.2787 | PPL: 72.14 | LR: 7.021103e-05\n",
      "  Step 1600/4407 - Loss: 4.2737 | PPL: 71.78 | LR: 7.089176e-05\n",
      "  Step 1600/4407 - Loss: 4.2737 | PPL: 71.78 | LR: 7.089176e-05\n",
      "  Step 1700/4407 - Loss: 4.2669 | PPL: 71.30 | LR: 7.157250e-05\n",
      "  Step 1700/4407 - Loss: 4.2669 | PPL: 71.30 | LR: 7.157250e-05\n",
      "  Step 1800/4407 - Loss: 4.2570 | PPL: 70.60 | LR: 7.225323e-05\n",
      "  Step 1800/4407 - Loss: 4.2570 | PPL: 70.60 | LR: 7.225323e-05\n",
      "  Step 1900/4407 - Loss: 4.2499 | PPL: 70.10 | LR: 7.293397e-05\n",
      "  Step 1900/4407 - Loss: 4.2499 | PPL: 70.10 | LR: 7.293397e-05\n",
      "  Step 2000/4407 - Loss: 4.2429 | PPL: 69.61 | LR: 7.361470e-05\n",
      "  Step 2000/4407 - Loss: 4.2429 | PPL: 69.61 | LR: 7.361470e-05\n",
      "  Step 2100/4407 - Loss: 4.2316 | PPL: 68.82 | LR: 7.429544e-05\n",
      "  Step 2100/4407 - Loss: 4.2316 | PPL: 68.82 | LR: 7.429544e-05\n",
      "  Step 2200/4407 - Loss: 4.2246 | PPL: 68.35 | LR: 7.497617e-05\n",
      "  Step 2200/4407 - Loss: 4.2246 | PPL: 68.35 | LR: 7.497617e-05\n",
      "  Step 2300/4407 - Loss: 4.2163 | PPL: 67.78 | LR: 7.565691e-05\n",
      "  Step 2300/4407 - Loss: 4.2163 | PPL: 67.78 | LR: 7.565691e-05\n",
      "  Step 2400/4407 - Loss: 4.2104 | PPL: 67.38 | LR: 7.633764e-05\n",
      "  Step 2400/4407 - Loss: 4.2104 | PPL: 67.38 | LR: 7.633764e-05\n",
      "  Step 2500/4407 - Loss: 4.2026 | PPL: 66.86 | LR: 7.701838e-05\n",
      "  Step 2500/4407 - Loss: 4.2026 | PPL: 66.86 | LR: 7.701838e-05\n",
      "  Step 2600/4407 - Loss: 4.1947 | PPL: 66.33 | LR: 7.769912e-05\n",
      "  Step 2600/4407 - Loss: 4.1947 | PPL: 66.33 | LR: 7.769912e-05\n",
      "  Step 2700/4407 - Loss: 4.1872 | PPL: 65.84 | LR: 7.837985e-05\n",
      "  Step 2700/4407 - Loss: 4.1872 | PPL: 65.84 | LR: 7.837985e-05\n",
      "  Step 2800/4407 - Loss: 4.1786 | PPL: 65.28 | LR: 7.906059e-05\n",
      "  Step 2800/4407 - Loss: 4.1786 | PPL: 65.28 | LR: 7.906059e-05\n",
      "  Step 2900/4407 - Loss: 4.1686 | PPL: 64.62 | LR: 7.974132e-05\n",
      "  Step 2900/4407 - Loss: 4.1686 | PPL: 64.62 | LR: 7.974132e-05\n",
      "  Step 3000/4407 - Loss: 4.1599 | PPL: 64.07 | LR: 8.042206e-05\n",
      "  Step 3000/4407 - Loss: 4.1599 | PPL: 64.07 | LR: 8.042206e-05\n",
      "  Step 3100/4407 - Loss: 4.1538 | PPL: 63.67 | LR: 8.110279e-05\n",
      "  Step 3100/4407 - Loss: 4.1538 | PPL: 63.67 | LR: 8.110279e-05\n",
      "  Step 3200/4407 - Loss: 4.1461 | PPL: 63.19 | LR: 8.178353e-05\n",
      "  Step 3200/4407 - Loss: 4.1461 | PPL: 63.19 | LR: 8.178353e-05\n",
      "  Step 3300/4407 - Loss: 4.1387 | PPL: 62.72 | LR: 8.246426e-05\n",
      "  Step 3300/4407 - Loss: 4.1387 | PPL: 62.72 | LR: 8.246426e-05\n",
      "  Step 3400/4407 - Loss: 4.1324 | PPL: 62.33 | LR: 8.314500e-05\n",
      "  Step 3400/4407 - Loss: 4.1324 | PPL: 62.33 | LR: 8.314500e-05\n",
      "  Step 3500/4407 - Loss: 4.1251 | PPL: 61.87 | LR: 8.382573e-05\n",
      "  Step 3500/4407 - Loss: 4.1251 | PPL: 61.87 | LR: 8.382573e-05\n",
      "  Step 3600/4407 - Loss: 4.1172 | PPL: 61.38 | LR: 8.450647e-05\n",
      "  Step 3600/4407 - Loss: 4.1172 | PPL: 61.38 | LR: 8.450647e-05\n",
      "  Step 3700/4407 - Loss: 4.1097 | PPL: 60.93 | LR: 8.518720e-05\n",
      "  Step 3700/4407 - Loss: 4.1097 | PPL: 60.93 | LR: 8.518720e-05\n",
      "  Step 3800/4407 - Loss: 4.1035 | PPL: 60.55 | LR: 8.586794e-05\n",
      "  Step 3800/4407 - Loss: 4.1035 | PPL: 60.55 | LR: 8.586794e-05\n",
      "  Step 3900/4407 - Loss: 4.0953 | PPL: 60.06 | LR: 8.654867e-05\n",
      "  Step 3900/4407 - Loss: 4.0953 | PPL: 60.06 | LR: 8.654867e-05\n",
      "  Step 4000/4407 - Loss: 4.0878 | PPL: 59.61 | LR: 8.722941e-05\n",
      "  Step 4000/4407 - Loss: 4.0878 | PPL: 59.61 | LR: 8.722941e-05\n",
      "  Step 4100/4407 - Loss: 4.0828 | PPL: 59.31 | LR: 8.791014e-05\n",
      "  Step 4100/4407 - Loss: 4.0828 | PPL: 59.31 | LR: 8.791014e-05\n",
      "  Step 4200/4407 - Loss: 4.0764 | PPL: 58.93 | LR: 8.859088e-05\n",
      "  Step 4200/4407 - Loss: 4.0764 | PPL: 58.93 | LR: 8.859088e-05\n",
      "  Step 4300/4407 - Loss: 4.0707 | PPL: 58.60 | LR: 8.927161e-05\n",
      "  Step 4300/4407 - Loss: 4.0707 | PPL: 58.60 | LR: 8.927161e-05\n",
      "  Step 4400/4407 - Loss: 4.0644 | PPL: 58.23 | LR: 8.995235e-05\n",
      "  Step 4400/4407 - Loss: 4.0644 | PPL: 58.23 | LR: 8.995235e-05\n",
      "Epoch 3/100 | Train Loss: 4.0640 | Train PPL: 58.21 | Val Loss: 3.8375 | Val PPL: 46.41\n",
      "Epoch 3/100 | Train Loss: 4.0640 | Train PPL: 58.21 | Val Loss: 3.8375 | Val PPL: 46.41\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 58.21\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 58.21\n",
      "  Step 100/4407 - Loss: 3.7697 | PPL: 43.37 | LR: 9.068074e-05\n",
      "  Step 100/4407 - Loss: 3.7697 | PPL: 43.37 | LR: 9.068074e-05\n",
      "  Step 200/4407 - Loss: 3.7354 | PPL: 41.90 | LR: 9.136147e-05\n",
      "  Step 200/4407 - Loss: 3.7354 | PPL: 41.90 | LR: 9.136147e-05\n",
      "  Step 300/4407 - Loss: 3.7208 | PPL: 41.30 | LR: 9.204221e-05\n",
      "  Step 300/4407 - Loss: 3.7208 | PPL: 41.30 | LR: 9.204221e-05\n",
      "  Step 400/4407 - Loss: 3.7099 | PPL: 40.85 | LR: 9.272294e-05\n",
      "  Step 400/4407 - Loss: 3.7099 | PPL: 40.85 | LR: 9.272294e-05\n",
      "  Step 500/4407 - Loss: 3.7042 | PPL: 40.62 | LR: 9.340368e-05\n",
      "  Step 500/4407 - Loss: 3.7042 | PPL: 40.62 | LR: 9.340368e-05\n",
      "  Step 600/4407 - Loss: 3.6921 | PPL: 40.13 | LR: 9.408441e-05\n",
      "  Step 600/4407 - Loss: 3.6921 | PPL: 40.13 | LR: 9.408441e-05\n",
      "  Step 700/4407 - Loss: 3.6928 | PPL: 40.16 | LR: 9.476515e-05\n",
      "  Step 700/4407 - Loss: 3.6928 | PPL: 40.16 | LR: 9.476515e-05\n",
      "  Step 800/4407 - Loss: 3.6863 | PPL: 39.90 | LR: 9.544588e-05\n",
      "  Step 800/4407 - Loss: 3.6863 | PPL: 39.90 | LR: 9.544588e-05\n",
      "  Step 900/4407 - Loss: 3.6837 | PPL: 39.79 | LR: 9.612662e-05\n",
      "  Step 900/4407 - Loss: 3.6837 | PPL: 39.79 | LR: 9.612662e-05\n",
      "  Step 1000/4407 - Loss: 3.6825 | PPL: 39.74 | LR: 9.680735e-05\n",
      "  Step 1000/4407 - Loss: 3.6825 | PPL: 39.74 | LR: 9.680735e-05\n",
      "  Step 1100/4407 - Loss: 3.6803 | PPL: 39.66 | LR: 9.748809e-05\n",
      "  Step 1100/4407 - Loss: 3.6803 | PPL: 39.66 | LR: 9.748809e-05\n",
      "  Step 1200/4407 - Loss: 3.6777 | PPL: 39.55 | LR: 9.816882e-05\n",
      "  Step 1200/4407 - Loss: 3.6777 | PPL: 39.55 | LR: 9.816882e-05\n",
      "  Step 1300/4407 - Loss: 3.6726 | PPL: 39.35 | LR: 9.884956e-05\n",
      "  Step 1300/4407 - Loss: 3.6726 | PPL: 39.35 | LR: 9.884956e-05\n",
      "  Step 1400/4407 - Loss: 3.6668 | PPL: 39.13 | LR: 9.953029e-05\n",
      "  Step 1400/4407 - Loss: 3.6668 | PPL: 39.13 | LR: 9.953029e-05\n",
      "  Step 1500/4407 - Loss: 3.6572 | PPL: 38.75 | LR: 1.002110e-04\n",
      "  Step 1500/4407 - Loss: 3.6572 | PPL: 38.75 | LR: 1.002110e-04\n",
      "  Step 1600/4407 - Loss: 3.6511 | PPL: 38.52 | LR: 1.008918e-04\n",
      "  Step 1600/4407 - Loss: 3.6511 | PPL: 38.52 | LR: 1.008918e-04\n",
      "  Step 1700/4407 - Loss: 3.6468 | PPL: 38.35 | LR: 1.015725e-04\n",
      "  Step 1700/4407 - Loss: 3.6468 | PPL: 38.35 | LR: 1.015725e-04\n",
      "  Step 1800/4407 - Loss: 3.6406 | PPL: 38.11 | LR: 1.022532e-04\n",
      "  Step 1800/4407 - Loss: 3.6406 | PPL: 38.11 | LR: 1.022532e-04\n",
      "  Step 1900/4407 - Loss: 3.6365 | PPL: 37.96 | LR: 1.029340e-04\n",
      "  Step 1900/4407 - Loss: 3.6365 | PPL: 37.96 | LR: 1.029340e-04\n",
      "  Step 2000/4407 - Loss: 3.6322 | PPL: 37.80 | LR: 1.036147e-04\n",
      "  Step 2000/4407 - Loss: 3.6322 | PPL: 37.80 | LR: 1.036147e-04\n",
      "  Step 2100/4407 - Loss: 3.6274 | PPL: 37.62 | LR: 1.042954e-04\n",
      "  Step 2100/4407 - Loss: 3.6274 | PPL: 37.62 | LR: 1.042954e-04\n",
      "  Step 2200/4407 - Loss: 3.6245 | PPL: 37.51 | LR: 1.049762e-04\n",
      "  Step 2200/4407 - Loss: 3.6245 | PPL: 37.51 | LR: 1.049762e-04\n",
      "  Step 2300/4407 - Loss: 3.6210 | PPL: 37.38 | LR: 1.056569e-04\n",
      "  Step 2300/4407 - Loss: 3.6210 | PPL: 37.38 | LR: 1.056569e-04\n",
      "  Step 2400/4407 - Loss: 3.6170 | PPL: 37.23 | LR: 1.063376e-04\n",
      "  Step 2400/4407 - Loss: 3.6170 | PPL: 37.23 | LR: 1.063376e-04\n",
      "  Step 2500/4407 - Loss: 3.6126 | PPL: 37.06 | LR: 1.070184e-04\n",
      "  Step 2500/4407 - Loss: 3.6126 | PPL: 37.06 | LR: 1.070184e-04\n",
      "  Step 2600/4407 - Loss: 3.6079 | PPL: 36.89 | LR: 1.076991e-04\n",
      "  Step 2600/4407 - Loss: 3.6079 | PPL: 36.89 | LR: 1.076991e-04\n",
      "  Step 2700/4407 - Loss: 3.6020 | PPL: 36.67 | LR: 1.083799e-04\n",
      "  Step 2700/4407 - Loss: 3.6020 | PPL: 36.67 | LR: 1.083799e-04\n",
      "  Step 2800/4407 - Loss: 3.5968 | PPL: 36.48 | LR: 1.090606e-04\n",
      "  Step 2800/4407 - Loss: 3.5968 | PPL: 36.48 | LR: 1.090606e-04\n",
      "  Step 2900/4407 - Loss: 3.5929 | PPL: 36.34 | LR: 1.097413e-04\n",
      "  Step 2900/4407 - Loss: 3.5929 | PPL: 36.34 | LR: 1.097413e-04\n",
      "  Step 3000/4407 - Loss: 3.5888 | PPL: 36.19 | LR: 1.104221e-04\n",
      "  Step 3000/4407 - Loss: 3.5888 | PPL: 36.19 | LR: 1.104221e-04\n",
      "  Step 3100/4407 - Loss: 3.5866 | PPL: 36.11 | LR: 1.111028e-04\n",
      "  Step 3100/4407 - Loss: 3.5866 | PPL: 36.11 | LR: 1.111028e-04\n",
      "  Step 3200/4407 - Loss: 3.5818 | PPL: 35.94 | LR: 1.117835e-04\n",
      "  Step 3200/4407 - Loss: 3.5818 | PPL: 35.94 | LR: 1.117835e-04\n",
      "  Step 3300/4407 - Loss: 3.5766 | PPL: 35.75 | LR: 1.124643e-04\n",
      "  Step 3300/4407 - Loss: 3.5766 | PPL: 35.75 | LR: 1.124643e-04\n",
      "  Step 3400/4407 - Loss: 3.5726 | PPL: 35.61 | LR: 1.131450e-04\n",
      "  Step 3400/4407 - Loss: 3.5726 | PPL: 35.61 | LR: 1.131450e-04\n",
      "  Step 3500/4407 - Loss: 3.5668 | PPL: 35.40 | LR: 1.138257e-04\n",
      "  Step 3500/4407 - Loss: 3.5668 | PPL: 35.40 | LR: 1.138257e-04\n",
      "  Step 3600/4407 - Loss: 3.5627 | PPL: 35.26 | LR: 1.145065e-04\n",
      "  Step 3600/4407 - Loss: 3.5627 | PPL: 35.26 | LR: 1.145065e-04\n",
      "  Step 3700/4407 - Loss: 3.5584 | PPL: 35.11 | LR: 1.151872e-04\n",
      "  Step 3700/4407 - Loss: 3.5584 | PPL: 35.11 | LR: 1.151872e-04\n",
      "  Step 3800/4407 - Loss: 3.5551 | PPL: 34.99 | LR: 1.158679e-04\n",
      "  Step 3800/4407 - Loss: 3.5551 | PPL: 34.99 | LR: 1.158679e-04\n",
      "  Step 3900/4407 - Loss: 3.5497 | PPL: 34.80 | LR: 1.165487e-04\n",
      "  Step 3900/4407 - Loss: 3.5497 | PPL: 34.80 | LR: 1.165487e-04\n",
      "  Step 4000/4407 - Loss: 3.5446 | PPL: 34.63 | LR: 1.172294e-04\n",
      "  Step 4000/4407 - Loss: 3.5446 | PPL: 34.63 | LR: 1.172294e-04\n",
      "  Step 4100/4407 - Loss: 3.5403 | PPL: 34.48 | LR: 1.179101e-04\n",
      "  Step 4100/4407 - Loss: 3.5403 | PPL: 34.48 | LR: 1.179101e-04\n",
      "  Step 4200/4407 - Loss: 3.5359 | PPL: 34.33 | LR: 1.185909e-04\n",
      "  Step 4200/4407 - Loss: 3.5359 | PPL: 34.33 | LR: 1.185909e-04\n",
      "  Step 4300/4407 - Loss: 3.5327 | PPL: 34.22 | LR: 1.192716e-04\n",
      "  Step 4300/4407 - Loss: 3.5327 | PPL: 34.22 | LR: 1.192716e-04\n",
      "  Step 4400/4407 - Loss: 3.5290 | PPL: 34.09 | LR: 1.199523e-04\n",
      "  Step 4400/4407 - Loss: 3.5290 | PPL: 34.09 | LR: 1.199523e-04\n",
      "Epoch 4/100 | Train Loss: 3.5287 | Train PPL: 34.08 | Val Loss: 3.4522 | Val PPL: 31.57\n",
      "Epoch 4/100 | Train Loss: 3.5287 | Train PPL: 34.08 | Val Loss: 3.4522 | Val PPL: 31.57\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 34.08\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 34.08\n",
      "  Step 100/4407 - Loss: 3.2957 | PPL: 27.00 | LR: 1.206807e-04\n",
      "  Step 100/4407 - Loss: 3.2957 | PPL: 27.00 | LR: 1.206807e-04\n",
      "  Step 200/4407 - Loss: 3.2918 | PPL: 26.89 | LR: 1.213615e-04\n",
      "  Step 200/4407 - Loss: 3.2918 | PPL: 26.89 | LR: 1.213615e-04\n",
      "  Step 300/4407 - Loss: 3.2815 | PPL: 26.62 | LR: 1.220422e-04\n",
      "  Step 300/4407 - Loss: 3.2815 | PPL: 26.62 | LR: 1.220422e-04\n",
      "  Step 400/4407 - Loss: 3.2790 | PPL: 26.55 | LR: 1.227229e-04\n",
      "  Step 400/4407 - Loss: 3.2790 | PPL: 26.55 | LR: 1.227229e-04\n",
      "  Step 500/4407 - Loss: 3.2857 | PPL: 26.73 | LR: 1.234037e-04\n",
      "  Step 500/4407 - Loss: 3.2857 | PPL: 26.73 | LR: 1.234037e-04\n",
      "  Step 600/4407 - Loss: 3.2856 | PPL: 26.73 | LR: 1.240844e-04\n",
      "  Step 600/4407 - Loss: 3.2856 | PPL: 26.73 | LR: 1.240844e-04\n",
      "  Step 700/4407 - Loss: 3.2859 | PPL: 26.73 | LR: 1.247651e-04\n",
      "  Step 700/4407 - Loss: 3.2859 | PPL: 26.73 | LR: 1.247651e-04\n",
      "  Step 800/4407 - Loss: 3.2847 | PPL: 26.70 | LR: 1.254459e-04\n",
      "  Step 800/4407 - Loss: 3.2847 | PPL: 26.70 | LR: 1.254459e-04\n",
      "  Step 900/4407 - Loss: 3.2797 | PPL: 26.57 | LR: 1.261266e-04\n",
      "  Step 900/4407 - Loss: 3.2797 | PPL: 26.57 | LR: 1.261266e-04\n",
      "  Step 1000/4407 - Loss: 3.2822 | PPL: 26.63 | LR: 1.268074e-04\n",
      "  Step 1000/4407 - Loss: 3.2822 | PPL: 26.63 | LR: 1.268074e-04\n",
      "  Step 1100/4407 - Loss: 3.2839 | PPL: 26.68 | LR: 1.274881e-04\n",
      "  Step 1100/4407 - Loss: 3.2839 | PPL: 26.68 | LR: 1.274881e-04\n",
      "  Step 1200/4407 - Loss: 3.2786 | PPL: 26.54 | LR: 1.281688e-04\n",
      "  Step 1200/4407 - Loss: 3.2786 | PPL: 26.54 | LR: 1.281688e-04\n",
      "  Step 1300/4407 - Loss: 3.2817 | PPL: 26.62 | LR: 1.288496e-04\n",
      "  Step 1300/4407 - Loss: 3.2817 | PPL: 26.62 | LR: 1.288496e-04\n",
      "  Step 1400/4407 - Loss: 3.2805 | PPL: 26.59 | LR: 1.295303e-04\n",
      "  Step 1400/4407 - Loss: 3.2805 | PPL: 26.59 | LR: 1.295303e-04\n",
      "  Step 1500/4407 - Loss: 3.2728 | PPL: 26.38 | LR: 1.302110e-04\n",
      "  Step 1500/4407 - Loss: 3.2728 | PPL: 26.38 | LR: 1.302110e-04\n",
      "  Step 1600/4407 - Loss: 3.2725 | PPL: 26.38 | LR: 1.308918e-04\n",
      "  Step 1600/4407 - Loss: 3.2725 | PPL: 26.38 | LR: 1.308918e-04\n",
      "  Step 1700/4407 - Loss: 3.2687 | PPL: 26.28 | LR: 1.315725e-04\n",
      "  Step 1700/4407 - Loss: 3.2687 | PPL: 26.28 | LR: 1.315725e-04\n",
      "  Step 1800/4407 - Loss: 3.2638 | PPL: 26.15 | LR: 1.322532e-04\n",
      "  Step 1800/4407 - Loss: 3.2638 | PPL: 26.15 | LR: 1.322532e-04\n",
      "  Step 1900/4407 - Loss: 3.2595 | PPL: 26.04 | LR: 1.329340e-04\n",
      "  Step 1900/4407 - Loss: 3.2595 | PPL: 26.04 | LR: 1.329340e-04\n",
      "  Step 2000/4407 - Loss: 3.2575 | PPL: 25.99 | LR: 1.336147e-04\n",
      "  Step 2000/4407 - Loss: 3.2575 | PPL: 25.99 | LR: 1.336147e-04\n",
      "  Step 2100/4407 - Loss: 3.2511 | PPL: 25.82 | LR: 1.342954e-04\n",
      "  Step 2100/4407 - Loss: 3.2511 | PPL: 25.82 | LR: 1.342954e-04\n",
      "  Step 2200/4407 - Loss: 3.2479 | PPL: 25.74 | LR: 1.349762e-04\n",
      "  Step 2200/4407 - Loss: 3.2479 | PPL: 25.74 | LR: 1.349762e-04\n",
      "  Step 2300/4407 - Loss: 3.2445 | PPL: 25.65 | LR: 1.356569e-04\n",
      "  Step 2300/4407 - Loss: 3.2445 | PPL: 25.65 | LR: 1.356569e-04\n",
      "  Step 2400/4407 - Loss: 3.2406 | PPL: 25.55 | LR: 1.363376e-04\n",
      "  Step 2400/4407 - Loss: 3.2406 | PPL: 25.55 | LR: 1.363376e-04\n",
      "  Step 2500/4407 - Loss: 3.2381 | PPL: 25.49 | LR: 1.370184e-04\n",
      "  Step 2500/4407 - Loss: 3.2381 | PPL: 25.49 | LR: 1.370184e-04\n",
      "  Step 2600/4407 - Loss: 3.2367 | PPL: 25.45 | LR: 1.376991e-04\n",
      "  Step 2600/4407 - Loss: 3.2367 | PPL: 25.45 | LR: 1.376991e-04\n",
      "  Step 2700/4407 - Loss: 3.2338 | PPL: 25.38 | LR: 1.383799e-04\n",
      "  Step 2700/4407 - Loss: 3.2338 | PPL: 25.38 | LR: 1.383799e-04\n",
      "  Step 2800/4407 - Loss: 3.2308 | PPL: 25.30 | LR: 1.390606e-04\n",
      "  Step 2800/4407 - Loss: 3.2308 | PPL: 25.30 | LR: 1.390606e-04\n",
      "  Step 2900/4407 - Loss: 3.2272 | PPL: 25.21 | LR: 1.397413e-04\n",
      "  Step 2900/4407 - Loss: 3.2272 | PPL: 25.21 | LR: 1.397413e-04\n",
      "  Step 3000/4407 - Loss: 3.2252 | PPL: 25.16 | LR: 1.404221e-04\n",
      "  Step 3000/4407 - Loss: 3.2252 | PPL: 25.16 | LR: 1.404221e-04\n",
      "  Step 3100/4407 - Loss: 3.2213 | PPL: 25.06 | LR: 1.411028e-04\n",
      "  Step 3100/4407 - Loss: 3.2213 | PPL: 25.06 | LR: 1.411028e-04\n",
      "  Step 3200/4407 - Loss: 3.2177 | PPL: 24.97 | LR: 1.417835e-04\n",
      "  Step 3200/4407 - Loss: 3.2177 | PPL: 24.97 | LR: 1.417835e-04\n",
      "  Step 3300/4407 - Loss: 3.2149 | PPL: 24.90 | LR: 1.424643e-04\n",
      "  Step 3300/4407 - Loss: 3.2149 | PPL: 24.90 | LR: 1.424643e-04\n",
      "  Step 3400/4407 - Loss: 3.2129 | PPL: 24.85 | LR: 1.431450e-04\n",
      "  Step 3400/4407 - Loss: 3.2129 | PPL: 24.85 | LR: 1.431450e-04\n",
      "  Step 3500/4407 - Loss: 3.2093 | PPL: 24.76 | LR: 1.438257e-04\n",
      "  Step 3500/4407 - Loss: 3.2093 | PPL: 24.76 | LR: 1.438257e-04\n",
      "  Step 3600/4407 - Loss: 3.2074 | PPL: 24.71 | LR: 1.445065e-04\n",
      "  Step 3600/4407 - Loss: 3.2074 | PPL: 24.71 | LR: 1.445065e-04\n",
      "  Step 3700/4407 - Loss: 3.2043 | PPL: 24.64 | LR: 1.451872e-04\n",
      "  Step 3700/4407 - Loss: 3.2043 | PPL: 24.64 | LR: 1.451872e-04\n",
      "  Step 3800/4407 - Loss: 3.2011 | PPL: 24.56 | LR: 1.458679e-04\n",
      "  Step 3800/4407 - Loss: 3.2011 | PPL: 24.56 | LR: 1.458679e-04\n",
      "  Step 3900/4407 - Loss: 3.1975 | PPL: 24.47 | LR: 1.465487e-04\n",
      "  Step 3900/4407 - Loss: 3.1975 | PPL: 24.47 | LR: 1.465487e-04\n",
      "  Step 4000/4407 - Loss: 3.1953 | PPL: 24.42 | LR: 1.472294e-04\n",
      "  Step 4000/4407 - Loss: 3.1953 | PPL: 24.42 | LR: 1.472294e-04\n",
      "  Step 4100/4407 - Loss: 3.1925 | PPL: 24.35 | LR: 1.479101e-04\n",
      "  Step 4100/4407 - Loss: 3.1925 | PPL: 24.35 | LR: 1.479101e-04\n",
      "  Step 4200/4407 - Loss: 3.1888 | PPL: 24.26 | LR: 1.485909e-04\n",
      "  Step 4200/4407 - Loss: 3.1888 | PPL: 24.26 | LR: 1.485909e-04\n",
      "  Step 4300/4407 - Loss: 3.1856 | PPL: 24.18 | LR: 1.492716e-04\n",
      "  Step 4300/4407 - Loss: 3.1856 | PPL: 24.18 | LR: 1.492716e-04\n",
      "  Step 4400/4407 - Loss: 3.1832 | PPL: 24.12 | LR: 1.499523e-04\n",
      "  Step 4400/4407 - Loss: 3.1832 | PPL: 24.12 | LR: 1.499523e-04\n",
      "Epoch 5/100 | Train Loss: 3.1831 | Train PPL: 24.12 | Val Loss: 3.2114 | Val PPL: 24.81\n",
      "Epoch 5/100 | Train Loss: 3.1831 | Train PPL: 24.12 | Val Loss: 3.2114 | Val PPL: 24.81\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 24.12\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 24.12\n",
      "  Step 100/4407 - Loss: 3.0251 | PPL: 20.60 | LR: 1.506807e-04\n",
      "  Step 100/4407 - Loss: 3.0251 | PPL: 20.60 | LR: 1.506807e-04\n",
      "  Step 200/4407 - Loss: 3.0373 | PPL: 20.85 | LR: 1.513615e-04\n",
      "  Step 200/4407 - Loss: 3.0373 | PPL: 20.85 | LR: 1.513615e-04\n",
      "  Step 300/4407 - Loss: 3.0309 | PPL: 20.72 | LR: 1.520422e-04\n",
      "  Step 300/4407 - Loss: 3.0309 | PPL: 20.72 | LR: 1.520422e-04\n",
      "  Step 400/4407 - Loss: 3.0273 | PPL: 20.64 | LR: 1.527229e-04\n",
      "  Step 400/4407 - Loss: 3.0273 | PPL: 20.64 | LR: 1.527229e-04\n",
      "  Step 500/4407 - Loss: 3.0160 | PPL: 20.41 | LR: 1.534037e-04\n",
      "  Step 500/4407 - Loss: 3.0160 | PPL: 20.41 | LR: 1.534037e-04\n",
      "  Step 600/4407 - Loss: 3.0069 | PPL: 20.23 | LR: 1.540844e-04\n",
      "  Step 600/4407 - Loss: 3.0069 | PPL: 20.23 | LR: 1.540844e-04\n",
      "  Step 700/4407 - Loss: 3.0037 | PPL: 20.16 | LR: 1.547651e-04\n",
      "  Step 700/4407 - Loss: 3.0037 | PPL: 20.16 | LR: 1.547651e-04\n",
      "  Step 800/4407 - Loss: 3.0064 | PPL: 20.21 | LR: 1.554459e-04\n",
      "  Step 800/4407 - Loss: 3.0064 | PPL: 20.21 | LR: 1.554459e-04\n",
      "  Step 900/4407 - Loss: 3.0092 | PPL: 20.27 | LR: 1.561266e-04\n",
      "  Step 900/4407 - Loss: 3.0092 | PPL: 20.27 | LR: 1.561266e-04\n",
      "  Step 1000/4407 - Loss: 3.0070 | PPL: 20.23 | LR: 1.568074e-04\n",
      "  Step 1000/4407 - Loss: 3.0070 | PPL: 20.23 | LR: 1.568074e-04\n",
      "  Step 1100/4407 - Loss: 3.0074 | PPL: 20.23 | LR: 1.574881e-04\n",
      "  Step 1100/4407 - Loss: 3.0074 | PPL: 20.23 | LR: 1.574881e-04\n",
      "  Step 1200/4407 - Loss: 3.0055 | PPL: 20.20 | LR: 1.581688e-04\n",
      "  Step 1200/4407 - Loss: 3.0055 | PPL: 20.20 | LR: 1.581688e-04\n",
      "  Step 1300/4407 - Loss: 3.0006 | PPL: 20.10 | LR: 1.588496e-04\n",
      "  Step 1300/4407 - Loss: 3.0006 | PPL: 20.10 | LR: 1.588496e-04\n",
      "  Step 1400/4407 - Loss: 3.0021 | PPL: 20.13 | LR: 1.595303e-04\n",
      "  Step 1400/4407 - Loss: 3.0021 | PPL: 20.13 | LR: 1.595303e-04\n",
      "  Step 1500/4407 - Loss: 2.9981 | PPL: 20.05 | LR: 1.602110e-04\n",
      "  Step 1500/4407 - Loss: 2.9981 | PPL: 20.05 | LR: 1.602110e-04\n",
      "  Step 1600/4407 - Loss: 2.9957 | PPL: 20.00 | LR: 1.608918e-04\n",
      "  Step 1600/4407 - Loss: 2.9957 | PPL: 20.00 | LR: 1.608918e-04\n",
      "  Step 1700/4407 - Loss: 2.9942 | PPL: 19.97 | LR: 1.615725e-04\n",
      "  Step 1700/4407 - Loss: 2.9942 | PPL: 19.97 | LR: 1.615725e-04\n",
      "  Step 1800/4407 - Loss: 2.9911 | PPL: 19.91 | LR: 1.622532e-04\n",
      "  Step 1800/4407 - Loss: 2.9911 | PPL: 19.91 | LR: 1.622532e-04\n",
      "  Step 1900/4407 - Loss: 2.9902 | PPL: 19.89 | LR: 1.629340e-04\n",
      "  Step 1900/4407 - Loss: 2.9902 | PPL: 19.89 | LR: 1.629340e-04\n",
      "  Step 2000/4407 - Loss: 2.9888 | PPL: 19.86 | LR: 1.636147e-04\n",
      "  Step 2000/4407 - Loss: 2.9888 | PPL: 19.86 | LR: 1.636147e-04\n",
      "  Step 2100/4407 - Loss: 2.9879 | PPL: 19.84 | LR: 1.642954e-04\n",
      "  Step 2100/4407 - Loss: 2.9879 | PPL: 19.84 | LR: 1.642954e-04\n",
      "  Step 2200/4407 - Loss: 2.9876 | PPL: 19.84 | LR: 1.649762e-04\n",
      "  Step 2200/4407 - Loss: 2.9876 | PPL: 19.84 | LR: 1.649762e-04\n",
      "  Step 2300/4407 - Loss: 2.9859 | PPL: 19.80 | LR: 1.656569e-04\n",
      "  Step 2300/4407 - Loss: 2.9859 | PPL: 19.80 | LR: 1.656569e-04\n",
      "  Step 2400/4407 - Loss: 2.9837 | PPL: 19.76 | LR: 1.663376e-04\n",
      "  Step 2400/4407 - Loss: 2.9837 | PPL: 19.76 | LR: 1.663376e-04\n",
      "  Step 2500/4407 - Loss: 2.9833 | PPL: 19.75 | LR: 1.670184e-04\n",
      "  Step 2500/4407 - Loss: 2.9833 | PPL: 19.75 | LR: 1.670184e-04\n",
      "  Step 2600/4407 - Loss: 2.9826 | PPL: 19.74 | LR: 1.676991e-04\n",
      "  Step 2600/4407 - Loss: 2.9826 | PPL: 19.74 | LR: 1.676991e-04\n",
      "  Step 2700/4407 - Loss: 2.9823 | PPL: 19.73 | LR: 1.683799e-04\n",
      "  Step 2700/4407 - Loss: 2.9823 | PPL: 19.73 | LR: 1.683799e-04\n",
      "  Step 2800/4407 - Loss: 2.9801 | PPL: 19.69 | LR: 1.690606e-04\n",
      "  Step 2800/4407 - Loss: 2.9801 | PPL: 19.69 | LR: 1.690606e-04\n",
      "  Step 2900/4407 - Loss: 2.9771 | PPL: 19.63 | LR: 1.697413e-04\n",
      "  Step 2900/4407 - Loss: 2.9771 | PPL: 19.63 | LR: 1.697413e-04\n",
      "  Step 3000/4407 - Loss: 2.9771 | PPL: 19.63 | LR: 1.704221e-04\n",
      "  Step 3000/4407 - Loss: 2.9771 | PPL: 19.63 | LR: 1.704221e-04\n",
      "  Step 3100/4407 - Loss: 2.9762 | PPL: 19.61 | LR: 1.711028e-04\n",
      "  Step 3100/4407 - Loss: 2.9762 | PPL: 19.61 | LR: 1.711028e-04\n",
      "  Step 3200/4407 - Loss: 2.9741 | PPL: 19.57 | LR: 1.717835e-04\n",
      "  Step 3200/4407 - Loss: 2.9741 | PPL: 19.57 | LR: 1.717835e-04\n",
      "  Step 3300/4407 - Loss: 2.9722 | PPL: 19.53 | LR: 1.724643e-04\n",
      "  Step 3300/4407 - Loss: 2.9722 | PPL: 19.53 | LR: 1.724643e-04\n",
      "  Step 3400/4407 - Loss: 2.9710 | PPL: 19.51 | LR: 1.731450e-04\n",
      "  Step 3400/4407 - Loss: 2.9710 | PPL: 19.51 | LR: 1.731450e-04\n",
      "  Step 3500/4407 - Loss: 2.9694 | PPL: 19.48 | LR: 1.738257e-04\n",
      "  Step 3500/4407 - Loss: 2.9694 | PPL: 19.48 | LR: 1.738257e-04\n",
      "  Step 3600/4407 - Loss: 2.9675 | PPL: 19.44 | LR: 1.745065e-04\n",
      "  Step 3600/4407 - Loss: 2.9675 | PPL: 19.44 | LR: 1.745065e-04\n",
      "  Step 3700/4407 - Loss: 2.9658 | PPL: 19.41 | LR: 1.751872e-04\n",
      "  Step 3700/4407 - Loss: 2.9658 | PPL: 19.41 | LR: 1.751872e-04\n",
      "  Step 3800/4407 - Loss: 2.9634 | PPL: 19.36 | LR: 1.758679e-04\n",
      "  Step 3800/4407 - Loss: 2.9634 | PPL: 19.36 | LR: 1.758679e-04\n",
      "  Step 3900/4407 - Loss: 2.9634 | PPL: 19.36 | LR: 1.765487e-04\n",
      "  Step 3900/4407 - Loss: 2.9634 | PPL: 19.36 | LR: 1.765487e-04\n",
      "  Step 4000/4407 - Loss: 2.9608 | PPL: 19.31 | LR: 1.772294e-04\n",
      "  Step 4000/4407 - Loss: 2.9608 | PPL: 19.31 | LR: 1.772294e-04\n",
      "  Step 4100/4407 - Loss: 2.9574 | PPL: 19.25 | LR: 1.779101e-04\n",
      "  Step 4100/4407 - Loss: 2.9574 | PPL: 19.25 | LR: 1.779101e-04\n",
      "  Step 4200/4407 - Loss: 2.9564 | PPL: 19.23 | LR: 1.785909e-04\n",
      "  Step 4200/4407 - Loss: 2.9564 | PPL: 19.23 | LR: 1.785909e-04\n",
      "  Step 4300/4407 - Loss: 2.9551 | PPL: 19.20 | LR: 1.792716e-04\n",
      "  Step 4300/4407 - Loss: 2.9551 | PPL: 19.20 | LR: 1.792716e-04\n",
      "  Step 4400/4407 - Loss: 2.9529 | PPL: 19.16 | LR: 1.799523e-04\n",
      "  Step 4400/4407 - Loss: 2.9529 | PPL: 19.16 | LR: 1.799523e-04\n",
      "Epoch 6/100 | Train Loss: 2.9531 | Train PPL: 19.17 | Val Loss: 3.0577 | Val PPL: 21.28\n",
      "Epoch 6/100 | Train Loss: 2.9531 | Train PPL: 19.17 | Val Loss: 3.0577 | Val PPL: 21.28\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 19.17\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 19.17\n",
      "  Step 100/4407 - Loss: 2.7987 | PPL: 16.42 | LR: 1.806807e-04\n",
      "  Step 100/4407 - Loss: 2.7987 | PPL: 16.42 | LR: 1.806807e-04\n",
      "  Step 200/4407 - Loss: 2.8373 | PPL: 17.07 | LR: 1.813615e-04\n",
      "  Step 200/4407 - Loss: 2.8373 | PPL: 17.07 | LR: 1.813615e-04\n",
      "  Step 300/4407 - Loss: 2.8384 | PPL: 17.09 | LR: 1.820422e-04\n",
      "  Step 300/4407 - Loss: 2.8384 | PPL: 17.09 | LR: 1.820422e-04\n",
      "  Step 400/4407 - Loss: 2.8379 | PPL: 17.08 | LR: 1.827229e-04\n",
      "  Step 400/4407 - Loss: 2.8379 | PPL: 17.08 | LR: 1.827229e-04\n",
      "  Step 500/4407 - Loss: 2.8396 | PPL: 17.11 | LR: 1.834037e-04\n",
      "  Step 500/4407 - Loss: 2.8396 | PPL: 17.11 | LR: 1.834037e-04\n",
      "  Step 600/4407 - Loss: 2.8362 | PPL: 17.05 | LR: 1.840844e-04\n",
      "  Step 600/4407 - Loss: 2.8362 | PPL: 17.05 | LR: 1.840844e-04\n",
      "  Step 700/4407 - Loss: 2.8307 | PPL: 16.96 | LR: 1.847651e-04\n",
      "  Step 700/4407 - Loss: 2.8307 | PPL: 16.96 | LR: 1.847651e-04\n",
      "  Step 800/4407 - Loss: 2.8255 | PPL: 16.87 | LR: 1.854459e-04\n",
      "  Step 800/4407 - Loss: 2.8255 | PPL: 16.87 | LR: 1.854459e-04\n",
      "  Step 900/4407 - Loss: 2.8227 | PPL: 16.82 | LR: 1.861266e-04\n",
      "  Step 900/4407 - Loss: 2.8227 | PPL: 16.82 | LR: 1.861266e-04\n",
      "  Step 1000/4407 - Loss: 2.8258 | PPL: 16.88 | LR: 1.868074e-04\n",
      "  Step 1000/4407 - Loss: 2.8258 | PPL: 16.88 | LR: 1.868074e-04\n",
      "  Step 1100/4407 - Loss: 2.8258 | PPL: 16.88 | LR: 1.874881e-04\n",
      "  Step 1100/4407 - Loss: 2.8258 | PPL: 16.88 | LR: 1.874881e-04\n",
      "  Step 1200/4407 - Loss: 2.8246 | PPL: 16.85 | LR: 1.881688e-04\n",
      "  Step 1200/4407 - Loss: 2.8246 | PPL: 16.85 | LR: 1.881688e-04\n",
      "  Step 1300/4407 - Loss: 2.8262 | PPL: 16.88 | LR: 1.888496e-04\n",
      "  Step 1300/4407 - Loss: 2.8262 | PPL: 16.88 | LR: 1.888496e-04\n",
      "  Step 1400/4407 - Loss: 2.8237 | PPL: 16.84 | LR: 1.895303e-04\n",
      "  Step 1400/4407 - Loss: 2.8237 | PPL: 16.84 | LR: 1.895303e-04\n",
      "  Step 1500/4407 - Loss: 2.8216 | PPL: 16.80 | LR: 1.902110e-04\n",
      "  Step 1500/4407 - Loss: 2.8216 | PPL: 16.80 | LR: 1.902110e-04\n",
      "  Step 1600/4407 - Loss: 2.8233 | PPL: 16.83 | LR: 1.908918e-04\n",
      "  Step 1600/4407 - Loss: 2.8233 | PPL: 16.83 | LR: 1.908918e-04\n",
      "  Step 1700/4407 - Loss: 2.8209 | PPL: 16.79 | LR: 1.915725e-04\n",
      "  Step 1700/4407 - Loss: 2.8209 | PPL: 16.79 | LR: 1.915725e-04\n",
      "  Step 1800/4407 - Loss: 2.8205 | PPL: 16.79 | LR: 1.922532e-04\n",
      "  Step 1800/4407 - Loss: 2.8205 | PPL: 16.79 | LR: 1.922532e-04\n",
      "  Step 1900/4407 - Loss: 2.8185 | PPL: 16.75 | LR: 1.929340e-04\n",
      "  Step 1900/4407 - Loss: 2.8185 | PPL: 16.75 | LR: 1.929340e-04\n",
      "  Step 2000/4407 - Loss: 2.8187 | PPL: 16.75 | LR: 1.936147e-04\n",
      "  Step 2000/4407 - Loss: 2.8187 | PPL: 16.75 | LR: 1.936147e-04\n",
      "  Step 2100/4407 - Loss: 2.8208 | PPL: 16.79 | LR: 1.942954e-04\n",
      "  Step 2100/4407 - Loss: 2.8208 | PPL: 16.79 | LR: 1.942954e-04\n",
      "  Step 2200/4407 - Loss: 2.8186 | PPL: 16.75 | LR: 1.949762e-04\n",
      "  Step 2200/4407 - Loss: 2.8186 | PPL: 16.75 | LR: 1.949762e-04\n",
      "  Step 2300/4407 - Loss: 2.8176 | PPL: 16.74 | LR: 1.956569e-04\n",
      "  Step 2300/4407 - Loss: 2.8176 | PPL: 16.74 | LR: 1.956569e-04\n",
      "  Step 2400/4407 - Loss: 2.8167 | PPL: 16.72 | LR: 1.963376e-04\n",
      "  Step 2400/4407 - Loss: 2.8167 | PPL: 16.72 | LR: 1.963376e-04\n",
      "  Step 2500/4407 - Loss: 2.8171 | PPL: 16.73 | LR: 1.970184e-04\n",
      "  Step 2500/4407 - Loss: 2.8171 | PPL: 16.73 | LR: 1.970184e-04\n",
      "  Step 2600/4407 - Loss: 2.8158 | PPL: 16.71 | LR: 1.976991e-04\n",
      "  Step 2600/4407 - Loss: 2.8158 | PPL: 16.71 | LR: 1.976991e-04\n",
      "  Step 2700/4407 - Loss: 2.8143 | PPL: 16.68 | LR: 1.983799e-04\n",
      "  Step 2700/4407 - Loss: 2.8143 | PPL: 16.68 | LR: 1.983799e-04\n",
      "  Step 2800/4407 - Loss: 2.8145 | PPL: 16.68 | LR: 1.990606e-04\n",
      "  Step 2800/4407 - Loss: 2.8145 | PPL: 16.68 | LR: 1.990606e-04\n",
      "  Step 2900/4407 - Loss: 2.8118 | PPL: 16.64 | LR: 1.997413e-04\n",
      "  Step 2900/4407 - Loss: 2.8118 | PPL: 16.64 | LR: 1.997413e-04\n",
      "  Step 3000/4407 - Loss: 2.8093 | PPL: 16.60 | LR: 2.004221e-04\n",
      "  Step 3000/4407 - Loss: 2.8093 | PPL: 16.60 | LR: 2.004221e-04\n",
      "  Step 3100/4407 - Loss: 2.8075 | PPL: 16.57 | LR: 2.011028e-04\n",
      "  Step 3100/4407 - Loss: 2.8075 | PPL: 16.57 | LR: 2.011028e-04\n",
      "  Step 3200/4407 - Loss: 2.8074 | PPL: 16.57 | LR: 2.017835e-04\n",
      "  Step 3200/4407 - Loss: 2.8074 | PPL: 16.57 | LR: 2.017835e-04\n",
      "  Step 3300/4407 - Loss: 2.8074 | PPL: 16.57 | LR: 2.024643e-04\n",
      "  Step 3300/4407 - Loss: 2.8074 | PPL: 16.57 | LR: 2.024643e-04\n",
      "  Step 3400/4407 - Loss: 2.8064 | PPL: 16.55 | LR: 2.031450e-04\n",
      "  Step 3400/4407 - Loss: 2.8064 | PPL: 16.55 | LR: 2.031450e-04\n",
      "  Step 3500/4407 - Loss: 2.8056 | PPL: 16.54 | LR: 2.038257e-04\n",
      "  Step 3500/4407 - Loss: 2.8056 | PPL: 16.54 | LR: 2.038257e-04\n",
      "  Step 3600/4407 - Loss: 2.8035 | PPL: 16.50 | LR: 2.045065e-04\n",
      "  Step 3600/4407 - Loss: 2.8035 | PPL: 16.50 | LR: 2.045065e-04\n",
      "  Step 3700/4407 - Loss: 2.8018 | PPL: 16.47 | LR: 2.051872e-04\n",
      "  Step 3700/4407 - Loss: 2.8018 | PPL: 16.47 | LR: 2.051872e-04\n",
      "  Step 3800/4407 - Loss: 2.8013 | PPL: 16.47 | LR: 2.058679e-04\n",
      "  Step 3800/4407 - Loss: 2.8013 | PPL: 16.47 | LR: 2.058679e-04\n",
      "  Step 3900/4407 - Loss: 2.8005 | PPL: 16.45 | LR: 2.065487e-04\n",
      "  Step 3900/4407 - Loss: 2.8005 | PPL: 16.45 | LR: 2.065487e-04\n",
      "  Step 4000/4407 - Loss: 2.7997 | PPL: 16.44 | LR: 2.072294e-04\n",
      "  Step 4000/4407 - Loss: 2.7997 | PPL: 16.44 | LR: 2.072294e-04\n",
      "  Step 4100/4407 - Loss: 2.7989 | PPL: 16.43 | LR: 2.079101e-04\n",
      "  Step 4100/4407 - Loss: 2.7989 | PPL: 16.43 | LR: 2.079101e-04\n",
      "  Step 4200/4407 - Loss: 2.7969 | PPL: 16.39 | LR: 2.085909e-04\n",
      "  Step 4200/4407 - Loss: 2.7969 | PPL: 16.39 | LR: 2.085909e-04\n",
      "  Step 4300/4407 - Loss: 2.7969 | PPL: 16.39 | LR: 2.092716e-04\n",
      "  Step 4300/4407 - Loss: 2.7969 | PPL: 16.39 | LR: 2.092716e-04\n",
      "  Step 4400/4407 - Loss: 2.7971 | PPL: 16.40 | LR: 2.099523e-04\n",
      "  Step 4400/4407 - Loss: 2.7971 | PPL: 16.40 | LR: 2.099523e-04\n",
      "Epoch 7/100 | Train Loss: 2.7971 | Train PPL: 16.40 | Val Loss: 2.9634 | Val PPL: 19.36\n",
      "Epoch 7/100 | Train Loss: 2.7971 | Train PPL: 16.40 | Val Loss: 2.9634 | Val PPL: 19.36\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 16.40\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 16.40\n",
      "  Step 100/4407 - Loss: 2.6668 | PPL: 14.39 | LR: 2.106807e-04\n",
      "  Step 100/4407 - Loss: 2.6668 | PPL: 14.39 | LR: 2.106807e-04\n",
      "  Step 200/4407 - Loss: 2.7104 | PPL: 15.04 | LR: 2.113615e-04\n",
      "  Step 200/4407 - Loss: 2.7104 | PPL: 15.04 | LR: 2.113615e-04\n",
      "  Step 300/4407 - Loss: 2.7046 | PPL: 14.95 | LR: 2.120422e-04\n",
      "  Step 300/4407 - Loss: 2.7046 | PPL: 14.95 | LR: 2.120422e-04\n",
      "  Step 400/4407 - Loss: 2.7022 | PPL: 14.91 | LR: 2.127229e-04\n",
      "  Step 400/4407 - Loss: 2.7022 | PPL: 14.91 | LR: 2.127229e-04\n",
      "  Step 500/4407 - Loss: 2.7004 | PPL: 14.89 | LR: 2.134037e-04\n",
      "  Step 500/4407 - Loss: 2.7004 | PPL: 14.89 | LR: 2.134037e-04\n",
      "  Step 600/4407 - Loss: 2.6925 | PPL: 14.77 | LR: 2.140844e-04\n",
      "  Step 600/4407 - Loss: 2.6925 | PPL: 14.77 | LR: 2.140844e-04\n",
      "  Step 700/4407 - Loss: 2.6892 | PPL: 14.72 | LR: 2.147651e-04\n",
      "  Step 700/4407 - Loss: 2.6892 | PPL: 14.72 | LR: 2.147651e-04\n",
      "  Step 800/4407 - Loss: 2.6968 | PPL: 14.83 | LR: 2.154459e-04\n",
      "  Step 800/4407 - Loss: 2.6968 | PPL: 14.83 | LR: 2.154459e-04\n",
      "  Step 900/4407 - Loss: 2.6921 | PPL: 14.76 | LR: 2.161266e-04\n",
      "  Step 900/4407 - Loss: 2.6921 | PPL: 14.76 | LR: 2.161266e-04\n",
      "  Step 1000/4407 - Loss: 2.6961 | PPL: 14.82 | LR: 2.168074e-04\n",
      "  Step 1000/4407 - Loss: 2.6961 | PPL: 14.82 | LR: 2.168074e-04\n",
      "  Step 1100/4407 - Loss: 2.6968 | PPL: 14.83 | LR: 2.174881e-04\n",
      "  Step 1100/4407 - Loss: 2.6968 | PPL: 14.83 | LR: 2.174881e-04\n",
      "  Step 1200/4407 - Loss: 2.7036 | PPL: 14.93 | LR: 2.181688e-04\n",
      "  Step 1200/4407 - Loss: 2.7036 | PPL: 14.93 | LR: 2.181688e-04\n",
      "  Step 1300/4407 - Loss: 2.7016 | PPL: 14.90 | LR: 2.188496e-04\n",
      "  Step 1300/4407 - Loss: 2.7016 | PPL: 14.90 | LR: 2.188496e-04\n",
      "  Step 1400/4407 - Loss: 2.7039 | PPL: 14.94 | LR: 2.195303e-04\n",
      "  Step 1400/4407 - Loss: 2.7039 | PPL: 14.94 | LR: 2.195303e-04\n",
      "  Step 1500/4407 - Loss: 2.7019 | PPL: 14.91 | LR: 2.202110e-04\n",
      "  Step 1500/4407 - Loss: 2.7019 | PPL: 14.91 | LR: 2.202110e-04\n",
      "  Step 1600/4407 - Loss: 2.6975 | PPL: 14.84 | LR: 2.208918e-04\n",
      "  Step 1600/4407 - Loss: 2.6975 | PPL: 14.84 | LR: 2.208918e-04\n",
      "  Step 1700/4407 - Loss: 2.6965 | PPL: 14.83 | LR: 2.215725e-04\n",
      "  Step 1700/4407 - Loss: 2.6965 | PPL: 14.83 | LR: 2.215725e-04\n",
      "  Step 1800/4407 - Loss: 2.6977 | PPL: 14.84 | LR: 2.222532e-04\n",
      "  Step 1800/4407 - Loss: 2.6977 | PPL: 14.84 | LR: 2.222532e-04\n",
      "  Step 1900/4407 - Loss: 2.6989 | PPL: 14.86 | LR: 2.229340e-04\n",
      "  Step 1900/4407 - Loss: 2.6989 | PPL: 14.86 | LR: 2.229340e-04\n",
      "  Step 2000/4407 - Loss: 2.6987 | PPL: 14.86 | LR: 2.236147e-04\n",
      "  Step 2000/4407 - Loss: 2.6987 | PPL: 14.86 | LR: 2.236147e-04\n",
      "  Step 2100/4407 - Loss: 2.6988 | PPL: 14.86 | LR: 2.242954e-04\n",
      "  Step 2100/4407 - Loss: 2.6988 | PPL: 14.86 | LR: 2.242954e-04\n",
      "  Step 2200/4407 - Loss: 2.6975 | PPL: 14.84 | LR: 2.249762e-04\n",
      "  Step 2200/4407 - Loss: 2.6975 | PPL: 14.84 | LR: 2.249762e-04\n",
      "  Step 2300/4407 - Loss: 2.6973 | PPL: 14.84 | LR: 2.256569e-04\n",
      "  Step 2300/4407 - Loss: 2.6973 | PPL: 14.84 | LR: 2.256569e-04\n",
      "  Step 2400/4407 - Loss: 2.6960 | PPL: 14.82 | LR: 2.263376e-04\n",
      "  Step 2400/4407 - Loss: 2.6960 | PPL: 14.82 | LR: 2.263376e-04\n",
      "  Step 2500/4407 - Loss: 2.6972 | PPL: 14.84 | LR: 2.270184e-04\n",
      "  Step 2500/4407 - Loss: 2.6972 | PPL: 14.84 | LR: 2.270184e-04\n",
      "  Step 2600/4407 - Loss: 2.6955 | PPL: 14.81 | LR: 2.276991e-04\n",
      "  Step 2600/4407 - Loss: 2.6955 | PPL: 14.81 | LR: 2.276991e-04\n",
      "  Step 2700/4407 - Loss: 2.6948 | PPL: 14.80 | LR: 2.283799e-04\n",
      "  Step 2700/4407 - Loss: 2.6948 | PPL: 14.80 | LR: 2.283799e-04\n",
      "  Step 2800/4407 - Loss: 2.6940 | PPL: 14.79 | LR: 2.290606e-04\n",
      "  Step 2800/4407 - Loss: 2.6940 | PPL: 14.79 | LR: 2.290606e-04\n",
      "  Step 2900/4407 - Loss: 2.6940 | PPL: 14.79 | LR: 2.297413e-04\n",
      "  Step 2900/4407 - Loss: 2.6940 | PPL: 14.79 | LR: 2.297413e-04\n",
      "  Step 3000/4407 - Loss: 2.6915 | PPL: 14.75 | LR: 2.304221e-04\n",
      "  Step 3000/4407 - Loss: 2.6915 | PPL: 14.75 | LR: 2.304221e-04\n",
      "  Step 3100/4407 - Loss: 2.6898 | PPL: 14.73 | LR: 2.311028e-04\n",
      "  Step 3100/4407 - Loss: 2.6898 | PPL: 14.73 | LR: 2.311028e-04\n",
      "  Step 3200/4407 - Loss: 2.6898 | PPL: 14.73 | LR: 2.317835e-04\n",
      "  Step 3200/4407 - Loss: 2.6898 | PPL: 14.73 | LR: 2.317835e-04\n",
      "  Step 3300/4407 - Loss: 2.6890 | PPL: 14.72 | LR: 2.324643e-04\n",
      "  Step 3300/4407 - Loss: 2.6890 | PPL: 14.72 | LR: 2.324643e-04\n",
      "  Step 3400/4407 - Loss: 2.6908 | PPL: 14.74 | LR: 2.331450e-04\n",
      "  Step 3400/4407 - Loss: 2.6908 | PPL: 14.74 | LR: 2.331450e-04\n",
      "  Step 3500/4407 - Loss: 2.6901 | PPL: 14.73 | LR: 2.338257e-04\n",
      "  Step 3500/4407 - Loss: 2.6901 | PPL: 14.73 | LR: 2.338257e-04\n",
      "  Step 3600/4407 - Loss: 2.6908 | PPL: 14.74 | LR: 2.345065e-04\n",
      "  Step 3600/4407 - Loss: 2.6908 | PPL: 14.74 | LR: 2.345065e-04\n",
      "  Step 3700/4407 - Loss: 2.6907 | PPL: 14.74 | LR: 2.351872e-04\n",
      "  Step 3700/4407 - Loss: 2.6907 | PPL: 14.74 | LR: 2.351872e-04\n",
      "  Step 3800/4407 - Loss: 2.6904 | PPL: 14.74 | LR: 2.358679e-04\n",
      "  Step 3800/4407 - Loss: 2.6904 | PPL: 14.74 | LR: 2.358679e-04\n",
      "  Step 3900/4407 - Loss: 2.6888 | PPL: 14.71 | LR: 2.365487e-04\n",
      "  Step 3900/4407 - Loss: 2.6888 | PPL: 14.71 | LR: 2.365487e-04\n",
      "  Step 4000/4407 - Loss: 2.6885 | PPL: 14.71 | LR: 2.372294e-04\n",
      "  Step 4000/4407 - Loss: 2.6885 | PPL: 14.71 | LR: 2.372294e-04\n",
      "  Step 4100/4407 - Loss: 2.6893 | PPL: 14.72 | LR: 2.379101e-04\n",
      "  Step 4100/4407 - Loss: 2.6893 | PPL: 14.72 | LR: 2.379101e-04\n",
      "  Step 4200/4407 - Loss: 2.6895 | PPL: 14.72 | LR: 2.385909e-04\n",
      "  Step 4200/4407 - Loss: 2.6895 | PPL: 14.72 | LR: 2.385909e-04\n",
      "  Step 4300/4407 - Loss: 2.6885 | PPL: 14.71 | LR: 2.392716e-04\n",
      "  Step 4300/4407 - Loss: 2.6885 | PPL: 14.71 | LR: 2.392716e-04\n",
      "  Step 4400/4407 - Loss: 2.6875 | PPL: 14.69 | LR: 2.399523e-04\n",
      "  Step 4400/4407 - Loss: 2.6875 | PPL: 14.69 | LR: 2.399523e-04\n",
      "Epoch 8/100 | Train Loss: 2.6877 | Train PPL: 14.70 | Val Loss: 2.9025 | Val PPL: 18.22\n",
      "Epoch 8/100 | Train Loss: 2.6877 | Train PPL: 14.70 | Val Loss: 2.9025 | Val PPL: 18.22\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 14.70\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 14.70\n",
      "  Step 100/4407 - Loss: 2.6132 | PPL: 13.64 | LR: 2.406807e-04\n",
      "  Step 100/4407 - Loss: 2.6132 | PPL: 13.64 | LR: 2.406807e-04\n",
      "  Step 200/4407 - Loss: 2.6193 | PPL: 13.73 | LR: 2.413615e-04\n",
      "  Step 200/4407 - Loss: 2.6193 | PPL: 13.73 | LR: 2.413615e-04\n",
      "  Step 300/4407 - Loss: 2.6064 | PPL: 13.55 | LR: 2.420422e-04\n",
      "  Step 300/4407 - Loss: 2.6064 | PPL: 13.55 | LR: 2.420422e-04\n",
      "  Step 400/4407 - Loss: 2.6061 | PPL: 13.55 | LR: 2.427229e-04\n",
      "  Step 400/4407 - Loss: 2.6061 | PPL: 13.55 | LR: 2.427229e-04\n",
      "  Step 500/4407 - Loss: 2.6120 | PPL: 13.63 | LR: 2.434037e-04\n",
      "  Step 500/4407 - Loss: 2.6120 | PPL: 13.63 | LR: 2.434037e-04\n",
      "  Step 600/4407 - Loss: 2.6068 | PPL: 13.56 | LR: 2.440844e-04\n",
      "  Step 600/4407 - Loss: 2.6068 | PPL: 13.56 | LR: 2.440844e-04\n",
      "  Step 700/4407 - Loss: 2.6067 | PPL: 13.55 | LR: 2.447651e-04\n",
      "  Step 700/4407 - Loss: 2.6067 | PPL: 13.55 | LR: 2.447651e-04\n",
      "  Step 800/4407 - Loss: 2.6115 | PPL: 13.62 | LR: 2.454459e-04\n",
      "  Step 800/4407 - Loss: 2.6115 | PPL: 13.62 | LR: 2.454459e-04\n",
      "  Step 900/4407 - Loss: 2.6098 | PPL: 13.60 | LR: 2.461266e-04\n",
      "  Step 900/4407 - Loss: 2.6098 | PPL: 13.60 | LR: 2.461266e-04\n",
      "  Step 1000/4407 - Loss: 2.6091 | PPL: 13.59 | LR: 2.468074e-04\n",
      "  Step 1000/4407 - Loss: 2.6091 | PPL: 13.59 | LR: 2.468074e-04\n",
      "  Step 1100/4407 - Loss: 2.6077 | PPL: 13.57 | LR: 2.474881e-04\n",
      "  Step 1100/4407 - Loss: 2.6077 | PPL: 13.57 | LR: 2.474881e-04\n",
      "  Step 1200/4407 - Loss: 2.6077 | PPL: 13.57 | LR: 2.481688e-04\n",
      "  Step 1200/4407 - Loss: 2.6077 | PPL: 13.57 | LR: 2.481688e-04\n",
      "  Step 1300/4407 - Loss: 2.6083 | PPL: 13.58 | LR: 2.488496e-04\n",
      "  Step 1300/4407 - Loss: 2.6083 | PPL: 13.58 | LR: 2.488496e-04\n",
      "  Step 1400/4407 - Loss: 2.6109 | PPL: 13.61 | LR: 2.495303e-04\n",
      "  Step 1400/4407 - Loss: 2.6109 | PPL: 13.61 | LR: 2.495303e-04\n",
      "  Step 1500/4407 - Loss: 2.6122 | PPL: 13.63 | LR: 2.502110e-04\n",
      "  Step 1500/4407 - Loss: 2.6122 | PPL: 13.63 | LR: 2.502110e-04\n",
      "  Step 1600/4407 - Loss: 2.6125 | PPL: 13.63 | LR: 2.508918e-04\n",
      "  Step 1600/4407 - Loss: 2.6125 | PPL: 13.63 | LR: 2.508918e-04\n",
      "  Step 1700/4407 - Loss: 2.6149 | PPL: 13.67 | LR: 2.515725e-04\n",
      "  Step 1700/4407 - Loss: 2.6149 | PPL: 13.67 | LR: 2.515725e-04\n",
      "  Step 1800/4407 - Loss: 2.6132 | PPL: 13.64 | LR: 2.522532e-04\n",
      "  Step 1800/4407 - Loss: 2.6132 | PPL: 13.64 | LR: 2.522532e-04\n",
      "  Step 1900/4407 - Loss: 2.6124 | PPL: 13.63 | LR: 2.529340e-04\n",
      "  Step 1900/4407 - Loss: 2.6124 | PPL: 13.63 | LR: 2.529340e-04\n",
      "  Step 2000/4407 - Loss: 2.6120 | PPL: 13.63 | LR: 2.536147e-04\n",
      "  Step 2000/4407 - Loss: 2.6120 | PPL: 13.63 | LR: 2.536147e-04\n",
      "  Step 2100/4407 - Loss: 2.6114 | PPL: 13.62 | LR: 2.542954e-04\n",
      "  Step 2100/4407 - Loss: 2.6114 | PPL: 13.62 | LR: 2.542954e-04\n",
      "  Step 2200/4407 - Loss: 2.6098 | PPL: 13.60 | LR: 2.549762e-04\n",
      "  Step 2200/4407 - Loss: 2.6098 | PPL: 13.60 | LR: 2.549762e-04\n",
      "  Step 2300/4407 - Loss: 2.6099 | PPL: 13.60 | LR: 2.556569e-04\n",
      "  Step 2300/4407 - Loss: 2.6099 | PPL: 13.60 | LR: 2.556569e-04\n",
      "  Step 2400/4407 - Loss: 2.6079 | PPL: 13.57 | LR: 2.563376e-04\n",
      "  Step 2400/4407 - Loss: 2.6079 | PPL: 13.57 | LR: 2.563376e-04\n",
      "  Step 2500/4407 - Loss: 2.6097 | PPL: 13.60 | LR: 2.570184e-04\n",
      "  Step 2500/4407 - Loss: 2.6097 | PPL: 13.60 | LR: 2.570184e-04\n",
      "  Step 2600/4407 - Loss: 2.6098 | PPL: 13.60 | LR: 2.576991e-04\n",
      "  Step 2600/4407 - Loss: 2.6098 | PPL: 13.60 | LR: 2.576991e-04\n",
      "  Step 2700/4407 - Loss: 2.6097 | PPL: 13.60 | LR: 2.583799e-04\n",
      "  Step 2700/4407 - Loss: 2.6097 | PPL: 13.60 | LR: 2.583799e-04\n",
      "  Step 2800/4407 - Loss: 2.6089 | PPL: 13.58 | LR: 2.590606e-04\n",
      "  Step 2800/4407 - Loss: 2.6089 | PPL: 13.58 | LR: 2.590606e-04\n",
      "  Step 2900/4407 - Loss: 2.6087 | PPL: 13.58 | LR: 2.597413e-04\n",
      "  Step 2900/4407 - Loss: 2.6087 | PPL: 13.58 | LR: 2.597413e-04\n",
      "  Step 3000/4407 - Loss: 2.6078 | PPL: 13.57 | LR: 2.604221e-04\n",
      "  Step 3000/4407 - Loss: 2.6078 | PPL: 13.57 | LR: 2.604221e-04\n",
      "  Step 3100/4407 - Loss: 2.6076 | PPL: 13.57 | LR: 2.611028e-04\n",
      "  Step 3100/4407 - Loss: 2.6076 | PPL: 13.57 | LR: 2.611028e-04\n",
      "  Step 3200/4407 - Loss: 2.6063 | PPL: 13.55 | LR: 2.617835e-04\n",
      "  Step 3200/4407 - Loss: 2.6063 | PPL: 13.55 | LR: 2.617835e-04\n",
      "  Step 3300/4407 - Loss: 2.6073 | PPL: 13.56 | LR: 2.624643e-04\n",
      "  Step 3300/4407 - Loss: 2.6073 | PPL: 13.56 | LR: 2.624643e-04\n",
      "  Step 3400/4407 - Loss: 2.6087 | PPL: 13.58 | LR: 2.631450e-04\n",
      "  Step 3400/4407 - Loss: 2.6087 | PPL: 13.58 | LR: 2.631450e-04\n",
      "  Step 3500/4407 - Loss: 2.6076 | PPL: 13.57 | LR: 2.638257e-04\n",
      "  Step 3500/4407 - Loss: 2.6076 | PPL: 13.57 | LR: 2.638257e-04\n",
      "  Step 3600/4407 - Loss: 2.6086 | PPL: 13.58 | LR: 2.645065e-04\n",
      "  Step 3600/4407 - Loss: 2.6086 | PPL: 13.58 | LR: 2.645065e-04\n",
      "  Step 3700/4407 - Loss: 2.6089 | PPL: 13.58 | LR: 2.651872e-04\n",
      "  Step 3700/4407 - Loss: 2.6089 | PPL: 13.58 | LR: 2.651872e-04\n",
      "  Step 3800/4407 - Loss: 2.6087 | PPL: 13.58 | LR: 2.658679e-04\n",
      "  Step 3800/4407 - Loss: 2.6087 | PPL: 13.58 | LR: 2.658679e-04\n",
      "  Step 3900/4407 - Loss: 2.6092 | PPL: 13.59 | LR: 2.665487e-04\n",
      "  Step 3900/4407 - Loss: 2.6092 | PPL: 13.59 | LR: 2.665487e-04\n",
      "  Step 4000/4407 - Loss: 2.6097 | PPL: 13.60 | LR: 2.672294e-04\n",
      "  Step 4000/4407 - Loss: 2.6097 | PPL: 13.60 | LR: 2.672294e-04\n",
      "  Step 4100/4407 - Loss: 2.6075 | PPL: 13.57 | LR: 2.679101e-04\n",
      "  Step 4100/4407 - Loss: 2.6075 | PPL: 13.57 | LR: 2.679101e-04\n",
      "  Step 4200/4407 - Loss: 2.6073 | PPL: 13.56 | LR: 2.685909e-04\n",
      "  Step 4200/4407 - Loss: 2.6073 | PPL: 13.56 | LR: 2.685909e-04\n",
      "  Step 4300/4407 - Loss: 2.6073 | PPL: 13.56 | LR: 2.692716e-04\n",
      "  Step 4300/4407 - Loss: 2.6073 | PPL: 13.56 | LR: 2.692716e-04\n",
      "  Step 4400/4407 - Loss: 2.6087 | PPL: 13.58 | LR: 2.699523e-04\n",
      "  Step 4400/4407 - Loss: 2.6087 | PPL: 13.58 | LR: 2.699523e-04\n",
      "Epoch 9/100 | Train Loss: 2.6086 | Train PPL: 13.58 | Val Loss: 2.8577 | Val PPL: 17.42\n",
      "Epoch 9/100 | Train Loss: 2.6086 | Train PPL: 13.58 | Val Loss: 2.8577 | Val PPL: 17.42\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 13.58\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 13.58\n",
      "  Step 100/4407 - Loss: 2.5464 | PPL: 12.76 | LR: 2.706807e-04\n",
      "  Step 100/4407 - Loss: 2.5464 | PPL: 12.76 | LR: 2.706807e-04\n",
      "  Step 200/4407 - Loss: 2.5280 | PPL: 12.53 | LR: 2.713615e-04\n",
      "  Step 200/4407 - Loss: 2.5280 | PPL: 12.53 | LR: 2.713615e-04\n",
      "  Step 300/4407 - Loss: 2.5258 | PPL: 12.50 | LR: 2.720422e-04\n",
      "  Step 300/4407 - Loss: 2.5258 | PPL: 12.50 | LR: 2.720422e-04\n",
      "  Step 400/4407 - Loss: 2.5297 | PPL: 12.55 | LR: 2.727229e-04\n",
      "  Step 400/4407 - Loss: 2.5297 | PPL: 12.55 | LR: 2.727229e-04\n",
      "  Step 500/4407 - Loss: 2.5330 | PPL: 12.59 | LR: 2.734037e-04\n",
      "  Step 500/4407 - Loss: 2.5330 | PPL: 12.59 | LR: 2.734037e-04\n",
      "  Step 600/4407 - Loss: 2.5329 | PPL: 12.59 | LR: 2.740844e-04\n",
      "  Step 600/4407 - Loss: 2.5329 | PPL: 12.59 | LR: 2.740844e-04\n",
      "  Step 700/4407 - Loss: 2.5323 | PPL: 12.58 | LR: 2.747651e-04\n",
      "  Step 700/4407 - Loss: 2.5323 | PPL: 12.58 | LR: 2.747651e-04\n",
      "  Step 800/4407 - Loss: 2.5318 | PPL: 12.58 | LR: 2.754459e-04\n",
      "  Step 800/4407 - Loss: 2.5318 | PPL: 12.58 | LR: 2.754459e-04\n",
      "  Step 900/4407 - Loss: 2.5341 | PPL: 12.60 | LR: 2.761266e-04\n",
      "  Step 900/4407 - Loss: 2.5341 | PPL: 12.60 | LR: 2.761266e-04\n",
      "  Step 1000/4407 - Loss: 2.5376 | PPL: 12.65 | LR: 2.768074e-04\n",
      "  Step 1000/4407 - Loss: 2.5376 | PPL: 12.65 | LR: 2.768074e-04\n",
      "  Step 1100/4407 - Loss: 2.5388 | PPL: 12.66 | LR: 2.774881e-04\n",
      "  Step 1100/4407 - Loss: 2.5388 | PPL: 12.66 | LR: 2.774881e-04\n",
      "  Step 1200/4407 - Loss: 2.5377 | PPL: 12.65 | LR: 2.781688e-04\n",
      "  Step 1200/4407 - Loss: 2.5377 | PPL: 12.65 | LR: 2.781688e-04\n",
      "  Step 1300/4407 - Loss: 2.5374 | PPL: 12.65 | LR: 2.788496e-04\n",
      "  Step 1300/4407 - Loss: 2.5374 | PPL: 12.65 | LR: 2.788496e-04\n",
      "  Step 1400/4407 - Loss: 2.5407 | PPL: 12.69 | LR: 2.795303e-04\n",
      "  Step 1400/4407 - Loss: 2.5407 | PPL: 12.69 | LR: 2.795303e-04\n",
      "  Step 1500/4407 - Loss: 2.5428 | PPL: 12.72 | LR: 2.802110e-04\n",
      "  Step 1500/4407 - Loss: 2.5428 | PPL: 12.72 | LR: 2.802110e-04\n",
      "  Step 1600/4407 - Loss: 2.5420 | PPL: 12.70 | LR: 2.808918e-04\n",
      "  Step 1600/4407 - Loss: 2.5420 | PPL: 12.70 | LR: 2.808918e-04\n",
      "  Step 1700/4407 - Loss: 2.5419 | PPL: 12.70 | LR: 2.815725e-04\n",
      "  Step 1700/4407 - Loss: 2.5419 | PPL: 12.70 | LR: 2.815725e-04\n",
      "  Step 1800/4407 - Loss: 2.5445 | PPL: 12.74 | LR: 2.822532e-04\n",
      "  Step 1800/4407 - Loss: 2.5445 | PPL: 12.74 | LR: 2.822532e-04\n",
      "  Step 1900/4407 - Loss: 2.5432 | PPL: 12.72 | LR: 2.829340e-04\n",
      "  Step 1900/4407 - Loss: 2.5432 | PPL: 12.72 | LR: 2.829340e-04\n",
      "  Step 2000/4407 - Loss: 2.5427 | PPL: 12.71 | LR: 2.836147e-04\n",
      "  Step 2000/4407 - Loss: 2.5427 | PPL: 12.71 | LR: 2.836147e-04\n",
      "  Step 2100/4407 - Loss: 2.5408 | PPL: 12.69 | LR: 2.842954e-04\n",
      "  Step 2100/4407 - Loss: 2.5408 | PPL: 12.69 | LR: 2.842954e-04\n",
      "  Step 2200/4407 - Loss: 2.5411 | PPL: 12.69 | LR: 2.849762e-04\n",
      "  Step 2200/4407 - Loss: 2.5411 | PPL: 12.69 | LR: 2.849762e-04\n",
      "  Step 2300/4407 - Loss: 2.5411 | PPL: 12.69 | LR: 2.856569e-04\n",
      "  Step 2300/4407 - Loss: 2.5411 | PPL: 12.69 | LR: 2.856569e-04\n",
      "  Step 2400/4407 - Loss: 2.5430 | PPL: 12.72 | LR: 2.863376e-04\n",
      "  Step 2400/4407 - Loss: 2.5430 | PPL: 12.72 | LR: 2.863376e-04\n",
      "  Step 2500/4407 - Loss: 2.5446 | PPL: 12.74 | LR: 2.870184e-04\n",
      "  Step 2500/4407 - Loss: 2.5446 | PPL: 12.74 | LR: 2.870184e-04\n",
      "  Step 2600/4407 - Loss: 2.5442 | PPL: 12.73 | LR: 2.876991e-04\n",
      "  Step 2600/4407 - Loss: 2.5442 | PPL: 12.73 | LR: 2.876991e-04\n",
      "  Step 2700/4407 - Loss: 2.5440 | PPL: 12.73 | LR: 2.883799e-04\n",
      "  Step 2700/4407 - Loss: 2.5440 | PPL: 12.73 | LR: 2.883799e-04\n",
      "  Step 2800/4407 - Loss: 2.5442 | PPL: 12.73 | LR: 2.890606e-04\n",
      "  Step 2800/4407 - Loss: 2.5442 | PPL: 12.73 | LR: 2.890606e-04\n",
      "  Step 2900/4407 - Loss: 2.5458 | PPL: 12.75 | LR: 2.897413e-04\n",
      "  Step 2900/4407 - Loss: 2.5458 | PPL: 12.75 | LR: 2.897413e-04\n",
      "  Step 3000/4407 - Loss: 2.5473 | PPL: 12.77 | LR: 2.904221e-04\n",
      "  Step 3000/4407 - Loss: 2.5473 | PPL: 12.77 | LR: 2.904221e-04\n",
      "  Step 3100/4407 - Loss: 2.5463 | PPL: 12.76 | LR: 2.911028e-04\n",
      "  Step 3100/4407 - Loss: 2.5463 | PPL: 12.76 | LR: 2.911028e-04\n",
      "  Step 3200/4407 - Loss: 2.5472 | PPL: 12.77 | LR: 2.917835e-04\n",
      "  Step 3200/4407 - Loss: 2.5472 | PPL: 12.77 | LR: 2.917835e-04\n",
      "  Step 3300/4407 - Loss: 2.5469 | PPL: 12.77 | LR: 2.924643e-04\n",
      "  Step 3300/4407 - Loss: 2.5469 | PPL: 12.77 | LR: 2.924643e-04\n",
      "  Step 3400/4407 - Loss: 2.5468 | PPL: 12.77 | LR: 2.931450e-04\n",
      "  Step 3400/4407 - Loss: 2.5468 | PPL: 12.77 | LR: 2.931450e-04\n",
      "  Step 3500/4407 - Loss: 2.5478 | PPL: 12.78 | LR: 2.938257e-04\n",
      "  Step 3500/4407 - Loss: 2.5478 | PPL: 12.78 | LR: 2.938257e-04\n",
      "  Step 3600/4407 - Loss: 2.5471 | PPL: 12.77 | LR: 2.945065e-04\n",
      "  Step 3600/4407 - Loss: 2.5471 | PPL: 12.77 | LR: 2.945065e-04\n",
      "  Step 3700/4407 - Loss: 2.5485 | PPL: 12.79 | LR: 2.951872e-04\n",
      "  Step 3700/4407 - Loss: 2.5485 | PPL: 12.79 | LR: 2.951872e-04\n",
      "  Step 3800/4407 - Loss: 2.5489 | PPL: 12.79 | LR: 2.958679e-04\n",
      "  Step 3800/4407 - Loss: 2.5489 | PPL: 12.79 | LR: 2.958679e-04\n",
      "  Step 3900/4407 - Loss: 2.5498 | PPL: 12.80 | LR: 2.965487e-04\n",
      "  Step 3900/4407 - Loss: 2.5498 | PPL: 12.80 | LR: 2.965487e-04\n",
      "  Step 4000/4407 - Loss: 2.5492 | PPL: 12.80 | LR: 2.972294e-04\n",
      "  Step 4000/4407 - Loss: 2.5492 | PPL: 12.80 | LR: 2.972294e-04\n",
      "  Step 4100/4407 - Loss: 2.5486 | PPL: 12.79 | LR: 2.979101e-04\n",
      "  Step 4100/4407 - Loss: 2.5486 | PPL: 12.79 | LR: 2.979101e-04\n",
      "  Step 4200/4407 - Loss: 2.5487 | PPL: 12.79 | LR: 2.985909e-04\n",
      "  Step 4200/4407 - Loss: 2.5487 | PPL: 12.79 | LR: 2.985909e-04\n",
      "  Step 4300/4407 - Loss: 2.5490 | PPL: 12.79 | LR: 2.992716e-04\n",
      "  Step 4300/4407 - Loss: 2.5490 | PPL: 12.79 | LR: 2.992716e-04\n",
      "  Step 4400/4407 - Loss: 2.5485 | PPL: 12.79 | LR: 2.999523e-04\n",
      "  Step 4400/4407 - Loss: 2.5485 | PPL: 12.79 | LR: 2.999523e-04\n",
      "Epoch 10/100 | Train Loss: 2.5483 | Train PPL: 12.79 | Val Loss: 2.8173 | Val PPL: 16.73\n",
      "Epoch 10/100 | Train Loss: 2.5483 | Train PPL: 12.79 | Val Loss: 2.8173 | Val PPL: 16.73\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 12.79\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 12.79\n",
      "  Step 100/4407 - Loss: 2.4499 | PPL: 11.59 | LR: 3.000000e-04\n",
      "  Step 100/4407 - Loss: 2.4499 | PPL: 11.59 | LR: 3.000000e-04\n",
      "  Step 200/4407 - Loss: 2.4502 | PPL: 11.59 | LR: 2.999998e-04\n",
      "  Step 200/4407 - Loss: 2.4502 | PPL: 11.59 | LR: 2.999998e-04\n",
      "  Step 300/4407 - Loss: 2.4636 | PPL: 11.75 | LR: 2.999996e-04\n",
      "  Step 300/4407 - Loss: 2.4636 | PPL: 11.75 | LR: 2.999996e-04\n",
      "  Step 400/4407 - Loss: 2.4747 | PPL: 11.88 | LR: 2.999992e-04\n",
      "  Step 400/4407 - Loss: 2.4747 | PPL: 11.88 | LR: 2.999992e-04\n",
      "  Step 500/4407 - Loss: 2.4721 | PPL: 11.85 | LR: 2.999988e-04\n",
      "  Step 500/4407 - Loss: 2.4721 | PPL: 11.85 | LR: 2.999988e-04\n",
      "  Step 600/4407 - Loss: 2.4738 | PPL: 11.87 | LR: 2.999983e-04\n",
      "  Step 600/4407 - Loss: 2.4738 | PPL: 11.87 | LR: 2.999983e-04\n",
      "  Step 700/4407 - Loss: 2.4739 | PPL: 11.87 | LR: 2.999977e-04\n",
      "  Step 700/4407 - Loss: 2.4739 | PPL: 11.87 | LR: 2.999977e-04\n",
      "  Step 800/4407 - Loss: 2.4830 | PPL: 11.98 | LR: 2.999970e-04\n",
      "  Step 800/4407 - Loss: 2.4830 | PPL: 11.98 | LR: 2.999970e-04\n",
      "  Step 900/4407 - Loss: 2.4813 | PPL: 11.96 | LR: 2.999962e-04\n",
      "  Step 900/4407 - Loss: 2.4813 | PPL: 11.96 | LR: 2.999962e-04\n",
      "  Step 1000/4407 - Loss: 2.4788 | PPL: 11.93 | LR: 2.999953e-04\n",
      "  Step 1000/4407 - Loss: 2.4788 | PPL: 11.93 | LR: 2.999953e-04\n",
      "  Step 1100/4407 - Loss: 2.4777 | PPL: 11.91 | LR: 2.999943e-04\n",
      "  Step 1100/4407 - Loss: 2.4777 | PPL: 11.91 | LR: 2.999943e-04\n",
      "  Step 1200/4407 - Loss: 2.4827 | PPL: 11.97 | LR: 2.999932e-04\n",
      "  Step 1200/4407 - Loss: 2.4827 | PPL: 11.97 | LR: 2.999932e-04\n",
      "  Step 1300/4407 - Loss: 2.4850 | PPL: 12.00 | LR: 2.999920e-04\n",
      "  Step 1300/4407 - Loss: 2.4850 | PPL: 12.00 | LR: 2.999920e-04\n",
      "  Step 1400/4407 - Loss: 2.4846 | PPL: 12.00 | LR: 2.999908e-04\n",
      "  Step 1400/4407 - Loss: 2.4846 | PPL: 12.00 | LR: 2.999908e-04\n",
      "  Step 1500/4407 - Loss: 2.4846 | PPL: 12.00 | LR: 2.999894e-04\n",
      "  Step 1500/4407 - Loss: 2.4846 | PPL: 12.00 | LR: 2.999894e-04\n",
      "  Step 1600/4407 - Loss: 2.4874 | PPL: 12.03 | LR: 2.999880e-04\n",
      "  Step 1600/4407 - Loss: 2.4874 | PPL: 12.03 | LR: 2.999880e-04\n",
      "  Step 1700/4407 - Loss: 2.4896 | PPL: 12.06 | LR: 2.999864e-04\n",
      "  Step 1700/4407 - Loss: 2.4896 | PPL: 12.06 | LR: 2.999864e-04\n",
      "  Step 1800/4407 - Loss: 2.4898 | PPL: 12.06 | LR: 2.999848e-04\n",
      "  Step 1800/4407 - Loss: 2.4898 | PPL: 12.06 | LR: 2.999848e-04\n",
      "  Step 1900/4407 - Loss: 2.4882 | PPL: 12.04 | LR: 2.999830e-04\n",
      "  Step 1900/4407 - Loss: 2.4882 | PPL: 12.04 | LR: 2.999830e-04\n",
      "  Step 2000/4407 - Loss: 2.4900 | PPL: 12.06 | LR: 2.999812e-04\n",
      "  Step 2000/4407 - Loss: 2.4900 | PPL: 12.06 | LR: 2.999812e-04\n",
      "  Step 2100/4407 - Loss: 2.4901 | PPL: 12.06 | LR: 2.999792e-04\n",
      "  Step 2100/4407 - Loss: 2.4901 | PPL: 12.06 | LR: 2.999792e-04\n",
      "  Step 2200/4407 - Loss: 2.4907 | PPL: 12.07 | LR: 2.999772e-04\n",
      "  Step 2200/4407 - Loss: 2.4907 | PPL: 12.07 | LR: 2.999772e-04\n",
      "  Step 2300/4407 - Loss: 2.4921 | PPL: 12.09 | LR: 2.999751e-04\n",
      "  Step 2300/4407 - Loss: 2.4921 | PPL: 12.09 | LR: 2.999751e-04\n",
      "  Step 2400/4407 - Loss: 2.4931 | PPL: 12.10 | LR: 2.999729e-04\n",
      "  Step 2400/4407 - Loss: 2.4931 | PPL: 12.10 | LR: 2.999729e-04\n",
      "  Step 2500/4407 - Loss: 2.4918 | PPL: 12.08 | LR: 2.999706e-04\n",
      "  Step 2500/4407 - Loss: 2.4918 | PPL: 12.08 | LR: 2.999706e-04\n",
      "  Step 2600/4407 - Loss: 2.4931 | PPL: 12.10 | LR: 2.999682e-04\n",
      "  Step 2600/4407 - Loss: 2.4931 | PPL: 12.10 | LR: 2.999682e-04\n",
      "  Step 2700/4407 - Loss: 2.4939 | PPL: 12.11 | LR: 2.999657e-04\n",
      "  Step 2700/4407 - Loss: 2.4939 | PPL: 12.11 | LR: 2.999657e-04\n",
      "  Step 2800/4407 - Loss: 2.4953 | PPL: 12.13 | LR: 2.999631e-04\n",
      "  Step 2800/4407 - Loss: 2.4953 | PPL: 12.13 | LR: 2.999631e-04\n",
      "  Step 2900/4407 - Loss: 2.4955 | PPL: 12.13 | LR: 2.999604e-04\n",
      "  Step 2900/4407 - Loss: 2.4955 | PPL: 12.13 | LR: 2.999604e-04\n",
      "  Step 3000/4407 - Loss: 2.4968 | PPL: 12.14 | LR: 2.999577e-04\n",
      "  Step 3000/4407 - Loss: 2.4968 | PPL: 12.14 | LR: 2.999577e-04\n",
      "  Step 3100/4407 - Loss: 2.4942 | PPL: 12.11 | LR: 2.999548e-04\n",
      "  Step 3100/4407 - Loss: 2.4942 | PPL: 12.11 | LR: 2.999548e-04\n",
      "  Step 3200/4407 - Loss: 2.4930 | PPL: 12.10 | LR: 2.999518e-04\n",
      "  Step 3200/4407 - Loss: 2.4930 | PPL: 12.10 | LR: 2.999518e-04\n",
      "  Step 3300/4407 - Loss: 2.4926 | PPL: 12.09 | LR: 2.999488e-04\n",
      "  Step 3300/4407 - Loss: 2.4926 | PPL: 12.09 | LR: 2.999488e-04\n",
      "  Step 3400/4407 - Loss: 2.4926 | PPL: 12.09 | LR: 2.999456e-04\n",
      "  Step 3400/4407 - Loss: 2.4926 | PPL: 12.09 | LR: 2.999456e-04\n",
      "  Step 3500/4407 - Loss: 2.4932 | PPL: 12.10 | LR: 2.999424e-04\n",
      "  Step 3500/4407 - Loss: 2.4932 | PPL: 12.10 | LR: 2.999424e-04\n",
      "  Step 3600/4407 - Loss: 2.4941 | PPL: 12.11 | LR: 2.999390e-04\n",
      "  Step 3600/4407 - Loss: 2.4941 | PPL: 12.11 | LR: 2.999390e-04\n",
      "  Step 3700/4407 - Loss: 2.4949 | PPL: 12.12 | LR: 2.999356e-04\n",
      "  Step 3700/4407 - Loss: 2.4949 | PPL: 12.12 | LR: 2.999356e-04\n",
      "  Step 3800/4407 - Loss: 2.4948 | PPL: 12.12 | LR: 2.999321e-04\n",
      "  Step 3800/4407 - Loss: 2.4948 | PPL: 12.12 | LR: 2.999321e-04\n",
      "  Step 3900/4407 - Loss: 2.4954 | PPL: 12.13 | LR: 2.999284e-04\n",
      "  Step 3900/4407 - Loss: 2.4954 | PPL: 12.13 | LR: 2.999284e-04\n",
      "  Step 4000/4407 - Loss: 2.4960 | PPL: 12.13 | LR: 2.999247e-04\n",
      "  Step 4000/4407 - Loss: 2.4960 | PPL: 12.13 | LR: 2.999247e-04\n",
      "  Step 4100/4407 - Loss: 2.4949 | PPL: 12.12 | LR: 2.999209e-04\n",
      "  Step 4100/4407 - Loss: 2.4949 | PPL: 12.12 | LR: 2.999209e-04\n",
      "  Step 4200/4407 - Loss: 2.4953 | PPL: 12.12 | LR: 2.999170e-04\n",
      "  Step 4200/4407 - Loss: 2.4953 | PPL: 12.12 | LR: 2.999170e-04\n",
      "  Step 4300/4407 - Loss: 2.4948 | PPL: 12.12 | LR: 2.999130e-04\n",
      "  Step 4300/4407 - Loss: 2.4948 | PPL: 12.12 | LR: 2.999130e-04\n",
      "  Step 4400/4407 - Loss: 2.4952 | PPL: 12.12 | LR: 2.999089e-04\n",
      "  Step 4400/4407 - Loss: 2.4952 | PPL: 12.12 | LR: 2.999089e-04\n",
      "Epoch 11/100 | Train Loss: 2.4953 | Train PPL: 12.12 | Val Loss: 2.7887 | Val PPL: 16.26\n",
      "Epoch 11/100 | Train Loss: 2.4953 | Train PPL: 12.12 | Val Loss: 2.7887 | Val PPL: 16.26\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 12.12\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 12.12\n",
      "  Step 100/4407 - Loss: 2.4285 | PPL: 11.34 | LR: 2.999044e-04\n",
      "  Step 100/4407 - Loss: 2.4285 | PPL: 11.34 | LR: 2.999044e-04\n",
      "  Step 200/4407 - Loss: 2.4325 | PPL: 11.39 | LR: 2.999001e-04\n",
      "  Step 200/4407 - Loss: 2.4325 | PPL: 11.39 | LR: 2.999001e-04\n",
      "  Step 300/4407 - Loss: 2.4336 | PPL: 11.40 | LR: 2.998958e-04\n",
      "  Step 300/4407 - Loss: 2.4336 | PPL: 11.40 | LR: 2.998958e-04\n",
      "  Step 400/4407 - Loss: 2.4412 | PPL: 11.49 | LR: 2.998913e-04\n",
      "  Step 400/4407 - Loss: 2.4412 | PPL: 11.49 | LR: 2.998913e-04\n",
      "  Step 500/4407 - Loss: 2.4354 | PPL: 11.42 | LR: 2.998867e-04\n",
      "  Step 500/4407 - Loss: 2.4354 | PPL: 11.42 | LR: 2.998867e-04\n",
      "  Step 600/4407 - Loss: 2.4359 | PPL: 11.43 | LR: 2.998821e-04\n",
      "  Step 600/4407 - Loss: 2.4359 | PPL: 11.43 | LR: 2.998821e-04\n",
      "  Step 700/4407 - Loss: 2.4411 | PPL: 11.49 | LR: 2.998773e-04\n",
      "  Step 700/4407 - Loss: 2.4411 | PPL: 11.49 | LR: 2.998773e-04\n",
      "  Step 800/4407 - Loss: 2.4397 | PPL: 11.47 | LR: 2.998724e-04\n",
      "  Step 800/4407 - Loss: 2.4397 | PPL: 11.47 | LR: 2.998724e-04\n",
      "  Step 900/4407 - Loss: 2.4393 | PPL: 11.47 | LR: 2.998675e-04\n",
      "  Step 900/4407 - Loss: 2.4393 | PPL: 11.47 | LR: 2.998675e-04\n",
      "  Step 1000/4407 - Loss: 2.4437 | PPL: 11.52 | LR: 2.998625e-04\n",
      "  Step 1000/4407 - Loss: 2.4437 | PPL: 11.52 | LR: 2.998625e-04\n",
      "  Step 1100/4407 - Loss: 2.4429 | PPL: 11.51 | LR: 2.998573e-04\n",
      "  Step 1100/4407 - Loss: 2.4429 | PPL: 11.51 | LR: 2.998573e-04\n",
      "  Step 1200/4407 - Loss: 2.4454 | PPL: 11.53 | LR: 2.998521e-04\n",
      "  Step 1200/4407 - Loss: 2.4454 | PPL: 11.53 | LR: 2.998521e-04\n",
      "  Step 1300/4407 - Loss: 2.4426 | PPL: 11.50 | LR: 2.998468e-04\n",
      "  Step 1300/4407 - Loss: 2.4426 | PPL: 11.50 | LR: 2.998468e-04\n",
      "  Step 1400/4407 - Loss: 2.4421 | PPL: 11.50 | LR: 2.998414e-04\n",
      "  Step 1400/4407 - Loss: 2.4421 | PPL: 11.50 | LR: 2.998414e-04\n",
      "  Step 1500/4407 - Loss: 2.4406 | PPL: 11.48 | LR: 2.998358e-04\n",
      "  Step 1500/4407 - Loss: 2.4406 | PPL: 11.48 | LR: 2.998358e-04\n",
      "  Step 1600/4407 - Loss: 2.4417 | PPL: 11.49 | LR: 2.998302e-04\n",
      "  Step 1600/4407 - Loss: 2.4417 | PPL: 11.49 | LR: 2.998302e-04\n",
      "  Step 1700/4407 - Loss: 2.4411 | PPL: 11.49 | LR: 2.998245e-04\n",
      "  Step 1700/4407 - Loss: 2.4411 | PPL: 11.49 | LR: 2.998245e-04\n",
      "  Step 1800/4407 - Loss: 2.4416 | PPL: 11.49 | LR: 2.998188e-04\n",
      "  Step 1800/4407 - Loss: 2.4416 | PPL: 11.49 | LR: 2.998188e-04\n",
      "  Step 1900/4407 - Loss: 2.4417 | PPL: 11.49 | LR: 2.998129e-04\n",
      "  Step 1900/4407 - Loss: 2.4417 | PPL: 11.49 | LR: 2.998129e-04\n",
      "  Step 2000/4407 - Loss: 2.4419 | PPL: 11.50 | LR: 2.998069e-04\n",
      "  Step 2000/4407 - Loss: 2.4419 | PPL: 11.50 | LR: 2.998069e-04\n",
      "  Step 2100/4407 - Loss: 2.4414 | PPL: 11.49 | LR: 2.998008e-04\n",
      "  Step 2100/4407 - Loss: 2.4414 | PPL: 11.49 | LR: 2.998008e-04\n",
      "  Step 2200/4407 - Loss: 2.4420 | PPL: 11.50 | LR: 2.997946e-04\n",
      "  Step 2200/4407 - Loss: 2.4420 | PPL: 11.50 | LR: 2.997946e-04\n",
      "  Step 2300/4407 - Loss: 2.4406 | PPL: 11.48 | LR: 2.997884e-04\n",
      "  Step 2300/4407 - Loss: 2.4406 | PPL: 11.48 | LR: 2.997884e-04\n",
      "  Step 2400/4407 - Loss: 2.4405 | PPL: 11.48 | LR: 2.997820e-04\n",
      "  Step 2400/4407 - Loss: 2.4405 | PPL: 11.48 | LR: 2.997820e-04\n",
      "  Step 2500/4407 - Loss: 2.4420 | PPL: 11.50 | LR: 2.997756e-04\n",
      "  Step 2500/4407 - Loss: 2.4420 | PPL: 11.50 | LR: 2.997756e-04\n",
      "  Step 2600/4407 - Loss: 2.4422 | PPL: 11.50 | LR: 2.997690e-04\n",
      "  Step 2600/4407 - Loss: 2.4422 | PPL: 11.50 | LR: 2.997690e-04\n",
      "  Step 2700/4407 - Loss: 2.4438 | PPL: 11.52 | LR: 2.997624e-04\n",
      "  Step 2700/4407 - Loss: 2.4438 | PPL: 11.52 | LR: 2.997624e-04\n",
      "  Step 2800/4407 - Loss: 2.4434 | PPL: 11.51 | LR: 2.997557e-04\n",
      "  Step 2800/4407 - Loss: 2.4434 | PPL: 11.51 | LR: 2.997557e-04\n",
      "  Step 2900/4407 - Loss: 2.4443 | PPL: 11.52 | LR: 2.997488e-04\n",
      "  Step 2900/4407 - Loss: 2.4443 | PPL: 11.52 | LR: 2.997488e-04\n",
      "  Step 3000/4407 - Loss: 2.4458 | PPL: 11.54 | LR: 2.997419e-04\n",
      "  Step 3000/4407 - Loss: 2.4458 | PPL: 11.54 | LR: 2.997419e-04\n",
      "  Step 3100/4407 - Loss: 2.4472 | PPL: 11.56 | LR: 2.997349e-04\n",
      "  Step 3100/4407 - Loss: 2.4472 | PPL: 11.56 | LR: 2.997349e-04\n",
      "  Step 3200/4407 - Loss: 2.4476 | PPL: 11.56 | LR: 2.997278e-04\n",
      "  Step 3200/4407 - Loss: 2.4476 | PPL: 11.56 | LR: 2.997278e-04\n",
      "  Step 3300/4407 - Loss: 2.4476 | PPL: 11.56 | LR: 2.997206e-04\n",
      "  Step 3300/4407 - Loss: 2.4476 | PPL: 11.56 | LR: 2.997206e-04\n",
      "  Step 3400/4407 - Loss: 2.4468 | PPL: 11.55 | LR: 2.997133e-04\n",
      "  Step 3400/4407 - Loss: 2.4468 | PPL: 11.55 | LR: 2.997133e-04\n",
      "  Step 3500/4407 - Loss: 2.4465 | PPL: 11.55 | LR: 2.997059e-04\n",
      "  Step 3500/4407 - Loss: 2.4465 | PPL: 11.55 | LR: 2.997059e-04\n",
      "  Step 3600/4407 - Loss: 2.4472 | PPL: 11.56 | LR: 2.996984e-04\n",
      "  Step 3600/4407 - Loss: 2.4472 | PPL: 11.56 | LR: 2.996984e-04\n",
      "  Step 3700/4407 - Loss: 2.4472 | PPL: 11.56 | LR: 2.996909e-04\n",
      "  Step 3700/4407 - Loss: 2.4472 | PPL: 11.56 | LR: 2.996909e-04\n",
      "  Step 3800/4407 - Loss: 2.4464 | PPL: 11.55 | LR: 2.996832e-04\n",
      "  Step 3800/4407 - Loss: 2.4464 | PPL: 11.55 | LR: 2.996832e-04\n",
      "  Step 3900/4407 - Loss: 2.4465 | PPL: 11.55 | LR: 2.996754e-04\n",
      "  Step 3900/4407 - Loss: 2.4465 | PPL: 11.55 | LR: 2.996754e-04\n",
      "  Step 4000/4407 - Loss: 2.4470 | PPL: 11.55 | LR: 2.996676e-04\n",
      "  Step 4000/4407 - Loss: 2.4470 | PPL: 11.55 | LR: 2.996676e-04\n",
      "  Step 4100/4407 - Loss: 2.4472 | PPL: 11.56 | LR: 2.996596e-04\n",
      "  Step 4100/4407 - Loss: 2.4472 | PPL: 11.56 | LR: 2.996596e-04\n",
      "  Step 4200/4407 - Loss: 2.4469 | PPL: 11.55 | LR: 2.996516e-04\n",
      "  Step 4200/4407 - Loss: 2.4469 | PPL: 11.55 | LR: 2.996516e-04\n",
      "  Step 4300/4407 - Loss: 2.4454 | PPL: 11.53 | LR: 2.996434e-04\n",
      "  Step 4300/4407 - Loss: 2.4454 | PPL: 11.53 | LR: 2.996434e-04\n",
      "  Step 4400/4407 - Loss: 2.4450 | PPL: 11.53 | LR: 2.996352e-04\n",
      "  Step 4400/4407 - Loss: 2.4450 | PPL: 11.53 | LR: 2.996352e-04\n",
      "Epoch 12/100 | Train Loss: 2.4451 | Train PPL: 11.53 | Val Loss: 2.7608 | Val PPL: 15.81\n",
      "Epoch 12/100 | Train Loss: 2.4451 | Train PPL: 11.53 | Val Loss: 2.7608 | Val PPL: 15.81\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 11.53\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 11.53\n",
      "  Step 100/4407 - Loss: 2.3724 | PPL: 10.72 | LR: 2.996263e-04\n",
      "  Step 100/4407 - Loss: 2.3724 | PPL: 10.72 | LR: 2.996263e-04\n",
      "  Step 200/4407 - Loss: 2.3604 | PPL: 10.60 | LR: 2.996178e-04\n",
      "  Step 200/4407 - Loss: 2.3604 | PPL: 10.60 | LR: 2.996178e-04\n",
      "  Step 300/4407 - Loss: 2.3692 | PPL: 10.69 | LR: 2.996093e-04\n",
      "  Step 300/4407 - Loss: 2.3692 | PPL: 10.69 | LR: 2.996093e-04\n",
      "  Step 400/4407 - Loss: 2.3698 | PPL: 10.70 | LR: 2.996007e-04\n",
      "  Step 400/4407 - Loss: 2.3698 | PPL: 10.70 | LR: 2.996007e-04\n",
      "  Step 500/4407 - Loss: 2.3723 | PPL: 10.72 | LR: 2.995920e-04\n",
      "  Step 500/4407 - Loss: 2.3723 | PPL: 10.72 | LR: 2.995920e-04\n",
      "  Step 600/4407 - Loss: 2.3693 | PPL: 10.69 | LR: 2.995832e-04\n",
      "  Step 600/4407 - Loss: 2.3693 | PPL: 10.69 | LR: 2.995832e-04\n",
      "  Step 700/4407 - Loss: 2.3701 | PPL: 10.70 | LR: 2.995743e-04\n",
      "  Step 700/4407 - Loss: 2.3701 | PPL: 10.70 | LR: 2.995743e-04\n",
      "  Step 800/4407 - Loss: 2.3729 | PPL: 10.73 | LR: 2.995653e-04\n",
      "  Step 800/4407 - Loss: 2.3729 | PPL: 10.73 | LR: 2.995653e-04\n",
      "  Step 900/4407 - Loss: 2.3789 | PPL: 10.79 | LR: 2.995562e-04\n",
      "  Step 900/4407 - Loss: 2.3789 | PPL: 10.79 | LR: 2.995562e-04\n",
      "  Step 1000/4407 - Loss: 2.3838 | PPL: 10.85 | LR: 2.995470e-04\n",
      "  Step 1000/4407 - Loss: 2.3838 | PPL: 10.85 | LR: 2.995470e-04\n",
      "  Step 1100/4407 - Loss: 2.3798 | PPL: 10.80 | LR: 2.995378e-04\n",
      "  Step 1100/4407 - Loss: 2.3798 | PPL: 10.80 | LR: 2.995378e-04\n",
      "  Step 1200/4407 - Loss: 2.3805 | PPL: 10.81 | LR: 2.995284e-04\n",
      "  Step 1200/4407 - Loss: 2.3805 | PPL: 10.81 | LR: 2.995284e-04\n",
      "  Step 1300/4407 - Loss: 2.3821 | PPL: 10.83 | LR: 2.995189e-04\n",
      "  Step 1300/4407 - Loss: 2.3821 | PPL: 10.83 | LR: 2.995189e-04\n",
      "  Step 1400/4407 - Loss: 2.3848 | PPL: 10.86 | LR: 2.995094e-04\n",
      "  Step 1400/4407 - Loss: 2.3848 | PPL: 10.86 | LR: 2.995094e-04\n",
      "  Step 1500/4407 - Loss: 2.3867 | PPL: 10.88 | LR: 2.994997e-04\n",
      "  Step 1500/4407 - Loss: 2.3867 | PPL: 10.88 | LR: 2.994997e-04\n",
      "  Step 1600/4407 - Loss: 2.3914 | PPL: 10.93 | LR: 2.994900e-04\n",
      "  Step 1600/4407 - Loss: 2.3914 | PPL: 10.93 | LR: 2.994900e-04\n",
      "  Step 1700/4407 - Loss: 2.3942 | PPL: 10.96 | LR: 2.994802e-04\n",
      "  Step 1700/4407 - Loss: 2.3942 | PPL: 10.96 | LR: 2.994802e-04\n",
      "  Step 1800/4407 - Loss: 2.3964 | PPL: 10.98 | LR: 2.994702e-04\n",
      "  Step 1800/4407 - Loss: 2.3964 | PPL: 10.98 | LR: 2.994702e-04\n",
      "  Step 1900/4407 - Loss: 2.3976 | PPL: 11.00 | LR: 2.994602e-04\n",
      "  Step 1900/4407 - Loss: 2.3976 | PPL: 11.00 | LR: 2.994602e-04\n",
      "  Step 2000/4407 - Loss: 2.3956 | PPL: 10.97 | LR: 2.994501e-04\n",
      "  Step 2000/4407 - Loss: 2.3956 | PPL: 10.97 | LR: 2.994501e-04\n",
      "  Step 2100/4407 - Loss: 2.3967 | PPL: 10.99 | LR: 2.994399e-04\n",
      "  Step 2100/4407 - Loss: 2.3967 | PPL: 10.99 | LR: 2.994399e-04\n",
      "  Step 2200/4407 - Loss: 2.3989 | PPL: 11.01 | LR: 2.994296e-04\n",
      "  Step 2200/4407 - Loss: 2.3989 | PPL: 11.01 | LR: 2.994296e-04\n",
      "  Step 2300/4407 - Loss: 2.4003 | PPL: 11.03 | LR: 2.994192e-04\n",
      "  Step 2300/4407 - Loss: 2.4003 | PPL: 11.03 | LR: 2.994192e-04\n",
      "  Step 2400/4407 - Loss: 2.3982 | PPL: 11.00 | LR: 2.994087e-04\n",
      "  Step 2400/4407 - Loss: 2.3982 | PPL: 11.00 | LR: 2.994087e-04\n",
      "  Step 2500/4407 - Loss: 2.3985 | PPL: 11.01 | LR: 2.993981e-04\n",
      "  Step 2500/4407 - Loss: 2.3985 | PPL: 11.01 | LR: 2.993981e-04\n",
      "  Step 2600/4407 - Loss: 2.3994 | PPL: 11.02 | LR: 2.993874e-04\n",
      "  Step 2600/4407 - Loss: 2.3994 | PPL: 11.02 | LR: 2.993874e-04\n",
      "  Step 2700/4407 - Loss: 2.3988 | PPL: 11.01 | LR: 2.993766e-04\n",
      "  Step 2700/4407 - Loss: 2.3988 | PPL: 11.01 | LR: 2.993766e-04\n",
      "  Step 2800/4407 - Loss: 2.3986 | PPL: 11.01 | LR: 2.993658e-04\n",
      "  Step 2800/4407 - Loss: 2.3986 | PPL: 11.01 | LR: 2.993658e-04\n",
      "  Step 2900/4407 - Loss: 2.3996 | PPL: 11.02 | LR: 2.993548e-04\n",
      "  Step 2900/4407 - Loss: 2.3996 | PPL: 11.02 | LR: 2.993548e-04\n",
      "  Step 3000/4407 - Loss: 2.3981 | PPL: 11.00 | LR: 2.993438e-04\n",
      "  Step 3000/4407 - Loss: 2.3981 | PPL: 11.00 | LR: 2.993438e-04\n",
      "  Step 3100/4407 - Loss: 2.3981 | PPL: 11.00 | LR: 2.993326e-04\n",
      "  Step 3100/4407 - Loss: 2.3981 | PPL: 11.00 | LR: 2.993326e-04\n",
      "  Step 3200/4407 - Loss: 2.3992 | PPL: 11.01 | LR: 2.993214e-04\n",
      "  Step 3200/4407 - Loss: 2.3992 | PPL: 11.01 | LR: 2.993214e-04\n",
      "  Step 3300/4407 - Loss: 2.3993 | PPL: 11.02 | LR: 2.993100e-04\n",
      "  Step 3300/4407 - Loss: 2.3993 | PPL: 11.02 | LR: 2.993100e-04\n",
      "  Step 3400/4407 - Loss: 2.3987 | PPL: 11.01 | LR: 2.992986e-04\n",
      "  Step 3400/4407 - Loss: 2.3987 | PPL: 11.01 | LR: 2.992986e-04\n",
      "  Step 3500/4407 - Loss: 2.4003 | PPL: 11.03 | LR: 2.992871e-04\n",
      "  Step 3500/4407 - Loss: 2.4003 | PPL: 11.03 | LR: 2.992871e-04\n",
      "  Step 3600/4407 - Loss: 2.4025 | PPL: 11.05 | LR: 2.992755e-04\n",
      "  Step 3600/4407 - Loss: 2.4025 | PPL: 11.05 | LR: 2.992755e-04\n",
      "  Step 3700/4407 - Loss: 2.4043 | PPL: 11.07 | LR: 2.992637e-04\n",
      "  Step 3700/4407 - Loss: 2.4043 | PPL: 11.07 | LR: 2.992637e-04\n",
      "  Step 3800/4407 - Loss: 2.4049 | PPL: 11.08 | LR: 2.992519e-04\n",
      "  Step 3800/4407 - Loss: 2.4049 | PPL: 11.08 | LR: 2.992519e-04\n",
      "  Step 3900/4407 - Loss: 2.4045 | PPL: 11.07 | LR: 2.992400e-04\n",
      "  Step 3900/4407 - Loss: 2.4045 | PPL: 11.07 | LR: 2.992400e-04\n",
      "  Step 4000/4407 - Loss: 2.4054 | PPL: 11.08 | LR: 2.992281e-04\n",
      "  Step 4000/4407 - Loss: 2.4054 | PPL: 11.08 | LR: 2.992281e-04\n",
      "  Step 4100/4407 - Loss: 2.4050 | PPL: 11.08 | LR: 2.992160e-04\n",
      "  Step 4100/4407 - Loss: 2.4050 | PPL: 11.08 | LR: 2.992160e-04\n",
      "  Step 4200/4407 - Loss: 2.4056 | PPL: 11.08 | LR: 2.992038e-04\n",
      "  Step 4200/4407 - Loss: 2.4056 | PPL: 11.08 | LR: 2.992038e-04\n",
      "  Step 4300/4407 - Loss: 2.4059 | PPL: 11.09 | LR: 2.991915e-04\n",
      "  Step 4300/4407 - Loss: 2.4059 | PPL: 11.09 | LR: 2.991915e-04\n",
      "  Step 4400/4407 - Loss: 2.4051 | PPL: 11.08 | LR: 2.991792e-04\n",
      "  Step 4400/4407 - Loss: 2.4051 | PPL: 11.08 | LR: 2.991792e-04\n",
      "Epoch 13/100 | Train Loss: 2.4049 | Train PPL: 11.08 | Val Loss: 2.7398 | Val PPL: 15.48\n",
      "Epoch 13/100 | Train Loss: 2.4049 | Train PPL: 11.08 | Val Loss: 2.7398 | Val PPL: 15.48\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 11.08\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 11.08\n",
      "  Step 100/4407 - Loss: 2.3532 | PPL: 10.52 | LR: 2.991658e-04\n",
      "  Step 100/4407 - Loss: 2.3532 | PPL: 10.52 | LR: 2.991658e-04\n",
      "  Step 200/4407 - Loss: 2.3580 | PPL: 10.57 | LR: 2.991533e-04\n",
      "  Step 200/4407 - Loss: 2.3580 | PPL: 10.57 | LR: 2.991533e-04\n",
      "  Step 300/4407 - Loss: 2.3549 | PPL: 10.54 | LR: 2.991406e-04\n",
      "  Step 300/4407 - Loss: 2.3549 | PPL: 10.54 | LR: 2.991406e-04\n",
      "  Step 400/4407 - Loss: 2.3542 | PPL: 10.53 | LR: 2.991279e-04\n",
      "  Step 400/4407 - Loss: 2.3542 | PPL: 10.53 | LR: 2.991279e-04\n",
      "  Step 500/4407 - Loss: 2.3557 | PPL: 10.55 | LR: 2.991150e-04\n",
      "  Step 500/4407 - Loss: 2.3557 | PPL: 10.55 | LR: 2.991150e-04\n",
      "  Step 600/4407 - Loss: 2.3655 | PPL: 10.65 | LR: 2.991021e-04\n",
      "  Step 600/4407 - Loss: 2.3655 | PPL: 10.65 | LR: 2.991021e-04\n",
      "  Step 700/4407 - Loss: 2.3567 | PPL: 10.56 | LR: 2.990891e-04\n",
      "  Step 700/4407 - Loss: 2.3567 | PPL: 10.56 | LR: 2.990891e-04\n",
      "  Step 800/4407 - Loss: 2.3620 | PPL: 10.61 | LR: 2.990759e-04\n",
      "  Step 800/4407 - Loss: 2.3620 | PPL: 10.61 | LR: 2.990759e-04\n",
      "  Step 900/4407 - Loss: 2.3654 | PPL: 10.65 | LR: 2.990627e-04\n",
      "  Step 900/4407 - Loss: 2.3654 | PPL: 10.65 | LR: 2.990627e-04\n",
      "  Step 1000/4407 - Loss: 2.3652 | PPL: 10.65 | LR: 2.990494e-04\n",
      "  Step 1000/4407 - Loss: 2.3652 | PPL: 10.65 | LR: 2.990494e-04\n",
      "  Step 1100/4407 - Loss: 2.3639 | PPL: 10.63 | LR: 2.990360e-04\n",
      "  Step 1100/4407 - Loss: 2.3639 | PPL: 10.63 | LR: 2.990360e-04\n",
      "  Step 1200/4407 - Loss: 2.3649 | PPL: 10.64 | LR: 2.990225e-04\n",
      "  Step 1200/4407 - Loss: 2.3649 | PPL: 10.64 | LR: 2.990225e-04\n",
      "  Step 1300/4407 - Loss: 2.3638 | PPL: 10.63 | LR: 2.990089e-04\n",
      "  Step 1300/4407 - Loss: 2.3638 | PPL: 10.63 | LR: 2.990089e-04\n",
      "  Step 1400/4407 - Loss: 2.3621 | PPL: 10.61 | LR: 2.989952e-04\n",
      "  Step 1400/4407 - Loss: 2.3621 | PPL: 10.61 | LR: 2.989952e-04\n",
      "  Step 1500/4407 - Loss: 2.3618 | PPL: 10.61 | LR: 2.989815e-04\n",
      "  Step 1500/4407 - Loss: 2.3618 | PPL: 10.61 | LR: 2.989815e-04\n",
      "  Step 1600/4407 - Loss: 2.3602 | PPL: 10.59 | LR: 2.989676e-04\n",
      "  Step 1600/4407 - Loss: 2.3602 | PPL: 10.59 | LR: 2.989676e-04\n",
      "  Step 1700/4407 - Loss: 2.3596 | PPL: 10.59 | LR: 2.989536e-04\n",
      "  Step 1700/4407 - Loss: 2.3596 | PPL: 10.59 | LR: 2.989536e-04\n",
      "  Step 1800/4407 - Loss: 2.3618 | PPL: 10.61 | LR: 2.989396e-04\n",
      "  Step 1800/4407 - Loss: 2.3618 | PPL: 10.61 | LR: 2.989396e-04\n",
      "  Step 1900/4407 - Loss: 2.3639 | PPL: 10.63 | LR: 2.989254e-04\n",
      "  Step 1900/4407 - Loss: 2.3639 | PPL: 10.63 | LR: 2.989254e-04\n",
      "  Step 2000/4407 - Loss: 2.3647 | PPL: 10.64 | LR: 2.989112e-04\n",
      "  Step 2000/4407 - Loss: 2.3647 | PPL: 10.64 | LR: 2.989112e-04\n",
      "  Step 2100/4407 - Loss: 2.3632 | PPL: 10.62 | LR: 2.988969e-04\n",
      "  Step 2100/4407 - Loss: 2.3632 | PPL: 10.62 | LR: 2.988969e-04\n",
      "  Step 2200/4407 - Loss: 2.3649 | PPL: 10.64 | LR: 2.988824e-04\n",
      "  Step 2200/4407 - Loss: 2.3649 | PPL: 10.64 | LR: 2.988824e-04\n",
      "  Step 2300/4407 - Loss: 2.3635 | PPL: 10.63 | LR: 2.988679e-04\n",
      "  Step 2300/4407 - Loss: 2.3635 | PPL: 10.63 | LR: 2.988679e-04\n",
      "  Step 2400/4407 - Loss: 2.3652 | PPL: 10.65 | LR: 2.988533e-04\n",
      "  Step 2400/4407 - Loss: 2.3652 | PPL: 10.65 | LR: 2.988533e-04\n",
      "  Step 2500/4407 - Loss: 2.3675 | PPL: 10.67 | LR: 2.988386e-04\n",
      "  Step 2500/4407 - Loss: 2.3675 | PPL: 10.67 | LR: 2.988386e-04\n",
      "  Step 2600/4407 - Loss: 2.3678 | PPL: 10.67 | LR: 2.988238e-04\n",
      "  Step 2600/4407 - Loss: 2.3678 | PPL: 10.67 | LR: 2.988238e-04\n",
      "  Step 2700/4407 - Loss: 2.3677 | PPL: 10.67 | LR: 2.988089e-04\n",
      "  Step 2700/4407 - Loss: 2.3677 | PPL: 10.67 | LR: 2.988089e-04\n",
      "  Step 2800/4407 - Loss: 2.3684 | PPL: 10.68 | LR: 2.987939e-04\n",
      "  Step 2800/4407 - Loss: 2.3684 | PPL: 10.68 | LR: 2.987939e-04\n",
      "  Step 2900/4407 - Loss: 2.3681 | PPL: 10.68 | LR: 2.987788e-04\n",
      "  Step 2900/4407 - Loss: 2.3681 | PPL: 10.68 | LR: 2.987788e-04\n",
      "  Step 3000/4407 - Loss: 2.3691 | PPL: 10.69 | LR: 2.987636e-04\n",
      "  Step 3000/4407 - Loss: 2.3691 | PPL: 10.69 | LR: 2.987636e-04\n",
      "  Step 3100/4407 - Loss: 2.3691 | PPL: 10.69 | LR: 2.987484e-04\n",
      "  Step 3100/4407 - Loss: 2.3691 | PPL: 10.69 | LR: 2.987484e-04\n",
      "  Step 3200/4407 - Loss: 2.3680 | PPL: 10.68 | LR: 2.987330e-04\n",
      "  Step 3200/4407 - Loss: 2.3680 | PPL: 10.68 | LR: 2.987330e-04\n",
      "  Step 3300/4407 - Loss: 2.3673 | PPL: 10.67 | LR: 2.987175e-04\n",
      "  Step 3300/4407 - Loss: 2.3673 | PPL: 10.67 | LR: 2.987175e-04\n",
      "  Step 3400/4407 - Loss: 2.3679 | PPL: 10.68 | LR: 2.987020e-04\n",
      "  Step 3400/4407 - Loss: 2.3679 | PPL: 10.68 | LR: 2.987020e-04\n",
      "  Step 3500/4407 - Loss: 2.3680 | PPL: 10.68 | LR: 2.986864e-04\n",
      "  Step 3500/4407 - Loss: 2.3680 | PPL: 10.68 | LR: 2.986864e-04\n",
      "  Step 3600/4407 - Loss: 2.3682 | PPL: 10.68 | LR: 2.986706e-04\n",
      "  Step 3600/4407 - Loss: 2.3682 | PPL: 10.68 | LR: 2.986706e-04\n",
      "  Step 3700/4407 - Loss: 2.3692 | PPL: 10.69 | LR: 2.986548e-04\n",
      "  Step 3700/4407 - Loss: 2.3692 | PPL: 10.69 | LR: 2.986548e-04\n",
      "  Step 3800/4407 - Loss: 2.3697 | PPL: 10.69 | LR: 2.986389e-04\n",
      "  Step 3800/4407 - Loss: 2.3697 | PPL: 10.69 | LR: 2.986389e-04\n",
      "  Step 3900/4407 - Loss: 2.3701 | PPL: 10.70 | LR: 2.986228e-04\n",
      "  Step 3900/4407 - Loss: 2.3701 | PPL: 10.70 | LR: 2.986228e-04\n",
      "  Step 4000/4407 - Loss: 2.3705 | PPL: 10.70 | LR: 2.986067e-04\n",
      "  Step 4000/4407 - Loss: 2.3705 | PPL: 10.70 | LR: 2.986067e-04\n",
      "  Step 4100/4407 - Loss: 2.3699 | PPL: 10.70 | LR: 2.985905e-04\n",
      "  Step 4100/4407 - Loss: 2.3699 | PPL: 10.70 | LR: 2.985905e-04\n",
      "  Step 4200/4407 - Loss: 2.3706 | PPL: 10.70 | LR: 2.985742e-04\n",
      "  Step 4200/4407 - Loss: 2.3706 | PPL: 10.70 | LR: 2.985742e-04\n",
      "  Step 4300/4407 - Loss: 2.3710 | PPL: 10.71 | LR: 2.985578e-04\n",
      "  Step 4300/4407 - Loss: 2.3710 | PPL: 10.71 | LR: 2.985578e-04\n",
      "  Step 4400/4407 - Loss: 2.3710 | PPL: 10.71 | LR: 2.985414e-04\n",
      "  Step 4400/4407 - Loss: 2.3710 | PPL: 10.71 | LR: 2.985414e-04\n",
      "Epoch 14/100 | Train Loss: 2.3712 | Train PPL: 10.71 | Val Loss: 2.7221 | Val PPL: 15.21\n",
      "Epoch 14/100 | Train Loss: 2.3712 | Train PPL: 10.71 | Val Loss: 2.7221 | Val PPL: 15.21\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 10.71\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 10.71\n",
      "  Step 100/4407 - Loss: 2.2489 | PPL: 9.48 | LR: 2.985236e-04\n",
      "  Step 100/4407 - Loss: 2.2489 | PPL: 9.48 | LR: 2.985236e-04\n",
      "  Step 200/4407 - Loss: 2.2779 | PPL: 9.76 | LR: 2.985070e-04\n",
      "  Step 200/4407 - Loss: 2.2779 | PPL: 9.76 | LR: 2.985070e-04\n",
      "  Step 300/4407 - Loss: 2.2912 | PPL: 9.89 | LR: 2.984902e-04\n",
      "  Step 300/4407 - Loss: 2.2912 | PPL: 9.89 | LR: 2.984902e-04\n",
      "  Step 400/4407 - Loss: 2.2978 | PPL: 9.95 | LR: 2.984733e-04\n",
      "  Step 400/4407 - Loss: 2.2978 | PPL: 9.95 | LR: 2.984733e-04\n",
      "  Step 500/4407 - Loss: 2.3051 | PPL: 10.03 | LR: 2.984564e-04\n",
      "  Step 500/4407 - Loss: 2.3051 | PPL: 10.03 | LR: 2.984564e-04\n",
      "  Step 600/4407 - Loss: 2.3068 | PPL: 10.04 | LR: 2.984393e-04\n",
      "  Step 600/4407 - Loss: 2.3068 | PPL: 10.04 | LR: 2.984393e-04\n",
      "  Step 700/4407 - Loss: 2.3022 | PPL: 10.00 | LR: 2.984222e-04\n",
      "  Step 700/4407 - Loss: 2.3022 | PPL: 10.00 | LR: 2.984222e-04\n",
      "  Step 800/4407 - Loss: 2.3102 | PPL: 10.08 | LR: 2.984049e-04\n",
      "  Step 800/4407 - Loss: 2.3102 | PPL: 10.08 | LR: 2.984049e-04\n",
      "  Step 900/4407 - Loss: 2.3134 | PPL: 10.11 | LR: 2.983876e-04\n",
      "  Step 900/4407 - Loss: 2.3134 | PPL: 10.11 | LR: 2.983876e-04\n",
      "  Step 1000/4407 - Loss: 2.3154 | PPL: 10.13 | LR: 2.983702e-04\n",
      "  Step 1000/4407 - Loss: 2.3154 | PPL: 10.13 | LR: 2.983702e-04\n",
      "  Step 1100/4407 - Loss: 2.3197 | PPL: 10.17 | LR: 2.983527e-04\n",
      "  Step 1100/4407 - Loss: 2.3197 | PPL: 10.17 | LR: 2.983527e-04\n",
      "  Step 1200/4407 - Loss: 2.3214 | PPL: 10.19 | LR: 2.983351e-04\n",
      "  Step 1200/4407 - Loss: 2.3214 | PPL: 10.19 | LR: 2.983351e-04\n",
      "  Step 1300/4407 - Loss: 2.3224 | PPL: 10.20 | LR: 2.983174e-04\n",
      "  Step 1300/4407 - Loss: 2.3224 | PPL: 10.20 | LR: 2.983174e-04\n",
      "  Step 1400/4407 - Loss: 2.3241 | PPL: 10.22 | LR: 2.982996e-04\n",
      "  Step 1400/4407 - Loss: 2.3241 | PPL: 10.22 | LR: 2.982996e-04\n",
      "  Step 1500/4407 - Loss: 2.3291 | PPL: 10.27 | LR: 2.982817e-04\n",
      "  Step 1500/4407 - Loss: 2.3291 | PPL: 10.27 | LR: 2.982817e-04\n",
      "  Step 1600/4407 - Loss: 2.3294 | PPL: 10.27 | LR: 2.982637e-04\n",
      "  Step 1600/4407 - Loss: 2.3294 | PPL: 10.27 | LR: 2.982637e-04\n",
      "  Step 1700/4407 - Loss: 2.3276 | PPL: 10.25 | LR: 2.982457e-04\n",
      "  Step 1700/4407 - Loss: 2.3276 | PPL: 10.25 | LR: 2.982457e-04\n",
      "  Step 1800/4407 - Loss: 2.3293 | PPL: 10.27 | LR: 2.982275e-04\n",
      "  Step 1800/4407 - Loss: 2.3293 | PPL: 10.27 | LR: 2.982275e-04\n",
      "  Step 1900/4407 - Loss: 2.3299 | PPL: 10.28 | LR: 2.982092e-04\n",
      "  Step 1900/4407 - Loss: 2.3299 | PPL: 10.28 | LR: 2.982092e-04\n",
      "  Step 2000/4407 - Loss: 2.3288 | PPL: 10.27 | LR: 2.981909e-04\n",
      "  Step 2000/4407 - Loss: 2.3288 | PPL: 10.27 | LR: 2.981909e-04\n",
      "  Step 2100/4407 - Loss: 2.3298 | PPL: 10.28 | LR: 2.981724e-04\n",
      "  Step 2100/4407 - Loss: 2.3298 | PPL: 10.28 | LR: 2.981724e-04\n",
      "  Step 2200/4407 - Loss: 2.3291 | PPL: 10.27 | LR: 2.981539e-04\n",
      "  Step 2200/4407 - Loss: 2.3291 | PPL: 10.27 | LR: 2.981539e-04\n",
      "  Step 2300/4407 - Loss: 2.3309 | PPL: 10.29 | LR: 2.981353e-04\n",
      "  Step 2300/4407 - Loss: 2.3309 | PPL: 10.29 | LR: 2.981353e-04\n",
      "  Step 2400/4407 - Loss: 2.3297 | PPL: 10.27 | LR: 2.981165e-04\n",
      "  Step 2400/4407 - Loss: 2.3297 | PPL: 10.27 | LR: 2.981165e-04\n",
      "  Step 2500/4407 - Loss: 2.3302 | PPL: 10.28 | LR: 2.980977e-04\n",
      "  Step 2500/4407 - Loss: 2.3302 | PPL: 10.28 | LR: 2.980977e-04\n",
      "  Step 2600/4407 - Loss: 2.3302 | PPL: 10.28 | LR: 2.980788e-04\n",
      "  Step 2600/4407 - Loss: 2.3302 | PPL: 10.28 | LR: 2.980788e-04\n",
      "  Step 2700/4407 - Loss: 2.3303 | PPL: 10.28 | LR: 2.980598e-04\n",
      "  Step 2700/4407 - Loss: 2.3303 | PPL: 10.28 | LR: 2.980598e-04\n",
      "  Step 2800/4407 - Loss: 2.3318 | PPL: 10.30 | LR: 2.980407e-04\n",
      "  Step 2800/4407 - Loss: 2.3318 | PPL: 10.30 | LR: 2.980407e-04\n",
      "  Step 2900/4407 - Loss: 2.3324 | PPL: 10.30 | LR: 2.980215e-04\n",
      "  Step 2900/4407 - Loss: 2.3324 | PPL: 10.30 | LR: 2.980215e-04\n",
      "  Step 3000/4407 - Loss: 2.3343 | PPL: 10.32 | LR: 2.980023e-04\n",
      "  Step 3000/4407 - Loss: 2.3343 | PPL: 10.32 | LR: 2.980023e-04\n",
      "  Step 3100/4407 - Loss: 2.3347 | PPL: 10.33 | LR: 2.979829e-04\n",
      "  Step 3100/4407 - Loss: 2.3347 | PPL: 10.33 | LR: 2.979829e-04\n",
      "  Step 3200/4407 - Loss: 2.3344 | PPL: 10.32 | LR: 2.979634e-04\n",
      "  Step 3200/4407 - Loss: 2.3344 | PPL: 10.32 | LR: 2.979634e-04\n",
      "  Step 3300/4407 - Loss: 2.3351 | PPL: 10.33 | LR: 2.979439e-04\n",
      "  Step 3300/4407 - Loss: 2.3351 | PPL: 10.33 | LR: 2.979439e-04\n",
      "  Step 3400/4407 - Loss: 2.3368 | PPL: 10.35 | LR: 2.979242e-04\n",
      "  Step 3400/4407 - Loss: 2.3368 | PPL: 10.35 | LR: 2.979242e-04\n",
      "  Step 3500/4407 - Loss: 2.3374 | PPL: 10.35 | LR: 2.979045e-04\n",
      "  Step 3500/4407 - Loss: 2.3374 | PPL: 10.35 | LR: 2.979045e-04\n",
      "  Step 3600/4407 - Loss: 2.3378 | PPL: 10.36 | LR: 2.978846e-04\n",
      "  Step 3600/4407 - Loss: 2.3378 | PPL: 10.36 | LR: 2.978846e-04\n",
      "  Step 3700/4407 - Loss: 2.3394 | PPL: 10.38 | LR: 2.978647e-04\n",
      "  Step 3700/4407 - Loss: 2.3394 | PPL: 10.38 | LR: 2.978647e-04\n",
      "  Step 3800/4407 - Loss: 2.3394 | PPL: 10.38 | LR: 2.978447e-04\n",
      "  Step 3800/4407 - Loss: 2.3394 | PPL: 10.38 | LR: 2.978447e-04\n",
      "  Step 3900/4407 - Loss: 2.3402 | PPL: 10.38 | LR: 2.978246e-04\n",
      "  Step 3900/4407 - Loss: 2.3402 | PPL: 10.38 | LR: 2.978246e-04\n",
      "  Step 4000/4407 - Loss: 2.3415 | PPL: 10.40 | LR: 2.978044e-04\n",
      "  Step 4000/4407 - Loss: 2.3415 | PPL: 10.40 | LR: 2.978044e-04\n",
      "  Step 4100/4407 - Loss: 2.3418 | PPL: 10.40 | LR: 2.977841e-04\n",
      "  Step 4100/4407 - Loss: 2.3418 | PPL: 10.40 | LR: 2.977841e-04\n",
      "  Step 4200/4407 - Loss: 2.3425 | PPL: 10.41 | LR: 2.977637e-04\n",
      "  Step 4200/4407 - Loss: 2.3425 | PPL: 10.41 | LR: 2.977637e-04\n",
      "  Step 4300/4407 - Loss: 2.3433 | PPL: 10.42 | LR: 2.977432e-04\n",
      "  Step 4300/4407 - Loss: 2.3433 | PPL: 10.42 | LR: 2.977432e-04\n",
      "  Step 4400/4407 - Loss: 2.3432 | PPL: 10.41 | LR: 2.977226e-04\n",
      "  Step 4400/4407 - Loss: 2.3432 | PPL: 10.41 | LR: 2.977226e-04\n",
      "Epoch 15/100 | Train Loss: 2.3431 | Train PPL: 10.41 | Val Loss: 2.7100 | Val PPL: 15.03\n",
      "Epoch 15/100 | Train Loss: 2.3431 | Train PPL: 10.41 | Val Loss: 2.7100 | Val PPL: 15.03\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 10.41\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 10.41\n",
      "  Step 100/4407 - Loss: 2.2804 | PPL: 9.78 | LR: 2.977005e-04\n",
      "  Step 100/4407 - Loss: 2.2804 | PPL: 9.78 | LR: 2.977005e-04\n",
      "  Step 200/4407 - Loss: 2.2778 | PPL: 9.75 | LR: 2.976797e-04\n",
      "  Step 200/4407 - Loss: 2.2778 | PPL: 9.75 | LR: 2.976797e-04\n",
      "  Step 300/4407 - Loss: 2.2684 | PPL: 9.66 | LR: 2.976589e-04\n",
      "  Step 300/4407 - Loss: 2.2684 | PPL: 9.66 | LR: 2.976589e-04\n",
      "  Step 400/4407 - Loss: 2.2676 | PPL: 9.66 | LR: 2.976379e-04\n",
      "  Step 400/4407 - Loss: 2.2676 | PPL: 9.66 | LR: 2.976379e-04\n",
      "  Step 500/4407 - Loss: 2.2745 | PPL: 9.72 | LR: 2.976168e-04\n",
      "  Step 500/4407 - Loss: 2.2745 | PPL: 9.72 | LR: 2.976168e-04\n",
      "  Step 600/4407 - Loss: 2.2815 | PPL: 9.79 | LR: 2.975957e-04\n",
      "  Step 600/4407 - Loss: 2.2815 | PPL: 9.79 | LR: 2.975957e-04\n",
      "  Step 700/4407 - Loss: 2.2861 | PPL: 9.84 | LR: 2.975745e-04\n",
      "  Step 700/4407 - Loss: 2.2861 | PPL: 9.84 | LR: 2.975745e-04\n",
      "  Step 800/4407 - Loss: 2.2847 | PPL: 9.82 | LR: 2.975531e-04\n",
      "  Step 800/4407 - Loss: 2.2847 | PPL: 9.82 | LR: 2.975531e-04\n",
      "  Step 900/4407 - Loss: 2.2921 | PPL: 9.90 | LR: 2.975317e-04\n",
      "  Step 900/4407 - Loss: 2.2921 | PPL: 9.90 | LR: 2.975317e-04\n",
      "  Step 1000/4407 - Loss: 2.2930 | PPL: 9.90 | LR: 2.975102e-04\n",
      "  Step 1000/4407 - Loss: 2.2930 | PPL: 9.90 | LR: 2.975102e-04\n",
      "  Step 1100/4407 - Loss: 2.2957 | PPL: 9.93 | LR: 2.974886e-04\n",
      "  Step 1100/4407 - Loss: 2.2957 | PPL: 9.93 | LR: 2.974886e-04\n",
      "  Step 1200/4407 - Loss: 2.2957 | PPL: 9.93 | LR: 2.974669e-04\n",
      "  Step 1200/4407 - Loss: 2.2957 | PPL: 9.93 | LR: 2.974669e-04\n",
      "  Step 1300/4407 - Loss: 2.2976 | PPL: 9.95 | LR: 2.974451e-04\n",
      "  Step 1300/4407 - Loss: 2.2976 | PPL: 9.95 | LR: 2.974451e-04\n",
      "  Step 1400/4407 - Loss: 2.2995 | PPL: 9.97 | LR: 2.974232e-04\n",
      "  Step 1400/4407 - Loss: 2.2995 | PPL: 9.97 | LR: 2.974232e-04\n",
      "  Step 1500/4407 - Loss: 2.2998 | PPL: 9.97 | LR: 2.974013e-04\n",
      "  Step 1500/4407 - Loss: 2.2998 | PPL: 9.97 | LR: 2.974013e-04\n",
      "  Step 1600/4407 - Loss: 2.3033 | PPL: 10.01 | LR: 2.973792e-04\n",
      "  Step 1600/4407 - Loss: 2.3033 | PPL: 10.01 | LR: 2.973792e-04\n",
      "  Step 1700/4407 - Loss: 2.3062 | PPL: 10.04 | LR: 2.973571e-04\n",
      "  Step 1700/4407 - Loss: 2.3062 | PPL: 10.04 | LR: 2.973571e-04\n",
      "  Step 1800/4407 - Loss: 2.3083 | PPL: 10.06 | LR: 2.973348e-04\n",
      "  Step 1800/4407 - Loss: 2.3083 | PPL: 10.06 | LR: 2.973348e-04\n",
      "  Step 1900/4407 - Loss: 2.3061 | PPL: 10.03 | LR: 2.973125e-04\n",
      "  Step 1900/4407 - Loss: 2.3061 | PPL: 10.03 | LR: 2.973125e-04\n",
      "  Step 2000/4407 - Loss: 2.3060 | PPL: 10.03 | LR: 2.972900e-04\n",
      "  Step 2000/4407 - Loss: 2.3060 | PPL: 10.03 | LR: 2.972900e-04\n",
      "  Step 2100/4407 - Loss: 2.3083 | PPL: 10.06 | LR: 2.972675e-04\n",
      "  Step 2100/4407 - Loss: 2.3083 | PPL: 10.06 | LR: 2.972675e-04\n",
      "  Step 2200/4407 - Loss: 2.3101 | PPL: 10.08 | LR: 2.972449e-04\n",
      "  Step 2200/4407 - Loss: 2.3101 | PPL: 10.08 | LR: 2.972449e-04\n",
      "  Step 2300/4407 - Loss: 2.3076 | PPL: 10.05 | LR: 2.972222e-04\n",
      "  Step 2300/4407 - Loss: 2.3076 | PPL: 10.05 | LR: 2.972222e-04\n",
      "  Step 2400/4407 - Loss: 2.3113 | PPL: 10.09 | LR: 2.971994e-04\n",
      "  Step 2400/4407 - Loss: 2.3113 | PPL: 10.09 | LR: 2.971994e-04\n",
      "  Step 2500/4407 - Loss: 2.3116 | PPL: 10.09 | LR: 2.971765e-04\n",
      "  Step 2500/4407 - Loss: 2.3116 | PPL: 10.09 | LR: 2.971765e-04\n",
      "  Step 2600/4407 - Loss: 2.3123 | PPL: 10.10 | LR: 2.971535e-04\n",
      "  Step 2600/4407 - Loss: 2.3123 | PPL: 10.10 | LR: 2.971535e-04\n",
      "  Step 2700/4407 - Loss: 2.3113 | PPL: 10.09 | LR: 2.971304e-04\n",
      "  Step 2700/4407 - Loss: 2.3113 | PPL: 10.09 | LR: 2.971304e-04\n",
      "  Step 2800/4407 - Loss: 2.3123 | PPL: 10.10 | LR: 2.971072e-04\n",
      "  Step 2800/4407 - Loss: 2.3123 | PPL: 10.10 | LR: 2.971072e-04\n",
      "  Step 2900/4407 - Loss: 2.3133 | PPL: 10.11 | LR: 2.970839e-04\n",
      "  Step 2900/4407 - Loss: 2.3133 | PPL: 10.11 | LR: 2.970839e-04\n",
      "  Step 3000/4407 - Loss: 2.3143 | PPL: 10.12 | LR: 2.970606e-04\n",
      "  Step 3000/4407 - Loss: 2.3143 | PPL: 10.12 | LR: 2.970606e-04\n",
      "  Step 3100/4407 - Loss: 2.3128 | PPL: 10.10 | LR: 2.970371e-04\n",
      "  Step 3100/4407 - Loss: 2.3128 | PPL: 10.10 | LR: 2.970371e-04\n",
      "  Step 3200/4407 - Loss: 2.3137 | PPL: 10.11 | LR: 2.970136e-04\n",
      "  Step 3200/4407 - Loss: 2.3137 | PPL: 10.11 | LR: 2.970136e-04\n",
      "  Step 3300/4407 - Loss: 2.3140 | PPL: 10.12 | LR: 2.969899e-04\n",
      "  Step 3300/4407 - Loss: 2.3140 | PPL: 10.12 | LR: 2.969899e-04\n",
      "  Step 3400/4407 - Loss: 2.3158 | PPL: 10.13 | LR: 2.969662e-04\n",
      "  Step 3400/4407 - Loss: 2.3158 | PPL: 10.13 | LR: 2.969662e-04\n",
      "  Step 3500/4407 - Loss: 2.3164 | PPL: 10.14 | LR: 2.969424e-04\n",
      "  Step 3500/4407 - Loss: 2.3164 | PPL: 10.14 | LR: 2.969424e-04\n",
      "  Step 3600/4407 - Loss: 2.3161 | PPL: 10.14 | LR: 2.969185e-04\n",
      "  Step 3600/4407 - Loss: 2.3161 | PPL: 10.14 | LR: 2.969185e-04\n",
      "  Step 3700/4407 - Loss: 2.3155 | PPL: 10.13 | LR: 2.968945e-04\n",
      "  Step 3700/4407 - Loss: 2.3155 | PPL: 10.13 | LR: 2.968945e-04\n",
      "  Step 3800/4407 - Loss: 2.3172 | PPL: 10.15 | LR: 2.968704e-04\n",
      "  Step 3800/4407 - Loss: 2.3172 | PPL: 10.15 | LR: 2.968704e-04\n",
      "  Step 3900/4407 - Loss: 2.3183 | PPL: 10.16 | LR: 2.968462e-04\n",
      "  Step 3900/4407 - Loss: 2.3183 | PPL: 10.16 | LR: 2.968462e-04\n",
      "  Step 4000/4407 - Loss: 2.3175 | PPL: 10.15 | LR: 2.968219e-04\n",
      "  Step 4000/4407 - Loss: 2.3175 | PPL: 10.15 | LR: 2.968219e-04\n",
      "  Step 4100/4407 - Loss: 2.3180 | PPL: 10.16 | LR: 2.967975e-04\n",
      "  Step 4100/4407 - Loss: 2.3180 | PPL: 10.16 | LR: 2.967975e-04\n",
      "  Step 4200/4407 - Loss: 2.3181 | PPL: 10.16 | LR: 2.967731e-04\n",
      "  Step 4200/4407 - Loss: 2.3181 | PPL: 10.16 | LR: 2.967731e-04\n",
      "  Step 4300/4407 - Loss: 2.3177 | PPL: 10.15 | LR: 2.967485e-04\n",
      "  Step 4300/4407 - Loss: 2.3177 | PPL: 10.15 | LR: 2.967485e-04\n",
      "  Step 4400/4407 - Loss: 2.3192 | PPL: 10.17 | LR: 2.967239e-04\n",
      "  Step 4400/4407 - Loss: 2.3192 | PPL: 10.17 | LR: 2.967239e-04\n",
      "Epoch 16/100 | Train Loss: 2.3193 | Train PPL: 10.17 | Val Loss: 2.6952 | Val PPL: 14.81\n",
      "Epoch 16/100 | Train Loss: 2.3193 | Train PPL: 10.17 | Val Loss: 2.6952 | Val PPL: 14.81\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 10.17\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 10.17\n",
      "  Step 100/4407 - Loss: 2.1996 | PPL: 9.02 | LR: 2.966974e-04\n",
      "  Step 100/4407 - Loss: 2.1996 | PPL: 9.02 | LR: 2.966974e-04\n",
      "  Step 200/4407 - Loss: 2.2270 | PPL: 9.27 | LR: 2.966726e-04\n",
      "  Step 200/4407 - Loss: 2.2270 | PPL: 9.27 | LR: 2.966726e-04\n",
      "  Step 300/4407 - Loss: 2.2330 | PPL: 9.33 | LR: 2.966476e-04\n",
      "  Step 300/4407 - Loss: 2.2330 | PPL: 9.33 | LR: 2.966476e-04\n",
      "  Step 400/4407 - Loss: 2.2448 | PPL: 9.44 | LR: 2.966226e-04\n",
      "  Step 400/4407 - Loss: 2.2448 | PPL: 9.44 | LR: 2.966226e-04\n",
      "  Step 500/4407 - Loss: 2.2449 | PPL: 9.44 | LR: 2.965975e-04\n",
      "  Step 500/4407 - Loss: 2.2449 | PPL: 9.44 | LR: 2.965975e-04\n",
      "  Step 600/4407 - Loss: 2.2549 | PPL: 9.53 | LR: 2.965723e-04\n",
      "  Step 600/4407 - Loss: 2.2549 | PPL: 9.53 | LR: 2.965723e-04\n",
      "  Step 700/4407 - Loss: 2.2592 | PPL: 9.58 | LR: 2.965470e-04\n",
      "  Step 700/4407 - Loss: 2.2592 | PPL: 9.58 | LR: 2.965470e-04\n",
      "  Step 800/4407 - Loss: 2.2614 | PPL: 9.60 | LR: 2.965216e-04\n",
      "  Step 800/4407 - Loss: 2.2614 | PPL: 9.60 | LR: 2.965216e-04\n",
      "  Step 900/4407 - Loss: 2.2615 | PPL: 9.60 | LR: 2.964961e-04\n",
      "  Step 900/4407 - Loss: 2.2615 | PPL: 9.60 | LR: 2.964961e-04\n",
      "  Step 1000/4407 - Loss: 2.2624 | PPL: 9.61 | LR: 2.964705e-04\n",
      "  Step 1000/4407 - Loss: 2.2624 | PPL: 9.61 | LR: 2.964705e-04\n",
      "  Step 1100/4407 - Loss: 2.2639 | PPL: 9.62 | LR: 2.964449e-04\n",
      "  Step 1100/4407 - Loss: 2.2639 | PPL: 9.62 | LR: 2.964449e-04\n",
      "  Step 1200/4407 - Loss: 2.2671 | PPL: 9.65 | LR: 2.964191e-04\n",
      "  Step 1200/4407 - Loss: 2.2671 | PPL: 9.65 | LR: 2.964191e-04\n",
      "  Step 1300/4407 - Loss: 2.2679 | PPL: 9.66 | LR: 2.963932e-04\n",
      "  Step 1300/4407 - Loss: 2.2679 | PPL: 9.66 | LR: 2.963932e-04\n",
      "  Step 1400/4407 - Loss: 2.2678 | PPL: 9.66 | LR: 2.963673e-04\n",
      "  Step 1400/4407 - Loss: 2.2678 | PPL: 9.66 | LR: 2.963673e-04\n",
      "  Step 1500/4407 - Loss: 2.2726 | PPL: 9.70 | LR: 2.963413e-04\n",
      "  Step 1500/4407 - Loss: 2.2726 | PPL: 9.70 | LR: 2.963413e-04\n",
      "  Step 1600/4407 - Loss: 2.2733 | PPL: 9.71 | LR: 2.963151e-04\n",
      "  Step 1600/4407 - Loss: 2.2733 | PPL: 9.71 | LR: 2.963151e-04\n",
      "  Step 1700/4407 - Loss: 2.2742 | PPL: 9.72 | LR: 2.962889e-04\n",
      "  Step 1700/4407 - Loss: 2.2742 | PPL: 9.72 | LR: 2.962889e-04\n",
      "  Step 1800/4407 - Loss: 2.2750 | PPL: 9.73 | LR: 2.962626e-04\n",
      "  Step 1800/4407 - Loss: 2.2750 | PPL: 9.73 | LR: 2.962626e-04\n",
      "  Step 1900/4407 - Loss: 2.2745 | PPL: 9.72 | LR: 2.962362e-04\n",
      "  Step 1900/4407 - Loss: 2.2745 | PPL: 9.72 | LR: 2.962362e-04\n",
      "  Step 2000/4407 - Loss: 2.2792 | PPL: 9.77 | LR: 2.962097e-04\n",
      "  Step 2000/4407 - Loss: 2.2792 | PPL: 9.77 | LR: 2.962097e-04\n",
      "  Step 2100/4407 - Loss: 2.2801 | PPL: 9.78 | LR: 2.961831e-04\n",
      "  Step 2100/4407 - Loss: 2.2801 | PPL: 9.78 | LR: 2.961831e-04\n",
      "  Step 2200/4407 - Loss: 2.2819 | PPL: 9.79 | LR: 2.961564e-04\n",
      "  Step 2200/4407 - Loss: 2.2819 | PPL: 9.79 | LR: 2.961564e-04\n",
      "  Step 2300/4407 - Loss: 2.2831 | PPL: 9.81 | LR: 2.961297e-04\n",
      "  Step 2300/4407 - Loss: 2.2831 | PPL: 9.81 | LR: 2.961297e-04\n",
      "  Step 2400/4407 - Loss: 2.2846 | PPL: 9.82 | LR: 2.961028e-04\n",
      "  Step 2400/4407 - Loss: 2.2846 | PPL: 9.82 | LR: 2.961028e-04\n",
      "  Step 2500/4407 - Loss: 2.2861 | PPL: 9.84 | LR: 2.960759e-04\n",
      "  Step 2500/4407 - Loss: 2.2861 | PPL: 9.84 | LR: 2.960759e-04\n",
      "  Step 2600/4407 - Loss: 2.2869 | PPL: 9.84 | LR: 2.960488e-04\n",
      "  Step 2600/4407 - Loss: 2.2869 | PPL: 9.84 | LR: 2.960488e-04\n",
      "  Step 2700/4407 - Loss: 2.2867 | PPL: 9.84 | LR: 2.960217e-04\n",
      "  Step 2700/4407 - Loss: 2.2867 | PPL: 9.84 | LR: 2.960217e-04\n",
      "  Step 2800/4407 - Loss: 2.2886 | PPL: 9.86 | LR: 2.959945e-04\n",
      "  Step 2800/4407 - Loss: 2.2886 | PPL: 9.86 | LR: 2.959945e-04\n",
      "  Step 2900/4407 - Loss: 2.2900 | PPL: 9.88 | LR: 2.959671e-04\n",
      "  Step 2900/4407 - Loss: 2.2900 | PPL: 9.88 | LR: 2.959671e-04\n",
      "  Step 3000/4407 - Loss: 2.2900 | PPL: 9.87 | LR: 2.959397e-04\n",
      "  Step 3000/4407 - Loss: 2.2900 | PPL: 9.87 | LR: 2.959397e-04\n",
      "  Step 3100/4407 - Loss: 2.2902 | PPL: 9.88 | LR: 2.959122e-04\n",
      "  Step 3100/4407 - Loss: 2.2902 | PPL: 9.88 | LR: 2.959122e-04\n",
      "  Step 3200/4407 - Loss: 2.2920 | PPL: 9.90 | LR: 2.958846e-04\n",
      "  Step 3200/4407 - Loss: 2.2920 | PPL: 9.90 | LR: 2.958846e-04\n",
      "  Step 3300/4407 - Loss: 2.2924 | PPL: 9.90 | LR: 2.958569e-04\n",
      "  Step 3300/4407 - Loss: 2.2924 | PPL: 9.90 | LR: 2.958569e-04\n",
      "  Step 3400/4407 - Loss: 2.2926 | PPL: 9.90 | LR: 2.958292e-04\n",
      "  Step 3400/4407 - Loss: 2.2926 | PPL: 9.90 | LR: 2.958292e-04\n",
      "  Step 3500/4407 - Loss: 2.2929 | PPL: 9.90 | LR: 2.958013e-04\n",
      "  Step 3500/4407 - Loss: 2.2929 | PPL: 9.90 | LR: 2.958013e-04\n",
      "  Step 3600/4407 - Loss: 2.2941 | PPL: 9.92 | LR: 2.957733e-04\n",
      "  Step 3600/4407 - Loss: 2.2941 | PPL: 9.92 | LR: 2.957733e-04\n",
      "  Step 3700/4407 - Loss: 2.2955 | PPL: 9.93 | LR: 2.957453e-04\n",
      "  Step 3700/4407 - Loss: 2.2955 | PPL: 9.93 | LR: 2.957453e-04\n",
      "  Step 3800/4407 - Loss: 2.2961 | PPL: 9.94 | LR: 2.957171e-04\n",
      "  Step 3800/4407 - Loss: 2.2961 | PPL: 9.94 | LR: 2.957171e-04\n",
      "  Step 3900/4407 - Loss: 2.2962 | PPL: 9.94 | LR: 2.956889e-04\n",
      "  Step 3900/4407 - Loss: 2.2962 | PPL: 9.94 | LR: 2.956889e-04\n",
      "  Step 4000/4407 - Loss: 2.2967 | PPL: 9.94 | LR: 2.956606e-04\n",
      "  Step 4000/4407 - Loss: 2.2967 | PPL: 9.94 | LR: 2.956606e-04\n",
      "  Step 4100/4407 - Loss: 2.2964 | PPL: 9.94 | LR: 2.956322e-04\n",
      "  Step 4100/4407 - Loss: 2.2964 | PPL: 9.94 | LR: 2.956322e-04\n",
      "  Step 4200/4407 - Loss: 2.2969 | PPL: 9.94 | LR: 2.956037e-04\n",
      "  Step 4200/4407 - Loss: 2.2969 | PPL: 9.94 | LR: 2.956037e-04\n",
      "  Step 4300/4407 - Loss: 2.2983 | PPL: 9.96 | LR: 2.955751e-04\n",
      "  Step 4300/4407 - Loss: 2.2983 | PPL: 9.96 | LR: 2.955751e-04\n",
      "  Step 4400/4407 - Loss: 2.2985 | PPL: 9.96 | LR: 2.955464e-04\n",
      "  Step 4400/4407 - Loss: 2.2985 | PPL: 9.96 | LR: 2.955464e-04\n",
      "Epoch 17/100 | Train Loss: 2.2985 | Train PPL: 9.96 | Val Loss: 2.6856 | Val PPL: 14.67\n",
      "Epoch 17/100 | Train Loss: 2.2985 | Train PPL: 9.96 | Val Loss: 2.6856 | Val PPL: 14.67\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.96\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.96\n",
      "  Step 100/4407 - Loss: 2.2512 | PPL: 9.50 | LR: 2.955156e-04\n",
      "  Step 100/4407 - Loss: 2.2512 | PPL: 9.50 | LR: 2.955156e-04\n",
      "  Step 200/4407 - Loss: 2.2669 | PPL: 9.65 | LR: 2.954867e-04\n",
      "  Step 200/4407 - Loss: 2.2669 | PPL: 9.65 | LR: 2.954867e-04\n",
      "  Step 300/4407 - Loss: 2.2646 | PPL: 9.63 | LR: 2.954577e-04\n",
      "  Step 300/4407 - Loss: 2.2646 | PPL: 9.63 | LR: 2.954577e-04\n",
      "  Step 400/4407 - Loss: 2.2738 | PPL: 9.72 | LR: 2.954287e-04\n",
      "  Step 400/4407 - Loss: 2.2738 | PPL: 9.72 | LR: 2.954287e-04\n",
      "  Step 500/4407 - Loss: 2.2782 | PPL: 9.76 | LR: 2.953995e-04\n",
      "  Step 500/4407 - Loss: 2.2782 | PPL: 9.76 | LR: 2.953995e-04\n",
      "  Step 600/4407 - Loss: 2.2780 | PPL: 9.76 | LR: 2.953703e-04\n",
      "  Step 600/4407 - Loss: 2.2780 | PPL: 9.76 | LR: 2.953703e-04\n",
      "  Step 700/4407 - Loss: 2.2722 | PPL: 9.70 | LR: 2.953409e-04\n",
      "  Step 700/4407 - Loss: 2.2722 | PPL: 9.70 | LR: 2.953409e-04\n",
      "  Step 800/4407 - Loss: 2.2675 | PPL: 9.66 | LR: 2.953115e-04\n",
      "  Step 800/4407 - Loss: 2.2675 | PPL: 9.66 | LR: 2.953115e-04\n",
      "  Step 900/4407 - Loss: 2.2679 | PPL: 9.66 | LR: 2.952820e-04\n",
      "  Step 900/4407 - Loss: 2.2679 | PPL: 9.66 | LR: 2.952820e-04\n",
      "  Step 1000/4407 - Loss: 2.2713 | PPL: 9.69 | LR: 2.952524e-04\n",
      "  Step 1000/4407 - Loss: 2.2713 | PPL: 9.69 | LR: 2.952524e-04\n",
      "  Step 1100/4407 - Loss: 2.2724 | PPL: 9.70 | LR: 2.952227e-04\n",
      "  Step 1100/4407 - Loss: 2.2724 | PPL: 9.70 | LR: 2.952227e-04\n",
      "  Step 1200/4407 - Loss: 2.2696 | PPL: 9.68 | LR: 2.951929e-04\n",
      "  Step 1200/4407 - Loss: 2.2696 | PPL: 9.68 | LR: 2.951929e-04\n",
      "  Step 1300/4407 - Loss: 2.2665 | PPL: 9.65 | LR: 2.951630e-04\n",
      "  Step 1300/4407 - Loss: 2.2665 | PPL: 9.65 | LR: 2.951630e-04\n",
      "  Step 1400/4407 - Loss: 2.2655 | PPL: 9.64 | LR: 2.951330e-04\n",
      "  Step 1400/4407 - Loss: 2.2655 | PPL: 9.64 | LR: 2.951330e-04\n",
      "  Step 1500/4407 - Loss: 2.2674 | PPL: 9.65 | LR: 2.951030e-04\n",
      "  Step 1500/4407 - Loss: 2.2674 | PPL: 9.65 | LR: 2.951030e-04\n",
      "  Step 1600/4407 - Loss: 2.2696 | PPL: 9.68 | LR: 2.950728e-04\n",
      "  Step 1600/4407 - Loss: 2.2696 | PPL: 9.68 | LR: 2.950728e-04\n",
      "  Step 1700/4407 - Loss: 2.2698 | PPL: 9.68 | LR: 2.950425e-04\n",
      "  Step 1700/4407 - Loss: 2.2698 | PPL: 9.68 | LR: 2.950425e-04\n",
      "  Step 1800/4407 - Loss: 2.2686 | PPL: 9.67 | LR: 2.950122e-04\n",
      "  Step 1800/4407 - Loss: 2.2686 | PPL: 9.67 | LR: 2.950122e-04\n",
      "  Step 1900/4407 - Loss: 2.2715 | PPL: 9.69 | LR: 2.949818e-04\n",
      "  Step 1900/4407 - Loss: 2.2715 | PPL: 9.69 | LR: 2.949818e-04\n",
      "  Step 2000/4407 - Loss: 2.2724 | PPL: 9.70 | LR: 2.949513e-04\n",
      "  Step 2000/4407 - Loss: 2.2724 | PPL: 9.70 | LR: 2.949513e-04\n",
      "  Step 2100/4407 - Loss: 2.2735 | PPL: 9.71 | LR: 2.949207e-04\n",
      "  Step 2100/4407 - Loss: 2.2735 | PPL: 9.71 | LR: 2.949207e-04\n",
      "  Step 2200/4407 - Loss: 2.2707 | PPL: 9.69 | LR: 2.948900e-04\n",
      "  Step 2200/4407 - Loss: 2.2707 | PPL: 9.69 | LR: 2.948900e-04\n",
      "  Step 2300/4407 - Loss: 2.2721 | PPL: 9.70 | LR: 2.948592e-04\n",
      "  Step 2300/4407 - Loss: 2.2721 | PPL: 9.70 | LR: 2.948592e-04\n",
      "  Step 2400/4407 - Loss: 2.2731 | PPL: 9.71 | LR: 2.948283e-04\n",
      "  Step 2400/4407 - Loss: 2.2731 | PPL: 9.71 | LR: 2.948283e-04\n",
      "  Step 2500/4407 - Loss: 2.2740 | PPL: 9.72 | LR: 2.947973e-04\n",
      "  Step 2500/4407 - Loss: 2.2740 | PPL: 9.72 | LR: 2.947973e-04\n",
      "  Step 2600/4407 - Loss: 2.2761 | PPL: 9.74 | LR: 2.947662e-04\n",
      "  Step 2600/4407 - Loss: 2.2761 | PPL: 9.74 | LR: 2.947662e-04\n",
      "  Step 2700/4407 - Loss: 2.2773 | PPL: 9.75 | LR: 2.947351e-04\n",
      "  Step 2700/4407 - Loss: 2.2773 | PPL: 9.75 | LR: 2.947351e-04\n",
      "  Step 2800/4407 - Loss: 2.2796 | PPL: 9.77 | LR: 2.947038e-04\n",
      "  Step 2800/4407 - Loss: 2.2796 | PPL: 9.77 | LR: 2.947038e-04\n",
      "  Step 2900/4407 - Loss: 2.2799 | PPL: 9.78 | LR: 2.946725e-04\n",
      "  Step 2900/4407 - Loss: 2.2799 | PPL: 9.78 | LR: 2.946725e-04\n",
      "  Step 3000/4407 - Loss: 2.2804 | PPL: 9.78 | LR: 2.946411e-04\n",
      "  Step 3000/4407 - Loss: 2.2804 | PPL: 9.78 | LR: 2.946411e-04\n",
      "  Step 3100/4407 - Loss: 2.2799 | PPL: 9.78 | LR: 2.946095e-04\n",
      "  Step 3100/4407 - Loss: 2.2799 | PPL: 9.78 | LR: 2.946095e-04\n",
      "  Step 3200/4407 - Loss: 2.2791 | PPL: 9.77 | LR: 2.945779e-04\n",
      "  Step 3200/4407 - Loss: 2.2791 | PPL: 9.77 | LR: 2.945779e-04\n",
      "  Step 3300/4407 - Loss: 2.2787 | PPL: 9.76 | LR: 2.945462e-04\n",
      "  Step 3300/4407 - Loss: 2.2787 | PPL: 9.76 | LR: 2.945462e-04\n",
      "  Step 3400/4407 - Loss: 2.2790 | PPL: 9.77 | LR: 2.945144e-04\n",
      "  Step 3400/4407 - Loss: 2.2790 | PPL: 9.77 | LR: 2.945144e-04\n",
      "  Step 3500/4407 - Loss: 2.2779 | PPL: 9.76 | LR: 2.944826e-04\n",
      "  Step 3500/4407 - Loss: 2.2779 | PPL: 9.76 | LR: 2.944826e-04\n",
      "  Step 3600/4407 - Loss: 2.2779 | PPL: 9.76 | LR: 2.944506e-04\n",
      "  Step 3600/4407 - Loss: 2.2779 | PPL: 9.76 | LR: 2.944506e-04\n",
      "  Step 3700/4407 - Loss: 2.2777 | PPL: 9.75 | LR: 2.944185e-04\n",
      "  Step 3700/4407 - Loss: 2.2777 | PPL: 9.75 | LR: 2.944185e-04\n",
      "  Step 3800/4407 - Loss: 2.2781 | PPL: 9.76 | LR: 2.943864e-04\n",
      "  Step 3800/4407 - Loss: 2.2781 | PPL: 9.76 | LR: 2.943864e-04\n",
      "  Step 3900/4407 - Loss: 2.2783 | PPL: 9.76 | LR: 2.943541e-04\n",
      "  Step 3900/4407 - Loss: 2.2783 | PPL: 9.76 | LR: 2.943541e-04\n",
      "  Step 4000/4407 - Loss: 2.2779 | PPL: 9.76 | LR: 2.943218e-04\n",
      "  Step 4000/4407 - Loss: 2.2779 | PPL: 9.76 | LR: 2.943218e-04\n",
      "  Step 4100/4407 - Loss: 2.2781 | PPL: 9.76 | LR: 2.942894e-04\n",
      "  Step 4100/4407 - Loss: 2.2781 | PPL: 9.76 | LR: 2.942894e-04\n",
      "  Step 4200/4407 - Loss: 2.2792 | PPL: 9.77 | LR: 2.942569e-04\n",
      "  Step 4200/4407 - Loss: 2.2792 | PPL: 9.77 | LR: 2.942569e-04\n",
      "  Step 4300/4407 - Loss: 2.2796 | PPL: 9.77 | LR: 2.942242e-04\n",
      "  Step 4300/4407 - Loss: 2.2796 | PPL: 9.77 | LR: 2.942242e-04\n",
      "  Step 4400/4407 - Loss: 2.2803 | PPL: 9.78 | LR: 2.941915e-04\n",
      "  Step 4400/4407 - Loss: 2.2803 | PPL: 9.78 | LR: 2.941915e-04\n",
      "Epoch 18/100 | Train Loss: 2.2801 | Train PPL: 9.78 | Val Loss: 2.6794 | Val PPL: 14.58\n",
      "Epoch 18/100 | Train Loss: 2.2801 | Train PPL: 9.78 | Val Loss: 2.6794 | Val PPL: 14.58\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.78\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.78\n",
      "  Step 100/4407 - Loss: 2.2438 | PPL: 9.43 | LR: 2.941565e-04\n",
      "  Step 100/4407 - Loss: 2.2438 | PPL: 9.43 | LR: 2.941565e-04\n",
      "  Step 200/4407 - Loss: 2.2498 | PPL: 9.49 | LR: 2.941236e-04\n",
      "  Step 200/4407 - Loss: 2.2498 | PPL: 9.49 | LR: 2.941236e-04\n",
      "  Step 300/4407 - Loss: 2.2348 | PPL: 9.34 | LR: 2.940906e-04\n",
      "  Step 300/4407 - Loss: 2.2348 | PPL: 9.34 | LR: 2.940906e-04\n",
      "  Step 400/4407 - Loss: 2.2476 | PPL: 9.47 | LR: 2.940575e-04\n",
      "  Step 400/4407 - Loss: 2.2476 | PPL: 9.47 | LR: 2.940575e-04\n",
      "  Step 500/4407 - Loss: 2.2450 | PPL: 9.44 | LR: 2.940244e-04\n",
      "  Step 500/4407 - Loss: 2.2450 | PPL: 9.44 | LR: 2.940244e-04\n",
      "  Step 600/4407 - Loss: 2.2466 | PPL: 9.46 | LR: 2.939911e-04\n",
      "  Step 600/4407 - Loss: 2.2466 | PPL: 9.46 | LR: 2.939911e-04\n",
      "  Step 700/4407 - Loss: 2.2447 | PPL: 9.44 | LR: 2.939578e-04\n",
      "  Step 700/4407 - Loss: 2.2447 | PPL: 9.44 | LR: 2.939578e-04\n",
      "  Step 800/4407 - Loss: 2.2497 | PPL: 9.48 | LR: 2.939244e-04\n",
      "  Step 800/4407 - Loss: 2.2497 | PPL: 9.48 | LR: 2.939244e-04\n",
      "  Step 900/4407 - Loss: 2.2484 | PPL: 9.47 | LR: 2.938909e-04\n",
      "  Step 900/4407 - Loss: 2.2484 | PPL: 9.47 | LR: 2.938909e-04\n",
      "  Step 1000/4407 - Loss: 2.2450 | PPL: 9.44 | LR: 2.938572e-04\n",
      "  Step 1000/4407 - Loss: 2.2450 | PPL: 9.44 | LR: 2.938572e-04\n",
      "  Step 1100/4407 - Loss: 2.2452 | PPL: 9.44 | LR: 2.938236e-04\n",
      "  Step 1100/4407 - Loss: 2.2452 | PPL: 9.44 | LR: 2.938236e-04\n",
      "  Step 1200/4407 - Loss: 2.2474 | PPL: 9.46 | LR: 2.937898e-04\n",
      "  Step 1200/4407 - Loss: 2.2474 | PPL: 9.46 | LR: 2.937898e-04\n",
      "  Step 1300/4407 - Loss: 2.2481 | PPL: 9.47 | LR: 2.937559e-04\n",
      "  Step 1300/4407 - Loss: 2.2481 | PPL: 9.47 | LR: 2.937559e-04\n",
      "  Step 1400/4407 - Loss: 2.2501 | PPL: 9.49 | LR: 2.937219e-04\n",
      "  Step 1400/4407 - Loss: 2.2501 | PPL: 9.49 | LR: 2.937219e-04\n",
      "  Step 1500/4407 - Loss: 2.2507 | PPL: 9.49 | LR: 2.936879e-04\n",
      "  Step 1500/4407 - Loss: 2.2507 | PPL: 9.49 | LR: 2.936879e-04\n",
      "  Step 1600/4407 - Loss: 2.2510 | PPL: 9.50 | LR: 2.936537e-04\n",
      "  Step 1600/4407 - Loss: 2.2510 | PPL: 9.50 | LR: 2.936537e-04\n",
      "  Step 1700/4407 - Loss: 2.2500 | PPL: 9.49 | LR: 2.936195e-04\n",
      "  Step 1700/4407 - Loss: 2.2500 | PPL: 9.49 | LR: 2.936195e-04\n",
      "  Step 1800/4407 - Loss: 2.2508 | PPL: 9.50 | LR: 2.935851e-04\n",
      "  Step 1800/4407 - Loss: 2.2508 | PPL: 9.50 | LR: 2.935851e-04\n",
      "  Step 1900/4407 - Loss: 2.2517 | PPL: 9.50 | LR: 2.935507e-04\n",
      "  Step 1900/4407 - Loss: 2.2517 | PPL: 9.50 | LR: 2.935507e-04\n",
      "  Step 2000/4407 - Loss: 2.2507 | PPL: 9.49 | LR: 2.935162e-04\n",
      "  Step 2000/4407 - Loss: 2.2507 | PPL: 9.49 | LR: 2.935162e-04\n",
      "  Step 2100/4407 - Loss: 2.2520 | PPL: 9.51 | LR: 2.934816e-04\n",
      "  Step 2100/4407 - Loss: 2.2520 | PPL: 9.51 | LR: 2.934816e-04\n",
      "  Step 2200/4407 - Loss: 2.2523 | PPL: 9.51 | LR: 2.934469e-04\n",
      "  Step 2200/4407 - Loss: 2.2523 | PPL: 9.51 | LR: 2.934469e-04\n",
      "  Step 2300/4407 - Loss: 2.2548 | PPL: 9.53 | LR: 2.934122e-04\n",
      "  Step 2300/4407 - Loss: 2.2548 | PPL: 9.53 | LR: 2.934122e-04\n",
      "  Step 2400/4407 - Loss: 2.2556 | PPL: 9.54 | LR: 2.933773e-04\n",
      "  Step 2400/4407 - Loss: 2.2556 | PPL: 9.54 | LR: 2.933773e-04\n",
      "  Step 2500/4407 - Loss: 2.2564 | PPL: 9.55 | LR: 2.933423e-04\n",
      "  Step 2500/4407 - Loss: 2.2564 | PPL: 9.55 | LR: 2.933423e-04\n",
      "  Step 2600/4407 - Loss: 2.2576 | PPL: 9.56 | LR: 2.933073e-04\n",
      "  Step 2600/4407 - Loss: 2.2576 | PPL: 9.56 | LR: 2.933073e-04\n",
      "  Step 2700/4407 - Loss: 2.2575 | PPL: 9.56 | LR: 2.932721e-04\n",
      "  Step 2700/4407 - Loss: 2.2575 | PPL: 9.56 | LR: 2.932721e-04\n",
      "  Step 2800/4407 - Loss: 2.2571 | PPL: 9.56 | LR: 2.932369e-04\n",
      "  Step 2800/4407 - Loss: 2.2571 | PPL: 9.56 | LR: 2.932369e-04\n",
      "  Step 2900/4407 - Loss: 2.2568 | PPL: 9.55 | LR: 2.932016e-04\n",
      "  Step 2900/4407 - Loss: 2.2568 | PPL: 9.55 | LR: 2.932016e-04\n",
      "  Step 3000/4407 - Loss: 2.2595 | PPL: 9.58 | LR: 2.931662e-04\n",
      "  Step 3000/4407 - Loss: 2.2595 | PPL: 9.58 | LR: 2.931662e-04\n",
      "  Step 3100/4407 - Loss: 2.2612 | PPL: 9.59 | LR: 2.931307e-04\n",
      "  Step 3100/4407 - Loss: 2.2612 | PPL: 9.59 | LR: 2.931307e-04\n",
      "  Step 3200/4407 - Loss: 2.2599 | PPL: 9.58 | LR: 2.930951e-04\n",
      "  Step 3200/4407 - Loss: 2.2599 | PPL: 9.58 | LR: 2.930951e-04\n",
      "  Step 3300/4407 - Loss: 2.2605 | PPL: 9.59 | LR: 2.930594e-04\n",
      "  Step 3300/4407 - Loss: 2.2605 | PPL: 9.59 | LR: 2.930594e-04\n",
      "  Step 3400/4407 - Loss: 2.2613 | PPL: 9.60 | LR: 2.930237e-04\n",
      "  Step 3400/4407 - Loss: 2.2613 | PPL: 9.60 | LR: 2.930237e-04\n",
      "  Step 3500/4407 - Loss: 2.2618 | PPL: 9.60 | LR: 2.929878e-04\n",
      "  Step 3500/4407 - Loss: 2.2618 | PPL: 9.60 | LR: 2.929878e-04\n",
      "  Step 3600/4407 - Loss: 2.2615 | PPL: 9.60 | LR: 2.929518e-04\n",
      "  Step 3600/4407 - Loss: 2.2615 | PPL: 9.60 | LR: 2.929518e-04\n",
      "  Step 3700/4407 - Loss: 2.2621 | PPL: 9.60 | LR: 2.929158e-04\n",
      "  Step 3700/4407 - Loss: 2.2621 | PPL: 9.60 | LR: 2.929158e-04\n",
      "  Step 3800/4407 - Loss: 2.2626 | PPL: 9.61 | LR: 2.928797e-04\n",
      "  Step 3800/4407 - Loss: 2.2626 | PPL: 9.61 | LR: 2.928797e-04\n",
      "  Step 3900/4407 - Loss: 2.2626 | PPL: 9.61 | LR: 2.928435e-04\n",
      "  Step 3900/4407 - Loss: 2.2626 | PPL: 9.61 | LR: 2.928435e-04\n",
      "  Step 4000/4407 - Loss: 2.2635 | PPL: 9.62 | LR: 2.928072e-04\n",
      "  Step 4000/4407 - Loss: 2.2635 | PPL: 9.62 | LR: 2.928072e-04\n",
      "  Step 4100/4407 - Loss: 2.2628 | PPL: 9.61 | LR: 2.927708e-04\n",
      "  Step 4100/4407 - Loss: 2.2628 | PPL: 9.61 | LR: 2.927708e-04\n",
      "  Step 4200/4407 - Loss: 2.2626 | PPL: 9.61 | LR: 2.927343e-04\n",
      "  Step 4200/4407 - Loss: 2.2626 | PPL: 9.61 | LR: 2.927343e-04\n",
      "  Step 4300/4407 - Loss: 2.2631 | PPL: 9.61 | LR: 2.926977e-04\n",
      "  Step 4300/4407 - Loss: 2.2631 | PPL: 9.61 | LR: 2.926977e-04\n",
      "  Step 4400/4407 - Loss: 2.2634 | PPL: 9.62 | LR: 2.926610e-04\n",
      "  Step 4400/4407 - Loss: 2.2634 | PPL: 9.62 | LR: 2.926610e-04\n",
      "Epoch 19/100 | Train Loss: 2.2636 | Train PPL: 9.62 | Val Loss: 2.6699 | Val PPL: 14.44\n",
      "Epoch 19/100 | Train Loss: 2.2636 | Train PPL: 9.62 | Val Loss: 2.6699 | Val PPL: 14.44\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.62\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.62\n",
      "  Step 100/4407 - Loss: 2.2063 | PPL: 9.08 | LR: 2.926217e-04\n",
      "  Step 100/4407 - Loss: 2.2063 | PPL: 9.08 | LR: 2.926217e-04\n",
      "  Step 200/4407 - Loss: 2.1985 | PPL: 9.01 | LR: 2.925849e-04\n",
      "  Step 200/4407 - Loss: 2.1985 | PPL: 9.01 | LR: 2.925849e-04\n",
      "  Step 300/4407 - Loss: 2.2156 | PPL: 9.17 | LR: 2.925479e-04\n",
      "  Step 300/4407 - Loss: 2.2156 | PPL: 9.17 | LR: 2.925479e-04\n",
      "  Step 400/4407 - Loss: 2.2146 | PPL: 9.16 | LR: 2.925109e-04\n",
      "  Step 400/4407 - Loss: 2.2146 | PPL: 9.16 | LR: 2.925109e-04\n",
      "  Step 500/4407 - Loss: 2.2193 | PPL: 9.20 | LR: 2.924738e-04\n",
      "  Step 500/4407 - Loss: 2.2193 | PPL: 9.20 | LR: 2.924738e-04\n",
      "  Step 600/4407 - Loss: 2.2210 | PPL: 9.22 | LR: 2.924366e-04\n",
      "  Step 600/4407 - Loss: 2.2210 | PPL: 9.22 | LR: 2.924366e-04\n",
      "  Step 700/4407 - Loss: 2.2270 | PPL: 9.27 | LR: 2.923993e-04\n",
      "  Step 700/4407 - Loss: 2.2270 | PPL: 9.27 | LR: 2.923993e-04\n",
      "  Step 800/4407 - Loss: 2.2179 | PPL: 9.19 | LR: 2.923619e-04\n",
      "  Step 800/4407 - Loss: 2.2179 | PPL: 9.19 | LR: 2.923619e-04\n",
      "  Step 900/4407 - Loss: 2.2193 | PPL: 9.20 | LR: 2.923244e-04\n",
      "  Step 900/4407 - Loss: 2.2193 | PPL: 9.20 | LR: 2.923244e-04\n",
      "  Step 1000/4407 - Loss: 2.2201 | PPL: 9.21 | LR: 2.922869e-04\n",
      "  Step 1000/4407 - Loss: 2.2201 | PPL: 9.21 | LR: 2.922869e-04\n",
      "  Step 1100/4407 - Loss: 2.2216 | PPL: 9.22 | LR: 2.922492e-04\n",
      "  Step 1100/4407 - Loss: 2.2216 | PPL: 9.22 | LR: 2.922492e-04\n",
      "  Step 1200/4407 - Loss: 2.2197 | PPL: 9.21 | LR: 2.922115e-04\n",
      "  Step 1200/4407 - Loss: 2.2197 | PPL: 9.21 | LR: 2.922115e-04\n",
      "  Step 1300/4407 - Loss: 2.2238 | PPL: 9.24 | LR: 2.921736e-04\n",
      "  Step 1300/4407 - Loss: 2.2238 | PPL: 9.24 | LR: 2.921736e-04\n",
      "  Step 1400/4407 - Loss: 2.2243 | PPL: 9.25 | LR: 2.921357e-04\n",
      "  Step 1400/4407 - Loss: 2.2243 | PPL: 9.25 | LR: 2.921357e-04\n",
      "  Step 1500/4407 - Loss: 2.2245 | PPL: 9.25 | LR: 2.920977e-04\n",
      "  Step 1500/4407 - Loss: 2.2245 | PPL: 9.25 | LR: 2.920977e-04\n",
      "  Step 1600/4407 - Loss: 2.2272 | PPL: 9.27 | LR: 2.920596e-04\n",
      "  Step 1600/4407 - Loss: 2.2272 | PPL: 9.27 | LR: 2.920596e-04\n",
      "  Step 1700/4407 - Loss: 2.2290 | PPL: 9.29 | LR: 2.920214e-04\n",
      "  Step 1700/4407 - Loss: 2.2290 | PPL: 9.29 | LR: 2.920214e-04\n",
      "  Step 1800/4407 - Loss: 2.2299 | PPL: 9.30 | LR: 2.919831e-04\n",
      "  Step 1800/4407 - Loss: 2.2299 | PPL: 9.30 | LR: 2.919831e-04\n",
      "  Step 1900/4407 - Loss: 2.2322 | PPL: 9.32 | LR: 2.919448e-04\n",
      "  Step 1900/4407 - Loss: 2.2322 | PPL: 9.32 | LR: 2.919448e-04\n",
      "  Step 2000/4407 - Loss: 2.2329 | PPL: 9.33 | LR: 2.919063e-04\n",
      "  Step 2000/4407 - Loss: 2.2329 | PPL: 9.33 | LR: 2.919063e-04\n",
      "  Step 2100/4407 - Loss: 2.2349 | PPL: 9.35 | LR: 2.918678e-04\n",
      "  Step 2100/4407 - Loss: 2.2349 | PPL: 9.35 | LR: 2.918678e-04\n",
      "  Step 2200/4407 - Loss: 2.2354 | PPL: 9.35 | LR: 2.918291e-04\n",
      "  Step 2200/4407 - Loss: 2.2354 | PPL: 9.35 | LR: 2.918291e-04\n",
      "  Step 2300/4407 - Loss: 2.2346 | PPL: 9.34 | LR: 2.917904e-04\n",
      "  Step 2300/4407 - Loss: 2.2346 | PPL: 9.34 | LR: 2.917904e-04\n",
      "  Step 2400/4407 - Loss: 2.2358 | PPL: 9.35 | LR: 2.917516e-04\n",
      "  Step 2400/4407 - Loss: 2.2358 | PPL: 9.35 | LR: 2.917516e-04\n",
      "  Step 2500/4407 - Loss: 2.2352 | PPL: 9.35 | LR: 2.917127e-04\n",
      "  Step 2500/4407 - Loss: 2.2352 | PPL: 9.35 | LR: 2.917127e-04\n",
      "  Step 2600/4407 - Loss: 2.2379 | PPL: 9.37 | LR: 2.916737e-04\n",
      "  Step 2600/4407 - Loss: 2.2379 | PPL: 9.37 | LR: 2.916737e-04\n",
      "  Step 2700/4407 - Loss: 2.2407 | PPL: 9.40 | LR: 2.916346e-04\n",
      "  Step 2700/4407 - Loss: 2.2407 | PPL: 9.40 | LR: 2.916346e-04\n",
      "  Step 2800/4407 - Loss: 2.2408 | PPL: 9.40 | LR: 2.915955e-04\n",
      "  Step 2800/4407 - Loss: 2.2408 | PPL: 9.40 | LR: 2.915955e-04\n",
      "  Step 2900/4407 - Loss: 2.2394 | PPL: 9.39 | LR: 2.915562e-04\n",
      "  Step 2900/4407 - Loss: 2.2394 | PPL: 9.39 | LR: 2.915562e-04\n",
      "  Step 3000/4407 - Loss: 2.2407 | PPL: 9.40 | LR: 2.915169e-04\n",
      "  Step 3000/4407 - Loss: 2.2407 | PPL: 9.40 | LR: 2.915169e-04\n",
      "  Step 3100/4407 - Loss: 2.2423 | PPL: 9.42 | LR: 2.914774e-04\n",
      "  Step 3100/4407 - Loss: 2.2423 | PPL: 9.42 | LR: 2.914774e-04\n",
      "  Step 3200/4407 - Loss: 2.2426 | PPL: 9.42 | LR: 2.914379e-04\n",
      "  Step 3200/4407 - Loss: 2.2426 | PPL: 9.42 | LR: 2.914379e-04\n",
      "  Step 3300/4407 - Loss: 2.2427 | PPL: 9.42 | LR: 2.913983e-04\n",
      "  Step 3300/4407 - Loss: 2.2427 | PPL: 9.42 | LR: 2.913983e-04\n",
      "  Step 3400/4407 - Loss: 2.2428 | PPL: 9.42 | LR: 2.913586e-04\n",
      "  Step 3400/4407 - Loss: 2.2428 | PPL: 9.42 | LR: 2.913586e-04\n",
      "  Step 3500/4407 - Loss: 2.2452 | PPL: 9.44 | LR: 2.913188e-04\n",
      "  Step 3500/4407 - Loss: 2.2452 | PPL: 9.44 | LR: 2.913188e-04\n",
      "  Step 3600/4407 - Loss: 2.2450 | PPL: 9.44 | LR: 2.912789e-04\n",
      "  Step 3600/4407 - Loss: 2.2450 | PPL: 9.44 | LR: 2.912789e-04\n",
      "  Step 3700/4407 - Loss: 2.2461 | PPL: 9.45 | LR: 2.912390e-04\n",
      "  Step 3700/4407 - Loss: 2.2461 | PPL: 9.45 | LR: 2.912390e-04\n",
      "  Step 3800/4407 - Loss: 2.2465 | PPL: 9.45 | LR: 2.911989e-04\n",
      "  Step 3800/4407 - Loss: 2.2465 | PPL: 9.45 | LR: 2.911989e-04\n",
      "  Step 3900/4407 - Loss: 2.2461 | PPL: 9.45 | LR: 2.911588e-04\n",
      "  Step 3900/4407 - Loss: 2.2461 | PPL: 9.45 | LR: 2.911588e-04\n",
      "  Step 4000/4407 - Loss: 2.2465 | PPL: 9.45 | LR: 2.911185e-04\n",
      "  Step 4000/4407 - Loss: 2.2465 | PPL: 9.45 | LR: 2.911185e-04\n",
      "  Step 4100/4407 - Loss: 2.2466 | PPL: 9.46 | LR: 2.910782e-04\n",
      "  Step 4100/4407 - Loss: 2.2466 | PPL: 9.46 | LR: 2.910782e-04\n",
      "  Step 4200/4407 - Loss: 2.2473 | PPL: 9.46 | LR: 2.910378e-04\n",
      "  Step 4200/4407 - Loss: 2.2473 | PPL: 9.46 | LR: 2.910378e-04\n",
      "  Step 4300/4407 - Loss: 2.2479 | PPL: 9.47 | LR: 2.909973e-04\n",
      "  Step 4300/4407 - Loss: 2.2479 | PPL: 9.47 | LR: 2.909973e-04\n",
      "  Step 4400/4407 - Loss: 2.2493 | PPL: 9.48 | LR: 2.909567e-04\n",
      "  Step 4400/4407 - Loss: 2.2493 | PPL: 9.48 | LR: 2.909567e-04\n",
      "Epoch 20/100 | Train Loss: 2.2491 | Train PPL: 9.48 | Val Loss: 2.6685 | Val PPL: 14.42\n",
      "Epoch 20/100 | Train Loss: 2.2491 | Train PPL: 9.48 | Val Loss: 2.6685 | Val PPL: 14.42\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.48\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.48\n",
      "  Step 100/4407 - Loss: 2.1864 | PPL: 8.90 | LR: 2.909132e-04\n",
      "  Step 100/4407 - Loss: 2.1864 | PPL: 8.90 | LR: 2.909132e-04\n",
      "  Step 200/4407 - Loss: 2.1901 | PPL: 8.94 | LR: 2.908724e-04\n",
      "  Step 200/4407 - Loss: 2.1901 | PPL: 8.94 | LR: 2.908724e-04\n",
      "  Step 300/4407 - Loss: 2.1929 | PPL: 8.96 | LR: 2.908316e-04\n",
      "  Step 300/4407 - Loss: 2.1929 | PPL: 8.96 | LR: 2.908316e-04\n",
      "  Step 400/4407 - Loss: 2.1924 | PPL: 8.96 | LR: 2.907906e-04\n",
      "  Step 400/4407 - Loss: 2.1924 | PPL: 8.96 | LR: 2.907906e-04\n",
      "  Step 500/4407 - Loss: 2.1977 | PPL: 9.00 | LR: 2.907496e-04\n",
      "  Step 500/4407 - Loss: 2.1977 | PPL: 9.00 | LR: 2.907496e-04\n",
      "  Step 600/4407 - Loss: 2.1973 | PPL: 9.00 | LR: 2.907085e-04\n",
      "  Step 600/4407 - Loss: 2.1973 | PPL: 9.00 | LR: 2.907085e-04\n",
      "  Step 700/4407 - Loss: 2.2008 | PPL: 9.03 | LR: 2.906673e-04\n",
      "  Step 700/4407 - Loss: 2.2008 | PPL: 9.03 | LR: 2.906673e-04\n",
      "  Step 800/4407 - Loss: 2.2062 | PPL: 9.08 | LR: 2.906260e-04\n",
      "  Step 800/4407 - Loss: 2.2062 | PPL: 9.08 | LR: 2.906260e-04\n",
      "  Step 900/4407 - Loss: 2.2046 | PPL: 9.07 | LR: 2.905846e-04\n",
      "  Step 900/4407 - Loss: 2.2046 | PPL: 9.07 | LR: 2.905846e-04\n",
      "  Step 1000/4407 - Loss: 2.2080 | PPL: 9.10 | LR: 2.905431e-04\n",
      "  Step 1000/4407 - Loss: 2.2080 | PPL: 9.10 | LR: 2.905431e-04\n",
      "  Step 1100/4407 - Loss: 2.2063 | PPL: 9.08 | LR: 2.905016e-04\n",
      "  Step 1100/4407 - Loss: 2.2063 | PPL: 9.08 | LR: 2.905016e-04\n",
      "  Step 1200/4407 - Loss: 2.2089 | PPL: 9.11 | LR: 2.904599e-04\n",
      "  Step 1200/4407 - Loss: 2.2089 | PPL: 9.11 | LR: 2.904599e-04\n",
      "  Step 1300/4407 - Loss: 2.2091 | PPL: 9.11 | LR: 2.904182e-04\n",
      "  Step 1300/4407 - Loss: 2.2091 | PPL: 9.11 | LR: 2.904182e-04\n",
      "  Step 1400/4407 - Loss: 2.2085 | PPL: 9.10 | LR: 2.903763e-04\n",
      "  Step 1400/4407 - Loss: 2.2085 | PPL: 9.10 | LR: 2.903763e-04\n",
      "  Step 1500/4407 - Loss: 2.2119 | PPL: 9.13 | LR: 2.903344e-04\n",
      "  Step 1500/4407 - Loss: 2.2119 | PPL: 9.13 | LR: 2.903344e-04\n",
      "  Step 1600/4407 - Loss: 2.2134 | PPL: 9.15 | LR: 2.902924e-04\n",
      "  Step 1600/4407 - Loss: 2.2134 | PPL: 9.15 | LR: 2.902924e-04\n",
      "  Step 1700/4407 - Loss: 2.2152 | PPL: 9.16 | LR: 2.902503e-04\n",
      "  Step 1700/4407 - Loss: 2.2152 | PPL: 9.16 | LR: 2.902503e-04\n",
      "  Step 1800/4407 - Loss: 2.2163 | PPL: 9.17 | LR: 2.902082e-04\n",
      "  Step 1800/4407 - Loss: 2.2163 | PPL: 9.17 | LR: 2.902082e-04\n",
      "  Step 1900/4407 - Loss: 2.2182 | PPL: 9.19 | LR: 2.901659e-04\n",
      "  Step 1900/4407 - Loss: 2.2182 | PPL: 9.19 | LR: 2.901659e-04\n",
      "  Step 2000/4407 - Loss: 2.2193 | PPL: 9.20 | LR: 2.901235e-04\n",
      "  Step 2000/4407 - Loss: 2.2193 | PPL: 9.20 | LR: 2.901235e-04\n",
      "  Step 2100/4407 - Loss: 2.2199 | PPL: 9.21 | LR: 2.900811e-04\n",
      "  Step 2100/4407 - Loss: 2.2199 | PPL: 9.21 | LR: 2.900811e-04\n",
      "  Step 2200/4407 - Loss: 2.2220 | PPL: 9.23 | LR: 2.900386e-04\n",
      "  Step 2200/4407 - Loss: 2.2220 | PPL: 9.23 | LR: 2.900386e-04\n",
      "  Step 2300/4407 - Loss: 2.2219 | PPL: 9.23 | LR: 2.899959e-04\n",
      "  Step 2300/4407 - Loss: 2.2219 | PPL: 9.23 | LR: 2.899959e-04\n",
      "  Step 2400/4407 - Loss: 2.2231 | PPL: 9.24 | LR: 2.899532e-04\n",
      "  Step 2400/4407 - Loss: 2.2231 | PPL: 9.24 | LR: 2.899532e-04\n",
      "  Step 2500/4407 - Loss: 2.2249 | PPL: 9.25 | LR: 2.899104e-04\n",
      "  Step 2500/4407 - Loss: 2.2249 | PPL: 9.25 | LR: 2.899104e-04\n",
      "  Step 2600/4407 - Loss: 2.2250 | PPL: 9.25 | LR: 2.898676e-04\n",
      "  Step 2600/4407 - Loss: 2.2250 | PPL: 9.25 | LR: 2.898676e-04\n",
      "  Step 2700/4407 - Loss: 2.2256 | PPL: 9.26 | LR: 2.898246e-04\n",
      "  Step 2700/4407 - Loss: 2.2256 | PPL: 9.26 | LR: 2.898246e-04\n",
      "  Step 2800/4407 - Loss: 2.2278 | PPL: 9.28 | LR: 2.897815e-04\n",
      "  Step 2800/4407 - Loss: 2.2278 | PPL: 9.28 | LR: 2.897815e-04\n",
      "  Step 2900/4407 - Loss: 2.2291 | PPL: 9.29 | LR: 2.897384e-04\n",
      "  Step 2900/4407 - Loss: 2.2291 | PPL: 9.29 | LR: 2.897384e-04\n",
      "  Step 3000/4407 - Loss: 2.2289 | PPL: 9.29 | LR: 2.896951e-04\n",
      "  Step 3000/4407 - Loss: 2.2289 | PPL: 9.29 | LR: 2.896951e-04\n",
      "  Step 3100/4407 - Loss: 2.2304 | PPL: 9.30 | LR: 2.896518e-04\n",
      "  Step 3100/4407 - Loss: 2.2304 | PPL: 9.30 | LR: 2.896518e-04\n",
      "  Step 3200/4407 - Loss: 2.2307 | PPL: 9.31 | LR: 2.896084e-04\n",
      "  Step 3200/4407 - Loss: 2.2307 | PPL: 9.31 | LR: 2.896084e-04\n",
      "  Step 3300/4407 - Loss: 2.2310 | PPL: 9.31 | LR: 2.895649e-04\n",
      "  Step 3300/4407 - Loss: 2.2310 | PPL: 9.31 | LR: 2.895649e-04\n",
      "  Step 3400/4407 - Loss: 2.2304 | PPL: 9.30 | LR: 2.895213e-04\n",
      "  Step 3400/4407 - Loss: 2.2304 | PPL: 9.30 | LR: 2.895213e-04\n",
      "  Step 3500/4407 - Loss: 2.2314 | PPL: 9.31 | LR: 2.894777e-04\n",
      "  Step 3500/4407 - Loss: 2.2314 | PPL: 9.31 | LR: 2.894777e-04\n",
      "  Step 3600/4407 - Loss: 2.2336 | PPL: 9.33 | LR: 2.894339e-04\n",
      "  Step 3600/4407 - Loss: 2.2336 | PPL: 9.33 | LR: 2.894339e-04\n",
      "  Step 3700/4407 - Loss: 2.2341 | PPL: 9.34 | LR: 2.893901e-04\n",
      "  Step 3700/4407 - Loss: 2.2341 | PPL: 9.34 | LR: 2.893901e-04\n",
      "  Step 3800/4407 - Loss: 2.2341 | PPL: 9.34 | LR: 2.893461e-04\n",
      "  Step 3800/4407 - Loss: 2.2341 | PPL: 9.34 | LR: 2.893461e-04\n",
      "  Step 3900/4407 - Loss: 2.2347 | PPL: 9.34 | LR: 2.893021e-04\n",
      "  Step 3900/4407 - Loss: 2.2347 | PPL: 9.34 | LR: 2.893021e-04\n",
      "  Step 4000/4407 - Loss: 2.2350 | PPL: 9.35 | LR: 2.892580e-04\n",
      "  Step 4000/4407 - Loss: 2.2350 | PPL: 9.35 | LR: 2.892580e-04\n",
      "  Step 4100/4407 - Loss: 2.2351 | PPL: 9.35 | LR: 2.892138e-04\n",
      "  Step 4100/4407 - Loss: 2.2351 | PPL: 9.35 | LR: 2.892138e-04\n",
      "  Step 4200/4407 - Loss: 2.2359 | PPL: 9.35 | LR: 2.891695e-04\n",
      "  Step 4200/4407 - Loss: 2.2359 | PPL: 9.35 | LR: 2.891695e-04\n",
      "  Step 4300/4407 - Loss: 2.2353 | PPL: 9.35 | LR: 2.891252e-04\n",
      "  Step 4300/4407 - Loss: 2.2353 | PPL: 9.35 | LR: 2.891252e-04\n",
      "  Step 4400/4407 - Loss: 2.2355 | PPL: 9.35 | LR: 2.890807e-04\n",
      "  Step 4400/4407 - Loss: 2.2355 | PPL: 9.35 | LR: 2.890807e-04\n",
      "Epoch 21/100 | Train Loss: 2.2353 | Train PPL: 9.35 | Val Loss: 2.6743 | Val PPL: 14.50\n",
      "Epoch 21/100 | Train Loss: 2.2353 | Train PPL: 9.35 | Val Loss: 2.6743 | Val PPL: 14.50\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.35\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.35\n",
      "  Step 100/4407 - Loss: 2.1876 | PPL: 8.91 | LR: 2.890330e-04\n",
      "  Step 100/4407 - Loss: 2.1876 | PPL: 8.91 | LR: 2.890330e-04\n",
      "  Step 200/4407 - Loss: 2.1930 | PPL: 8.96 | LR: 2.889884e-04\n",
      "  Step 200/4407 - Loss: 2.1930 | PPL: 8.96 | LR: 2.889884e-04\n",
      "  Step 300/4407 - Loss: 2.2011 | PPL: 9.03 | LR: 2.889437e-04\n",
      "  Step 300/4407 - Loss: 2.2011 | PPL: 9.03 | LR: 2.889437e-04\n",
      "  Step 400/4407 - Loss: 2.1949 | PPL: 8.98 | LR: 2.888989e-04\n",
      "  Step 400/4407 - Loss: 2.1949 | PPL: 8.98 | LR: 2.888989e-04\n",
      "  Step 500/4407 - Loss: 2.1983 | PPL: 9.01 | LR: 2.888540e-04\n",
      "  Step 500/4407 - Loss: 2.1983 | PPL: 9.01 | LR: 2.888540e-04\n",
      "  Step 600/4407 - Loss: 2.1952 | PPL: 8.98 | LR: 2.888090e-04\n",
      "  Step 600/4407 - Loss: 2.1952 | PPL: 8.98 | LR: 2.888090e-04\n",
      "  Step 700/4407 - Loss: 2.2021 | PPL: 9.04 | LR: 2.887639e-04\n",
      "  Step 700/4407 - Loss: 2.2021 | PPL: 9.04 | LR: 2.887639e-04\n",
      "  Step 800/4407 - Loss: 2.2039 | PPL: 9.06 | LR: 2.887187e-04\n",
      "  Step 800/4407 - Loss: 2.2039 | PPL: 9.06 | LR: 2.887187e-04\n",
      "  Step 900/4407 - Loss: 2.2060 | PPL: 9.08 | LR: 2.886735e-04\n",
      "  Step 900/4407 - Loss: 2.2060 | PPL: 9.08 | LR: 2.886735e-04\n",
      "  Step 1000/4407 - Loss: 2.2025 | PPL: 9.05 | LR: 2.886281e-04\n",
      "  Step 1000/4407 - Loss: 2.2025 | PPL: 9.05 | LR: 2.886281e-04\n",
      "  Step 1100/4407 - Loss: 2.2047 | PPL: 9.07 | LR: 2.885827e-04\n",
      "  Step 1100/4407 - Loss: 2.2047 | PPL: 9.07 | LR: 2.885827e-04\n",
      "  Step 1200/4407 - Loss: 2.2048 | PPL: 9.07 | LR: 2.885372e-04\n",
      "  Step 1200/4407 - Loss: 2.2048 | PPL: 9.07 | LR: 2.885372e-04\n",
      "  Step 1300/4407 - Loss: 2.2059 | PPL: 9.08 | LR: 2.884916e-04\n",
      "  Step 1300/4407 - Loss: 2.2059 | PPL: 9.08 | LR: 2.884916e-04\n",
      "  Step 1400/4407 - Loss: 2.2070 | PPL: 9.09 | LR: 2.884459e-04\n",
      "  Step 1400/4407 - Loss: 2.2070 | PPL: 9.09 | LR: 2.884459e-04\n",
      "  Step 1500/4407 - Loss: 2.2076 | PPL: 9.09 | LR: 2.884002e-04\n",
      "  Step 1500/4407 - Loss: 2.2076 | PPL: 9.09 | LR: 2.884002e-04\n",
      "  Step 1600/4407 - Loss: 2.2098 | PPL: 9.11 | LR: 2.883543e-04\n",
      "  Step 1600/4407 - Loss: 2.2098 | PPL: 9.11 | LR: 2.883543e-04\n",
      "  Step 1700/4407 - Loss: 2.2092 | PPL: 9.11 | LR: 2.883084e-04\n",
      "  Step 1700/4407 - Loss: 2.2092 | PPL: 9.11 | LR: 2.883084e-04\n",
      "  Step 1800/4407 - Loss: 2.2086 | PPL: 9.10 | LR: 2.882623e-04\n",
      "  Step 1800/4407 - Loss: 2.2086 | PPL: 9.10 | LR: 2.882623e-04\n",
      "  Step 1900/4407 - Loss: 2.2105 | PPL: 9.12 | LR: 2.882162e-04\n",
      "  Step 1900/4407 - Loss: 2.2105 | PPL: 9.12 | LR: 2.882162e-04\n",
      "  Step 2000/4407 - Loss: 2.2107 | PPL: 9.12 | LR: 2.881700e-04\n",
      "  Step 2000/4407 - Loss: 2.2107 | PPL: 9.12 | LR: 2.881700e-04\n",
      "  Step 2100/4407 - Loss: 2.2104 | PPL: 9.12 | LR: 2.881237e-04\n",
      "  Step 2100/4407 - Loss: 2.2104 | PPL: 9.12 | LR: 2.881237e-04\n",
      "  Step 2200/4407 - Loss: 2.2111 | PPL: 9.13 | LR: 2.880774e-04\n",
      "  Step 2200/4407 - Loss: 2.2111 | PPL: 9.13 | LR: 2.880774e-04\n",
      "  Step 2300/4407 - Loss: 2.2107 | PPL: 9.12 | LR: 2.880309e-04\n",
      "  Step 2300/4407 - Loss: 2.2107 | PPL: 9.12 | LR: 2.880309e-04\n",
      "  Step 2400/4407 - Loss: 2.2114 | PPL: 9.13 | LR: 2.879843e-04\n",
      "  Step 2400/4407 - Loss: 2.2114 | PPL: 9.13 | LR: 2.879843e-04\n",
      "  Step 2500/4407 - Loss: 2.2101 | PPL: 9.12 | LR: 2.879377e-04\n",
      "  Step 2500/4407 - Loss: 2.2101 | PPL: 9.12 | LR: 2.879377e-04\n",
      "  Step 2600/4407 - Loss: 2.2120 | PPL: 9.13 | LR: 2.878910e-04\n",
      "  Step 2600/4407 - Loss: 2.2120 | PPL: 9.13 | LR: 2.878910e-04\n",
      "  Step 2700/4407 - Loss: 2.2118 | PPL: 9.13 | LR: 2.878442e-04\n",
      "  Step 2700/4407 - Loss: 2.2118 | PPL: 9.13 | LR: 2.878442e-04\n",
      "  Step 2800/4407 - Loss: 2.2124 | PPL: 9.14 | LR: 2.877973e-04\n",
      "  Step 2800/4407 - Loss: 2.2124 | PPL: 9.14 | LR: 2.877973e-04\n",
      "  Step 2900/4407 - Loss: 2.2135 | PPL: 9.15 | LR: 2.877503e-04\n",
      "  Step 2900/4407 - Loss: 2.2135 | PPL: 9.15 | LR: 2.877503e-04\n",
      "  Step 3000/4407 - Loss: 2.2140 | PPL: 9.15 | LR: 2.877032e-04\n",
      "  Step 3000/4407 - Loss: 2.2140 | PPL: 9.15 | LR: 2.877032e-04\n",
      "  Step 3100/4407 - Loss: 2.2139 | PPL: 9.15 | LR: 2.876561e-04\n",
      "  Step 3100/4407 - Loss: 2.2139 | PPL: 9.15 | LR: 2.876561e-04\n",
      "  Step 3200/4407 - Loss: 2.2152 | PPL: 9.16 | LR: 2.876088e-04\n",
      "  Step 3200/4407 - Loss: 2.2152 | PPL: 9.16 | LR: 2.876088e-04\n",
      "  Step 3300/4407 - Loss: 2.2152 | PPL: 9.16 | LR: 2.875615e-04\n",
      "  Step 3300/4407 - Loss: 2.2152 | PPL: 9.16 | LR: 2.875615e-04\n",
      "  Step 3400/4407 - Loss: 2.2157 | PPL: 9.17 | LR: 2.875141e-04\n",
      "  Step 3400/4407 - Loss: 2.2157 | PPL: 9.17 | LR: 2.875141e-04\n",
      "  Step 3500/4407 - Loss: 2.2165 | PPL: 9.18 | LR: 2.874666e-04\n",
      "  Step 3500/4407 - Loss: 2.2165 | PPL: 9.18 | LR: 2.874666e-04\n",
      "  Step 3600/4407 - Loss: 2.2168 | PPL: 9.18 | LR: 2.874190e-04\n",
      "  Step 3600/4407 - Loss: 2.2168 | PPL: 9.18 | LR: 2.874190e-04\n",
      "  Step 3700/4407 - Loss: 2.2176 | PPL: 9.19 | LR: 2.873713e-04\n",
      "  Step 3700/4407 - Loss: 2.2176 | PPL: 9.19 | LR: 2.873713e-04\n",
      "  Step 3800/4407 - Loss: 2.2179 | PPL: 9.19 | LR: 2.873236e-04\n",
      "  Step 3800/4407 - Loss: 2.2179 | PPL: 9.19 | LR: 2.873236e-04\n",
      "  Step 3900/4407 - Loss: 2.2191 | PPL: 9.20 | LR: 2.872757e-04\n",
      "  Step 3900/4407 - Loss: 2.2191 | PPL: 9.20 | LR: 2.872757e-04\n",
      "  Step 4000/4407 - Loss: 2.2200 | PPL: 9.21 | LR: 2.872278e-04\n",
      "  Step 4000/4407 - Loss: 2.2200 | PPL: 9.21 | LR: 2.872278e-04\n",
      "  Step 4100/4407 - Loss: 2.2207 | PPL: 9.21 | LR: 2.871798e-04\n",
      "  Step 4100/4407 - Loss: 2.2207 | PPL: 9.21 | LR: 2.871798e-04\n",
      "  Step 4200/4407 - Loss: 2.2214 | PPL: 9.22 | LR: 2.871317e-04\n",
      "  Step 4200/4407 - Loss: 2.2214 | PPL: 9.22 | LR: 2.871317e-04\n",
      "  Step 4300/4407 - Loss: 2.2218 | PPL: 9.22 | LR: 2.870835e-04\n",
      "  Step 4300/4407 - Loss: 2.2218 | PPL: 9.22 | LR: 2.870835e-04\n",
      "  Step 4400/4407 - Loss: 2.2231 | PPL: 9.24 | LR: 2.870352e-04\n",
      "  Step 4400/4407 - Loss: 2.2231 | PPL: 9.24 | LR: 2.870352e-04\n",
      "Epoch 22/100 | Train Loss: 2.2234 | Train PPL: 9.24 | Val Loss: 2.6535 | Val PPL: 14.20\n",
      "Epoch 22/100 | Train Loss: 2.2234 | Train PPL: 9.24 | Val Loss: 2.6535 | Val PPL: 14.20\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.24\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.24\n",
      "  Step 100/4407 - Loss: 2.1783 | PPL: 8.83 | LR: 2.869835e-04\n",
      "  Step 100/4407 - Loss: 2.1783 | PPL: 8.83 | LR: 2.869835e-04\n",
      "  Step 200/4407 - Loss: 2.1918 | PPL: 8.95 | LR: 2.869350e-04\n",
      "  Step 200/4407 - Loss: 2.1918 | PPL: 8.95 | LR: 2.869350e-04\n",
      "  Step 300/4407 - Loss: 2.1824 | PPL: 8.87 | LR: 2.868865e-04\n",
      "  Step 300/4407 - Loss: 2.1824 | PPL: 8.87 | LR: 2.868865e-04\n",
      "  Step 400/4407 - Loss: 2.1835 | PPL: 8.88 | LR: 2.868378e-04\n",
      "  Step 400/4407 - Loss: 2.1835 | PPL: 8.88 | LR: 2.868378e-04\n",
      "  Step 500/4407 - Loss: 2.1890 | PPL: 8.93 | LR: 2.867891e-04\n",
      "  Step 500/4407 - Loss: 2.1890 | PPL: 8.93 | LR: 2.867891e-04\n",
      "  Step 600/4407 - Loss: 2.1902 | PPL: 8.94 | LR: 2.867403e-04\n",
      "  Step 600/4407 - Loss: 2.1902 | PPL: 8.94 | LR: 2.867403e-04\n",
      "  Step 700/4407 - Loss: 2.1897 | PPL: 8.93 | LR: 2.866914e-04\n",
      "  Step 700/4407 - Loss: 2.1897 | PPL: 8.93 | LR: 2.866914e-04\n",
      "  Step 800/4407 - Loss: 2.1907 | PPL: 8.94 | LR: 2.866425e-04\n",
      "  Step 800/4407 - Loss: 2.1907 | PPL: 8.94 | LR: 2.866425e-04\n",
      "  Step 900/4407 - Loss: 2.1905 | PPL: 8.94 | LR: 2.865934e-04\n",
      "  Step 900/4407 - Loss: 2.1905 | PPL: 8.94 | LR: 2.865934e-04\n",
      "  Step 1000/4407 - Loss: 2.1914 | PPL: 8.95 | LR: 2.865443e-04\n",
      "  Step 1000/4407 - Loss: 2.1914 | PPL: 8.95 | LR: 2.865443e-04\n",
      "  Step 1100/4407 - Loss: 2.1900 | PPL: 8.94 | LR: 2.864951e-04\n",
      "  Step 1100/4407 - Loss: 2.1900 | PPL: 8.94 | LR: 2.864951e-04\n",
      "  Step 1200/4407 - Loss: 2.1888 | PPL: 8.92 | LR: 2.864457e-04\n",
      "  Step 1200/4407 - Loss: 2.1888 | PPL: 8.92 | LR: 2.864457e-04\n",
      "  Step 1300/4407 - Loss: 2.1887 | PPL: 8.92 | LR: 2.863963e-04\n",
      "  Step 1300/4407 - Loss: 2.1887 | PPL: 8.92 | LR: 2.863963e-04\n",
      "  Step 1400/4407 - Loss: 2.1917 | PPL: 8.95 | LR: 2.863469e-04\n",
      "  Step 1400/4407 - Loss: 2.1917 | PPL: 8.95 | LR: 2.863469e-04\n",
      "  Step 1500/4407 - Loss: 2.1957 | PPL: 8.99 | LR: 2.862973e-04\n",
      "  Step 1500/4407 - Loss: 2.1957 | PPL: 8.99 | LR: 2.862973e-04\n",
      "  Step 1600/4407 - Loss: 2.1951 | PPL: 8.98 | LR: 2.862476e-04\n",
      "  Step 1600/4407 - Loss: 2.1951 | PPL: 8.98 | LR: 2.862476e-04\n",
      "  Step 1700/4407 - Loss: 2.1972 | PPL: 9.00 | LR: 2.861979e-04\n",
      "  Step 1700/4407 - Loss: 2.1972 | PPL: 9.00 | LR: 2.861979e-04\n",
      "  Step 1800/4407 - Loss: 2.1981 | PPL: 9.01 | LR: 2.861481e-04\n",
      "  Step 1800/4407 - Loss: 2.1981 | PPL: 9.01 | LR: 2.861481e-04\n",
      "  Step 1900/4407 - Loss: 2.1999 | PPL: 9.02 | LR: 2.860982e-04\n",
      "  Step 1900/4407 - Loss: 2.1999 | PPL: 9.02 | LR: 2.860982e-04\n",
      "  Step 2000/4407 - Loss: 2.1982 | PPL: 9.01 | LR: 2.860482e-04\n",
      "  Step 2000/4407 - Loss: 2.1982 | PPL: 9.01 | LR: 2.860482e-04\n",
      "  Step 2100/4407 - Loss: 2.2001 | PPL: 9.03 | LR: 2.859981e-04\n",
      "  Step 2100/4407 - Loss: 2.2001 | PPL: 9.03 | LR: 2.859981e-04\n",
      "  Step 2200/4407 - Loss: 2.2005 | PPL: 9.03 | LR: 2.859479e-04\n",
      "  Step 2200/4407 - Loss: 2.2005 | PPL: 9.03 | LR: 2.859479e-04\n",
      "  Step 2300/4407 - Loss: 2.2009 | PPL: 9.03 | LR: 2.858977e-04\n",
      "  Step 2300/4407 - Loss: 2.2009 | PPL: 9.03 | LR: 2.858977e-04\n",
      "  Step 2400/4407 - Loss: 2.2011 | PPL: 9.04 | LR: 2.858473e-04\n",
      "  Step 2400/4407 - Loss: 2.2011 | PPL: 9.04 | LR: 2.858473e-04\n",
      "  Step 2500/4407 - Loss: 2.2019 | PPL: 9.04 | LR: 2.857969e-04\n",
      "  Step 2500/4407 - Loss: 2.2019 | PPL: 9.04 | LR: 2.857969e-04\n",
      "  Step 2600/4407 - Loss: 2.2032 | PPL: 9.05 | LR: 2.857464e-04\n",
      "  Step 2600/4407 - Loss: 2.2032 | PPL: 9.05 | LR: 2.857464e-04\n",
      "  Step 2700/4407 - Loss: 2.2038 | PPL: 9.06 | LR: 2.856958e-04\n",
      "  Step 2700/4407 - Loss: 2.2038 | PPL: 9.06 | LR: 2.856958e-04\n",
      "  Step 2800/4407 - Loss: 2.2035 | PPL: 9.06 | LR: 2.856451e-04\n",
      "  Step 2800/4407 - Loss: 2.2035 | PPL: 9.06 | LR: 2.856451e-04\n",
      "  Step 2900/4407 - Loss: 2.2050 | PPL: 9.07 | LR: 2.855944e-04\n",
      "  Step 2900/4407 - Loss: 2.2050 | PPL: 9.07 | LR: 2.855944e-04\n",
      "  Step 3000/4407 - Loss: 2.2051 | PPL: 9.07 | LR: 2.855435e-04\n",
      "  Step 3000/4407 - Loss: 2.2051 | PPL: 9.07 | LR: 2.855435e-04\n",
      "  Step 3100/4407 - Loss: 2.2064 | PPL: 9.08 | LR: 2.854926e-04\n",
      "  Step 3100/4407 - Loss: 2.2064 | PPL: 9.08 | LR: 2.854926e-04\n",
      "  Step 3200/4407 - Loss: 2.2057 | PPL: 9.08 | LR: 2.854416e-04\n",
      "  Step 3200/4407 - Loss: 2.2057 | PPL: 9.08 | LR: 2.854416e-04\n",
      "  Step 3300/4407 - Loss: 2.2052 | PPL: 9.07 | LR: 2.853905e-04\n",
      "  Step 3300/4407 - Loss: 2.2052 | PPL: 9.07 | LR: 2.853905e-04\n",
      "  Step 3400/4407 - Loss: 2.2066 | PPL: 9.08 | LR: 2.853393e-04\n",
      "  Step 3400/4407 - Loss: 2.2066 | PPL: 9.08 | LR: 2.853393e-04\n",
      "  Step 3500/4407 - Loss: 2.2061 | PPL: 9.08 | LR: 2.852880e-04\n",
      "  Step 3500/4407 - Loss: 2.2061 | PPL: 9.08 | LR: 2.852880e-04\n",
      "  Step 3600/4407 - Loss: 2.2061 | PPL: 9.08 | LR: 2.852367e-04\n",
      "  Step 3600/4407 - Loss: 2.2061 | PPL: 9.08 | LR: 2.852367e-04\n",
      "  Step 3700/4407 - Loss: 2.2078 | PPL: 9.10 | LR: 2.851852e-04\n",
      "  Step 3700/4407 - Loss: 2.2078 | PPL: 9.10 | LR: 2.851852e-04\n",
      "  Step 3800/4407 - Loss: 2.2091 | PPL: 9.11 | LR: 2.851337e-04\n",
      "  Step 3800/4407 - Loss: 2.2091 | PPL: 9.11 | LR: 2.851337e-04\n",
      "  Step 3900/4407 - Loss: 2.2097 | PPL: 9.11 | LR: 2.850821e-04\n",
      "  Step 3900/4407 - Loss: 2.2097 | PPL: 9.11 | LR: 2.850821e-04\n",
      "  Step 4000/4407 - Loss: 2.2108 | PPL: 9.12 | LR: 2.850304e-04\n",
      "  Step 4000/4407 - Loss: 2.2108 | PPL: 9.12 | LR: 2.850304e-04\n",
      "  Step 4100/4407 - Loss: 2.2114 | PPL: 9.13 | LR: 2.849786e-04\n",
      "  Step 4100/4407 - Loss: 2.2114 | PPL: 9.13 | LR: 2.849786e-04\n",
      "  Step 4200/4407 - Loss: 2.2107 | PPL: 9.12 | LR: 2.849267e-04\n",
      "  Step 4200/4407 - Loss: 2.2107 | PPL: 9.12 | LR: 2.849267e-04\n",
      "  Step 4300/4407 - Loss: 2.2105 | PPL: 9.12 | LR: 2.848748e-04\n",
      "  Step 4300/4407 - Loss: 2.2105 | PPL: 9.12 | LR: 2.848748e-04\n",
      "  Step 4400/4407 - Loss: 2.2118 | PPL: 9.13 | LR: 2.848228e-04\n",
      "  Step 4400/4407 - Loss: 2.2118 | PPL: 9.13 | LR: 2.848228e-04\n",
      "Epoch 23/100 | Train Loss: 2.2119 | Train PPL: 9.13 | Val Loss: 2.6541 | Val PPL: 14.21\n",
      "Epoch 23/100 | Train Loss: 2.2119 | Train PPL: 9.13 | Val Loss: 2.6541 | Val PPL: 14.21\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.13\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.13\n",
      "  Step 100/4407 - Loss: 2.1510 | PPL: 8.59 | LR: 2.847670e-04\n",
      "  Step 100/4407 - Loss: 2.1510 | PPL: 8.59 | LR: 2.847670e-04\n",
      "  Step 200/4407 - Loss: 2.1654 | PPL: 8.72 | LR: 2.847148e-04\n",
      "  Step 200/4407 - Loss: 2.1654 | PPL: 8.72 | LR: 2.847148e-04\n",
      "  Step 300/4407 - Loss: 2.1513 | PPL: 8.60 | LR: 2.846625e-04\n",
      "  Step 300/4407 - Loss: 2.1513 | PPL: 8.60 | LR: 2.846625e-04\n",
      "  Step 400/4407 - Loss: 2.1617 | PPL: 8.69 | LR: 2.846101e-04\n",
      "  Step 400/4407 - Loss: 2.1617 | PPL: 8.69 | LR: 2.846101e-04\n",
      "  Step 500/4407 - Loss: 2.1586 | PPL: 8.66 | LR: 2.845576e-04\n",
      "  Step 500/4407 - Loss: 2.1586 | PPL: 8.66 | LR: 2.845576e-04\n",
      "  Step 600/4407 - Loss: 2.1587 | PPL: 8.66 | LR: 2.845051e-04\n",
      "  Step 600/4407 - Loss: 2.1587 | PPL: 8.66 | LR: 2.845051e-04\n",
      "  Step 700/4407 - Loss: 2.1625 | PPL: 8.69 | LR: 2.844525e-04\n",
      "  Step 700/4407 - Loss: 2.1625 | PPL: 8.69 | LR: 2.844525e-04\n",
      "  Step 800/4407 - Loss: 2.1689 | PPL: 8.75 | LR: 2.843997e-04\n",
      "  Step 800/4407 - Loss: 2.1689 | PPL: 8.75 | LR: 2.843997e-04\n",
      "  Step 900/4407 - Loss: 2.1724 | PPL: 8.78 | LR: 2.843469e-04\n",
      "  Step 900/4407 - Loss: 2.1724 | PPL: 8.78 | LR: 2.843469e-04\n",
      "  Step 1000/4407 - Loss: 2.1728 | PPL: 8.78 | LR: 2.842941e-04\n",
      "  Step 1000/4407 - Loss: 2.1728 | PPL: 8.78 | LR: 2.842941e-04\n",
      "  Step 1100/4407 - Loss: 2.1766 | PPL: 8.82 | LR: 2.842411e-04\n",
      "  Step 1100/4407 - Loss: 2.1766 | PPL: 8.82 | LR: 2.842411e-04\n",
      "  Step 1200/4407 - Loss: 2.1736 | PPL: 8.79 | LR: 2.841880e-04\n",
      "  Step 1200/4407 - Loss: 2.1736 | PPL: 8.79 | LR: 2.841880e-04\n",
      "  Step 1300/4407 - Loss: 2.1755 | PPL: 8.81 | LR: 2.841349e-04\n",
      "  Step 1300/4407 - Loss: 2.1755 | PPL: 8.81 | LR: 2.841349e-04\n",
      "  Step 1400/4407 - Loss: 2.1778 | PPL: 8.83 | LR: 2.840817e-04\n",
      "  Step 1400/4407 - Loss: 2.1778 | PPL: 8.83 | LR: 2.840817e-04\n",
      "  Step 1500/4407 - Loss: 2.1799 | PPL: 8.85 | LR: 2.840284e-04\n",
      "  Step 1500/4407 - Loss: 2.1799 | PPL: 8.85 | LR: 2.840284e-04\n",
      "  Step 1600/4407 - Loss: 2.1820 | PPL: 8.86 | LR: 2.839750e-04\n",
      "  Step 1600/4407 - Loss: 2.1820 | PPL: 8.86 | LR: 2.839750e-04\n",
      "  Step 1700/4407 - Loss: 2.1834 | PPL: 8.88 | LR: 2.839215e-04\n",
      "  Step 1700/4407 - Loss: 2.1834 | PPL: 8.88 | LR: 2.839215e-04\n",
      "  Step 1800/4407 - Loss: 2.1819 | PPL: 8.86 | LR: 2.838679e-04\n",
      "  Step 1800/4407 - Loss: 2.1819 | PPL: 8.86 | LR: 2.838679e-04\n",
      "  Step 1900/4407 - Loss: 2.1860 | PPL: 8.90 | LR: 2.838143e-04\n",
      "  Step 1900/4407 - Loss: 2.1860 | PPL: 8.90 | LR: 2.838143e-04\n",
      "  Step 2000/4407 - Loss: 2.1872 | PPL: 8.91 | LR: 2.837606e-04\n",
      "  Step 2000/4407 - Loss: 2.1872 | PPL: 8.91 | LR: 2.837606e-04\n",
      "  Step 2100/4407 - Loss: 2.1871 | PPL: 8.91 | LR: 2.837068e-04\n",
      "  Step 2100/4407 - Loss: 2.1871 | PPL: 8.91 | LR: 2.837068e-04\n",
      "  Step 2200/4407 - Loss: 2.1879 | PPL: 8.92 | LR: 2.836529e-04\n",
      "  Step 2200/4407 - Loss: 2.1879 | PPL: 8.92 | LR: 2.836529e-04\n",
      "  Step 2300/4407 - Loss: 2.1915 | PPL: 8.95 | LR: 2.835989e-04\n",
      "  Step 2300/4407 - Loss: 2.1915 | PPL: 8.95 | LR: 2.835989e-04\n",
      "  Step 2400/4407 - Loss: 2.1922 | PPL: 8.95 | LR: 2.835448e-04\n",
      "  Step 2400/4407 - Loss: 2.1922 | PPL: 8.95 | LR: 2.835448e-04\n",
      "  Step 2500/4407 - Loss: 2.1946 | PPL: 8.98 | LR: 2.834907e-04\n",
      "  Step 2500/4407 - Loss: 2.1946 | PPL: 8.98 | LR: 2.834907e-04\n",
      "  Step 2600/4407 - Loss: 2.1964 | PPL: 8.99 | LR: 2.834365e-04\n",
      "  Step 2600/4407 - Loss: 2.1964 | PPL: 8.99 | LR: 2.834365e-04\n",
      "  Step 2700/4407 - Loss: 2.1963 | PPL: 8.99 | LR: 2.833821e-04\n",
      "  Step 2700/4407 - Loss: 2.1963 | PPL: 8.99 | LR: 2.833821e-04\n",
      "  Step 2800/4407 - Loss: 2.1957 | PPL: 8.99 | LR: 2.833277e-04\n",
      "  Step 2800/4407 - Loss: 2.1957 | PPL: 8.99 | LR: 2.833277e-04\n",
      "  Step 2900/4407 - Loss: 2.1953 | PPL: 8.98 | LR: 2.832733e-04\n",
      "  Step 2900/4407 - Loss: 2.1953 | PPL: 8.98 | LR: 2.832733e-04\n",
      "  Step 3000/4407 - Loss: 2.1965 | PPL: 8.99 | LR: 2.832187e-04\n",
      "  Step 3000/4407 - Loss: 2.1965 | PPL: 8.99 | LR: 2.832187e-04\n",
      "  Step 3100/4407 - Loss: 2.1968 | PPL: 9.00 | LR: 2.831641e-04\n",
      "  Step 3100/4407 - Loss: 2.1968 | PPL: 9.00 | LR: 2.831641e-04\n",
      "  Step 3200/4407 - Loss: 2.1968 | PPL: 9.00 | LR: 2.831093e-04\n",
      "  Step 3200/4407 - Loss: 2.1968 | PPL: 9.00 | LR: 2.831093e-04\n",
      "  Step 3300/4407 - Loss: 2.1957 | PPL: 8.99 | LR: 2.830545e-04\n",
      "  Step 3300/4407 - Loss: 2.1957 | PPL: 8.99 | LR: 2.830545e-04\n",
      "  Step 3400/4407 - Loss: 2.1971 | PPL: 9.00 | LR: 2.829996e-04\n",
      "  Step 3400/4407 - Loss: 2.1971 | PPL: 9.00 | LR: 2.829996e-04\n",
      "  Step 3500/4407 - Loss: 2.1972 | PPL: 9.00 | LR: 2.829446e-04\n",
      "  Step 3500/4407 - Loss: 2.1972 | PPL: 9.00 | LR: 2.829446e-04\n",
      "  Step 3600/4407 - Loss: 2.1980 | PPL: 9.01 | LR: 2.828896e-04\n",
      "  Step 3600/4407 - Loss: 2.1980 | PPL: 9.01 | LR: 2.828896e-04\n",
      "  Step 3700/4407 - Loss: 2.1999 | PPL: 9.02 | LR: 2.828344e-04\n",
      "  Step 3700/4407 - Loss: 2.1999 | PPL: 9.02 | LR: 2.828344e-04\n",
      "  Step 3800/4407 - Loss: 2.1999 | PPL: 9.02 | LR: 2.827792e-04\n",
      "  Step 3800/4407 - Loss: 2.1999 | PPL: 9.02 | LR: 2.827792e-04\n",
      "  Step 3900/4407 - Loss: 2.2009 | PPL: 9.03 | LR: 2.827239e-04\n",
      "  Step 3900/4407 - Loss: 2.2009 | PPL: 9.03 | LR: 2.827239e-04\n",
      "  Step 4000/4407 - Loss: 2.1997 | PPL: 9.02 | LR: 2.826685e-04\n",
      "  Step 4000/4407 - Loss: 2.1997 | PPL: 9.02 | LR: 2.826685e-04\n",
      "  Step 4100/4407 - Loss: 2.2005 | PPL: 9.03 | LR: 2.826130e-04\n",
      "  Step 4100/4407 - Loss: 2.2005 | PPL: 9.03 | LR: 2.826130e-04\n",
      "  Step 4200/4407 - Loss: 2.2010 | PPL: 9.03 | LR: 2.825574e-04\n",
      "  Step 4200/4407 - Loss: 2.2010 | PPL: 9.03 | LR: 2.825574e-04\n",
      "  Step 4300/4407 - Loss: 2.2010 | PPL: 9.03 | LR: 2.825018e-04\n",
      "  Step 4300/4407 - Loss: 2.2010 | PPL: 9.03 | LR: 2.825018e-04\n",
      "  Step 4400/4407 - Loss: 2.2015 | PPL: 9.04 | LR: 2.824460e-04\n",
      "  Step 4400/4407 - Loss: 2.2015 | PPL: 9.04 | LR: 2.824460e-04\n",
      "Epoch 24/100 | Train Loss: 2.2014 | Train PPL: 9.04 | Val Loss: 2.6481 | Val PPL: 14.13\n",
      "Epoch 24/100 | Train Loss: 2.2014 | Train PPL: 9.04 | Val Loss: 2.6481 | Val PPL: 14.13\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.04\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 9.04\n",
      "  Step 100/4407 - Loss: 2.1381 | PPL: 8.48 | LR: 2.823863e-04\n",
      "  Step 100/4407 - Loss: 2.1381 | PPL: 8.48 | LR: 2.823863e-04\n",
      "  Step 200/4407 - Loss: 2.1440 | PPL: 8.53 | LR: 2.823304e-04\n",
      "  Step 200/4407 - Loss: 2.1440 | PPL: 8.53 | LR: 2.823304e-04\n",
      "  Step 300/4407 - Loss: 2.1421 | PPL: 8.52 | LR: 2.822744e-04\n",
      "  Step 300/4407 - Loss: 2.1421 | PPL: 8.52 | LR: 2.822744e-04\n",
      "  Step 400/4407 - Loss: 2.1570 | PPL: 8.64 | LR: 2.822184e-04\n",
      "  Step 400/4407 - Loss: 2.1570 | PPL: 8.64 | LR: 2.822184e-04\n",
      "  Step 500/4407 - Loss: 2.1576 | PPL: 8.65 | LR: 2.821622e-04\n",
      "  Step 500/4407 - Loss: 2.1576 | PPL: 8.65 | LR: 2.821622e-04\n",
      "  Step 600/4407 - Loss: 2.1617 | PPL: 8.69 | LR: 2.821060e-04\n",
      "  Step 600/4407 - Loss: 2.1617 | PPL: 8.69 | LR: 2.821060e-04\n",
      "  Step 700/4407 - Loss: 2.1661 | PPL: 8.72 | LR: 2.820497e-04\n",
      "  Step 700/4407 - Loss: 2.1661 | PPL: 8.72 | LR: 2.820497e-04\n",
      "  Step 800/4407 - Loss: 2.1652 | PPL: 8.72 | LR: 2.819933e-04\n",
      "  Step 800/4407 - Loss: 2.1652 | PPL: 8.72 | LR: 2.819933e-04\n",
      "  Step 900/4407 - Loss: 2.1687 | PPL: 8.75 | LR: 2.819368e-04\n",
      "  Step 900/4407 - Loss: 2.1687 | PPL: 8.75 | LR: 2.819368e-04\n",
      "  Step 1000/4407 - Loss: 2.1726 | PPL: 8.78 | LR: 2.818802e-04\n",
      "  Step 1000/4407 - Loss: 2.1726 | PPL: 8.78 | LR: 2.818802e-04\n",
      "  Step 1100/4407 - Loss: 2.1743 | PPL: 8.80 | LR: 2.818236e-04\n",
      "  Step 1100/4407 - Loss: 2.1743 | PPL: 8.80 | LR: 2.818236e-04\n",
      "  Step 1200/4407 - Loss: 2.1726 | PPL: 8.78 | LR: 2.817668e-04\n",
      "  Step 1200/4407 - Loss: 2.1726 | PPL: 8.78 | LR: 2.817668e-04\n",
      "  Step 1300/4407 - Loss: 2.1745 | PPL: 8.80 | LR: 2.817100e-04\n",
      "  Step 1300/4407 - Loss: 2.1745 | PPL: 8.80 | LR: 2.817100e-04\n",
      "  Step 1400/4407 - Loss: 2.1766 | PPL: 8.82 | LR: 2.816531e-04\n",
      "  Step 1400/4407 - Loss: 2.1766 | PPL: 8.82 | LR: 2.816531e-04\n",
      "  Step 1500/4407 - Loss: 2.1758 | PPL: 8.81 | LR: 2.815961e-04\n",
      "  Step 1500/4407 - Loss: 2.1758 | PPL: 8.81 | LR: 2.815961e-04\n",
      "  Step 1600/4407 - Loss: 2.1772 | PPL: 8.82 | LR: 2.815391e-04\n",
      "  Step 1600/4407 - Loss: 2.1772 | PPL: 8.82 | LR: 2.815391e-04\n",
      "  Step 1700/4407 - Loss: 2.1775 | PPL: 8.82 | LR: 2.814819e-04\n",
      "  Step 1700/4407 - Loss: 2.1775 | PPL: 8.82 | LR: 2.814819e-04\n",
      "  Step 1800/4407 - Loss: 2.1790 | PPL: 8.84 | LR: 2.814247e-04\n",
      "  Step 1800/4407 - Loss: 2.1790 | PPL: 8.84 | LR: 2.814247e-04\n",
      "  Step 1900/4407 - Loss: 2.1761 | PPL: 8.81 | LR: 2.813674e-04\n",
      "  Step 1900/4407 - Loss: 2.1761 | PPL: 8.81 | LR: 2.813674e-04\n",
      "  Step 2000/4407 - Loss: 2.1762 | PPL: 8.81 | LR: 2.813100e-04\n",
      "  Step 2000/4407 - Loss: 2.1762 | PPL: 8.81 | LR: 2.813100e-04\n",
      "  Step 2100/4407 - Loss: 2.1761 | PPL: 8.81 | LR: 2.812525e-04\n",
      "  Step 2100/4407 - Loss: 2.1761 | PPL: 8.81 | LR: 2.812525e-04\n",
      "  Step 2200/4407 - Loss: 2.1778 | PPL: 8.83 | LR: 2.811950e-04\n",
      "  Step 2200/4407 - Loss: 2.1778 | PPL: 8.83 | LR: 2.811950e-04\n",
      "  Step 2300/4407 - Loss: 2.1793 | PPL: 8.84 | LR: 2.811373e-04\n",
      "  Step 2300/4407 - Loss: 2.1793 | PPL: 8.84 | LR: 2.811373e-04\n",
      "  Step 2400/4407 - Loss: 2.1798 | PPL: 8.84 | LR: 2.810796e-04\n",
      "  Step 2400/4407 - Loss: 2.1798 | PPL: 8.84 | LR: 2.810796e-04\n",
      "  Step 2500/4407 - Loss: 2.1810 | PPL: 8.85 | LR: 2.810218e-04\n",
      "  Step 2500/4407 - Loss: 2.1810 | PPL: 8.85 | LR: 2.810218e-04\n",
      "  Step 2600/4407 - Loss: 2.1815 | PPL: 8.86 | LR: 2.809639e-04\n",
      "  Step 2600/4407 - Loss: 2.1815 | PPL: 8.86 | LR: 2.809639e-04\n",
      "  Step 2700/4407 - Loss: 2.1814 | PPL: 8.86 | LR: 2.809060e-04\n",
      "  Step 2700/4407 - Loss: 2.1814 | PPL: 8.86 | LR: 2.809060e-04\n",
      "  Step 2800/4407 - Loss: 2.1824 | PPL: 8.87 | LR: 2.808479e-04\n",
      "  Step 2800/4407 - Loss: 2.1824 | PPL: 8.87 | LR: 2.808479e-04\n",
      "  Step 2900/4407 - Loss: 2.1839 | PPL: 8.88 | LR: 2.807898e-04\n",
      "  Step 2900/4407 - Loss: 2.1839 | PPL: 8.88 | LR: 2.807898e-04\n",
      "  Step 3000/4407 - Loss: 2.1844 | PPL: 8.89 | LR: 2.807316e-04\n",
      "  Step 3000/4407 - Loss: 2.1844 | PPL: 8.89 | LR: 2.807316e-04\n",
      "  Step 3100/4407 - Loss: 2.1850 | PPL: 8.89 | LR: 2.806733e-04\n",
      "  Step 3100/4407 - Loss: 2.1850 | PPL: 8.89 | LR: 2.806733e-04\n",
      "  Step 3200/4407 - Loss: 2.1865 | PPL: 8.90 | LR: 2.806149e-04\n",
      "  Step 3200/4407 - Loss: 2.1865 | PPL: 8.90 | LR: 2.806149e-04\n",
      "  Step 3300/4407 - Loss: 2.1870 | PPL: 8.91 | LR: 2.805564e-04\n",
      "  Step 3300/4407 - Loss: 2.1870 | PPL: 8.91 | LR: 2.805564e-04\n",
      "  Step 3400/4407 - Loss: 2.1877 | PPL: 8.91 | LR: 2.804979e-04\n",
      "  Step 3400/4407 - Loss: 2.1877 | PPL: 8.91 | LR: 2.804979e-04\n",
      "  Step 3500/4407 - Loss: 2.1882 | PPL: 8.92 | LR: 2.804393e-04\n",
      "  Step 3500/4407 - Loss: 2.1882 | PPL: 8.92 | LR: 2.804393e-04\n",
      "  Step 3600/4407 - Loss: 2.1893 | PPL: 8.93 | LR: 2.803806e-04\n",
      "  Step 3600/4407 - Loss: 2.1893 | PPL: 8.93 | LR: 2.803806e-04\n",
      "  Step 3700/4407 - Loss: 2.1899 | PPL: 8.93 | LR: 2.803218e-04\n",
      "  Step 3700/4407 - Loss: 2.1899 | PPL: 8.93 | LR: 2.803218e-04\n",
      "  Step 3800/4407 - Loss: 2.1910 | PPL: 8.94 | LR: 2.802629e-04\n",
      "  Step 3800/4407 - Loss: 2.1910 | PPL: 8.94 | LR: 2.802629e-04\n",
      "  Step 3900/4407 - Loss: 2.1919 | PPL: 8.95 | LR: 2.802039e-04\n",
      "  Step 3900/4407 - Loss: 2.1919 | PPL: 8.95 | LR: 2.802039e-04\n",
      "  Step 4000/4407 - Loss: 2.1902 | PPL: 8.94 | LR: 2.801449e-04\n",
      "  Step 4000/4407 - Loss: 2.1902 | PPL: 8.94 | LR: 2.801449e-04\n",
      "  Step 4100/4407 - Loss: 2.1911 | PPL: 8.95 | LR: 2.800858e-04\n",
      "  Step 4100/4407 - Loss: 2.1911 | PPL: 8.95 | LR: 2.800858e-04\n",
      "  Step 4200/4407 - Loss: 2.1909 | PPL: 8.94 | LR: 2.800266e-04\n",
      "  Step 4200/4407 - Loss: 2.1909 | PPL: 8.94 | LR: 2.800266e-04\n",
      "  Step 4300/4407 - Loss: 2.1917 | PPL: 8.95 | LR: 2.799673e-04\n",
      "  Step 4300/4407 - Loss: 2.1917 | PPL: 8.95 | LR: 2.799673e-04\n",
      "  Step 4400/4407 - Loss: 2.1921 | PPL: 8.95 | LR: 2.799080e-04\n",
      "  Step 4400/4407 - Loss: 2.1921 | PPL: 8.95 | LR: 2.799080e-04\n",
      "Epoch 25/100 | Train Loss: 2.1920 | Train PPL: 8.95 | Val Loss: 2.6420 | Val PPL: 14.04\n",
      "Epoch 25/100 | Train Loss: 2.1920 | Train PPL: 8.95 | Val Loss: 2.6420 | Val PPL: 14.04\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.95\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.95\n",
      "  Step 100/4407 - Loss: 2.1303 | PPL: 8.42 | LR: 2.798444e-04\n",
      "  Step 100/4407 - Loss: 2.1303 | PPL: 8.42 | LR: 2.798444e-04\n",
      "  Step 200/4407 - Loss: 2.1562 | PPL: 8.64 | LR: 2.797848e-04\n",
      "  Step 200/4407 - Loss: 2.1562 | PPL: 8.64 | LR: 2.797848e-04\n",
      "  Step 300/4407 - Loss: 2.1524 | PPL: 8.61 | LR: 2.797252e-04\n",
      "  Step 300/4407 - Loss: 2.1524 | PPL: 8.61 | LR: 2.797252e-04\n",
      "  Step 400/4407 - Loss: 2.1505 | PPL: 8.59 | LR: 2.796655e-04\n",
      "  Step 400/4407 - Loss: 2.1505 | PPL: 8.59 | LR: 2.796655e-04\n",
      "  Step 500/4407 - Loss: 2.1579 | PPL: 8.65 | LR: 2.796058e-04\n",
      "  Step 500/4407 - Loss: 2.1579 | PPL: 8.65 | LR: 2.796058e-04\n",
      "  Step 600/4407 - Loss: 2.1614 | PPL: 8.68 | LR: 2.795459e-04\n",
      "  Step 600/4407 - Loss: 2.1614 | PPL: 8.68 | LR: 2.795459e-04\n",
      "  Step 700/4407 - Loss: 2.1567 | PPL: 8.64 | LR: 2.794860e-04\n",
      "  Step 700/4407 - Loss: 2.1567 | PPL: 8.64 | LR: 2.794860e-04\n",
      "  Step 800/4407 - Loss: 2.1516 | PPL: 8.60 | LR: 2.794260e-04\n",
      "  Step 800/4407 - Loss: 2.1516 | PPL: 8.60 | LR: 2.794260e-04\n",
      "  Step 900/4407 - Loss: 2.1517 | PPL: 8.60 | LR: 2.793659e-04\n",
      "  Step 900/4407 - Loss: 2.1517 | PPL: 8.60 | LR: 2.793659e-04\n",
      "  Step 1000/4407 - Loss: 2.1532 | PPL: 8.61 | LR: 2.793057e-04\n",
      "  Step 1000/4407 - Loss: 2.1532 | PPL: 8.61 | LR: 2.793057e-04\n",
      "  Step 1100/4407 - Loss: 2.1503 | PPL: 8.59 | LR: 2.792454e-04\n",
      "  Step 1100/4407 - Loss: 2.1503 | PPL: 8.59 | LR: 2.792454e-04\n",
      "  Step 1200/4407 - Loss: 2.1546 | PPL: 8.62 | LR: 2.791851e-04\n",
      "  Step 1200/4407 - Loss: 2.1546 | PPL: 8.62 | LR: 2.791851e-04\n",
      "  Step 1300/4407 - Loss: 2.1564 | PPL: 8.64 | LR: 2.791247e-04\n",
      "  Step 1300/4407 - Loss: 2.1564 | PPL: 8.64 | LR: 2.791247e-04\n",
      "  Step 1400/4407 - Loss: 2.1581 | PPL: 8.66 | LR: 2.790642e-04\n",
      "  Step 1400/4407 - Loss: 2.1581 | PPL: 8.66 | LR: 2.790642e-04\n",
      "  Step 1500/4407 - Loss: 2.1573 | PPL: 8.65 | LR: 2.790036e-04\n",
      "  Step 1500/4407 - Loss: 2.1573 | PPL: 8.65 | LR: 2.790036e-04\n",
      "  Step 1600/4407 - Loss: 2.1602 | PPL: 8.67 | LR: 2.789429e-04\n",
      "  Step 1600/4407 - Loss: 2.1602 | PPL: 8.67 | LR: 2.789429e-04\n",
      "  Step 1700/4407 - Loss: 2.1647 | PPL: 8.71 | LR: 2.788822e-04\n",
      "  Step 1700/4407 - Loss: 2.1647 | PPL: 8.71 | LR: 2.788822e-04\n",
      "  Step 1800/4407 - Loss: 2.1659 | PPL: 8.72 | LR: 2.788213e-04\n",
      "  Step 1800/4407 - Loss: 2.1659 | PPL: 8.72 | LR: 2.788213e-04\n",
      "  Step 1900/4407 - Loss: 2.1662 | PPL: 8.72 | LR: 2.787604e-04\n",
      "  Step 1900/4407 - Loss: 2.1662 | PPL: 8.72 | LR: 2.787604e-04\n",
      "  Step 2000/4407 - Loss: 2.1670 | PPL: 8.73 | LR: 2.786995e-04\n",
      "  Step 2000/4407 - Loss: 2.1670 | PPL: 8.73 | LR: 2.786995e-04\n",
      "  Step 2100/4407 - Loss: 2.1691 | PPL: 8.75 | LR: 2.786384e-04\n",
      "  Step 2100/4407 - Loss: 2.1691 | PPL: 8.75 | LR: 2.786384e-04\n",
      "  Step 2200/4407 - Loss: 2.1728 | PPL: 8.78 | LR: 2.785772e-04\n",
      "  Step 2200/4407 - Loss: 2.1728 | PPL: 8.78 | LR: 2.785772e-04\n",
      "  Step 2300/4407 - Loss: 2.1754 | PPL: 8.81 | LR: 2.785160e-04\n",
      "  Step 2300/4407 - Loss: 2.1754 | PPL: 8.81 | LR: 2.785160e-04\n",
      "  Step 2400/4407 - Loss: 2.1741 | PPL: 8.79 | LR: 2.784547e-04\n",
      "  Step 2400/4407 - Loss: 2.1741 | PPL: 8.79 | LR: 2.784547e-04\n",
      "  Step 2500/4407 - Loss: 2.1760 | PPL: 8.81 | LR: 2.783933e-04\n",
      "  Step 2500/4407 - Loss: 2.1760 | PPL: 8.81 | LR: 2.783933e-04\n",
      "  Step 2600/4407 - Loss: 2.1761 | PPL: 8.81 | LR: 2.783318e-04\n",
      "  Step 2600/4407 - Loss: 2.1761 | PPL: 8.81 | LR: 2.783318e-04\n",
      "  Step 2700/4407 - Loss: 2.1778 | PPL: 8.83 | LR: 2.782703e-04\n",
      "  Step 2700/4407 - Loss: 2.1778 | PPL: 8.83 | LR: 2.782703e-04\n",
      "  Step 2800/4407 - Loss: 2.1774 | PPL: 8.82 | LR: 2.782087e-04\n",
      "  Step 2800/4407 - Loss: 2.1774 | PPL: 8.82 | LR: 2.782087e-04\n",
      "  Step 2900/4407 - Loss: 2.1776 | PPL: 8.83 | LR: 2.781469e-04\n",
      "  Step 2900/4407 - Loss: 2.1776 | PPL: 8.83 | LR: 2.781469e-04\n",
      "  Step 3000/4407 - Loss: 2.1779 | PPL: 8.83 | LR: 2.780851e-04\n",
      "  Step 3000/4407 - Loss: 2.1779 | PPL: 8.83 | LR: 2.780851e-04\n",
      "  Step 3100/4407 - Loss: 2.1783 | PPL: 8.83 | LR: 2.780233e-04\n",
      "  Step 3100/4407 - Loss: 2.1783 | PPL: 8.83 | LR: 2.780233e-04\n",
      "  Step 3200/4407 - Loss: 2.1801 | PPL: 8.85 | LR: 2.779613e-04\n",
      "  Step 3200/4407 - Loss: 2.1801 | PPL: 8.85 | LR: 2.779613e-04\n",
      "  Step 3300/4407 - Loss: 2.1778 | PPL: 8.83 | LR: 2.778993e-04\n",
      "  Step 3300/4407 - Loss: 2.1778 | PPL: 8.83 | LR: 2.778993e-04\n",
      "  Step 3400/4407 - Loss: 2.1787 | PPL: 8.84 | LR: 2.778372e-04\n",
      "  Step 3400/4407 - Loss: 2.1787 | PPL: 8.84 | LR: 2.778372e-04\n",
      "  Step 3500/4407 - Loss: 2.1783 | PPL: 8.83 | LR: 2.777750e-04\n",
      "  Step 3500/4407 - Loss: 2.1783 | PPL: 8.83 | LR: 2.777750e-04\n",
      "  Step 3600/4407 - Loss: 2.1788 | PPL: 8.84 | LR: 2.777127e-04\n",
      "  Step 3600/4407 - Loss: 2.1788 | PPL: 8.84 | LR: 2.777127e-04\n",
      "  Step 3700/4407 - Loss: 2.1795 | PPL: 8.84 | LR: 2.776503e-04\n",
      "  Step 3700/4407 - Loss: 2.1795 | PPL: 8.84 | LR: 2.776503e-04\n",
      "  Step 3800/4407 - Loss: 2.1798 | PPL: 8.84 | LR: 2.775879e-04\n",
      "  Step 3800/4407 - Loss: 2.1798 | PPL: 8.84 | LR: 2.775879e-04\n",
      "  Step 3900/4407 - Loss: 2.1806 | PPL: 8.85 | LR: 2.775254e-04\n",
      "  Step 3900/4407 - Loss: 2.1806 | PPL: 8.85 | LR: 2.775254e-04\n",
      "  Step 4000/4407 - Loss: 2.1811 | PPL: 8.86 | LR: 2.774628e-04\n",
      "  Step 4000/4407 - Loss: 2.1811 | PPL: 8.86 | LR: 2.774628e-04\n",
      "  Step 4100/4407 - Loss: 2.1811 | PPL: 8.86 | LR: 2.774001e-04\n",
      "  Step 4100/4407 - Loss: 2.1811 | PPL: 8.86 | LR: 2.774001e-04\n",
      "  Step 4200/4407 - Loss: 2.1810 | PPL: 8.86 | LR: 2.773374e-04\n",
      "  Step 4200/4407 - Loss: 2.1810 | PPL: 8.86 | LR: 2.773374e-04\n",
      "  Step 4300/4407 - Loss: 2.1821 | PPL: 8.86 | LR: 2.772745e-04\n",
      "  Step 4300/4407 - Loss: 2.1821 | PPL: 8.86 | LR: 2.772745e-04\n",
      "  Step 4400/4407 - Loss: 2.1826 | PPL: 8.87 | LR: 2.772116e-04\n",
      "  Step 4400/4407 - Loss: 2.1826 | PPL: 8.87 | LR: 2.772116e-04\n",
      "Epoch 26/100 | Train Loss: 2.1829 | Train PPL: 8.87 | Val Loss: 2.6375 | Val PPL: 13.98\n",
      "Epoch 26/100 | Train Loss: 2.1829 | Train PPL: 8.87 | Val Loss: 2.6375 | Val PPL: 13.98\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.87\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.87\n",
      "  Step 100/4407 - Loss: 2.1204 | PPL: 8.33 | LR: 2.771442e-04\n",
      "  Step 100/4407 - Loss: 2.1204 | PPL: 8.33 | LR: 2.771442e-04\n",
      "  Step 200/4407 - Loss: 2.1402 | PPL: 8.50 | LR: 2.770811e-04\n",
      "  Step 200/4407 - Loss: 2.1402 | PPL: 8.50 | LR: 2.770811e-04\n",
      "  Step 300/4407 - Loss: 2.1398 | PPL: 8.50 | LR: 2.770180e-04\n",
      "  Step 300/4407 - Loss: 2.1398 | PPL: 8.50 | LR: 2.770180e-04\n",
      "  Step 400/4407 - Loss: 2.1405 | PPL: 8.50 | LR: 2.769547e-04\n",
      "  Step 400/4407 - Loss: 2.1405 | PPL: 8.50 | LR: 2.769547e-04\n",
      "  Step 500/4407 - Loss: 2.1425 | PPL: 8.52 | LR: 2.768914e-04\n",
      "  Step 500/4407 - Loss: 2.1425 | PPL: 8.52 | LR: 2.768914e-04\n",
      "  Step 600/4407 - Loss: 2.1391 | PPL: 8.49 | LR: 2.768280e-04\n",
      "  Step 600/4407 - Loss: 2.1391 | PPL: 8.49 | LR: 2.768280e-04\n",
      "  Step 700/4407 - Loss: 2.1433 | PPL: 8.53 | LR: 2.767645e-04\n",
      "  Step 700/4407 - Loss: 2.1433 | PPL: 8.53 | LR: 2.767645e-04\n",
      "  Step 800/4407 - Loss: 2.1476 | PPL: 8.56 | LR: 2.767010e-04\n",
      "  Step 800/4407 - Loss: 2.1476 | PPL: 8.56 | LR: 2.767010e-04\n",
      "  Step 900/4407 - Loss: 2.1481 | PPL: 8.57 | LR: 2.766373e-04\n",
      "  Step 900/4407 - Loss: 2.1481 | PPL: 8.57 | LR: 2.766373e-04\n",
      "  Step 1000/4407 - Loss: 2.1466 | PPL: 8.56 | LR: 2.765736e-04\n",
      "  Step 1000/4407 - Loss: 2.1466 | PPL: 8.56 | LR: 2.765736e-04\n",
      "  Step 1100/4407 - Loss: 2.1484 | PPL: 8.57 | LR: 2.765098e-04\n",
      "  Step 1100/4407 - Loss: 2.1484 | PPL: 8.57 | LR: 2.765098e-04\n",
      "  Step 1200/4407 - Loss: 2.1493 | PPL: 8.58 | LR: 2.764460e-04\n",
      "  Step 1200/4407 - Loss: 2.1493 | PPL: 8.58 | LR: 2.764460e-04\n",
      "  Step 1300/4407 - Loss: 2.1464 | PPL: 8.55 | LR: 2.763820e-04\n",
      "  Step 1300/4407 - Loss: 2.1464 | PPL: 8.55 | LR: 2.763820e-04\n",
      "  Step 1400/4407 - Loss: 2.1482 | PPL: 8.57 | LR: 2.763180e-04\n",
      "  Step 1400/4407 - Loss: 2.1482 | PPL: 8.57 | LR: 2.763180e-04\n",
      "  Step 1500/4407 - Loss: 2.1478 | PPL: 8.57 | LR: 2.762539e-04\n",
      "  Step 1500/4407 - Loss: 2.1478 | PPL: 8.57 | LR: 2.762539e-04\n",
      "  Step 1600/4407 - Loss: 2.1526 | PPL: 8.61 | LR: 2.761897e-04\n",
      "  Step 1600/4407 - Loss: 2.1526 | PPL: 8.61 | LR: 2.761897e-04\n",
      "  Step 1700/4407 - Loss: 2.1542 | PPL: 8.62 | LR: 2.761254e-04\n",
      "  Step 1700/4407 - Loss: 2.1542 | PPL: 8.62 | LR: 2.761254e-04\n",
      "  Step 1800/4407 - Loss: 2.1527 | PPL: 8.61 | LR: 2.760610e-04\n",
      "  Step 1800/4407 - Loss: 2.1527 | PPL: 8.61 | LR: 2.760610e-04\n",
      "  Step 1900/4407 - Loss: 2.1559 | PPL: 8.64 | LR: 2.759966e-04\n",
      "  Step 1900/4407 - Loss: 2.1559 | PPL: 8.64 | LR: 2.759966e-04\n",
      "  Step 2000/4407 - Loss: 2.1586 | PPL: 8.66 | LR: 2.759321e-04\n",
      "  Step 2000/4407 - Loss: 2.1586 | PPL: 8.66 | LR: 2.759321e-04\n",
      "  Step 2100/4407 - Loss: 2.1600 | PPL: 8.67 | LR: 2.758675e-04\n",
      "  Step 2100/4407 - Loss: 2.1600 | PPL: 8.67 | LR: 2.758675e-04\n",
      "  Step 2200/4407 - Loss: 2.1600 | PPL: 8.67 | LR: 2.758028e-04\n",
      "  Step 2200/4407 - Loss: 2.1600 | PPL: 8.67 | LR: 2.758028e-04\n",
      "  Step 2300/4407 - Loss: 2.1592 | PPL: 8.66 | LR: 2.757381e-04\n",
      "  Step 2300/4407 - Loss: 2.1592 | PPL: 8.66 | LR: 2.757381e-04\n",
      "  Step 2400/4407 - Loss: 2.1588 | PPL: 8.66 | LR: 2.756733e-04\n",
      "  Step 2400/4407 - Loss: 2.1588 | PPL: 8.66 | LR: 2.756733e-04\n",
      "  Step 2500/4407 - Loss: 2.1584 | PPL: 8.66 | LR: 2.756084e-04\n",
      "  Step 2500/4407 - Loss: 2.1584 | PPL: 8.66 | LR: 2.756084e-04\n",
      "  Step 2600/4407 - Loss: 2.1587 | PPL: 8.66 | LR: 2.755434e-04\n",
      "  Step 2600/4407 - Loss: 2.1587 | PPL: 8.66 | LR: 2.755434e-04\n",
      "  Step 2700/4407 - Loss: 2.1609 | PPL: 8.68 | LR: 2.754783e-04\n",
      "  Step 2700/4407 - Loss: 2.1609 | PPL: 8.68 | LR: 2.754783e-04\n",
      "  Step 2800/4407 - Loss: 2.1628 | PPL: 8.70 | LR: 2.754132e-04\n",
      "  Step 2800/4407 - Loss: 2.1628 | PPL: 8.70 | LR: 2.754132e-04\n",
      "  Step 2900/4407 - Loss: 2.1638 | PPL: 8.70 | LR: 2.753480e-04\n",
      "  Step 2900/4407 - Loss: 2.1638 | PPL: 8.70 | LR: 2.753480e-04\n",
      "  Step 3000/4407 - Loss: 2.1646 | PPL: 8.71 | LR: 2.752827e-04\n",
      "  Step 3000/4407 - Loss: 2.1646 | PPL: 8.71 | LR: 2.752827e-04\n",
      "  Step 3100/4407 - Loss: 2.1654 | PPL: 8.72 | LR: 2.752173e-04\n",
      "  Step 3100/4407 - Loss: 2.1654 | PPL: 8.72 | LR: 2.752173e-04\n",
      "  Step 3200/4407 - Loss: 2.1667 | PPL: 8.73 | LR: 2.751518e-04\n",
      "  Step 3200/4407 - Loss: 2.1667 | PPL: 8.73 | LR: 2.751518e-04\n",
      "  Step 3300/4407 - Loss: 2.1672 | PPL: 8.73 | LR: 2.750863e-04\n",
      "  Step 3300/4407 - Loss: 2.1672 | PPL: 8.73 | LR: 2.750863e-04\n",
      "  Step 3400/4407 - Loss: 2.1680 | PPL: 8.74 | LR: 2.750207e-04\n",
      "  Step 3400/4407 - Loss: 2.1680 | PPL: 8.74 | LR: 2.750207e-04\n",
      "  Step 3500/4407 - Loss: 2.1689 | PPL: 8.75 | LR: 2.749550e-04\n",
      "  Step 3500/4407 - Loss: 2.1689 | PPL: 8.75 | LR: 2.749550e-04\n",
      "  Step 3600/4407 - Loss: 2.1698 | PPL: 8.76 | LR: 2.748892e-04\n",
      "  Step 3600/4407 - Loss: 2.1698 | PPL: 8.76 | LR: 2.748892e-04\n",
      "  Step 3700/4407 - Loss: 2.1706 | PPL: 8.76 | LR: 2.748234e-04\n",
      "  Step 3700/4407 - Loss: 2.1706 | PPL: 8.76 | LR: 2.748234e-04\n",
      "  Step 3800/4407 - Loss: 2.1714 | PPL: 8.77 | LR: 2.747575e-04\n",
      "  Step 3800/4407 - Loss: 2.1714 | PPL: 8.77 | LR: 2.747575e-04\n",
      "  Step 3900/4407 - Loss: 2.1729 | PPL: 8.78 | LR: 2.746915e-04\n",
      "  Step 3900/4407 - Loss: 2.1729 | PPL: 8.78 | LR: 2.746915e-04\n",
      "  Step 4000/4407 - Loss: 2.1733 | PPL: 8.79 | LR: 2.746254e-04\n",
      "  Step 4000/4407 - Loss: 2.1733 | PPL: 8.79 | LR: 2.746254e-04\n",
      "  Step 4100/4407 - Loss: 2.1725 | PPL: 8.78 | LR: 2.745592e-04\n",
      "  Step 4100/4407 - Loss: 2.1725 | PPL: 8.78 | LR: 2.745592e-04\n",
      "  Step 4200/4407 - Loss: 2.1726 | PPL: 8.78 | LR: 2.744930e-04\n",
      "  Step 4200/4407 - Loss: 2.1726 | PPL: 8.78 | LR: 2.744930e-04\n",
      "  Step 4300/4407 - Loss: 2.1728 | PPL: 8.78 | LR: 2.744267e-04\n",
      "  Step 4300/4407 - Loss: 2.1728 | PPL: 8.78 | LR: 2.744267e-04\n",
      "  Step 4400/4407 - Loss: 2.1739 | PPL: 8.79 | LR: 2.743603e-04\n",
      "  Step 4400/4407 - Loss: 2.1739 | PPL: 8.79 | LR: 2.743603e-04\n",
      "Epoch 27/100 | Train Loss: 2.1740 | Train PPL: 8.79 | Val Loss: 2.6403 | Val PPL: 14.02\n",
      "Epoch 27/100 | Train Loss: 2.1740 | Train PPL: 8.79 | Val Loss: 2.6403 | Val PPL: 14.02\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.79\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.79\n",
      "  Step 100/4407 - Loss: 2.1193 | PPL: 8.33 | LR: 2.742892e-04\n",
      "  Step 100/4407 - Loss: 2.1193 | PPL: 8.33 | LR: 2.742892e-04\n",
      "  Step 200/4407 - Loss: 2.1158 | PPL: 8.30 | LR: 2.742226e-04\n",
      "  Step 200/4407 - Loss: 2.1158 | PPL: 8.30 | LR: 2.742226e-04\n",
      "  Step 300/4407 - Loss: 2.1226 | PPL: 8.35 | LR: 2.741560e-04\n",
      "  Step 300/4407 - Loss: 2.1226 | PPL: 8.35 | LR: 2.741560e-04\n",
      "  Step 400/4407 - Loss: 2.1254 | PPL: 8.38 | LR: 2.740893e-04\n",
      "  Step 400/4407 - Loss: 2.1254 | PPL: 8.38 | LR: 2.740893e-04\n",
      "  Step 500/4407 - Loss: 2.1226 | PPL: 8.35 | LR: 2.740225e-04\n",
      "  Step 500/4407 - Loss: 2.1226 | PPL: 8.35 | LR: 2.740225e-04\n",
      "  Step 600/4407 - Loss: 2.1300 | PPL: 8.41 | LR: 2.739556e-04\n",
      "  Step 600/4407 - Loss: 2.1300 | PPL: 8.41 | LR: 2.739556e-04\n",
      "  Step 700/4407 - Loss: 2.1296 | PPL: 8.41 | LR: 2.738887e-04\n",
      "  Step 700/4407 - Loss: 2.1296 | PPL: 8.41 | LR: 2.738887e-04\n",
      "  Step 800/4407 - Loss: 2.1314 | PPL: 8.43 | LR: 2.738216e-04\n",
      "  Step 800/4407 - Loss: 2.1314 | PPL: 8.43 | LR: 2.738216e-04\n",
      "  Step 900/4407 - Loss: 2.1344 | PPL: 8.45 | LR: 2.737545e-04\n",
      "  Step 900/4407 - Loss: 2.1344 | PPL: 8.45 | LR: 2.737545e-04\n",
      "  Step 1000/4407 - Loss: 2.1361 | PPL: 8.47 | LR: 2.736874e-04\n",
      "  Step 1000/4407 - Loss: 2.1361 | PPL: 8.47 | LR: 2.736874e-04\n",
      "  Step 1100/4407 - Loss: 2.1372 | PPL: 8.48 | LR: 2.736201e-04\n",
      "  Step 1100/4407 - Loss: 2.1372 | PPL: 8.48 | LR: 2.736201e-04\n",
      "  Step 1200/4407 - Loss: 2.1406 | PPL: 8.50 | LR: 2.735528e-04\n",
      "  Step 1200/4407 - Loss: 2.1406 | PPL: 8.50 | LR: 2.735528e-04\n",
      "  Step 1300/4407 - Loss: 2.1439 | PPL: 8.53 | LR: 2.734854e-04\n",
      "  Step 1300/4407 - Loss: 2.1439 | PPL: 8.53 | LR: 2.734854e-04\n",
      "  Step 1400/4407 - Loss: 2.1414 | PPL: 8.51 | LR: 2.734179e-04\n",
      "  Step 1400/4407 - Loss: 2.1414 | PPL: 8.51 | LR: 2.734179e-04\n",
      "  Step 1500/4407 - Loss: 2.1455 | PPL: 8.55 | LR: 2.733503e-04\n",
      "  Step 1500/4407 - Loss: 2.1455 | PPL: 8.55 | LR: 2.733503e-04\n",
      "  Step 1600/4407 - Loss: 2.1457 | PPL: 8.55 | LR: 2.732827e-04\n",
      "  Step 1600/4407 - Loss: 2.1457 | PPL: 8.55 | LR: 2.732827e-04\n",
      "  Step 1700/4407 - Loss: 2.1477 | PPL: 8.57 | LR: 2.732149e-04\n",
      "  Step 1700/4407 - Loss: 2.1477 | PPL: 8.57 | LR: 2.732149e-04\n",
      "  Step 1800/4407 - Loss: 2.1500 | PPL: 8.58 | LR: 2.731472e-04\n",
      "  Step 1800/4407 - Loss: 2.1500 | PPL: 8.58 | LR: 2.731472e-04\n",
      "  Step 1900/4407 - Loss: 2.1507 | PPL: 8.59 | LR: 2.730793e-04\n",
      "  Step 1900/4407 - Loss: 2.1507 | PPL: 8.59 | LR: 2.730793e-04\n",
      "  Step 2000/4407 - Loss: 2.1519 | PPL: 8.60 | LR: 2.730113e-04\n",
      "  Step 2000/4407 - Loss: 2.1519 | PPL: 8.60 | LR: 2.730113e-04\n",
      "  Step 2100/4407 - Loss: 2.1525 | PPL: 8.61 | LR: 2.729433e-04\n",
      "  Step 2100/4407 - Loss: 2.1525 | PPL: 8.61 | LR: 2.729433e-04\n",
      "  Step 2200/4407 - Loss: 2.1540 | PPL: 8.62 | LR: 2.728752e-04\n",
      "  Step 2200/4407 - Loss: 2.1540 | PPL: 8.62 | LR: 2.728752e-04\n",
      "  Step 2300/4407 - Loss: 2.1530 | PPL: 8.61 | LR: 2.728070e-04\n",
      "  Step 2300/4407 - Loss: 2.1530 | PPL: 8.61 | LR: 2.728070e-04\n",
      "  Step 2400/4407 - Loss: 2.1548 | PPL: 8.63 | LR: 2.727387e-04\n",
      "  Step 2400/4407 - Loss: 2.1548 | PPL: 8.63 | LR: 2.727387e-04\n",
      "  Step 2500/4407 - Loss: 2.1542 | PPL: 8.62 | LR: 2.726704e-04\n",
      "  Step 2500/4407 - Loss: 2.1542 | PPL: 8.62 | LR: 2.726704e-04\n",
      "  Step 2600/4407 - Loss: 2.1536 | PPL: 8.62 | LR: 2.726020e-04\n",
      "  Step 2600/4407 - Loss: 2.1536 | PPL: 8.62 | LR: 2.726020e-04\n",
      "  Step 2700/4407 - Loss: 2.1550 | PPL: 8.63 | LR: 2.725335e-04\n",
      "  Step 2700/4407 - Loss: 2.1550 | PPL: 8.63 | LR: 2.725335e-04\n",
      "  Step 2800/4407 - Loss: 2.1565 | PPL: 8.64 | LR: 2.724649e-04\n",
      "  Step 2800/4407 - Loss: 2.1565 | PPL: 8.64 | LR: 2.724649e-04\n",
      "  Step 2900/4407 - Loss: 2.1574 | PPL: 8.65 | LR: 2.723963e-04\n",
      "  Step 2900/4407 - Loss: 2.1574 | PPL: 8.65 | LR: 2.723963e-04\n",
      "  Step 3000/4407 - Loss: 2.1583 | PPL: 8.66 | LR: 2.723276e-04\n",
      "  Step 3000/4407 - Loss: 2.1583 | PPL: 8.66 | LR: 2.723276e-04\n",
      "  Step 3100/4407 - Loss: 2.1586 | PPL: 8.66 | LR: 2.722588e-04\n",
      "  Step 3100/4407 - Loss: 2.1586 | PPL: 8.66 | LR: 2.722588e-04\n",
      "  Step 3200/4407 - Loss: 2.1593 | PPL: 8.67 | LR: 2.721899e-04\n",
      "  Step 3200/4407 - Loss: 2.1593 | PPL: 8.67 | LR: 2.721899e-04\n",
      "  Step 3300/4407 - Loss: 2.1598 | PPL: 8.67 | LR: 2.721209e-04\n",
      "  Step 3300/4407 - Loss: 2.1598 | PPL: 8.67 | LR: 2.721209e-04\n",
      "  Step 3400/4407 - Loss: 2.1608 | PPL: 8.68 | LR: 2.720519e-04\n",
      "  Step 3400/4407 - Loss: 2.1608 | PPL: 8.68 | LR: 2.720519e-04\n",
      "  Step 3500/4407 - Loss: 2.1621 | PPL: 8.69 | LR: 2.719828e-04\n",
      "  Step 3500/4407 - Loss: 2.1621 | PPL: 8.69 | LR: 2.719828e-04\n",
      "  Step 3600/4407 - Loss: 2.1626 | PPL: 8.69 | LR: 2.719136e-04\n",
      "  Step 3600/4407 - Loss: 2.1626 | PPL: 8.69 | LR: 2.719136e-04\n",
      "  Step 3700/4407 - Loss: 2.1637 | PPL: 8.70 | LR: 2.718444e-04\n",
      "  Step 3700/4407 - Loss: 2.1637 | PPL: 8.70 | LR: 2.718444e-04\n",
      "  Step 3800/4407 - Loss: 2.1646 | PPL: 8.71 | LR: 2.717750e-04\n",
      "  Step 3800/4407 - Loss: 2.1646 | PPL: 8.71 | LR: 2.717750e-04\n",
      "  Step 3900/4407 - Loss: 2.1656 | PPL: 8.72 | LR: 2.717056e-04\n",
      "  Step 3900/4407 - Loss: 2.1656 | PPL: 8.72 | LR: 2.717056e-04\n",
      "  Step 4000/4407 - Loss: 2.1670 | PPL: 8.73 | LR: 2.716361e-04\n",
      "  Step 4000/4407 - Loss: 2.1670 | PPL: 8.73 | LR: 2.716361e-04\n",
      "  Step 4100/4407 - Loss: 2.1661 | PPL: 8.72 | LR: 2.715666e-04\n",
      "  Step 4100/4407 - Loss: 2.1661 | PPL: 8.72 | LR: 2.715666e-04\n",
      "  Step 4200/4407 - Loss: 2.1655 | PPL: 8.72 | LR: 2.714969e-04\n",
      "  Step 4200/4407 - Loss: 2.1655 | PPL: 8.72 | LR: 2.714969e-04\n",
      "  Step 4300/4407 - Loss: 2.1658 | PPL: 8.72 | LR: 2.714272e-04\n",
      "  Step 4300/4407 - Loss: 2.1658 | PPL: 8.72 | LR: 2.714272e-04\n",
      "  Step 4400/4407 - Loss: 2.1662 | PPL: 8.73 | LR: 2.713574e-04\n",
      "  Step 4400/4407 - Loss: 2.1662 | PPL: 8.73 | LR: 2.713574e-04\n",
      "Epoch 28/100 | Train Loss: 2.1661 | Train PPL: 8.72 | Val Loss: 2.6309 | Val PPL: 13.89\n",
      "Epoch 28/100 | Train Loss: 2.1661 | Train PPL: 8.72 | Val Loss: 2.6309 | Val PPL: 13.89\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.72\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.72\n",
      "  Step 100/4407 - Loss: 2.0934 | PPL: 8.11 | LR: 2.712827e-04\n",
      "  Step 100/4407 - Loss: 2.0934 | PPL: 8.11 | LR: 2.712827e-04\n",
      "  Step 200/4407 - Loss: 2.1112 | PPL: 8.26 | LR: 2.712127e-04\n",
      "  Step 200/4407 - Loss: 2.1112 | PPL: 8.26 | LR: 2.712127e-04\n",
      "  Step 300/4407 - Loss: 2.1165 | PPL: 8.30 | LR: 2.711427e-04\n",
      "  Step 300/4407 - Loss: 2.1165 | PPL: 8.30 | LR: 2.711427e-04\n",
      "  Step 400/4407 - Loss: 2.1277 | PPL: 8.40 | LR: 2.710726e-04\n",
      "  Step 400/4407 - Loss: 2.1277 | PPL: 8.40 | LR: 2.710726e-04\n",
      "  Step 500/4407 - Loss: 2.1298 | PPL: 8.41 | LR: 2.710024e-04\n",
      "  Step 500/4407 - Loss: 2.1298 | PPL: 8.41 | LR: 2.710024e-04\n",
      "  Step 600/4407 - Loss: 2.1321 | PPL: 8.43 | LR: 2.709322e-04\n",
      "  Step 600/4407 - Loss: 2.1321 | PPL: 8.43 | LR: 2.709322e-04\n",
      "  Step 700/4407 - Loss: 2.1345 | PPL: 8.45 | LR: 2.708618e-04\n",
      "  Step 700/4407 - Loss: 2.1345 | PPL: 8.45 | LR: 2.708618e-04\n",
      "  Step 800/4407 - Loss: 2.1346 | PPL: 8.45 | LR: 2.707914e-04\n",
      "  Step 800/4407 - Loss: 2.1346 | PPL: 8.45 | LR: 2.707914e-04\n",
      "  Step 900/4407 - Loss: 2.1343 | PPL: 8.45 | LR: 2.707210e-04\n",
      "  Step 900/4407 - Loss: 2.1343 | PPL: 8.45 | LR: 2.707210e-04\n",
      "  Step 1000/4407 - Loss: 2.1318 | PPL: 8.43 | LR: 2.706504e-04\n",
      "  Step 1000/4407 - Loss: 2.1318 | PPL: 8.43 | LR: 2.706504e-04\n",
      "  Step 1100/4407 - Loss: 2.1296 | PPL: 8.41 | LR: 2.705798e-04\n",
      "  Step 1100/4407 - Loss: 2.1296 | PPL: 8.41 | LR: 2.705798e-04\n",
      "  Step 1200/4407 - Loss: 2.1309 | PPL: 8.42 | LR: 2.705091e-04\n",
      "  Step 1200/4407 - Loss: 2.1309 | PPL: 8.42 | LR: 2.705091e-04\n",
      "  Step 1300/4407 - Loss: 2.1287 | PPL: 8.40 | LR: 2.704383e-04\n",
      "  Step 1300/4407 - Loss: 2.1287 | PPL: 8.40 | LR: 2.704383e-04\n",
      "  Step 1400/4407 - Loss: 2.1304 | PPL: 8.42 | LR: 2.703674e-04\n",
      "  Step 1400/4407 - Loss: 2.1304 | PPL: 8.42 | LR: 2.703674e-04\n",
      "  Step 1500/4407 - Loss: 2.1343 | PPL: 8.45 | LR: 2.702965e-04\n",
      "  Step 1500/4407 - Loss: 2.1343 | PPL: 8.45 | LR: 2.702965e-04\n",
      "  Step 1600/4407 - Loss: 2.1357 | PPL: 8.46 | LR: 2.702255e-04\n",
      "  Step 1600/4407 - Loss: 2.1357 | PPL: 8.46 | LR: 2.702255e-04\n",
      "  Step 1700/4407 - Loss: 2.1368 | PPL: 8.47 | LR: 2.701544e-04\n",
      "  Step 1700/4407 - Loss: 2.1368 | PPL: 8.47 | LR: 2.701544e-04\n",
      "  Step 1800/4407 - Loss: 2.1377 | PPL: 8.48 | LR: 2.700832e-04\n",
      "  Step 1800/4407 - Loss: 2.1377 | PPL: 8.48 | LR: 2.700832e-04\n",
      "  Step 1900/4407 - Loss: 2.1372 | PPL: 8.48 | LR: 2.700120e-04\n",
      "  Step 1900/4407 - Loss: 2.1372 | PPL: 8.48 | LR: 2.700120e-04\n",
      "  Step 2000/4407 - Loss: 2.1361 | PPL: 8.47 | LR: 2.699407e-04\n",
      "  Step 2000/4407 - Loss: 2.1361 | PPL: 8.47 | LR: 2.699407e-04\n",
      "  Step 2100/4407 - Loss: 2.1377 | PPL: 8.48 | LR: 2.698693e-04\n",
      "  Step 2100/4407 - Loss: 2.1377 | PPL: 8.48 | LR: 2.698693e-04\n",
      "  Step 2200/4407 - Loss: 2.1393 | PPL: 8.49 | LR: 2.697978e-04\n",
      "  Step 2200/4407 - Loss: 2.1393 | PPL: 8.49 | LR: 2.697978e-04\n",
      "  Step 2300/4407 - Loss: 2.1403 | PPL: 8.50 | LR: 2.697263e-04\n",
      "  Step 2300/4407 - Loss: 2.1403 | PPL: 8.50 | LR: 2.697263e-04\n",
      "  Step 2400/4407 - Loss: 2.1409 | PPL: 8.51 | LR: 2.696547e-04\n",
      "  Step 2400/4407 - Loss: 2.1409 | PPL: 8.51 | LR: 2.696547e-04\n",
      "  Step 2500/4407 - Loss: 2.1432 | PPL: 8.53 | LR: 2.695830e-04\n",
      "  Step 2500/4407 - Loss: 2.1432 | PPL: 8.53 | LR: 2.695830e-04\n",
      "  Step 2600/4407 - Loss: 2.1450 | PPL: 8.54 | LR: 2.695112e-04\n",
      "  Step 2600/4407 - Loss: 2.1450 | PPL: 8.54 | LR: 2.695112e-04\n",
      "  Step 2700/4407 - Loss: 2.1470 | PPL: 8.56 | LR: 2.694394e-04\n",
      "  Step 2700/4407 - Loss: 2.1470 | PPL: 8.56 | LR: 2.694394e-04\n",
      "  Step 2800/4407 - Loss: 2.1468 | PPL: 8.56 | LR: 2.693675e-04\n",
      "  Step 2800/4407 - Loss: 2.1468 | PPL: 8.56 | LR: 2.693675e-04\n",
      "  Step 2900/4407 - Loss: 2.1474 | PPL: 8.56 | LR: 2.692955e-04\n",
      "  Step 2900/4407 - Loss: 2.1474 | PPL: 8.56 | LR: 2.692955e-04\n",
      "  Step 3000/4407 - Loss: 2.1482 | PPL: 8.57 | LR: 2.692234e-04\n",
      "  Step 3000/4407 - Loss: 2.1482 | PPL: 8.57 | LR: 2.692234e-04\n",
      "  Step 3100/4407 - Loss: 2.1489 | PPL: 8.58 | LR: 2.691513e-04\n",
      "  Step 3100/4407 - Loss: 2.1489 | PPL: 8.58 | LR: 2.691513e-04\n",
      "  Step 3200/4407 - Loss: 2.1506 | PPL: 8.59 | LR: 2.690791e-04\n",
      "  Step 3200/4407 - Loss: 2.1506 | PPL: 8.59 | LR: 2.690791e-04\n",
      "  Step 3300/4407 - Loss: 2.1511 | PPL: 8.59 | LR: 2.690068e-04\n",
      "  Step 3300/4407 - Loss: 2.1511 | PPL: 8.59 | LR: 2.690068e-04\n",
      "  Step 3400/4407 - Loss: 2.1523 | PPL: 8.60 | LR: 2.689344e-04\n",
      "  Step 3400/4407 - Loss: 2.1523 | PPL: 8.60 | LR: 2.689344e-04\n",
      "  Step 3500/4407 - Loss: 2.1534 | PPL: 8.61 | LR: 2.688620e-04\n",
      "  Step 3500/4407 - Loss: 2.1534 | PPL: 8.61 | LR: 2.688620e-04\n",
      "  Step 3600/4407 - Loss: 2.1542 | PPL: 8.62 | LR: 2.687895e-04\n",
      "  Step 3600/4407 - Loss: 2.1542 | PPL: 8.62 | LR: 2.687895e-04\n",
      "  Step 3700/4407 - Loss: 2.1548 | PPL: 8.63 | LR: 2.687169e-04\n",
      "  Step 3700/4407 - Loss: 2.1548 | PPL: 8.63 | LR: 2.687169e-04\n",
      "  Step 3800/4407 - Loss: 2.1553 | PPL: 8.63 | LR: 2.686442e-04\n",
      "  Step 3800/4407 - Loss: 2.1553 | PPL: 8.63 | LR: 2.686442e-04\n",
      "  Step 3900/4407 - Loss: 2.1557 | PPL: 8.63 | LR: 2.685715e-04\n",
      "  Step 3900/4407 - Loss: 2.1557 | PPL: 8.63 | LR: 2.685715e-04\n",
      "  Step 4000/4407 - Loss: 2.1566 | PPL: 8.64 | LR: 2.684987e-04\n",
      "  Step 4000/4407 - Loss: 2.1566 | PPL: 8.64 | LR: 2.684987e-04\n",
      "  Step 4100/4407 - Loss: 2.1567 | PPL: 8.64 | LR: 2.684258e-04\n",
      "  Step 4100/4407 - Loss: 2.1567 | PPL: 8.64 | LR: 2.684258e-04\n",
      "  Step 4200/4407 - Loss: 2.1569 | PPL: 8.64 | LR: 2.683529e-04\n",
      "  Step 4200/4407 - Loss: 2.1569 | PPL: 8.64 | LR: 2.683529e-04\n",
      "  Step 4300/4407 - Loss: 2.1582 | PPL: 8.66 | LR: 2.682798e-04\n",
      "  Step 4300/4407 - Loss: 2.1582 | PPL: 8.66 | LR: 2.682798e-04\n",
      "  Step 4400/4407 - Loss: 2.1587 | PPL: 8.66 | LR: 2.682067e-04\n",
      "  Step 4400/4407 - Loss: 2.1587 | PPL: 8.66 | LR: 2.682067e-04\n",
      "Epoch 29/100 | Train Loss: 2.1586 | Train PPL: 8.66 | Val Loss: 2.6291 | Val PPL: 13.86\n",
      "Epoch 29/100 | Train Loss: 2.1586 | Train PPL: 8.66 | Val Loss: 2.6291 | Val PPL: 13.86\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.66\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.66\n",
      "  Step 100/4407 - Loss: 2.0924 | PPL: 8.10 | LR: 2.681284e-04\n",
      "  Step 100/4407 - Loss: 2.0924 | PPL: 8.10 | LR: 2.681284e-04\n",
      "  Step 200/4407 - Loss: 2.1071 | PPL: 8.22 | LR: 2.680552e-04\n",
      "  Step 200/4407 - Loss: 2.1071 | PPL: 8.22 | LR: 2.680552e-04\n",
      "  Step 300/4407 - Loss: 2.1087 | PPL: 8.24 | LR: 2.679818e-04\n",
      "  Step 300/4407 - Loss: 2.1087 | PPL: 8.24 | LR: 2.679818e-04\n",
      "  Step 400/4407 - Loss: 2.1189 | PPL: 8.32 | LR: 2.679084e-04\n",
      "  Step 400/4407 - Loss: 2.1189 | PPL: 8.32 | LR: 2.679084e-04\n",
      "  Step 500/4407 - Loss: 2.1177 | PPL: 8.31 | LR: 2.678350e-04\n",
      "  Step 500/4407 - Loss: 2.1177 | PPL: 8.31 | LR: 2.678350e-04\n",
      "  Step 600/4407 - Loss: 2.1226 | PPL: 8.35 | LR: 2.677614e-04\n",
      "  Step 600/4407 - Loss: 2.1226 | PPL: 8.35 | LR: 2.677614e-04\n",
      "  Step 700/4407 - Loss: 2.1204 | PPL: 8.33 | LR: 2.676878e-04\n",
      "  Step 700/4407 - Loss: 2.1204 | PPL: 8.33 | LR: 2.676878e-04\n",
      "  Step 800/4407 - Loss: 2.1248 | PPL: 8.37 | LR: 2.676141e-04\n",
      "  Step 800/4407 - Loss: 2.1248 | PPL: 8.37 | LR: 2.676141e-04\n",
      "  Step 900/4407 - Loss: 2.1242 | PPL: 8.37 | LR: 2.675403e-04\n",
      "  Step 900/4407 - Loss: 2.1242 | PPL: 8.37 | LR: 2.675403e-04\n",
      "  Step 1000/4407 - Loss: 2.1242 | PPL: 8.37 | LR: 2.674664e-04\n",
      "  Step 1000/4407 - Loss: 2.1242 | PPL: 8.37 | LR: 2.674664e-04\n",
      "  Step 1100/4407 - Loss: 2.1262 | PPL: 8.38 | LR: 2.673925e-04\n",
      "  Step 1100/4407 - Loss: 2.1262 | PPL: 8.38 | LR: 2.673925e-04\n",
      "  Step 1200/4407 - Loss: 2.1250 | PPL: 8.37 | LR: 2.673185e-04\n",
      "  Step 1200/4407 - Loss: 2.1250 | PPL: 8.37 | LR: 2.673185e-04\n",
      "  Step 1300/4407 - Loss: 2.1240 | PPL: 8.36 | LR: 2.672445e-04\n",
      "  Step 1300/4407 - Loss: 2.1240 | PPL: 8.36 | LR: 2.672445e-04\n",
      "  Step 1400/4407 - Loss: 2.1264 | PPL: 8.38 | LR: 2.671703e-04\n",
      "  Step 1400/4407 - Loss: 2.1264 | PPL: 8.38 | LR: 2.671703e-04\n",
      "  Step 1500/4407 - Loss: 2.1269 | PPL: 8.39 | LR: 2.670961e-04\n",
      "  Step 1500/4407 - Loss: 2.1269 | PPL: 8.39 | LR: 2.670961e-04\n",
      "  Step 1600/4407 - Loss: 2.1286 | PPL: 8.40 | LR: 2.670218e-04\n",
      "  Step 1600/4407 - Loss: 2.1286 | PPL: 8.40 | LR: 2.670218e-04\n",
      "  Step 1700/4407 - Loss: 2.1280 | PPL: 8.40 | LR: 2.669474e-04\n",
      "  Step 1700/4407 - Loss: 2.1280 | PPL: 8.40 | LR: 2.669474e-04\n",
      "  Step 1800/4407 - Loss: 2.1280 | PPL: 8.40 | LR: 2.668730e-04\n",
      "  Step 1800/4407 - Loss: 2.1280 | PPL: 8.40 | LR: 2.668730e-04\n",
      "  Step 1900/4407 - Loss: 2.1289 | PPL: 8.41 | LR: 2.667985e-04\n",
      "  Step 1900/4407 - Loss: 2.1289 | PPL: 8.41 | LR: 2.667985e-04\n",
      "  Step 2000/4407 - Loss: 2.1304 | PPL: 8.42 | LR: 2.667239e-04\n",
      "  Step 2000/4407 - Loss: 2.1304 | PPL: 8.42 | LR: 2.667239e-04\n",
      "  Step 2100/4407 - Loss: 2.1319 | PPL: 8.43 | LR: 2.666492e-04\n",
      "  Step 2100/4407 - Loss: 2.1319 | PPL: 8.43 | LR: 2.666492e-04\n",
      "  Step 2200/4407 - Loss: 2.1338 | PPL: 8.45 | LR: 2.665745e-04\n",
      "  Step 2200/4407 - Loss: 2.1338 | PPL: 8.45 | LR: 2.665745e-04\n",
      "  Step 2300/4407 - Loss: 2.1337 | PPL: 8.45 | LR: 2.664997e-04\n",
      "  Step 2300/4407 - Loss: 2.1337 | PPL: 8.45 | LR: 2.664997e-04\n",
      "  Step 2400/4407 - Loss: 2.1335 | PPL: 8.44 | LR: 2.664248e-04\n",
      "  Step 2400/4407 - Loss: 2.1335 | PPL: 8.44 | LR: 2.664248e-04\n",
      "  Step 2500/4407 - Loss: 2.1348 | PPL: 8.46 | LR: 2.663499e-04\n",
      "  Step 2500/4407 - Loss: 2.1348 | PPL: 8.46 | LR: 2.663499e-04\n",
      "  Step 2600/4407 - Loss: 2.1347 | PPL: 8.45 | LR: 2.662749e-04\n",
      "  Step 2600/4407 - Loss: 2.1347 | PPL: 8.45 | LR: 2.662749e-04\n",
      "  Step 2700/4407 - Loss: 2.1349 | PPL: 8.46 | LR: 2.661998e-04\n",
      "  Step 2700/4407 - Loss: 2.1349 | PPL: 8.46 | LR: 2.661998e-04\n",
      "  Step 2800/4407 - Loss: 2.1373 | PPL: 8.48 | LR: 2.661246e-04\n",
      "  Step 2800/4407 - Loss: 2.1373 | PPL: 8.48 | LR: 2.661246e-04\n",
      "  Step 2900/4407 - Loss: 2.1380 | PPL: 8.48 | LR: 2.660493e-04\n",
      "  Step 2900/4407 - Loss: 2.1380 | PPL: 8.48 | LR: 2.660493e-04\n",
      "  Step 3000/4407 - Loss: 2.1411 | PPL: 8.51 | LR: 2.659740e-04\n",
      "  Step 3000/4407 - Loss: 2.1411 | PPL: 8.51 | LR: 2.659740e-04\n",
      "  Step 3100/4407 - Loss: 2.1422 | PPL: 8.52 | LR: 2.658986e-04\n",
      "  Step 3100/4407 - Loss: 2.1422 | PPL: 8.52 | LR: 2.658986e-04\n",
      "  Step 3200/4407 - Loss: 2.1429 | PPL: 8.52 | LR: 2.658232e-04\n",
      "  Step 3200/4407 - Loss: 2.1429 | PPL: 8.52 | LR: 2.658232e-04\n",
      "  Step 3300/4407 - Loss: 2.1440 | PPL: 8.53 | LR: 2.657477e-04\n",
      "  Step 3300/4407 - Loss: 2.1440 | PPL: 8.53 | LR: 2.657477e-04\n",
      "  Step 3400/4407 - Loss: 2.1458 | PPL: 8.55 | LR: 2.656720e-04\n",
      "  Step 3400/4407 - Loss: 2.1458 | PPL: 8.55 | LR: 2.656720e-04\n",
      "  Step 3500/4407 - Loss: 2.1457 | PPL: 8.55 | LR: 2.655964e-04\n",
      "  Step 3500/4407 - Loss: 2.1457 | PPL: 8.55 | LR: 2.655964e-04\n",
      "  Step 3600/4407 - Loss: 2.1457 | PPL: 8.55 | LR: 2.655206e-04\n",
      "  Step 3600/4407 - Loss: 2.1457 | PPL: 8.55 | LR: 2.655206e-04\n",
      "  Step 3700/4407 - Loss: 2.1460 | PPL: 8.55 | LR: 2.654448e-04\n",
      "  Step 3700/4407 - Loss: 2.1460 | PPL: 8.55 | LR: 2.654448e-04\n",
      "  Step 3800/4407 - Loss: 2.1470 | PPL: 8.56 | LR: 2.653689e-04\n",
      "  Step 3800/4407 - Loss: 2.1470 | PPL: 8.56 | LR: 2.653689e-04\n",
      "  Step 3900/4407 - Loss: 2.1476 | PPL: 8.56 | LR: 2.652929e-04\n",
      "  Step 3900/4407 - Loss: 2.1476 | PPL: 8.56 | LR: 2.652929e-04\n",
      "  Step 4000/4407 - Loss: 2.1481 | PPL: 8.57 | LR: 2.652169e-04\n",
      "  Step 4000/4407 - Loss: 2.1481 | PPL: 8.57 | LR: 2.652169e-04\n",
      "  Step 4100/4407 - Loss: 2.1489 | PPL: 8.58 | LR: 2.651408e-04\n",
      "  Step 4100/4407 - Loss: 2.1489 | PPL: 8.58 | LR: 2.651408e-04\n",
      "  Step 4200/4407 - Loss: 2.1496 | PPL: 8.58 | LR: 2.650646e-04\n",
      "  Step 4200/4407 - Loss: 2.1496 | PPL: 8.58 | LR: 2.650646e-04\n",
      "  Step 4300/4407 - Loss: 2.1503 | PPL: 8.59 | LR: 2.649883e-04\n",
      "  Step 4300/4407 - Loss: 2.1503 | PPL: 8.59 | LR: 2.649883e-04\n",
      "  Step 4400/4407 - Loss: 2.1518 | PPL: 8.60 | LR: 2.649120e-04\n",
      "  Step 4400/4407 - Loss: 2.1518 | PPL: 8.60 | LR: 2.649120e-04\n",
      "Epoch 30/100 | Train Loss: 2.1516 | Train PPL: 8.60 | Val Loss: 2.6298 | Val PPL: 13.87\n",
      "Epoch 30/100 | Train Loss: 2.1516 | Train PPL: 8.60 | Val Loss: 2.6298 | Val PPL: 13.87\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.60\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.60\n",
      "  Step 100/4407 - Loss: 2.0794 | PPL: 8.00 | LR: 2.648303e-04\n",
      "  Step 100/4407 - Loss: 2.0794 | PPL: 8.00 | LR: 2.648303e-04\n",
      "  Step 200/4407 - Loss: 2.1066 | PPL: 8.22 | LR: 2.647538e-04\n",
      "  Step 200/4407 - Loss: 2.1066 | PPL: 8.22 | LR: 2.647538e-04\n",
      "  Step 300/4407 - Loss: 2.1211 | PPL: 8.34 | LR: 2.646772e-04\n",
      "  Step 300/4407 - Loss: 2.1211 | PPL: 8.34 | LR: 2.646772e-04\n",
      "  Step 400/4407 - Loss: 2.1194 | PPL: 8.33 | LR: 2.646006e-04\n",
      "  Step 400/4407 - Loss: 2.1194 | PPL: 8.33 | LR: 2.646006e-04\n",
      "  Step 500/4407 - Loss: 2.1124 | PPL: 8.27 | LR: 2.645239e-04\n",
      "  Step 500/4407 - Loss: 2.1124 | PPL: 8.27 | LR: 2.645239e-04\n",
      "  Step 600/4407 - Loss: 2.1151 | PPL: 8.29 | LR: 2.644472e-04\n",
      "  Step 600/4407 - Loss: 2.1151 | PPL: 8.29 | LR: 2.644472e-04\n",
      "  Step 700/4407 - Loss: 2.1181 | PPL: 8.32 | LR: 2.643703e-04\n",
      "  Step 700/4407 - Loss: 2.1181 | PPL: 8.32 | LR: 2.643703e-04\n",
      "  Step 800/4407 - Loss: 2.1177 | PPL: 8.31 | LR: 2.642934e-04\n",
      "  Step 800/4407 - Loss: 2.1177 | PPL: 8.31 | LR: 2.642934e-04\n",
      "  Step 900/4407 - Loss: 2.1182 | PPL: 8.32 | LR: 2.642164e-04\n",
      "  Step 900/4407 - Loss: 2.1182 | PPL: 8.32 | LR: 2.642164e-04\n",
      "  Step 1000/4407 - Loss: 2.1183 | PPL: 8.32 | LR: 2.641394e-04\n",
      "  Step 1000/4407 - Loss: 2.1183 | PPL: 8.32 | LR: 2.641394e-04\n",
      "  Step 1100/4407 - Loss: 2.1174 | PPL: 8.31 | LR: 2.640622e-04\n",
      "  Step 1100/4407 - Loss: 2.1174 | PPL: 8.31 | LR: 2.640622e-04\n",
      "  Step 1200/4407 - Loss: 2.1152 | PPL: 8.29 | LR: 2.639850e-04\n",
      "  Step 1200/4407 - Loss: 2.1152 | PPL: 8.29 | LR: 2.639850e-04\n",
      "  Step 1300/4407 - Loss: 2.1139 | PPL: 8.28 | LR: 2.639078e-04\n",
      "  Step 1300/4407 - Loss: 2.1139 | PPL: 8.28 | LR: 2.639078e-04\n",
      "  Step 1400/4407 - Loss: 2.1158 | PPL: 8.30 | LR: 2.638304e-04\n",
      "  Step 1400/4407 - Loss: 2.1158 | PPL: 8.30 | LR: 2.638304e-04\n",
      "  Step 1500/4407 - Loss: 2.1211 | PPL: 8.34 | LR: 2.637530e-04\n",
      "  Step 1500/4407 - Loss: 2.1211 | PPL: 8.34 | LR: 2.637530e-04\n",
      "  Step 1600/4407 - Loss: 2.1230 | PPL: 8.36 | LR: 2.636756e-04\n",
      "  Step 1600/4407 - Loss: 2.1230 | PPL: 8.36 | LR: 2.636756e-04\n",
      "  Step 1700/4407 - Loss: 2.1240 | PPL: 8.36 | LR: 2.635980e-04\n",
      "  Step 1700/4407 - Loss: 2.1240 | PPL: 8.36 | LR: 2.635980e-04\n",
      "  Step 1800/4407 - Loss: 2.1241 | PPL: 8.37 | LR: 2.635204e-04\n",
      "  Step 1800/4407 - Loss: 2.1241 | PPL: 8.37 | LR: 2.635204e-04\n",
      "  Step 1900/4407 - Loss: 2.1265 | PPL: 8.39 | LR: 2.634427e-04\n",
      "  Step 1900/4407 - Loss: 2.1265 | PPL: 8.39 | LR: 2.634427e-04\n",
      "  Step 2000/4407 - Loss: 2.1274 | PPL: 8.39 | LR: 2.633649e-04\n",
      "  Step 2000/4407 - Loss: 2.1274 | PPL: 8.39 | LR: 2.633649e-04\n",
      "  Step 2100/4407 - Loss: 2.1292 | PPL: 8.41 | LR: 2.632871e-04\n",
      "  Step 2100/4407 - Loss: 2.1292 | PPL: 8.41 | LR: 2.632871e-04\n",
      "  Step 2200/4407 - Loss: 2.1316 | PPL: 8.43 | LR: 2.632092e-04\n",
      "  Step 2200/4407 - Loss: 2.1316 | PPL: 8.43 | LR: 2.632092e-04\n",
      "  Step 2300/4407 - Loss: 2.1330 | PPL: 8.44 | LR: 2.631312e-04\n",
      "  Step 2300/4407 - Loss: 2.1330 | PPL: 8.44 | LR: 2.631312e-04\n",
      "  Step 2400/4407 - Loss: 2.1333 | PPL: 8.44 | LR: 2.630531e-04\n",
      "  Step 2400/4407 - Loss: 2.1333 | PPL: 8.44 | LR: 2.630531e-04\n",
      "  Step 2500/4407 - Loss: 2.1332 | PPL: 8.44 | LR: 2.629750e-04\n",
      "  Step 2500/4407 - Loss: 2.1332 | PPL: 8.44 | LR: 2.629750e-04\n",
      "  Step 2600/4407 - Loss: 2.1347 | PPL: 8.45 | LR: 2.628968e-04\n",
      "  Step 2600/4407 - Loss: 2.1347 | PPL: 8.45 | LR: 2.628968e-04\n",
      "  Step 2700/4407 - Loss: 2.1353 | PPL: 8.46 | LR: 2.628186e-04\n",
      "  Step 2700/4407 - Loss: 2.1353 | PPL: 8.46 | LR: 2.628186e-04\n",
      "  Step 2800/4407 - Loss: 2.1351 | PPL: 8.46 | LR: 2.627402e-04\n",
      "  Step 2800/4407 - Loss: 2.1351 | PPL: 8.46 | LR: 2.627402e-04\n",
      "  Step 2900/4407 - Loss: 2.1358 | PPL: 8.46 | LR: 2.626618e-04\n",
      "  Step 2900/4407 - Loss: 2.1358 | PPL: 8.46 | LR: 2.626618e-04\n",
      "  Step 3000/4407 - Loss: 2.1365 | PPL: 8.47 | LR: 2.625833e-04\n",
      "  Step 3000/4407 - Loss: 2.1365 | PPL: 8.47 | LR: 2.625833e-04\n",
      "  Step 3100/4407 - Loss: 2.1368 | PPL: 8.47 | LR: 2.625048e-04\n",
      "  Step 3100/4407 - Loss: 2.1368 | PPL: 8.47 | LR: 2.625048e-04\n",
      "  Step 3200/4407 - Loss: 2.1384 | PPL: 8.49 | LR: 2.624262e-04\n",
      "  Step 3200/4407 - Loss: 2.1384 | PPL: 8.49 | LR: 2.624262e-04\n",
      "  Step 3300/4407 - Loss: 2.1397 | PPL: 8.50 | LR: 2.623475e-04\n",
      "  Step 3300/4407 - Loss: 2.1397 | PPL: 8.50 | LR: 2.623475e-04\n",
      "  Step 3400/4407 - Loss: 2.1398 | PPL: 8.50 | LR: 2.622687e-04\n",
      "  Step 3400/4407 - Loss: 2.1398 | PPL: 8.50 | LR: 2.622687e-04\n",
      "  Step 3500/4407 - Loss: 2.1410 | PPL: 8.51 | LR: 2.621899e-04\n",
      "  Step 3500/4407 - Loss: 2.1410 | PPL: 8.51 | LR: 2.621899e-04\n",
      "  Step 3600/4407 - Loss: 2.1414 | PPL: 8.51 | LR: 2.621110e-04\n",
      "  Step 3600/4407 - Loss: 2.1414 | PPL: 8.51 | LR: 2.621110e-04\n",
      "  Step 3700/4407 - Loss: 2.1409 | PPL: 8.51 | LR: 2.620320e-04\n",
      "  Step 3700/4407 - Loss: 2.1409 | PPL: 8.51 | LR: 2.620320e-04\n",
      "  Step 3800/4407 - Loss: 2.1408 | PPL: 8.51 | LR: 2.619530e-04\n",
      "  Step 3800/4407 - Loss: 2.1408 | PPL: 8.51 | LR: 2.619530e-04\n",
      "  Step 3900/4407 - Loss: 2.1412 | PPL: 8.51 | LR: 2.618739e-04\n",
      "  Step 3900/4407 - Loss: 2.1412 | PPL: 8.51 | LR: 2.618739e-04\n",
      "  Step 4000/4407 - Loss: 2.1420 | PPL: 8.52 | LR: 2.617947e-04\n",
      "  Step 4000/4407 - Loss: 2.1420 | PPL: 8.52 | LR: 2.617947e-04\n",
      "  Step 4100/4407 - Loss: 2.1424 | PPL: 8.52 | LR: 2.617155e-04\n",
      "  Step 4100/4407 - Loss: 2.1424 | PPL: 8.52 | LR: 2.617155e-04\n",
      "  Step 4200/4407 - Loss: 2.1428 | PPL: 8.52 | LR: 2.616361e-04\n",
      "  Step 4200/4407 - Loss: 2.1428 | PPL: 8.52 | LR: 2.616361e-04\n",
      "  Step 4300/4407 - Loss: 2.1428 | PPL: 8.52 | LR: 2.615567e-04\n",
      "  Step 4300/4407 - Loss: 2.1428 | PPL: 8.52 | LR: 2.615567e-04\n",
      "  Step 4400/4407 - Loss: 2.1439 | PPL: 8.53 | LR: 2.614773e-04\n",
      "  Step 4400/4407 - Loss: 2.1439 | PPL: 8.53 | LR: 2.614773e-04\n",
      "Epoch 31/100 | Train Loss: 2.1442 | Train PPL: 8.54 | Val Loss: 2.6288 | Val PPL: 13.86\n",
      "Epoch 31/100 | Train Loss: 2.1442 | Train PPL: 8.54 | Val Loss: 2.6288 | Val PPL: 13.86\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.54\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.54\n",
      "  Step 100/4407 - Loss: 2.0898 | PPL: 8.08 | LR: 2.613922e-04\n",
      "  Step 100/4407 - Loss: 2.0898 | PPL: 8.08 | LR: 2.613922e-04\n",
      "  Step 200/4407 - Loss: 2.0938 | PPL: 8.12 | LR: 2.613126e-04\n",
      "  Step 200/4407 - Loss: 2.0938 | PPL: 8.12 | LR: 2.613126e-04\n",
      "  Step 300/4407 - Loss: 2.1029 | PPL: 8.19 | LR: 2.612329e-04\n",
      "  Step 300/4407 - Loss: 2.1029 | PPL: 8.19 | LR: 2.612329e-04\n",
      "  Step 400/4407 - Loss: 2.1087 | PPL: 8.24 | LR: 2.611532e-04\n",
      "  Step 400/4407 - Loss: 2.1087 | PPL: 8.24 | LR: 2.611532e-04\n",
      "  Step 500/4407 - Loss: 2.1076 | PPL: 8.23 | LR: 2.610734e-04\n",
      "  Step 500/4407 - Loss: 2.1076 | PPL: 8.23 | LR: 2.610734e-04\n",
      "  Step 600/4407 - Loss: 2.1045 | PPL: 8.20 | LR: 2.609935e-04\n",
      "  Step 600/4407 - Loss: 2.1045 | PPL: 8.20 | LR: 2.609935e-04\n",
      "  Step 700/4407 - Loss: 2.1056 | PPL: 8.21 | LR: 2.609135e-04\n",
      "  Step 700/4407 - Loss: 2.1056 | PPL: 8.21 | LR: 2.609135e-04\n",
      "  Step 800/4407 - Loss: 2.1095 | PPL: 8.24 | LR: 2.608335e-04\n",
      "  Step 800/4407 - Loss: 2.1095 | PPL: 8.24 | LR: 2.608335e-04\n",
      "  Step 900/4407 - Loss: 2.1109 | PPL: 8.26 | LR: 2.607534e-04\n",
      "  Step 900/4407 - Loss: 2.1109 | PPL: 8.26 | LR: 2.607534e-04\n",
      "  Step 1000/4407 - Loss: 2.1097 | PPL: 8.25 | LR: 2.606732e-04\n",
      "  Step 1000/4407 - Loss: 2.1097 | PPL: 8.25 | LR: 2.606732e-04\n",
      "  Step 1100/4407 - Loss: 2.1115 | PPL: 8.26 | LR: 2.605930e-04\n",
      "  Step 1100/4407 - Loss: 2.1115 | PPL: 8.26 | LR: 2.605930e-04\n",
      "  Step 1200/4407 - Loss: 2.1114 | PPL: 8.26 | LR: 2.605127e-04\n",
      "  Step 1200/4407 - Loss: 2.1114 | PPL: 8.26 | LR: 2.605127e-04\n",
      "  Step 1300/4407 - Loss: 2.1150 | PPL: 8.29 | LR: 2.604323e-04\n",
      "  Step 1300/4407 - Loss: 2.1150 | PPL: 8.29 | LR: 2.604323e-04\n",
      "  Step 1400/4407 - Loss: 2.1160 | PPL: 8.30 | LR: 2.603519e-04\n",
      "  Step 1400/4407 - Loss: 2.1160 | PPL: 8.30 | LR: 2.603519e-04\n",
      "  Step 1500/4407 - Loss: 2.1182 | PPL: 8.32 | LR: 2.602714e-04\n",
      "  Step 1500/4407 - Loss: 2.1182 | PPL: 8.32 | LR: 2.602714e-04\n",
      "  Step 1600/4407 - Loss: 2.1216 | PPL: 8.34 | LR: 2.601908e-04\n",
      "  Step 1600/4407 - Loss: 2.1216 | PPL: 8.34 | LR: 2.601908e-04\n",
      "  Step 1700/4407 - Loss: 2.1215 | PPL: 8.34 | LR: 2.601102e-04\n",
      "  Step 1700/4407 - Loss: 2.1215 | PPL: 8.34 | LR: 2.601102e-04\n",
      "  Step 1800/4407 - Loss: 2.1262 | PPL: 8.38 | LR: 2.600294e-04\n",
      "  Step 1800/4407 - Loss: 2.1262 | PPL: 8.38 | LR: 2.600294e-04\n",
      "  Step 1900/4407 - Loss: 2.1278 | PPL: 8.40 | LR: 2.599487e-04\n",
      "  Step 1900/4407 - Loss: 2.1278 | PPL: 8.40 | LR: 2.599487e-04\n",
      "  Step 2000/4407 - Loss: 2.1273 | PPL: 8.39 | LR: 2.598678e-04\n",
      "  Step 2000/4407 - Loss: 2.1273 | PPL: 8.39 | LR: 2.598678e-04\n",
      "  Step 2100/4407 - Loss: 2.1276 | PPL: 8.39 | LR: 2.597869e-04\n",
      "  Step 2100/4407 - Loss: 2.1276 | PPL: 8.39 | LR: 2.597869e-04\n",
      "  Step 2200/4407 - Loss: 2.1276 | PPL: 8.39 | LR: 2.597059e-04\n",
      "  Step 2200/4407 - Loss: 2.1276 | PPL: 8.39 | LR: 2.597059e-04\n",
      "  Step 2300/4407 - Loss: 2.1281 | PPL: 8.40 | LR: 2.596248e-04\n",
      "  Step 2300/4407 - Loss: 2.1281 | PPL: 8.40 | LR: 2.596248e-04\n",
      "  Step 2400/4407 - Loss: 2.1290 | PPL: 8.41 | LR: 2.595437e-04\n",
      "  Step 2400/4407 - Loss: 2.1290 | PPL: 8.41 | LR: 2.595437e-04\n",
      "  Step 2500/4407 - Loss: 2.1316 | PPL: 8.43 | LR: 2.594625e-04\n",
      "  Step 2500/4407 - Loss: 2.1316 | PPL: 8.43 | LR: 2.594625e-04\n",
      "  Step 2600/4407 - Loss: 2.1343 | PPL: 8.45 | LR: 2.593812e-04\n",
      "  Step 2600/4407 - Loss: 2.1343 | PPL: 8.45 | LR: 2.593812e-04\n",
      "  Step 2700/4407 - Loss: 2.1342 | PPL: 8.45 | LR: 2.592999e-04\n",
      "  Step 2700/4407 - Loss: 2.1342 | PPL: 8.45 | LR: 2.592999e-04\n",
      "  Step 2800/4407 - Loss: 2.1348 | PPL: 8.46 | LR: 2.592185e-04\n",
      "  Step 2800/4407 - Loss: 2.1348 | PPL: 8.46 | LR: 2.592185e-04\n",
      "  Step 2900/4407 - Loss: 2.1356 | PPL: 8.46 | LR: 2.591370e-04\n",
      "  Step 2900/4407 - Loss: 2.1356 | PPL: 8.46 | LR: 2.591370e-04\n",
      "  Step 3000/4407 - Loss: 2.1356 | PPL: 8.46 | LR: 2.590555e-04\n",
      "  Step 3000/4407 - Loss: 2.1356 | PPL: 8.46 | LR: 2.590555e-04\n",
      "  Step 3100/4407 - Loss: 2.1356 | PPL: 8.46 | LR: 2.589739e-04\n",
      "  Step 3100/4407 - Loss: 2.1356 | PPL: 8.46 | LR: 2.589739e-04\n",
      "  Step 3200/4407 - Loss: 2.1362 | PPL: 8.47 | LR: 2.588922e-04\n",
      "  Step 3200/4407 - Loss: 2.1362 | PPL: 8.47 | LR: 2.588922e-04\n",
      "  Step 3300/4407 - Loss: 2.1356 | PPL: 8.46 | LR: 2.588104e-04\n",
      "  Step 3300/4407 - Loss: 2.1356 | PPL: 8.46 | LR: 2.588104e-04\n",
      "  Step 3400/4407 - Loss: 2.1367 | PPL: 8.47 | LR: 2.587286e-04\n",
      "  Step 3400/4407 - Loss: 2.1367 | PPL: 8.47 | LR: 2.587286e-04\n",
      "  Step 3500/4407 - Loss: 2.1368 | PPL: 8.47 | LR: 2.586468e-04\n",
      "  Step 3500/4407 - Loss: 2.1368 | PPL: 8.47 | LR: 2.586468e-04\n",
      "  Step 3600/4407 - Loss: 2.1371 | PPL: 8.48 | LR: 2.585648e-04\n",
      "  Step 3600/4407 - Loss: 2.1371 | PPL: 8.48 | LR: 2.585648e-04\n",
      "  Step 3700/4407 - Loss: 2.1376 | PPL: 8.48 | LR: 2.584828e-04\n",
      "  Step 3700/4407 - Loss: 2.1376 | PPL: 8.48 | LR: 2.584828e-04\n",
      "  Step 3800/4407 - Loss: 2.1377 | PPL: 8.48 | LR: 2.584007e-04\n",
      "  Step 3800/4407 - Loss: 2.1377 | PPL: 8.48 | LR: 2.584007e-04\n",
      "  Step 3900/4407 - Loss: 2.1377 | PPL: 8.48 | LR: 2.583185e-04\n",
      "  Step 3900/4407 - Loss: 2.1377 | PPL: 8.48 | LR: 2.583185e-04\n",
      "  Step 4000/4407 - Loss: 2.1369 | PPL: 8.47 | LR: 2.582363e-04\n",
      "  Step 4000/4407 - Loss: 2.1369 | PPL: 8.47 | LR: 2.582363e-04\n",
      "  Step 4100/4407 - Loss: 2.1365 | PPL: 8.47 | LR: 2.581540e-04\n",
      "  Step 4100/4407 - Loss: 2.1365 | PPL: 8.47 | LR: 2.581540e-04\n",
      "  Step 4200/4407 - Loss: 2.1370 | PPL: 8.47 | LR: 2.580717e-04\n",
      "  Step 4200/4407 - Loss: 2.1370 | PPL: 8.47 | LR: 2.580717e-04\n",
      "  Step 4300/4407 - Loss: 2.1367 | PPL: 8.47 | LR: 2.579892e-04\n",
      "  Step 4300/4407 - Loss: 2.1367 | PPL: 8.47 | LR: 2.579892e-04\n",
      "  Step 4400/4407 - Loss: 2.1377 | PPL: 8.48 | LR: 2.579067e-04\n",
      "  Step 4400/4407 - Loss: 2.1377 | PPL: 8.48 | LR: 2.579067e-04\n",
      "Epoch 32/100 | Train Loss: 2.1379 | Train PPL: 8.48 | Val Loss: 2.6293 | Val PPL: 13.86\n",
      "Epoch 32/100 | Train Loss: 2.1379 | Train PPL: 8.48 | Val Loss: 2.6293 | Val PPL: 13.86\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.48\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.48\n",
      "  Step 100/4407 - Loss: 2.0683 | PPL: 7.91 | LR: 2.578184e-04\n",
      "  Step 100/4407 - Loss: 2.0683 | PPL: 7.91 | LR: 2.578184e-04\n",
      "  Step 200/4407 - Loss: 2.0834 | PPL: 8.03 | LR: 2.577358e-04\n",
      "  Step 200/4407 - Loss: 2.0834 | PPL: 8.03 | LR: 2.577358e-04\n",
      "  Step 300/4407 - Loss: 2.0859 | PPL: 8.05 | LR: 2.576531e-04\n",
      "  Step 300/4407 - Loss: 2.0859 | PPL: 8.05 | LR: 2.576531e-04\n",
      "  Step 400/4407 - Loss: 2.0939 | PPL: 8.12 | LR: 2.575703e-04\n",
      "  Step 400/4407 - Loss: 2.0939 | PPL: 8.12 | LR: 2.575703e-04\n",
      "  Step 500/4407 - Loss: 2.1046 | PPL: 8.20 | LR: 2.574875e-04\n",
      "  Step 500/4407 - Loss: 2.1046 | PPL: 8.20 | LR: 2.574875e-04\n",
      "  Step 600/4407 - Loss: 2.1060 | PPL: 8.22 | LR: 2.574046e-04\n",
      "  Step 600/4407 - Loss: 2.1060 | PPL: 8.22 | LR: 2.574046e-04\n",
      "  Step 700/4407 - Loss: 2.1066 | PPL: 8.22 | LR: 2.573216e-04\n",
      "  Step 700/4407 - Loss: 2.1066 | PPL: 8.22 | LR: 2.573216e-04\n",
      "  Step 800/4407 - Loss: 2.1115 | PPL: 8.26 | LR: 2.572385e-04\n",
      "  Step 800/4407 - Loss: 2.1115 | PPL: 8.26 | LR: 2.572385e-04\n",
      "  Step 900/4407 - Loss: 2.1094 | PPL: 8.24 | LR: 2.571554e-04\n",
      "  Step 900/4407 - Loss: 2.1094 | PPL: 8.24 | LR: 2.571554e-04\n",
      "  Step 1000/4407 - Loss: 2.1128 | PPL: 8.27 | LR: 2.570723e-04\n",
      "  Step 1000/4407 - Loss: 2.1128 | PPL: 8.27 | LR: 2.570723e-04\n",
      "  Step 1100/4407 - Loss: 2.1125 | PPL: 8.27 | LR: 2.569890e-04\n",
      "  Step 1100/4407 - Loss: 2.1125 | PPL: 8.27 | LR: 2.569890e-04\n",
      "  Step 1200/4407 - Loss: 2.1120 | PPL: 8.27 | LR: 2.569057e-04\n",
      "  Step 1200/4407 - Loss: 2.1120 | PPL: 8.27 | LR: 2.569057e-04\n",
      "  Step 1300/4407 - Loss: 2.1117 | PPL: 8.26 | LR: 2.568223e-04\n",
      "  Step 1300/4407 - Loss: 2.1117 | PPL: 8.26 | LR: 2.568223e-04\n",
      "  Step 1400/4407 - Loss: 2.1119 | PPL: 8.26 | LR: 2.567389e-04\n",
      "  Step 1400/4407 - Loss: 2.1119 | PPL: 8.26 | LR: 2.567389e-04\n",
      "  Step 1500/4407 - Loss: 2.1124 | PPL: 8.27 | LR: 2.566554e-04\n",
      "  Step 1500/4407 - Loss: 2.1124 | PPL: 8.27 | LR: 2.566554e-04\n",
      "  Step 1600/4407 - Loss: 2.1114 | PPL: 8.26 | LR: 2.565718e-04\n",
      "  Step 1600/4407 - Loss: 2.1114 | PPL: 8.26 | LR: 2.565718e-04\n",
      "  Step 1700/4407 - Loss: 2.1130 | PPL: 8.27 | LR: 2.564882e-04\n",
      "  Step 1700/4407 - Loss: 2.1130 | PPL: 8.27 | LR: 2.564882e-04\n",
      "  Step 1800/4407 - Loss: 2.1139 | PPL: 8.28 | LR: 2.564045e-04\n",
      "  Step 1800/4407 - Loss: 2.1139 | PPL: 8.28 | LR: 2.564045e-04\n",
      "  Step 1900/4407 - Loss: 2.1169 | PPL: 8.31 | LR: 2.563207e-04\n",
      "  Step 1900/4407 - Loss: 2.1169 | PPL: 8.31 | LR: 2.563207e-04\n",
      "  Step 2000/4407 - Loss: 2.1202 | PPL: 8.33 | LR: 2.562368e-04\n",
      "  Step 2000/4407 - Loss: 2.1202 | PPL: 8.33 | LR: 2.562368e-04\n",
      "  Step 2100/4407 - Loss: 2.1178 | PPL: 8.31 | LR: 2.561529e-04\n",
      "  Step 2100/4407 - Loss: 2.1178 | PPL: 8.31 | LR: 2.561529e-04\n",
      "  Step 2200/4407 - Loss: 2.1180 | PPL: 8.31 | LR: 2.560690e-04\n",
      "  Step 2200/4407 - Loss: 2.1180 | PPL: 8.31 | LR: 2.560690e-04\n",
      "  Step 2300/4407 - Loss: 2.1208 | PPL: 8.34 | LR: 2.559849e-04\n",
      "  Step 2300/4407 - Loss: 2.1208 | PPL: 8.34 | LR: 2.559849e-04\n",
      "  Step 2400/4407 - Loss: 2.1212 | PPL: 8.34 | LR: 2.559008e-04\n",
      "  Step 2400/4407 - Loss: 2.1212 | PPL: 8.34 | LR: 2.559008e-04\n",
      "  Step 2500/4407 - Loss: 2.1224 | PPL: 8.35 | LR: 2.558166e-04\n",
      "  Step 2500/4407 - Loss: 2.1224 | PPL: 8.35 | LR: 2.558166e-04\n",
      "  Step 2600/4407 - Loss: 2.1229 | PPL: 8.35 | LR: 2.557324e-04\n",
      "  Step 2600/4407 - Loss: 2.1229 | PPL: 8.35 | LR: 2.557324e-04\n",
      "  Step 2700/4407 - Loss: 2.1227 | PPL: 8.35 | LR: 2.556481e-04\n",
      "  Step 2700/4407 - Loss: 2.1227 | PPL: 8.35 | LR: 2.556481e-04\n",
      "  Step 2800/4407 - Loss: 2.1246 | PPL: 8.37 | LR: 2.555637e-04\n",
      "  Step 2800/4407 - Loss: 2.1246 | PPL: 8.37 | LR: 2.555637e-04\n",
      "  Step 2900/4407 - Loss: 2.1253 | PPL: 8.38 | LR: 2.554793e-04\n",
      "  Step 2900/4407 - Loss: 2.1253 | PPL: 8.38 | LR: 2.554793e-04\n",
      "  Step 3000/4407 - Loss: 2.1268 | PPL: 8.39 | LR: 2.553948e-04\n",
      "  Step 3000/4407 - Loss: 2.1268 | PPL: 8.39 | LR: 2.553948e-04\n",
      "  Step 3100/4407 - Loss: 2.1268 | PPL: 8.39 | LR: 2.553102e-04\n",
      "  Step 3100/4407 - Loss: 2.1268 | PPL: 8.39 | LR: 2.553102e-04\n",
      "  Step 3200/4407 - Loss: 2.1280 | PPL: 8.40 | LR: 2.552255e-04\n",
      "  Step 3200/4407 - Loss: 2.1280 | PPL: 8.40 | LR: 2.552255e-04\n",
      "  Step 3300/4407 - Loss: 2.1281 | PPL: 8.40 | LR: 2.551408e-04\n",
      "  Step 3300/4407 - Loss: 2.1281 | PPL: 8.40 | LR: 2.551408e-04\n",
      "  Step 3400/4407 - Loss: 2.1295 | PPL: 8.41 | LR: 2.550561e-04\n",
      "  Step 3400/4407 - Loss: 2.1295 | PPL: 8.41 | LR: 2.550561e-04\n",
      "  Step 3500/4407 - Loss: 2.1298 | PPL: 8.41 | LR: 2.549712e-04\n",
      "  Step 3500/4407 - Loss: 2.1298 | PPL: 8.41 | LR: 2.549712e-04\n",
      "  Step 3600/4407 - Loss: 2.1295 | PPL: 8.41 | LR: 2.548863e-04\n",
      "  Step 3600/4407 - Loss: 2.1295 | PPL: 8.41 | LR: 2.548863e-04\n",
      "  Step 3700/4407 - Loss: 2.1301 | PPL: 8.42 | LR: 2.548014e-04\n",
      "  Step 3700/4407 - Loss: 2.1301 | PPL: 8.42 | LR: 2.548014e-04\n",
      "  Step 3800/4407 - Loss: 2.1296 | PPL: 8.41 | LR: 2.547163e-04\n",
      "  Step 3800/4407 - Loss: 2.1296 | PPL: 8.41 | LR: 2.547163e-04\n",
      "  Step 3900/4407 - Loss: 2.1295 | PPL: 8.41 | LR: 2.546312e-04\n",
      "  Step 3900/4407 - Loss: 2.1295 | PPL: 8.41 | LR: 2.546312e-04\n",
      "  Step 4000/4407 - Loss: 2.1299 | PPL: 8.41 | LR: 2.545461e-04\n",
      "  Step 4000/4407 - Loss: 2.1299 | PPL: 8.41 | LR: 2.545461e-04\n",
      "  Step 4100/4407 - Loss: 2.1305 | PPL: 8.42 | LR: 2.544608e-04\n",
      "  Step 4100/4407 - Loss: 2.1305 | PPL: 8.42 | LR: 2.544608e-04\n",
      "  Step 4200/4407 - Loss: 2.1305 | PPL: 8.42 | LR: 2.543755e-04\n",
      "  Step 4200/4407 - Loss: 2.1305 | PPL: 8.42 | LR: 2.543755e-04\n",
      "  Step 4300/4407 - Loss: 2.1315 | PPL: 8.43 | LR: 2.542902e-04\n",
      "  Step 4300/4407 - Loss: 2.1315 | PPL: 8.43 | LR: 2.542902e-04\n",
      "  Step 4400/4407 - Loss: 2.1313 | PPL: 8.43 | LR: 2.542047e-04\n",
      "  Step 4400/4407 - Loss: 2.1313 | PPL: 8.43 | LR: 2.542047e-04\n",
      "Epoch 33/100 | Train Loss: 2.1315 | Train PPL: 8.43 | Val Loss: 2.6254 | Val PPL: 13.81\n",
      "Epoch 33/100 | Train Loss: 2.1315 | Train PPL: 8.43 | Val Loss: 2.6254 | Val PPL: 13.81\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.43\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.43\n",
      "  Step 100/4407 - Loss: 2.0874 | PPL: 8.06 | LR: 2.541133e-04\n",
      "  Step 100/4407 - Loss: 2.0874 | PPL: 8.06 | LR: 2.541133e-04\n",
      "  Step 200/4407 - Loss: 2.0765 | PPL: 7.98 | LR: 2.540277e-04\n",
      "  Step 200/4407 - Loss: 2.0765 | PPL: 7.98 | LR: 2.540277e-04\n",
      "  Step 300/4407 - Loss: 2.0815 | PPL: 8.02 | LR: 2.539421e-04\n",
      "  Step 300/4407 - Loss: 2.0815 | PPL: 8.02 | LR: 2.539421e-04\n",
      "  Step 400/4407 - Loss: 2.0817 | PPL: 8.02 | LR: 2.538564e-04\n",
      "  Step 400/4407 - Loss: 2.0817 | PPL: 8.02 | LR: 2.538564e-04\n",
      "  Step 500/4407 - Loss: 2.0805 | PPL: 8.01 | LR: 2.537706e-04\n",
      "  Step 500/4407 - Loss: 2.0805 | PPL: 8.01 | LR: 2.537706e-04\n",
      "  Step 600/4407 - Loss: 2.0857 | PPL: 8.05 | LR: 2.536848e-04\n",
      "  Step 600/4407 - Loss: 2.0857 | PPL: 8.05 | LR: 2.536848e-04\n",
      "  Step 700/4407 - Loss: 2.0851 | PPL: 8.05 | LR: 2.535989e-04\n",
      "  Step 700/4407 - Loss: 2.0851 | PPL: 8.05 | LR: 2.535989e-04\n",
      "  Step 800/4407 - Loss: 2.0860 | PPL: 8.05 | LR: 2.535129e-04\n",
      "  Step 800/4407 - Loss: 2.0860 | PPL: 8.05 | LR: 2.535129e-04\n",
      "  Step 900/4407 - Loss: 2.0873 | PPL: 8.06 | LR: 2.534269e-04\n",
      "  Step 900/4407 - Loss: 2.0873 | PPL: 8.06 | LR: 2.534269e-04\n",
      "  Step 1000/4407 - Loss: 2.0893 | PPL: 8.08 | LR: 2.533408e-04\n",
      "  Step 1000/4407 - Loss: 2.0893 | PPL: 8.08 | LR: 2.533408e-04\n",
      "  Step 1100/4407 - Loss: 2.0946 | PPL: 8.12 | LR: 2.532547e-04\n",
      "  Step 1100/4407 - Loss: 2.0946 | PPL: 8.12 | LR: 2.532547e-04\n",
      "  Step 1200/4407 - Loss: 2.0979 | PPL: 8.15 | LR: 2.531685e-04\n",
      "  Step 1200/4407 - Loss: 2.0979 | PPL: 8.15 | LR: 2.531685e-04\n",
      "  Step 1300/4407 - Loss: 2.0994 | PPL: 8.16 | LR: 2.530822e-04\n",
      "  Step 1300/4407 - Loss: 2.0994 | PPL: 8.16 | LR: 2.530822e-04\n",
      "  Step 1400/4407 - Loss: 2.0995 | PPL: 8.16 | LR: 2.529959e-04\n",
      "  Step 1400/4407 - Loss: 2.0995 | PPL: 8.16 | LR: 2.529959e-04\n",
      "  Step 1500/4407 - Loss: 2.1038 | PPL: 8.20 | LR: 2.529095e-04\n",
      "  Step 1500/4407 - Loss: 2.1038 | PPL: 8.20 | LR: 2.529095e-04\n",
      "  Step 1600/4407 - Loss: 2.1049 | PPL: 8.21 | LR: 2.528230e-04\n",
      "  Step 1600/4407 - Loss: 2.1049 | PPL: 8.21 | LR: 2.528230e-04\n",
      "  Step 1700/4407 - Loss: 2.1072 | PPL: 8.23 | LR: 2.527364e-04\n",
      "  Step 1700/4407 - Loss: 2.1072 | PPL: 8.23 | LR: 2.527364e-04\n",
      "  Step 1800/4407 - Loss: 2.1093 | PPL: 8.24 | LR: 2.526498e-04\n",
      "  Step 1800/4407 - Loss: 2.1093 | PPL: 8.24 | LR: 2.526498e-04\n",
      "  Step 1900/4407 - Loss: 2.1080 | PPL: 8.23 | LR: 2.525632e-04\n",
      "  Step 1900/4407 - Loss: 2.1080 | PPL: 8.23 | LR: 2.525632e-04\n",
      "  Step 2000/4407 - Loss: 2.1095 | PPL: 8.24 | LR: 2.524764e-04\n",
      "  Step 2000/4407 - Loss: 2.1095 | PPL: 8.24 | LR: 2.524764e-04\n",
      "  Step 2100/4407 - Loss: 2.1109 | PPL: 8.26 | LR: 2.523897e-04\n",
      "  Step 2100/4407 - Loss: 2.1109 | PPL: 8.26 | LR: 2.523897e-04\n",
      "  Step 2200/4407 - Loss: 2.1095 | PPL: 8.24 | LR: 2.523028e-04\n",
      "  Step 2200/4407 - Loss: 2.1095 | PPL: 8.24 | LR: 2.523028e-04\n",
      "  Step 2300/4407 - Loss: 2.1132 | PPL: 8.27 | LR: 2.522159e-04\n",
      "  Step 2300/4407 - Loss: 2.1132 | PPL: 8.27 | LR: 2.522159e-04\n",
      "  Step 2400/4407 - Loss: 2.1142 | PPL: 8.28 | LR: 2.521289e-04\n",
      "  Step 2400/4407 - Loss: 2.1142 | PPL: 8.28 | LR: 2.521289e-04\n",
      "  Step 2500/4407 - Loss: 2.1160 | PPL: 8.30 | LR: 2.520418e-04\n",
      "  Step 2500/4407 - Loss: 2.1160 | PPL: 8.30 | LR: 2.520418e-04\n",
      "  Step 2600/4407 - Loss: 2.1159 | PPL: 8.30 | LR: 2.519547e-04\n",
      "  Step 2600/4407 - Loss: 2.1159 | PPL: 8.30 | LR: 2.519547e-04\n",
      "  Step 2700/4407 - Loss: 2.1170 | PPL: 8.31 | LR: 2.518675e-04\n",
      "  Step 2700/4407 - Loss: 2.1170 | PPL: 8.31 | LR: 2.518675e-04\n",
      "  Step 2800/4407 - Loss: 2.1187 | PPL: 8.32 | LR: 2.517803e-04\n",
      "  Step 2800/4407 - Loss: 2.1187 | PPL: 8.32 | LR: 2.517803e-04\n",
      "  Step 2900/4407 - Loss: 2.1188 | PPL: 8.32 | LR: 2.516930e-04\n",
      "  Step 2900/4407 - Loss: 2.1188 | PPL: 8.32 | LR: 2.516930e-04\n",
      "  Step 3000/4407 - Loss: 2.1213 | PPL: 8.34 | LR: 2.516056e-04\n",
      "  Step 3000/4407 - Loss: 2.1213 | PPL: 8.34 | LR: 2.516056e-04\n",
      "  Step 3100/4407 - Loss: 2.1211 | PPL: 8.34 | LR: 2.515182e-04\n",
      "  Step 3100/4407 - Loss: 2.1211 | PPL: 8.34 | LR: 2.515182e-04\n",
      "  Step 3200/4407 - Loss: 2.1225 | PPL: 8.35 | LR: 2.514307e-04\n",
      "  Step 3200/4407 - Loss: 2.1225 | PPL: 8.35 | LR: 2.514307e-04\n",
      "  Step 3300/4407 - Loss: 2.1217 | PPL: 8.35 | LR: 2.513431e-04\n",
      "  Step 3300/4407 - Loss: 2.1217 | PPL: 8.35 | LR: 2.513431e-04\n",
      "  Step 3400/4407 - Loss: 2.1223 | PPL: 8.35 | LR: 2.512555e-04\n",
      "  Step 3400/4407 - Loss: 2.1223 | PPL: 8.35 | LR: 2.512555e-04\n",
      "  Step 3500/4407 - Loss: 2.1225 | PPL: 8.35 | LR: 2.511678e-04\n",
      "  Step 3500/4407 - Loss: 2.1225 | PPL: 8.35 | LR: 2.511678e-04\n",
      "  Step 3600/4407 - Loss: 2.1220 | PPL: 8.35 | LR: 2.510801e-04\n",
      "  Step 3600/4407 - Loss: 2.1220 | PPL: 8.35 | LR: 2.510801e-04\n",
      "  Step 3700/4407 - Loss: 2.1226 | PPL: 8.35 | LR: 2.509922e-04\n",
      "  Step 3700/4407 - Loss: 2.1226 | PPL: 8.35 | LR: 2.509922e-04\n",
      "  Step 3800/4407 - Loss: 2.1232 | PPL: 8.36 | LR: 2.509044e-04\n",
      "  Step 3800/4407 - Loss: 2.1232 | PPL: 8.36 | LR: 2.509044e-04\n",
      "  Step 3900/4407 - Loss: 2.1230 | PPL: 8.36 | LR: 2.508164e-04\n",
      "  Step 3900/4407 - Loss: 2.1230 | PPL: 8.36 | LR: 2.508164e-04\n",
      "  Step 4000/4407 - Loss: 2.1238 | PPL: 8.36 | LR: 2.507284e-04\n",
      "  Step 4000/4407 - Loss: 2.1238 | PPL: 8.36 | LR: 2.507284e-04\n",
      "  Step 4100/4407 - Loss: 2.1242 | PPL: 8.37 | LR: 2.506404e-04\n",
      "  Step 4100/4407 - Loss: 2.1242 | PPL: 8.37 | LR: 2.506404e-04\n",
      "  Step 4200/4407 - Loss: 2.1238 | PPL: 8.36 | LR: 2.505522e-04\n",
      "  Step 4200/4407 - Loss: 2.1238 | PPL: 8.36 | LR: 2.505522e-04\n",
      "  Step 4300/4407 - Loss: 2.1239 | PPL: 8.36 | LR: 2.504640e-04\n",
      "  Step 4300/4407 - Loss: 2.1239 | PPL: 8.36 | LR: 2.504640e-04\n",
      "  Step 4400/4407 - Loss: 2.1252 | PPL: 8.37 | LR: 2.503758e-04\n",
      "  Step 4400/4407 - Loss: 2.1252 | PPL: 8.37 | LR: 2.503758e-04\n",
      "Epoch 34/100 | Train Loss: 2.1253 | Train PPL: 8.38 | Val Loss: 2.6198 | Val PPL: 13.73\n",
      "Epoch 34/100 | Train Loss: 2.1253 | Train PPL: 8.38 | Val Loss: 2.6198 | Val PPL: 13.73\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.38\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.38\n",
      "  Step 100/4407 - Loss: 2.0749 | PPL: 7.96 | LR: 2.502813e-04\n",
      "  Step 100/4407 - Loss: 2.0749 | PPL: 7.96 | LR: 2.502813e-04\n",
      "  Step 200/4407 - Loss: 2.0911 | PPL: 8.09 | LR: 2.501929e-04\n",
      "  Step 200/4407 - Loss: 2.0911 | PPL: 8.09 | LR: 2.501929e-04\n",
      "  Step 300/4407 - Loss: 2.1088 | PPL: 8.24 | LR: 2.501044e-04\n",
      "  Step 300/4407 - Loss: 2.1088 | PPL: 8.24 | LR: 2.501044e-04\n",
      "  Step 400/4407 - Loss: 2.0973 | PPL: 8.14 | LR: 2.500159e-04\n",
      "  Step 400/4407 - Loss: 2.0973 | PPL: 8.14 | LR: 2.500159e-04\n",
      "  Step 500/4407 - Loss: 2.0891 | PPL: 8.08 | LR: 2.499273e-04\n",
      "  Step 500/4407 - Loss: 2.0891 | PPL: 8.08 | LR: 2.499273e-04\n",
      "  Step 600/4407 - Loss: 2.0822 | PPL: 8.02 | LR: 2.498387e-04\n",
      "  Step 600/4407 - Loss: 2.0822 | PPL: 8.02 | LR: 2.498387e-04\n",
      "  Step 700/4407 - Loss: 2.0815 | PPL: 8.02 | LR: 2.497500e-04\n",
      "  Step 700/4407 - Loss: 2.0815 | PPL: 8.02 | LR: 2.497500e-04\n",
      "  Step 800/4407 - Loss: 2.0834 | PPL: 8.03 | LR: 2.496612e-04\n",
      "  Step 800/4407 - Loss: 2.0834 | PPL: 8.03 | LR: 2.496612e-04\n",
      "  Step 900/4407 - Loss: 2.0904 | PPL: 8.09 | LR: 2.495724e-04\n",
      "  Step 900/4407 - Loss: 2.0904 | PPL: 8.09 | LR: 2.495724e-04\n",
      "  Step 1000/4407 - Loss: 2.0950 | PPL: 8.13 | LR: 2.494835e-04\n",
      "  Step 1000/4407 - Loss: 2.0950 | PPL: 8.13 | LR: 2.494835e-04\n",
      "  Step 1100/4407 - Loss: 2.0946 | PPL: 8.12 | LR: 2.493946e-04\n",
      "  Step 1100/4407 - Loss: 2.0946 | PPL: 8.12 | LR: 2.493946e-04\n",
      "  Step 1200/4407 - Loss: 2.0960 | PPL: 8.13 | LR: 2.493056e-04\n",
      "  Step 1200/4407 - Loss: 2.0960 | PPL: 8.13 | LR: 2.493056e-04\n",
      "  Step 1300/4407 - Loss: 2.0980 | PPL: 8.15 | LR: 2.492165e-04\n",
      "  Step 1300/4407 - Loss: 2.0980 | PPL: 8.15 | LR: 2.492165e-04\n",
      "  Step 1400/4407 - Loss: 2.0999 | PPL: 8.17 | LR: 2.491273e-04\n",
      "  Step 1400/4407 - Loss: 2.0999 | PPL: 8.17 | LR: 2.491273e-04\n",
      "  Step 1500/4407 - Loss: 2.1011 | PPL: 8.18 | LR: 2.490381e-04\n",
      "  Step 1500/4407 - Loss: 2.1011 | PPL: 8.18 | LR: 2.490381e-04\n",
      "  Step 1600/4407 - Loss: 2.1026 | PPL: 8.19 | LR: 2.489489e-04\n",
      "  Step 1600/4407 - Loss: 2.1026 | PPL: 8.19 | LR: 2.489489e-04\n",
      "  Step 1700/4407 - Loss: 2.1036 | PPL: 8.20 | LR: 2.488595e-04\n",
      "  Step 1700/4407 - Loss: 2.1036 | PPL: 8.20 | LR: 2.488595e-04\n",
      "  Step 1800/4407 - Loss: 2.1047 | PPL: 8.20 | LR: 2.487702e-04\n",
      "  Step 1800/4407 - Loss: 2.1047 | PPL: 8.20 | LR: 2.487702e-04\n",
      "  Step 1900/4407 - Loss: 2.1047 | PPL: 8.20 | LR: 2.486807e-04\n",
      "  Step 1900/4407 - Loss: 2.1047 | PPL: 8.20 | LR: 2.486807e-04\n",
      "  Step 2000/4407 - Loss: 2.1041 | PPL: 8.20 | LR: 2.485912e-04\n",
      "  Step 2000/4407 - Loss: 2.1041 | PPL: 8.20 | LR: 2.485912e-04\n",
      "  Step 2100/4407 - Loss: 2.1068 | PPL: 8.22 | LR: 2.485016e-04\n",
      "  Step 2100/4407 - Loss: 2.1068 | PPL: 8.22 | LR: 2.485016e-04\n",
      "  Step 2200/4407 - Loss: 2.1087 | PPL: 8.24 | LR: 2.484120e-04\n",
      "  Step 2200/4407 - Loss: 2.1087 | PPL: 8.24 | LR: 2.484120e-04\n",
      "  Step 2300/4407 - Loss: 2.1091 | PPL: 8.24 | LR: 2.483223e-04\n",
      "  Step 2300/4407 - Loss: 2.1091 | PPL: 8.24 | LR: 2.483223e-04\n",
      "  Step 2400/4407 - Loss: 2.1105 | PPL: 8.25 | LR: 2.482325e-04\n",
      "  Step 2400/4407 - Loss: 2.1105 | PPL: 8.25 | LR: 2.482325e-04\n",
      "  Step 2500/4407 - Loss: 2.1110 | PPL: 8.26 | LR: 2.481427e-04\n",
      "  Step 2500/4407 - Loss: 2.1110 | PPL: 8.26 | LR: 2.481427e-04\n",
      "  Step 2600/4407 - Loss: 2.1120 | PPL: 8.26 | LR: 2.480528e-04\n",
      "  Step 2600/4407 - Loss: 2.1120 | PPL: 8.26 | LR: 2.480528e-04\n",
      "  Step 2700/4407 - Loss: 2.1118 | PPL: 8.26 | LR: 2.479629e-04\n",
      "  Step 2700/4407 - Loss: 2.1118 | PPL: 8.26 | LR: 2.479629e-04\n",
      "  Step 2800/4407 - Loss: 2.1142 | PPL: 8.28 | LR: 2.478729e-04\n",
      "  Step 2800/4407 - Loss: 2.1142 | PPL: 8.28 | LR: 2.478729e-04\n",
      "  Step 2900/4407 - Loss: 2.1126 | PPL: 8.27 | LR: 2.477828e-04\n",
      "  Step 2900/4407 - Loss: 2.1126 | PPL: 8.27 | LR: 2.477828e-04\n",
      "  Step 3000/4407 - Loss: 2.1126 | PPL: 8.27 | LR: 2.476927e-04\n",
      "  Step 3000/4407 - Loss: 2.1126 | PPL: 8.27 | LR: 2.476927e-04\n",
      "  Step 3100/4407 - Loss: 2.1134 | PPL: 8.28 | LR: 2.476025e-04\n",
      "  Step 3100/4407 - Loss: 2.1134 | PPL: 8.28 | LR: 2.476025e-04\n",
      "  Step 3200/4407 - Loss: 2.1134 | PPL: 8.28 | LR: 2.475123e-04\n",
      "  Step 3200/4407 - Loss: 2.1134 | PPL: 8.28 | LR: 2.475123e-04\n",
      "  Step 3300/4407 - Loss: 2.1152 | PPL: 8.29 | LR: 2.474220e-04\n",
      "  Step 3300/4407 - Loss: 2.1152 | PPL: 8.29 | LR: 2.474220e-04\n",
      "  Step 3400/4407 - Loss: 2.1157 | PPL: 8.30 | LR: 2.473316e-04\n",
      "  Step 3400/4407 - Loss: 2.1157 | PPL: 8.30 | LR: 2.473316e-04\n",
      "  Step 3500/4407 - Loss: 2.1151 | PPL: 8.29 | LR: 2.472411e-04\n",
      "  Step 3500/4407 - Loss: 2.1151 | PPL: 8.29 | LR: 2.472411e-04\n",
      "  Step 3600/4407 - Loss: 2.1162 | PPL: 8.30 | LR: 2.471507e-04\n",
      "  Step 3600/4407 - Loss: 2.1162 | PPL: 8.30 | LR: 2.471507e-04\n",
      "  Step 3700/4407 - Loss: 2.1171 | PPL: 8.31 | LR: 2.470601e-04\n",
      "  Step 3700/4407 - Loss: 2.1171 | PPL: 8.31 | LR: 2.470601e-04\n",
      "  Step 3800/4407 - Loss: 2.1181 | PPL: 8.32 | LR: 2.469695e-04\n",
      "  Step 3800/4407 - Loss: 2.1181 | PPL: 8.32 | LR: 2.469695e-04\n",
      "  Step 3900/4407 - Loss: 2.1190 | PPL: 8.32 | LR: 2.468788e-04\n",
      "  Step 3900/4407 - Loss: 2.1190 | PPL: 8.32 | LR: 2.468788e-04\n",
      "  Step 4000/4407 - Loss: 2.1195 | PPL: 8.33 | LR: 2.467881e-04\n",
      "  Step 4000/4407 - Loss: 2.1195 | PPL: 8.33 | LR: 2.467881e-04\n",
      "  Step 4100/4407 - Loss: 2.1198 | PPL: 8.33 | LR: 2.466973e-04\n",
      "  Step 4100/4407 - Loss: 2.1198 | PPL: 8.33 | LR: 2.466973e-04\n",
      "  Step 4200/4407 - Loss: 2.1190 | PPL: 8.32 | LR: 2.466064e-04\n",
      "  Step 4200/4407 - Loss: 2.1190 | PPL: 8.32 | LR: 2.466064e-04\n",
      "  Step 4300/4407 - Loss: 2.1186 | PPL: 8.32 | LR: 2.465155e-04\n",
      "  Step 4300/4407 - Loss: 2.1186 | PPL: 8.32 | LR: 2.465155e-04\n",
      "  Step 4400/4407 - Loss: 2.1195 | PPL: 8.33 | LR: 2.464245e-04\n",
      "  Step 4400/4407 - Loss: 2.1195 | PPL: 8.33 | LR: 2.464245e-04\n",
      "Epoch 35/100 | Train Loss: 2.1195 | Train PPL: 8.33 | Val Loss: 2.6241 | Val PPL: 13.79\n",
      "Epoch 35/100 | Train Loss: 2.1195 | Train PPL: 8.33 | Val Loss: 2.6241 | Val PPL: 13.79\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.33\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.33\n",
      "  Step 100/4407 - Loss: 2.0610 | PPL: 7.85 | LR: 2.463271e-04\n",
      "  Step 100/4407 - Loss: 2.0610 | PPL: 7.85 | LR: 2.463271e-04\n",
      "  Step 200/4407 - Loss: 2.0593 | PPL: 7.84 | LR: 2.462360e-04\n",
      "  Step 200/4407 - Loss: 2.0593 | PPL: 7.84 | LR: 2.462360e-04\n",
      "  Step 300/4407 - Loss: 2.0606 | PPL: 7.85 | LR: 2.461448e-04\n",
      "  Step 300/4407 - Loss: 2.0606 | PPL: 7.85 | LR: 2.461448e-04\n",
      "  Step 400/4407 - Loss: 2.0755 | PPL: 7.97 | LR: 2.460536e-04\n",
      "  Step 400/4407 - Loss: 2.0755 | PPL: 7.97 | LR: 2.460536e-04\n",
      "  Step 500/4407 - Loss: 2.0772 | PPL: 7.98 | LR: 2.459623e-04\n",
      "  Step 500/4407 - Loss: 2.0772 | PPL: 7.98 | LR: 2.459623e-04\n",
      "  Step 600/4407 - Loss: 2.0765 | PPL: 7.98 | LR: 2.458710e-04\n",
      "  Step 600/4407 - Loss: 2.0765 | PPL: 7.98 | LR: 2.458710e-04\n",
      "  Step 700/4407 - Loss: 2.0791 | PPL: 8.00 | LR: 2.457796e-04\n",
      "  Step 700/4407 - Loss: 2.0791 | PPL: 8.00 | LR: 2.457796e-04\n",
      "  Step 800/4407 - Loss: 2.0773 | PPL: 7.98 | LR: 2.456881e-04\n",
      "  Step 800/4407 - Loss: 2.0773 | PPL: 7.98 | LR: 2.456881e-04\n",
      "  Step 900/4407 - Loss: 2.0808 | PPL: 8.01 | LR: 2.455966e-04\n",
      "  Step 900/4407 - Loss: 2.0808 | PPL: 8.01 | LR: 2.455966e-04\n",
      "  Step 1000/4407 - Loss: 2.0831 | PPL: 8.03 | LR: 2.455050e-04\n",
      "  Step 1000/4407 - Loss: 2.0831 | PPL: 8.03 | LR: 2.455050e-04\n",
      "  Step 1100/4407 - Loss: 2.0857 | PPL: 8.05 | LR: 2.454133e-04\n",
      "  Step 1100/4407 - Loss: 2.0857 | PPL: 8.05 | LR: 2.454133e-04\n",
      "  Step 1200/4407 - Loss: 2.0857 | PPL: 8.05 | LR: 2.453216e-04\n",
      "  Step 1200/4407 - Loss: 2.0857 | PPL: 8.05 | LR: 2.453216e-04\n",
      "  Step 1300/4407 - Loss: 2.0875 | PPL: 8.06 | LR: 2.452299e-04\n",
      "  Step 1300/4407 - Loss: 2.0875 | PPL: 8.06 | LR: 2.452299e-04\n",
      "  Step 1400/4407 - Loss: 2.0874 | PPL: 8.06 | LR: 2.451380e-04\n",
      "  Step 1400/4407 - Loss: 2.0874 | PPL: 8.06 | LR: 2.451380e-04\n",
      "  Step 1500/4407 - Loss: 2.0871 | PPL: 8.06 | LR: 2.450462e-04\n",
      "  Step 1500/4407 - Loss: 2.0871 | PPL: 8.06 | LR: 2.450462e-04\n",
      "  Step 1600/4407 - Loss: 2.0888 | PPL: 8.07 | LR: 2.449542e-04\n",
      "  Step 1600/4407 - Loss: 2.0888 | PPL: 8.07 | LR: 2.449542e-04\n",
      "  Step 1700/4407 - Loss: 2.0900 | PPL: 8.09 | LR: 2.448622e-04\n",
      "  Step 1700/4407 - Loss: 2.0900 | PPL: 8.09 | LR: 2.448622e-04\n",
      "  Step 1800/4407 - Loss: 2.0924 | PPL: 8.10 | LR: 2.447701e-04\n",
      "  Step 1800/4407 - Loss: 2.0924 | PPL: 8.10 | LR: 2.447701e-04\n",
      "  Step 1900/4407 - Loss: 2.0952 | PPL: 8.13 | LR: 2.446780e-04\n",
      "  Step 1900/4407 - Loss: 2.0952 | PPL: 8.13 | LR: 2.446780e-04\n",
      "  Step 2000/4407 - Loss: 2.0964 | PPL: 8.14 | LR: 2.445858e-04\n",
      "  Step 2000/4407 - Loss: 2.0964 | PPL: 8.14 | LR: 2.445858e-04\n",
      "  Step 2100/4407 - Loss: 2.0964 | PPL: 8.14 | LR: 2.444936e-04\n",
      "  Step 2100/4407 - Loss: 2.0964 | PPL: 8.14 | LR: 2.444936e-04\n",
      "  Step 2200/4407 - Loss: 2.0985 | PPL: 8.15 | LR: 2.444013e-04\n",
      "  Step 2200/4407 - Loss: 2.0985 | PPL: 8.15 | LR: 2.444013e-04\n",
      "  Step 2300/4407 - Loss: 2.0996 | PPL: 8.16 | LR: 2.443089e-04\n",
      "  Step 2300/4407 - Loss: 2.0996 | PPL: 8.16 | LR: 2.443089e-04\n",
      "  Step 2400/4407 - Loss: 2.1010 | PPL: 8.17 | LR: 2.442165e-04\n",
      "  Step 2400/4407 - Loss: 2.1010 | PPL: 8.17 | LR: 2.442165e-04\n",
      "  Step 2500/4407 - Loss: 2.1021 | PPL: 8.18 | LR: 2.441240e-04\n",
      "  Step 2500/4407 - Loss: 2.1021 | PPL: 8.18 | LR: 2.441240e-04\n",
      "  Step 2600/4407 - Loss: 2.1034 | PPL: 8.19 | LR: 2.440315e-04\n",
      "  Step 2600/4407 - Loss: 2.1034 | PPL: 8.19 | LR: 2.440315e-04\n",
      "  Step 2700/4407 - Loss: 2.1048 | PPL: 8.21 | LR: 2.439389e-04\n",
      "  Step 2700/4407 - Loss: 2.1048 | PPL: 8.21 | LR: 2.439389e-04\n",
      "  Step 2800/4407 - Loss: 2.1062 | PPL: 8.22 | LR: 2.438462e-04\n",
      "  Step 2800/4407 - Loss: 2.1062 | PPL: 8.22 | LR: 2.438462e-04\n",
      "  Step 2900/4407 - Loss: 2.1071 | PPL: 8.22 | LR: 2.437535e-04\n",
      "  Step 2900/4407 - Loss: 2.1071 | PPL: 8.22 | LR: 2.437535e-04\n",
      "  Step 3000/4407 - Loss: 2.1072 | PPL: 8.22 | LR: 2.436608e-04\n",
      "  Step 3000/4407 - Loss: 2.1072 | PPL: 8.22 | LR: 2.436608e-04\n",
      "  Step 3100/4407 - Loss: 2.1088 | PPL: 8.24 | LR: 2.435679e-04\n",
      "  Step 3100/4407 - Loss: 2.1088 | PPL: 8.24 | LR: 2.435679e-04\n",
      "  Step 3200/4407 - Loss: 2.1086 | PPL: 8.24 | LR: 2.434750e-04\n",
      "  Step 3200/4407 - Loss: 2.1086 | PPL: 8.24 | LR: 2.434750e-04\n",
      "  Step 3300/4407 - Loss: 2.1105 | PPL: 8.25 | LR: 2.433821e-04\n",
      "  Step 3300/4407 - Loss: 2.1105 | PPL: 8.25 | LR: 2.433821e-04\n",
      "  Step 3400/4407 - Loss: 2.1118 | PPL: 8.26 | LR: 2.432891e-04\n",
      "  Step 3400/4407 - Loss: 2.1118 | PPL: 8.26 | LR: 2.432891e-04\n",
      "  Step 3500/4407 - Loss: 2.1109 | PPL: 8.26 | LR: 2.431960e-04\n",
      "  Step 3500/4407 - Loss: 2.1109 | PPL: 8.26 | LR: 2.431960e-04\n",
      "  Step 3600/4407 - Loss: 2.1114 | PPL: 8.26 | LR: 2.431029e-04\n",
      "  Step 3600/4407 - Loss: 2.1114 | PPL: 8.26 | LR: 2.431029e-04\n",
      "  Step 3700/4407 - Loss: 2.1132 | PPL: 8.28 | LR: 2.430097e-04\n",
      "  Step 3700/4407 - Loss: 2.1132 | PPL: 8.28 | LR: 2.430097e-04\n",
      "  Step 3800/4407 - Loss: 2.1130 | PPL: 8.27 | LR: 2.429165e-04\n",
      "  Step 3800/4407 - Loss: 2.1130 | PPL: 8.27 | LR: 2.429165e-04\n",
      "  Step 3900/4407 - Loss: 2.1140 | PPL: 8.28 | LR: 2.428231e-04\n",
      "  Step 3900/4407 - Loss: 2.1140 | PPL: 8.28 | LR: 2.428231e-04\n",
      "  Step 4000/4407 - Loss: 2.1143 | PPL: 8.28 | LR: 2.427298e-04\n",
      "  Step 4000/4407 - Loss: 2.1143 | PPL: 8.28 | LR: 2.427298e-04\n",
      "  Step 4100/4407 - Loss: 2.1138 | PPL: 8.28 | LR: 2.426364e-04\n",
      "  Step 4100/4407 - Loss: 2.1138 | PPL: 8.28 | LR: 2.426364e-04\n",
      "  Step 4200/4407 - Loss: 2.1135 | PPL: 8.28 | LR: 2.425429e-04\n",
      "  Step 4200/4407 - Loss: 2.1135 | PPL: 8.28 | LR: 2.425429e-04\n",
      "  Step 4300/4407 - Loss: 2.1133 | PPL: 8.28 | LR: 2.424494e-04\n",
      "  Step 4300/4407 - Loss: 2.1133 | PPL: 8.28 | LR: 2.424494e-04\n",
      "  Step 4400/4407 - Loss: 2.1142 | PPL: 8.28 | LR: 2.423558e-04\n",
      "  Step 4400/4407 - Loss: 2.1142 | PPL: 8.28 | LR: 2.423558e-04\n",
      "Epoch 36/100 | Train Loss: 2.1139 | Train PPL: 8.28 | Val Loss: 2.6269 | Val PPL: 13.83\n",
      "Epoch 36/100 | Train Loss: 2.1139 | Train PPL: 8.28 | Val Loss: 2.6269 | Val PPL: 13.83\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.28\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.28\n",
      "  Step 100/4407 - Loss: 2.0723 | PPL: 7.94 | LR: 2.422556e-04\n",
      "  Step 100/4407 - Loss: 2.0723 | PPL: 7.94 | LR: 2.422556e-04\n",
      "  Step 200/4407 - Loss: 2.0681 | PPL: 7.91 | LR: 2.421619e-04\n",
      "  Step 200/4407 - Loss: 2.0681 | PPL: 7.91 | LR: 2.421619e-04\n",
      "  Step 300/4407 - Loss: 2.0708 | PPL: 7.93 | LR: 2.420681e-04\n",
      "  Step 300/4407 - Loss: 2.0708 | PPL: 7.93 | LR: 2.420681e-04\n",
      "  Step 400/4407 - Loss: 2.0699 | PPL: 7.92 | LR: 2.419743e-04\n",
      "  Step 400/4407 - Loss: 2.0699 | PPL: 7.92 | LR: 2.419743e-04\n",
      "  Step 500/4407 - Loss: 2.0815 | PPL: 8.02 | LR: 2.418804e-04\n",
      "  Step 500/4407 - Loss: 2.0815 | PPL: 8.02 | LR: 2.418804e-04\n",
      "  Step 600/4407 - Loss: 2.0792 | PPL: 8.00 | LR: 2.417864e-04\n",
      "  Step 600/4407 - Loss: 2.0792 | PPL: 8.00 | LR: 2.417864e-04\n",
      "  Step 700/4407 - Loss: 2.0804 | PPL: 8.01 | LR: 2.416924e-04\n",
      "  Step 700/4407 - Loss: 2.0804 | PPL: 8.01 | LR: 2.416924e-04\n",
      "  Step 800/4407 - Loss: 2.0771 | PPL: 7.98 | LR: 2.415984e-04\n",
      "  Step 800/4407 - Loss: 2.0771 | PPL: 7.98 | LR: 2.415984e-04\n",
      "  Step 900/4407 - Loss: 2.0837 | PPL: 8.03 | LR: 2.415043e-04\n",
      "  Step 900/4407 - Loss: 2.0837 | PPL: 8.03 | LR: 2.415043e-04\n",
      "  Step 1000/4407 - Loss: 2.0858 | PPL: 8.05 | LR: 2.414101e-04\n",
      "  Step 1000/4407 - Loss: 2.0858 | PPL: 8.05 | LR: 2.414101e-04\n",
      "  Step 1100/4407 - Loss: 2.0901 | PPL: 8.09 | LR: 2.413159e-04\n",
      "  Step 1100/4407 - Loss: 2.0901 | PPL: 8.09 | LR: 2.413159e-04\n",
      "  Step 1200/4407 - Loss: 2.0918 | PPL: 8.10 | LR: 2.412216e-04\n",
      "  Step 1200/4407 - Loss: 2.0918 | PPL: 8.10 | LR: 2.412216e-04\n",
      "  Step 1300/4407 - Loss: 2.0950 | PPL: 8.13 | LR: 2.411272e-04\n",
      "  Step 1300/4407 - Loss: 2.0950 | PPL: 8.13 | LR: 2.411272e-04\n",
      "  Step 1400/4407 - Loss: 2.0943 | PPL: 8.12 | LR: 2.410328e-04\n",
      "  Step 1400/4407 - Loss: 2.0943 | PPL: 8.12 | LR: 2.410328e-04\n",
      "  Step 1500/4407 - Loss: 2.0956 | PPL: 8.13 | LR: 2.409384e-04\n",
      "  Step 1500/4407 - Loss: 2.0956 | PPL: 8.13 | LR: 2.409384e-04\n",
      "  Step 1600/4407 - Loss: 2.0927 | PPL: 8.11 | LR: 2.408439e-04\n",
      "  Step 1600/4407 - Loss: 2.0927 | PPL: 8.11 | LR: 2.408439e-04\n",
      "  Step 1700/4407 - Loss: 2.0914 | PPL: 8.10 | LR: 2.407493e-04\n",
      "  Step 1700/4407 - Loss: 2.0914 | PPL: 8.10 | LR: 2.407493e-04\n",
      "  Step 1800/4407 - Loss: 2.0936 | PPL: 8.11 | LR: 2.406547e-04\n",
      "  Step 1800/4407 - Loss: 2.0936 | PPL: 8.11 | LR: 2.406547e-04\n",
      "  Step 1900/4407 - Loss: 2.0939 | PPL: 8.12 | LR: 2.405600e-04\n",
      "  Step 1900/4407 - Loss: 2.0939 | PPL: 8.12 | LR: 2.405600e-04\n",
      "  Step 2000/4407 - Loss: 2.0942 | PPL: 8.12 | LR: 2.404652e-04\n",
      "  Step 2000/4407 - Loss: 2.0942 | PPL: 8.12 | LR: 2.404652e-04\n",
      "  Step 2100/4407 - Loss: 2.0953 | PPL: 8.13 | LR: 2.403704e-04\n",
      "  Step 2100/4407 - Loss: 2.0953 | PPL: 8.13 | LR: 2.403704e-04\n",
      "  Step 2200/4407 - Loss: 2.0972 | PPL: 8.14 | LR: 2.402756e-04\n",
      "  Step 2200/4407 - Loss: 2.0972 | PPL: 8.14 | LR: 2.402756e-04\n",
      "  Step 2300/4407 - Loss: 2.0989 | PPL: 8.16 | LR: 2.401807e-04\n",
      "  Step 2300/4407 - Loss: 2.0989 | PPL: 8.16 | LR: 2.401807e-04\n",
      "  Step 2400/4407 - Loss: 2.0997 | PPL: 8.16 | LR: 2.400857e-04\n",
      "  Step 2400/4407 - Loss: 2.0997 | PPL: 8.16 | LR: 2.400857e-04\n",
      "  Step 2500/4407 - Loss: 2.1010 | PPL: 8.17 | LR: 2.399907e-04\n",
      "  Step 2500/4407 - Loss: 2.1010 | PPL: 8.17 | LR: 2.399907e-04\n",
      "  Step 2600/4407 - Loss: 2.1027 | PPL: 8.19 | LR: 2.398956e-04\n",
      "  Step 2600/4407 - Loss: 2.1027 | PPL: 8.19 | LR: 2.398956e-04\n",
      "  Step 2700/4407 - Loss: 2.1026 | PPL: 8.19 | LR: 2.398004e-04\n",
      "  Step 2700/4407 - Loss: 2.1026 | PPL: 8.19 | LR: 2.398004e-04\n",
      "  Step 2800/4407 - Loss: 2.1033 | PPL: 8.19 | LR: 2.397053e-04\n",
      "  Step 2800/4407 - Loss: 2.1033 | PPL: 8.19 | LR: 2.397053e-04\n",
      "  Step 2900/4407 - Loss: 2.1031 | PPL: 8.19 | LR: 2.396100e-04\n",
      "  Step 2900/4407 - Loss: 2.1031 | PPL: 8.19 | LR: 2.396100e-04\n",
      "  Step 3000/4407 - Loss: 2.1040 | PPL: 8.20 | LR: 2.395147e-04\n",
      "  Step 3000/4407 - Loss: 2.1040 | PPL: 8.20 | LR: 2.395147e-04\n",
      "  Step 3100/4407 - Loss: 2.1042 | PPL: 8.20 | LR: 2.394193e-04\n",
      "  Step 3100/4407 - Loss: 2.1042 | PPL: 8.20 | LR: 2.394193e-04\n",
      "  Step 3200/4407 - Loss: 2.1049 | PPL: 8.21 | LR: 2.393239e-04\n",
      "  Step 3200/4407 - Loss: 2.1049 | PPL: 8.21 | LR: 2.393239e-04\n",
      "  Step 3300/4407 - Loss: 2.1054 | PPL: 8.21 | LR: 2.392284e-04\n",
      "  Step 3300/4407 - Loss: 2.1054 | PPL: 8.21 | LR: 2.392284e-04\n",
      "  Step 3400/4407 - Loss: 2.1068 | PPL: 8.22 | LR: 2.391329e-04\n",
      "  Step 3400/4407 - Loss: 2.1068 | PPL: 8.22 | LR: 2.391329e-04\n",
      "  Step 3500/4407 - Loss: 2.1074 | PPL: 8.23 | LR: 2.390373e-04\n",
      "  Step 3500/4407 - Loss: 2.1074 | PPL: 8.23 | LR: 2.390373e-04\n",
      "  Step 3600/4407 - Loss: 2.1070 | PPL: 8.22 | LR: 2.389417e-04\n",
      "  Step 3600/4407 - Loss: 2.1070 | PPL: 8.22 | LR: 2.389417e-04\n",
      "  Step 3700/4407 - Loss: 2.1064 | PPL: 8.22 | LR: 2.388460e-04\n",
      "  Step 3700/4407 - Loss: 2.1064 | PPL: 8.22 | LR: 2.388460e-04\n",
      "  Step 3800/4407 - Loss: 2.1064 | PPL: 8.22 | LR: 2.387502e-04\n",
      "  Step 3800/4407 - Loss: 2.1064 | PPL: 8.22 | LR: 2.387502e-04\n",
      "  Step 3900/4407 - Loss: 2.1070 | PPL: 8.22 | LR: 2.386544e-04\n",
      "  Step 3900/4407 - Loss: 2.1070 | PPL: 8.22 | LR: 2.386544e-04\n",
      "  Step 4000/4407 - Loss: 2.1074 | PPL: 8.23 | LR: 2.385585e-04\n",
      "  Step 4000/4407 - Loss: 2.1074 | PPL: 8.23 | LR: 2.385585e-04\n",
      "  Step 4100/4407 - Loss: 2.1071 | PPL: 8.22 | LR: 2.384626e-04\n",
      "  Step 4100/4407 - Loss: 2.1071 | PPL: 8.22 | LR: 2.384626e-04\n",
      "  Step 4200/4407 - Loss: 2.1081 | PPL: 8.23 | LR: 2.383666e-04\n",
      "  Step 4200/4407 - Loss: 2.1081 | PPL: 8.23 | LR: 2.383666e-04\n",
      "  Step 4300/4407 - Loss: 2.1079 | PPL: 8.23 | LR: 2.382706e-04\n",
      "  Step 4300/4407 - Loss: 2.1079 | PPL: 8.23 | LR: 2.382706e-04\n",
      "  Step 4400/4407 - Loss: 2.1083 | PPL: 8.23 | LR: 2.381745e-04\n",
      "  Step 4400/4407 - Loss: 2.1083 | PPL: 8.23 | LR: 2.381745e-04\n",
      "Epoch 37/100 | Train Loss: 2.1082 | Train PPL: 8.23 | Val Loss: 2.6169 | Val PPL: 13.69\n",
      "Epoch 37/100 | Train Loss: 2.1082 | Train PPL: 8.23 | Val Loss: 2.6169 | Val PPL: 13.69\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.23\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.23\n",
      "  Step 100/4407 - Loss: 2.0555 | PPL: 7.81 | LR: 2.380716e-04\n",
      "  Step 100/4407 - Loss: 2.0555 | PPL: 7.81 | LR: 2.380716e-04\n",
      "  Step 200/4407 - Loss: 2.0390 | PPL: 7.68 | LR: 2.379754e-04\n",
      "  Step 200/4407 - Loss: 2.0390 | PPL: 7.68 | LR: 2.379754e-04\n",
      "  Step 300/4407 - Loss: 2.0528 | PPL: 7.79 | LR: 2.378792e-04\n",
      "  Step 300/4407 - Loss: 2.0528 | PPL: 7.79 | LR: 2.378792e-04\n",
      "  Step 400/4407 - Loss: 2.0559 | PPL: 7.81 | LR: 2.377829e-04\n",
      "  Step 400/4407 - Loss: 2.0559 | PPL: 7.81 | LR: 2.377829e-04\n",
      "  Step 500/4407 - Loss: 2.0519 | PPL: 7.78 | LR: 2.376865e-04\n",
      "  Step 500/4407 - Loss: 2.0519 | PPL: 7.78 | LR: 2.376865e-04\n",
      "  Step 600/4407 - Loss: 2.0553 | PPL: 7.81 | LR: 2.375901e-04\n",
      "  Step 600/4407 - Loss: 2.0553 | PPL: 7.81 | LR: 2.375901e-04\n",
      "  Step 700/4407 - Loss: 2.0598 | PPL: 7.84 | LR: 2.374936e-04\n",
      "  Step 700/4407 - Loss: 2.0598 | PPL: 7.84 | LR: 2.374936e-04\n",
      "  Step 800/4407 - Loss: 2.0615 | PPL: 7.86 | LR: 2.373971e-04\n",
      "  Step 800/4407 - Loss: 2.0615 | PPL: 7.86 | LR: 2.373971e-04\n",
      "  Step 900/4407 - Loss: 2.0622 | PPL: 7.86 | LR: 2.373005e-04\n",
      "  Step 900/4407 - Loss: 2.0622 | PPL: 7.86 | LR: 2.373005e-04\n",
      "  Step 1000/4407 - Loss: 2.0621 | PPL: 7.86 | LR: 2.372038e-04\n",
      "  Step 1000/4407 - Loss: 2.0621 | PPL: 7.86 | LR: 2.372038e-04\n",
      "  Step 1100/4407 - Loss: 2.0664 | PPL: 7.90 | LR: 2.371071e-04\n",
      "  Step 1100/4407 - Loss: 2.0664 | PPL: 7.90 | LR: 2.371071e-04\n",
      "  Step 1200/4407 - Loss: 2.0632 | PPL: 7.87 | LR: 2.370104e-04\n",
      "  Step 1200/4407 - Loss: 2.0632 | PPL: 7.87 | LR: 2.370104e-04\n",
      "  Step 1300/4407 - Loss: 2.0655 | PPL: 7.89 | LR: 2.369136e-04\n",
      "  Step 1300/4407 - Loss: 2.0655 | PPL: 7.89 | LR: 2.369136e-04\n",
      "  Step 1400/4407 - Loss: 2.0689 | PPL: 7.92 | LR: 2.368167e-04\n",
      "  Step 1400/4407 - Loss: 2.0689 | PPL: 7.92 | LR: 2.368167e-04\n",
      "  Step 1500/4407 - Loss: 2.0698 | PPL: 7.92 | LR: 2.367198e-04\n",
      "  Step 1500/4407 - Loss: 2.0698 | PPL: 7.92 | LR: 2.367198e-04\n",
      "  Step 1600/4407 - Loss: 2.0712 | PPL: 7.93 | LR: 2.366228e-04\n",
      "  Step 1600/4407 - Loss: 2.0712 | PPL: 7.93 | LR: 2.366228e-04\n",
      "  Step 1700/4407 - Loss: 2.0742 | PPL: 7.96 | LR: 2.365258e-04\n",
      "  Step 1700/4407 - Loss: 2.0742 | PPL: 7.96 | LR: 2.365258e-04\n",
      "  Step 1800/4407 - Loss: 2.0754 | PPL: 7.97 | LR: 2.364287e-04\n",
      "  Step 1800/4407 - Loss: 2.0754 | PPL: 7.97 | LR: 2.364287e-04\n",
      "  Step 1900/4407 - Loss: 2.0795 | PPL: 8.00 | LR: 2.363316e-04\n",
      "  Step 1900/4407 - Loss: 2.0795 | PPL: 8.00 | LR: 2.363316e-04\n",
      "  Step 2000/4407 - Loss: 2.0828 | PPL: 8.03 | LR: 2.362344e-04\n",
      "  Step 2000/4407 - Loss: 2.0828 | PPL: 8.03 | LR: 2.362344e-04\n",
      "  Step 2100/4407 - Loss: 2.0838 | PPL: 8.03 | LR: 2.361372e-04\n",
      "  Step 2100/4407 - Loss: 2.0838 | PPL: 8.03 | LR: 2.361372e-04\n",
      "  Step 2200/4407 - Loss: 2.0838 | PPL: 8.03 | LR: 2.360399e-04\n",
      "  Step 2200/4407 - Loss: 2.0838 | PPL: 8.03 | LR: 2.360399e-04\n",
      "  Step 2300/4407 - Loss: 2.0870 | PPL: 8.06 | LR: 2.359425e-04\n",
      "  Step 2300/4407 - Loss: 2.0870 | PPL: 8.06 | LR: 2.359425e-04\n",
      "  Step 2400/4407 - Loss: 2.0877 | PPL: 8.07 | LR: 2.358451e-04\n",
      "  Step 2400/4407 - Loss: 2.0877 | PPL: 8.07 | LR: 2.358451e-04\n",
      "  Step 2500/4407 - Loss: 2.0875 | PPL: 8.06 | LR: 2.357477e-04\n",
      "  Step 2500/4407 - Loss: 2.0875 | PPL: 8.06 | LR: 2.357477e-04\n",
      "  Step 2600/4407 - Loss: 2.0884 | PPL: 8.07 | LR: 2.356502e-04\n",
      "  Step 2600/4407 - Loss: 2.0884 | PPL: 8.07 | LR: 2.356502e-04\n",
      "  Step 2700/4407 - Loss: 2.0895 | PPL: 8.08 | LR: 2.355526e-04\n",
      "  Step 2700/4407 - Loss: 2.0895 | PPL: 8.08 | LR: 2.355526e-04\n",
      "  Step 2800/4407 - Loss: 2.0897 | PPL: 8.08 | LR: 2.354550e-04\n",
      "  Step 2800/4407 - Loss: 2.0897 | PPL: 8.08 | LR: 2.354550e-04\n",
      "  Step 2900/4407 - Loss: 2.0921 | PPL: 8.10 | LR: 2.353573e-04\n",
      "  Step 2900/4407 - Loss: 2.0921 | PPL: 8.10 | LR: 2.353573e-04\n",
      "  Step 3000/4407 - Loss: 2.0940 | PPL: 8.12 | LR: 2.352596e-04\n",
      "  Step 3000/4407 - Loss: 2.0940 | PPL: 8.12 | LR: 2.352596e-04\n",
      "  Step 3100/4407 - Loss: 2.0944 | PPL: 8.12 | LR: 2.351618e-04\n",
      "  Step 3100/4407 - Loss: 2.0944 | PPL: 8.12 | LR: 2.351618e-04\n",
      "  Step 3200/4407 - Loss: 2.0952 | PPL: 8.13 | LR: 2.350640e-04\n",
      "  Step 3200/4407 - Loss: 2.0952 | PPL: 8.13 | LR: 2.350640e-04\n",
      "  Step 3300/4407 - Loss: 2.0949 | PPL: 8.13 | LR: 2.349661e-04\n",
      "  Step 3300/4407 - Loss: 2.0949 | PPL: 8.13 | LR: 2.349661e-04\n",
      "  Step 3400/4407 - Loss: 2.0976 | PPL: 8.15 | LR: 2.348681e-04\n",
      "  Step 3400/4407 - Loss: 2.0976 | PPL: 8.15 | LR: 2.348681e-04\n",
      "  Step 3500/4407 - Loss: 2.0979 | PPL: 8.15 | LR: 2.347701e-04\n",
      "  Step 3500/4407 - Loss: 2.0979 | PPL: 8.15 | LR: 2.347701e-04\n",
      "  Step 3600/4407 - Loss: 2.0994 | PPL: 8.16 | LR: 2.346721e-04\n",
      "  Step 3600/4407 - Loss: 2.0994 | PPL: 8.16 | LR: 2.346721e-04\n",
      "  Step 3700/4407 - Loss: 2.1005 | PPL: 8.17 | LR: 2.345740e-04\n",
      "  Step 3700/4407 - Loss: 2.1005 | PPL: 8.17 | LR: 2.345740e-04\n",
      "  Step 3800/4407 - Loss: 2.0995 | PPL: 8.16 | LR: 2.344758e-04\n",
      "  Step 3800/4407 - Loss: 2.0995 | PPL: 8.16 | LR: 2.344758e-04\n",
      "  Step 3900/4407 - Loss: 2.0996 | PPL: 8.16 | LR: 2.343776e-04\n",
      "  Step 3900/4407 - Loss: 2.0996 | PPL: 8.16 | LR: 2.343776e-04\n",
      "  Step 4000/4407 - Loss: 2.1002 | PPL: 8.17 | LR: 2.342794e-04\n",
      "  Step 4000/4407 - Loss: 2.1002 | PPL: 8.17 | LR: 2.342794e-04\n",
      "  Step 4100/4407 - Loss: 2.1000 | PPL: 8.17 | LR: 2.341811e-04\n",
      "  Step 4100/4407 - Loss: 2.1000 | PPL: 8.17 | LR: 2.341811e-04\n",
      "  Step 4200/4407 - Loss: 2.1013 | PPL: 8.18 | LR: 2.340827e-04\n",
      "  Step 4200/4407 - Loss: 2.1013 | PPL: 8.18 | LR: 2.340827e-04\n",
      "  Step 4300/4407 - Loss: 2.1022 | PPL: 8.18 | LR: 2.339843e-04\n",
      "  Step 4300/4407 - Loss: 2.1022 | PPL: 8.18 | LR: 2.339843e-04\n",
      "  Step 4400/4407 - Loss: 2.1028 | PPL: 8.19 | LR: 2.338858e-04\n",
      "  Step 4400/4407 - Loss: 2.1028 | PPL: 8.19 | LR: 2.338858e-04\n",
      "Epoch 38/100 | Train Loss: 2.1028 | Train PPL: 8.19 | Val Loss: 2.6162 | Val PPL: 13.68\n",
      "Epoch 38/100 | Train Loss: 2.1028 | Train PPL: 8.19 | Val Loss: 2.6162 | Val PPL: 13.68\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.19\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.19\n",
      "  Step 100/4407 - Loss: 2.0875 | PPL: 8.06 | LR: 2.337804e-04\n",
      "  Step 100/4407 - Loss: 2.0875 | PPL: 8.06 | LR: 2.337804e-04\n",
      "  Step 200/4407 - Loss: 2.0608 | PPL: 7.85 | LR: 2.336818e-04\n",
      "  Step 200/4407 - Loss: 2.0608 | PPL: 7.85 | LR: 2.336818e-04\n",
      "  Step 300/4407 - Loss: 2.0812 | PPL: 8.01 | LR: 2.335832e-04\n",
      "  Step 300/4407 - Loss: 2.0812 | PPL: 8.01 | LR: 2.335832e-04\n",
      "  Step 400/4407 - Loss: 2.0736 | PPL: 7.95 | LR: 2.334845e-04\n",
      "  Step 400/4407 - Loss: 2.0736 | PPL: 7.95 | LR: 2.334845e-04\n",
      "  Step 500/4407 - Loss: 2.0692 | PPL: 7.92 | LR: 2.333858e-04\n",
      "  Step 500/4407 - Loss: 2.0692 | PPL: 7.92 | LR: 2.333858e-04\n",
      "  Step 600/4407 - Loss: 2.0710 | PPL: 7.93 | LR: 2.332870e-04\n",
      "  Step 600/4407 - Loss: 2.0710 | PPL: 7.93 | LR: 2.332870e-04\n",
      "  Step 700/4407 - Loss: 2.0688 | PPL: 7.92 | LR: 2.331882e-04\n",
      "  Step 700/4407 - Loss: 2.0688 | PPL: 7.92 | LR: 2.331882e-04\n",
      "  Step 800/4407 - Loss: 2.0657 | PPL: 7.89 | LR: 2.330893e-04\n",
      "  Step 800/4407 - Loss: 2.0657 | PPL: 7.89 | LR: 2.330893e-04\n",
      "  Step 900/4407 - Loss: 2.0672 | PPL: 7.90 | LR: 2.329903e-04\n",
      "  Step 900/4407 - Loss: 2.0672 | PPL: 7.90 | LR: 2.329903e-04\n",
      "  Step 1000/4407 - Loss: 2.0670 | PPL: 7.90 | LR: 2.328913e-04\n",
      "  Step 1000/4407 - Loss: 2.0670 | PPL: 7.90 | LR: 2.328913e-04\n",
      "  Step 1100/4407 - Loss: 2.0662 | PPL: 7.89 | LR: 2.327923e-04\n",
      "  Step 1100/4407 - Loss: 2.0662 | PPL: 7.89 | LR: 2.327923e-04\n",
      "  Step 1200/4407 - Loss: 2.0684 | PPL: 7.91 | LR: 2.326932e-04\n",
      "  Step 1200/4407 - Loss: 2.0684 | PPL: 7.91 | LR: 2.326932e-04\n",
      "  Step 1300/4407 - Loss: 2.0698 | PPL: 7.92 | LR: 2.325940e-04\n",
      "  Step 1300/4407 - Loss: 2.0698 | PPL: 7.92 | LR: 2.325940e-04\n",
      "  Step 1400/4407 - Loss: 2.0714 | PPL: 7.94 | LR: 2.324948e-04\n",
      "  Step 1400/4407 - Loss: 2.0714 | PPL: 7.94 | LR: 2.324948e-04\n",
      "  Step 1500/4407 - Loss: 2.0729 | PPL: 7.95 | LR: 2.323956e-04\n",
      "  Step 1500/4407 - Loss: 2.0729 | PPL: 7.95 | LR: 2.323956e-04\n",
      "  Step 1600/4407 - Loss: 2.0730 | PPL: 7.95 | LR: 2.322963e-04\n",
      "  Step 1600/4407 - Loss: 2.0730 | PPL: 7.95 | LR: 2.322963e-04\n",
      "  Step 1700/4407 - Loss: 2.0781 | PPL: 7.99 | LR: 2.321969e-04\n",
      "  Step 1700/4407 - Loss: 2.0781 | PPL: 7.99 | LR: 2.321969e-04\n",
      "  Step 1800/4407 - Loss: 2.0800 | PPL: 8.00 | LR: 2.320975e-04\n",
      "  Step 1800/4407 - Loss: 2.0800 | PPL: 8.00 | LR: 2.320975e-04\n",
      "  Step 1900/4407 - Loss: 2.0815 | PPL: 8.02 | LR: 2.319980e-04\n",
      "  Step 1900/4407 - Loss: 2.0815 | PPL: 8.02 | LR: 2.319980e-04\n",
      "  Step 2000/4407 - Loss: 2.0834 | PPL: 8.03 | LR: 2.318985e-04\n",
      "  Step 2000/4407 - Loss: 2.0834 | PPL: 8.03 | LR: 2.318985e-04\n",
      "  Step 2100/4407 - Loss: 2.0849 | PPL: 8.04 | LR: 2.317990e-04\n",
      "  Step 2100/4407 - Loss: 2.0849 | PPL: 8.04 | LR: 2.317990e-04\n",
      "  Step 2200/4407 - Loss: 2.0853 | PPL: 8.05 | LR: 2.316993e-04\n",
      "  Step 2200/4407 - Loss: 2.0853 | PPL: 8.05 | LR: 2.316993e-04\n",
      "  Step 2300/4407 - Loss: 2.0867 | PPL: 8.06 | LR: 2.315997e-04\n",
      "  Step 2300/4407 - Loss: 2.0867 | PPL: 8.06 | LR: 2.315997e-04\n",
      "  Step 2400/4407 - Loss: 2.0865 | PPL: 8.06 | LR: 2.315000e-04\n",
      "  Step 2400/4407 - Loss: 2.0865 | PPL: 8.06 | LR: 2.315000e-04\n",
      "  Step 2500/4407 - Loss: 2.0849 | PPL: 8.04 | LR: 2.314002e-04\n",
      "  Step 2500/4407 - Loss: 2.0849 | PPL: 8.04 | LR: 2.314002e-04\n",
      "  Step 2600/4407 - Loss: 2.0860 | PPL: 8.05 | LR: 2.313004e-04\n",
      "  Step 2600/4407 - Loss: 2.0860 | PPL: 8.05 | LR: 2.313004e-04\n",
      "  Step 2700/4407 - Loss: 2.0866 | PPL: 8.06 | LR: 2.312005e-04\n",
      "  Step 2700/4407 - Loss: 2.0866 | PPL: 8.06 | LR: 2.312005e-04\n",
      "  Step 2800/4407 - Loss: 2.0863 | PPL: 8.06 | LR: 2.311006e-04\n",
      "  Step 2800/4407 - Loss: 2.0863 | PPL: 8.06 | LR: 2.311006e-04\n",
      "  Step 2900/4407 - Loss: 2.0870 | PPL: 8.06 | LR: 2.310006e-04\n",
      "  Step 2900/4407 - Loss: 2.0870 | PPL: 8.06 | LR: 2.310006e-04\n",
      "  Step 3000/4407 - Loss: 2.0887 | PPL: 8.07 | LR: 2.309006e-04\n",
      "  Step 3000/4407 - Loss: 2.0887 | PPL: 8.07 | LR: 2.309006e-04\n",
      "  Step 3100/4407 - Loss: 2.0898 | PPL: 8.08 | LR: 2.308005e-04\n",
      "  Step 3100/4407 - Loss: 2.0898 | PPL: 8.08 | LR: 2.308005e-04\n",
      "  Step 3200/4407 - Loss: 2.0904 | PPL: 8.09 | LR: 2.307004e-04\n",
      "  Step 3200/4407 - Loss: 2.0904 | PPL: 8.09 | LR: 2.307004e-04\n",
      "  Step 3300/4407 - Loss: 2.0914 | PPL: 8.10 | LR: 2.306002e-04\n",
      "  Step 3300/4407 - Loss: 2.0914 | PPL: 8.10 | LR: 2.306002e-04\n",
      "  Step 3400/4407 - Loss: 2.0921 | PPL: 8.10 | LR: 2.305000e-04\n",
      "  Step 3400/4407 - Loss: 2.0921 | PPL: 8.10 | LR: 2.305000e-04\n",
      "  Step 3500/4407 - Loss: 2.0942 | PPL: 8.12 | LR: 2.303997e-04\n",
      "  Step 3500/4407 - Loss: 2.0942 | PPL: 8.12 | LR: 2.303997e-04\n",
      "  Step 3600/4407 - Loss: 2.0941 | PPL: 8.12 | LR: 2.302994e-04\n",
      "  Step 3600/4407 - Loss: 2.0941 | PPL: 8.12 | LR: 2.302994e-04\n",
      "  Step 3700/4407 - Loss: 2.0943 | PPL: 8.12 | LR: 2.301990e-04\n",
      "  Step 3700/4407 - Loss: 2.0943 | PPL: 8.12 | LR: 2.301990e-04\n",
      "  Step 3800/4407 - Loss: 2.0947 | PPL: 8.12 | LR: 2.300986e-04\n",
      "  Step 3800/4407 - Loss: 2.0947 | PPL: 8.12 | LR: 2.300986e-04\n",
      "  Step 3900/4407 - Loss: 2.0948 | PPL: 8.12 | LR: 2.299981e-04\n",
      "  Step 3900/4407 - Loss: 2.0948 | PPL: 8.12 | LR: 2.299981e-04\n",
      "  Step 4000/4407 - Loss: 2.0951 | PPL: 8.13 | LR: 2.298976e-04\n",
      "  Step 4000/4407 - Loss: 2.0951 | PPL: 8.13 | LR: 2.298976e-04\n",
      "  Step 4100/4407 - Loss: 2.0955 | PPL: 8.13 | LR: 2.297970e-04\n",
      "  Step 4100/4407 - Loss: 2.0955 | PPL: 8.13 | LR: 2.297970e-04\n",
      "  Step 4200/4407 - Loss: 2.0965 | PPL: 8.14 | LR: 2.296964e-04\n",
      "  Step 4200/4407 - Loss: 2.0965 | PPL: 8.14 | LR: 2.296964e-04\n",
      "  Step 4300/4407 - Loss: 2.0977 | PPL: 8.15 | LR: 2.295957e-04\n",
      "  Step 4300/4407 - Loss: 2.0977 | PPL: 8.15 | LR: 2.295957e-04\n",
      "  Step 4400/4407 - Loss: 2.0980 | PPL: 8.15 | LR: 2.294949e-04\n",
      "  Step 4400/4407 - Loss: 2.0980 | PPL: 8.15 | LR: 2.294949e-04\n",
      "Epoch 39/100 | Train Loss: 2.0979 | Train PPL: 8.15 | Val Loss: 2.6232 | Val PPL: 13.78\n",
      "Epoch 39/100 | Train Loss: 2.0979 | Train PPL: 8.15 | Val Loss: 2.6232 | Val PPL: 13.78\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.15\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.15\n",
      "  Step 100/4407 - Loss: 2.0840 | PPL: 8.04 | LR: 2.293871e-04\n",
      "  Step 100/4407 - Loss: 2.0840 | PPL: 8.04 | LR: 2.293871e-04\n",
      "  Step 200/4407 - Loss: 2.0736 | PPL: 7.95 | LR: 2.292863e-04\n",
      "  Step 200/4407 - Loss: 2.0736 | PPL: 7.95 | LR: 2.292863e-04\n",
      "  Step 300/4407 - Loss: 2.0819 | PPL: 8.02 | LR: 2.291854e-04\n",
      "  Step 300/4407 - Loss: 2.0819 | PPL: 8.02 | LR: 2.291854e-04\n",
      "  Step 400/4407 - Loss: 2.0791 | PPL: 8.00 | LR: 2.290845e-04\n",
      "  Step 400/4407 - Loss: 2.0791 | PPL: 8.00 | LR: 2.290845e-04\n",
      "  Step 500/4407 - Loss: 2.0723 | PPL: 7.94 | LR: 2.289835e-04\n",
      "  Step 500/4407 - Loss: 2.0723 | PPL: 7.94 | LR: 2.289835e-04\n",
      "  Step 600/4407 - Loss: 2.0844 | PPL: 8.04 | LR: 2.288825e-04\n",
      "  Step 600/4407 - Loss: 2.0844 | PPL: 8.04 | LR: 2.288825e-04\n",
      "  Step 700/4407 - Loss: 2.0835 | PPL: 8.03 | LR: 2.287814e-04\n",
      "  Step 700/4407 - Loss: 2.0835 | PPL: 8.03 | LR: 2.287814e-04\n",
      "  Step 800/4407 - Loss: 2.0796 | PPL: 8.00 | LR: 2.286802e-04\n",
      "  Step 800/4407 - Loss: 2.0796 | PPL: 8.00 | LR: 2.286802e-04\n",
      "  Step 900/4407 - Loss: 2.0771 | PPL: 7.98 | LR: 2.285791e-04\n",
      "  Step 900/4407 - Loss: 2.0771 | PPL: 7.98 | LR: 2.285791e-04\n",
      "  Step 1000/4407 - Loss: 2.0770 | PPL: 7.98 | LR: 2.284778e-04\n",
      "  Step 1000/4407 - Loss: 2.0770 | PPL: 7.98 | LR: 2.284778e-04\n",
      "  Step 1100/4407 - Loss: 2.0766 | PPL: 7.98 | LR: 2.283766e-04\n",
      "  Step 1100/4407 - Loss: 2.0766 | PPL: 7.98 | LR: 2.283766e-04\n",
      "  Step 1200/4407 - Loss: 2.0739 | PPL: 7.96 | LR: 2.282752e-04\n",
      "  Step 1200/4407 - Loss: 2.0739 | PPL: 7.96 | LR: 2.282752e-04\n",
      "  Step 1300/4407 - Loss: 2.0731 | PPL: 7.95 | LR: 2.281739e-04\n",
      "  Step 1300/4407 - Loss: 2.0731 | PPL: 7.95 | LR: 2.281739e-04\n",
      "  Step 1400/4407 - Loss: 2.0710 | PPL: 7.93 | LR: 2.280724e-04\n",
      "  Step 1400/4407 - Loss: 2.0710 | PPL: 7.93 | LR: 2.280724e-04\n",
      "  Step 1500/4407 - Loss: 2.0729 | PPL: 7.95 | LR: 2.279710e-04\n",
      "  Step 1500/4407 - Loss: 2.0729 | PPL: 7.95 | LR: 2.279710e-04\n",
      "  Step 1600/4407 - Loss: 2.0722 | PPL: 7.94 | LR: 2.278694e-04\n",
      "  Step 1600/4407 - Loss: 2.0722 | PPL: 7.94 | LR: 2.278694e-04\n",
      "  Step 1700/4407 - Loss: 2.0730 | PPL: 7.95 | LR: 2.277679e-04\n",
      "  Step 1700/4407 - Loss: 2.0730 | PPL: 7.95 | LR: 2.277679e-04\n",
      "  Step 1800/4407 - Loss: 2.0753 | PPL: 7.97 | LR: 2.276662e-04\n",
      "  Step 1800/4407 - Loss: 2.0753 | PPL: 7.97 | LR: 2.276662e-04\n",
      "  Step 1900/4407 - Loss: 2.0770 | PPL: 7.98 | LR: 2.275646e-04\n",
      "  Step 1900/4407 - Loss: 2.0770 | PPL: 7.98 | LR: 2.275646e-04\n",
      "  Step 2000/4407 - Loss: 2.0779 | PPL: 7.99 | LR: 2.274629e-04\n",
      "  Step 2000/4407 - Loss: 2.0779 | PPL: 7.99 | LR: 2.274629e-04\n",
      "  Step 2100/4407 - Loss: 2.0790 | PPL: 8.00 | LR: 2.273611e-04\n",
      "  Step 2100/4407 - Loss: 2.0790 | PPL: 8.00 | LR: 2.273611e-04\n",
      "  Step 2200/4407 - Loss: 2.0809 | PPL: 8.01 | LR: 2.272593e-04\n",
      "  Step 2200/4407 - Loss: 2.0809 | PPL: 8.01 | LR: 2.272593e-04\n",
      "  Step 2300/4407 - Loss: 2.0812 | PPL: 8.01 | LR: 2.271574e-04\n",
      "  Step 2300/4407 - Loss: 2.0812 | PPL: 8.01 | LR: 2.271574e-04\n",
      "  Step 2400/4407 - Loss: 2.0821 | PPL: 8.02 | LR: 2.270555e-04\n",
      "  Step 2400/4407 - Loss: 2.0821 | PPL: 8.02 | LR: 2.270555e-04\n",
      "  Step 2500/4407 - Loss: 2.0838 | PPL: 8.03 | LR: 2.269535e-04\n",
      "  Step 2500/4407 - Loss: 2.0838 | PPL: 8.03 | LR: 2.269535e-04\n",
      "  Step 2600/4407 - Loss: 2.0833 | PPL: 8.03 | LR: 2.268515e-04\n",
      "  Step 2600/4407 - Loss: 2.0833 | PPL: 8.03 | LR: 2.268515e-04\n",
      "  Step 2700/4407 - Loss: 2.0844 | PPL: 8.04 | LR: 2.267495e-04\n",
      "  Step 2700/4407 - Loss: 2.0844 | PPL: 8.04 | LR: 2.267495e-04\n",
      "  Step 2800/4407 - Loss: 2.0850 | PPL: 8.04 | LR: 2.266474e-04\n",
      "  Step 2800/4407 - Loss: 2.0850 | PPL: 8.04 | LR: 2.266474e-04\n",
      "  Step 2900/4407 - Loss: 2.0839 | PPL: 8.04 | LR: 2.265452e-04\n",
      "  Step 2900/4407 - Loss: 2.0839 | PPL: 8.04 | LR: 2.265452e-04\n",
      "  Step 3000/4407 - Loss: 2.0843 | PPL: 8.04 | LR: 2.264430e-04\n",
      "  Step 3000/4407 - Loss: 2.0843 | PPL: 8.04 | LR: 2.264430e-04\n",
      "  Step 3100/4407 - Loss: 2.0854 | PPL: 8.05 | LR: 2.263408e-04\n",
      "  Step 3100/4407 - Loss: 2.0854 | PPL: 8.05 | LR: 2.263408e-04\n",
      "  Step 3200/4407 - Loss: 2.0850 | PPL: 8.04 | LR: 2.262385e-04\n",
      "  Step 3200/4407 - Loss: 2.0850 | PPL: 8.04 | LR: 2.262385e-04\n",
      "  Step 3300/4407 - Loss: 2.0861 | PPL: 8.05 | LR: 2.261361e-04\n",
      "  Step 3300/4407 - Loss: 2.0861 | PPL: 8.05 | LR: 2.261361e-04\n",
      "  Step 3400/4407 - Loss: 2.0867 | PPL: 8.06 | LR: 2.260337e-04\n",
      "  Step 3400/4407 - Loss: 2.0867 | PPL: 8.06 | LR: 2.260337e-04\n",
      "  Step 3500/4407 - Loss: 2.0879 | PPL: 8.07 | LR: 2.259313e-04\n",
      "  Step 3500/4407 - Loss: 2.0879 | PPL: 8.07 | LR: 2.259313e-04\n",
      "  Step 3600/4407 - Loss: 2.0896 | PPL: 8.08 | LR: 2.258288e-04\n",
      "  Step 3600/4407 - Loss: 2.0896 | PPL: 8.08 | LR: 2.258288e-04\n",
      "  Step 3700/4407 - Loss: 2.0896 | PPL: 8.08 | LR: 2.257263e-04\n",
      "  Step 3700/4407 - Loss: 2.0896 | PPL: 8.08 | LR: 2.257263e-04\n",
      "  Step 3800/4407 - Loss: 2.0899 | PPL: 8.08 | LR: 2.256237e-04\n",
      "  Step 3800/4407 - Loss: 2.0899 | PPL: 8.08 | LR: 2.256237e-04\n",
      "  Step 3900/4407 - Loss: 2.0903 | PPL: 8.09 | LR: 2.255211e-04\n",
      "  Step 3900/4407 - Loss: 2.0903 | PPL: 8.09 | LR: 2.255211e-04\n",
      "  Step 4000/4407 - Loss: 2.0912 | PPL: 8.09 | LR: 2.254184e-04\n",
      "  Step 4000/4407 - Loss: 2.0912 | PPL: 8.09 | LR: 2.254184e-04\n",
      "  Step 4100/4407 - Loss: 2.0922 | PPL: 8.10 | LR: 2.253157e-04\n",
      "  Step 4100/4407 - Loss: 2.0922 | PPL: 8.10 | LR: 2.253157e-04\n",
      "  Step 4200/4407 - Loss: 2.0917 | PPL: 8.10 | LR: 2.252129e-04\n",
      "  Step 4200/4407 - Loss: 2.0917 | PPL: 8.10 | LR: 2.252129e-04\n",
      "  Step 4300/4407 - Loss: 2.0931 | PPL: 8.11 | LR: 2.251101e-04\n",
      "  Step 4300/4407 - Loss: 2.0931 | PPL: 8.11 | LR: 2.251101e-04\n",
      "  Step 4400/4407 - Loss: 2.0929 | PPL: 8.11 | LR: 2.250072e-04\n",
      "  Step 4400/4407 - Loss: 2.0929 | PPL: 8.11 | LR: 2.250072e-04\n",
      "Epoch 40/100 | Train Loss: 2.0927 | Train PPL: 8.11 | Val Loss: 2.6143 | Val PPL: 13.66\n",
      "Epoch 40/100 | Train Loss: 2.0927 | Train PPL: 8.11 | Val Loss: 2.6143 | Val PPL: 13.66\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.11\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.11\n",
      "  Step 100/4407 - Loss: 2.0580 | PPL: 7.83 | LR: 2.248971e-04\n",
      "  Step 100/4407 - Loss: 2.0580 | PPL: 7.83 | LR: 2.248971e-04\n",
      "  Step 200/4407 - Loss: 2.0467 | PPL: 7.74 | LR: 2.247941e-04\n",
      "  Step 200/4407 - Loss: 2.0467 | PPL: 7.74 | LR: 2.247941e-04\n",
      "  Step 300/4407 - Loss: 2.0368 | PPL: 7.67 | LR: 2.246911e-04\n",
      "  Step 300/4407 - Loss: 2.0368 | PPL: 7.67 | LR: 2.246911e-04\n",
      "  Step 400/4407 - Loss: 2.0459 | PPL: 7.74 | LR: 2.245881e-04\n",
      "  Step 400/4407 - Loss: 2.0459 | PPL: 7.74 | LR: 2.245881e-04\n",
      "  Step 500/4407 - Loss: 2.0365 | PPL: 7.66 | LR: 2.244849e-04\n",
      "  Step 500/4407 - Loss: 2.0365 | PPL: 7.66 | LR: 2.244849e-04\n",
      "  Step 600/4407 - Loss: 2.0448 | PPL: 7.73 | LR: 2.243818e-04\n",
      "  Step 600/4407 - Loss: 2.0448 | PPL: 7.73 | LR: 2.243818e-04\n",
      "  Step 700/4407 - Loss: 2.0447 | PPL: 7.73 | LR: 2.242786e-04\n",
      "  Step 700/4407 - Loss: 2.0447 | PPL: 7.73 | LR: 2.242786e-04\n",
      "  Step 800/4407 - Loss: 2.0456 | PPL: 7.73 | LR: 2.241754e-04\n",
      "  Step 800/4407 - Loss: 2.0456 | PPL: 7.73 | LR: 2.241754e-04\n",
      "  Step 900/4407 - Loss: 2.0427 | PPL: 7.71 | LR: 2.240721e-04\n",
      "  Step 900/4407 - Loss: 2.0427 | PPL: 7.71 | LR: 2.240721e-04\n",
      "  Step 1000/4407 - Loss: 2.0490 | PPL: 7.76 | LR: 2.239687e-04\n",
      "  Step 1000/4407 - Loss: 2.0490 | PPL: 7.76 | LR: 2.239687e-04\n",
      "  Step 1100/4407 - Loss: 2.0485 | PPL: 7.76 | LR: 2.238653e-04\n",
      "  Step 1100/4407 - Loss: 2.0485 | PPL: 7.76 | LR: 2.238653e-04\n",
      "  Step 1200/4407 - Loss: 2.0546 | PPL: 7.80 | LR: 2.237619e-04\n",
      "  Step 1200/4407 - Loss: 2.0546 | PPL: 7.80 | LR: 2.237619e-04\n",
      "  Step 1300/4407 - Loss: 2.0521 | PPL: 7.78 | LR: 2.236584e-04\n",
      "  Step 1300/4407 - Loss: 2.0521 | PPL: 7.78 | LR: 2.236584e-04\n",
      "  Step 1400/4407 - Loss: 2.0544 | PPL: 7.80 | LR: 2.235549e-04\n",
      "  Step 1400/4407 - Loss: 2.0544 | PPL: 7.80 | LR: 2.235549e-04\n",
      "  Step 1500/4407 - Loss: 2.0577 | PPL: 7.83 | LR: 2.234513e-04\n",
      "  Step 1500/4407 - Loss: 2.0577 | PPL: 7.83 | LR: 2.234513e-04\n",
      "  Step 1600/4407 - Loss: 2.0570 | PPL: 7.82 | LR: 2.233477e-04\n",
      "  Step 1600/4407 - Loss: 2.0570 | PPL: 7.82 | LR: 2.233477e-04\n",
      "  Step 1700/4407 - Loss: 2.0561 | PPL: 7.82 | LR: 2.232441e-04\n",
      "  Step 1700/4407 - Loss: 2.0561 | PPL: 7.82 | LR: 2.232441e-04\n",
      "  Step 1800/4407 - Loss: 2.0591 | PPL: 7.84 | LR: 2.231404e-04\n",
      "  Step 1800/4407 - Loss: 2.0591 | PPL: 7.84 | LR: 2.231404e-04\n",
      "  Step 1900/4407 - Loss: 2.0620 | PPL: 7.86 | LR: 2.230366e-04\n",
      "  Step 1900/4407 - Loss: 2.0620 | PPL: 7.86 | LR: 2.230366e-04\n",
      "  Step 2000/4407 - Loss: 2.0605 | PPL: 7.85 | LR: 2.229328e-04\n",
      "  Step 2000/4407 - Loss: 2.0605 | PPL: 7.85 | LR: 2.229328e-04\n",
      "  Step 2100/4407 - Loss: 2.0642 | PPL: 7.88 | LR: 2.228290e-04\n",
      "  Step 2100/4407 - Loss: 2.0642 | PPL: 7.88 | LR: 2.228290e-04\n",
      "  Step 2200/4407 - Loss: 2.0648 | PPL: 7.88 | LR: 2.227251e-04\n",
      "  Step 2200/4407 - Loss: 2.0648 | PPL: 7.88 | LR: 2.227251e-04\n",
      "  Step 2300/4407 - Loss: 2.0666 | PPL: 7.90 | LR: 2.226211e-04\n",
      "  Step 2300/4407 - Loss: 2.0666 | PPL: 7.90 | LR: 2.226211e-04\n",
      "  Step 2400/4407 - Loss: 2.0660 | PPL: 7.89 | LR: 2.225172e-04\n",
      "  Step 2400/4407 - Loss: 2.0660 | PPL: 7.89 | LR: 2.225172e-04\n",
      "  Step 2500/4407 - Loss: 2.0662 | PPL: 7.89 | LR: 2.224131e-04\n",
      "  Step 2500/4407 - Loss: 2.0662 | PPL: 7.89 | LR: 2.224131e-04\n",
      "  Step 2600/4407 - Loss: 2.0685 | PPL: 7.91 | LR: 2.223091e-04\n",
      "  Step 2600/4407 - Loss: 2.0685 | PPL: 7.91 | LR: 2.223091e-04\n",
      "  Step 2700/4407 - Loss: 2.0702 | PPL: 7.93 | LR: 2.222049e-04\n",
      "  Step 2700/4407 - Loss: 2.0702 | PPL: 7.93 | LR: 2.222049e-04\n",
      "  Step 2800/4407 - Loss: 2.0714 | PPL: 7.94 | LR: 2.221008e-04\n",
      "  Step 2800/4407 - Loss: 2.0714 | PPL: 7.94 | LR: 2.221008e-04\n",
      "  Step 2900/4407 - Loss: 2.0735 | PPL: 7.95 | LR: 2.219966e-04\n",
      "  Step 2900/4407 - Loss: 2.0735 | PPL: 7.95 | LR: 2.219966e-04\n",
      "  Step 3000/4407 - Loss: 2.0742 | PPL: 7.96 | LR: 2.218923e-04\n",
      "  Step 3000/4407 - Loss: 2.0742 | PPL: 7.96 | LR: 2.218923e-04\n",
      "  Step 3100/4407 - Loss: 2.0762 | PPL: 7.97 | LR: 2.217880e-04\n",
      "  Step 3100/4407 - Loss: 2.0762 | PPL: 7.97 | LR: 2.217880e-04\n",
      "  Step 3200/4407 - Loss: 2.0772 | PPL: 7.98 | LR: 2.216837e-04\n",
      "  Step 3200/4407 - Loss: 2.0772 | PPL: 7.98 | LR: 2.216837e-04\n",
      "  Step 3300/4407 - Loss: 2.0796 | PPL: 8.00 | LR: 2.215793e-04\n",
      "  Step 3300/4407 - Loss: 2.0796 | PPL: 8.00 | LR: 2.215793e-04\n",
      "  Step 3400/4407 - Loss: 2.0802 | PPL: 8.01 | LR: 2.214749e-04\n",
      "  Step 3400/4407 - Loss: 2.0802 | PPL: 8.01 | LR: 2.214749e-04\n",
      "  Step 3500/4407 - Loss: 2.0809 | PPL: 8.01 | LR: 2.213704e-04\n",
      "  Step 3500/4407 - Loss: 2.0809 | PPL: 8.01 | LR: 2.213704e-04\n",
      "  Step 3600/4407 - Loss: 2.0814 | PPL: 8.02 | LR: 2.212659e-04\n",
      "  Step 3600/4407 - Loss: 2.0814 | PPL: 8.02 | LR: 2.212659e-04\n",
      "  Step 3700/4407 - Loss: 2.0829 | PPL: 8.03 | LR: 2.211613e-04\n",
      "  Step 3700/4407 - Loss: 2.0829 | PPL: 8.03 | LR: 2.211613e-04\n",
      "  Step 3800/4407 - Loss: 2.0837 | PPL: 8.03 | LR: 2.210567e-04\n",
      "  Step 3800/4407 - Loss: 2.0837 | PPL: 8.03 | LR: 2.210567e-04\n",
      "  Step 3900/4407 - Loss: 2.0841 | PPL: 8.04 | LR: 2.209520e-04\n",
      "  Step 3900/4407 - Loss: 2.0841 | PPL: 8.04 | LR: 2.209520e-04\n",
      "  Step 4000/4407 - Loss: 2.0843 | PPL: 8.04 | LR: 2.208473e-04\n",
      "  Step 4000/4407 - Loss: 2.0843 | PPL: 8.04 | LR: 2.208473e-04\n",
      "  Step 4100/4407 - Loss: 2.0850 | PPL: 8.04 | LR: 2.207426e-04\n",
      "  Step 4100/4407 - Loss: 2.0850 | PPL: 8.04 | LR: 2.207426e-04\n",
      "  Step 4200/4407 - Loss: 2.0857 | PPL: 8.05 | LR: 2.206378e-04\n",
      "  Step 4200/4407 - Loss: 2.0857 | PPL: 8.05 | LR: 2.206378e-04\n",
      "  Step 4300/4407 - Loss: 2.0870 | PPL: 8.06 | LR: 2.205330e-04\n",
      "  Step 4300/4407 - Loss: 2.0870 | PPL: 8.06 | LR: 2.205330e-04\n",
      "  Step 4400/4407 - Loss: 2.0877 | PPL: 8.07 | LR: 2.204281e-04\n",
      "  Step 4400/4407 - Loss: 2.0877 | PPL: 8.07 | LR: 2.204281e-04\n",
      "Epoch 41/100 | Train Loss: 2.0878 | Train PPL: 8.07 | Val Loss: 2.6119 | Val PPL: 13.62\n",
      "Epoch 41/100 | Train Loss: 2.0878 | Train PPL: 8.07 | Val Loss: 2.6119 | Val PPL: 13.62\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.07\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.07\n",
      "  Step 100/4407 - Loss: 2.0134 | PPL: 7.49 | LR: 2.203158e-04\n",
      "  Step 100/4407 - Loss: 2.0134 | PPL: 7.49 | LR: 2.203158e-04\n",
      "  Step 200/4407 - Loss: 2.0444 | PPL: 7.72 | LR: 2.202108e-04\n",
      "  Step 200/4407 - Loss: 2.0444 | PPL: 7.72 | LR: 2.202108e-04\n",
      "  Step 300/4407 - Loss: 2.0563 | PPL: 7.82 | LR: 2.201058e-04\n",
      "  Step 300/4407 - Loss: 2.0563 | PPL: 7.82 | LR: 2.201058e-04\n",
      "  Step 400/4407 - Loss: 2.0498 | PPL: 7.77 | LR: 2.200008e-04\n",
      "  Step 400/4407 - Loss: 2.0498 | PPL: 7.77 | LR: 2.200008e-04\n",
      "  Step 500/4407 - Loss: 2.0516 | PPL: 7.78 | LR: 2.198957e-04\n",
      "  Step 500/4407 - Loss: 2.0516 | PPL: 7.78 | LR: 2.198957e-04\n",
      "  Step 600/4407 - Loss: 2.0529 | PPL: 7.79 | LR: 2.197905e-04\n",
      "  Step 600/4407 - Loss: 2.0529 | PPL: 7.79 | LR: 2.197905e-04\n",
      "  Step 700/4407 - Loss: 2.0569 | PPL: 7.82 | LR: 2.196853e-04\n",
      "  Step 700/4407 - Loss: 2.0569 | PPL: 7.82 | LR: 2.196853e-04\n",
      "  Step 800/4407 - Loss: 2.0581 | PPL: 7.83 | LR: 2.195801e-04\n",
      "  Step 800/4407 - Loss: 2.0581 | PPL: 7.83 | LR: 2.195801e-04\n",
      "  Step 900/4407 - Loss: 2.0587 | PPL: 7.84 | LR: 2.194748e-04\n",
      "  Step 900/4407 - Loss: 2.0587 | PPL: 7.84 | LR: 2.194748e-04\n",
      "  Step 1000/4407 - Loss: 2.0600 | PPL: 7.85 | LR: 2.193695e-04\n",
      "  Step 1000/4407 - Loss: 2.0600 | PPL: 7.85 | LR: 2.193695e-04\n",
      "  Step 1100/4407 - Loss: 2.0599 | PPL: 7.85 | LR: 2.192641e-04\n",
      "  Step 1100/4407 - Loss: 2.0599 | PPL: 7.85 | LR: 2.192641e-04\n",
      "  Step 1200/4407 - Loss: 2.0598 | PPL: 7.84 | LR: 2.191587e-04\n",
      "  Step 1200/4407 - Loss: 2.0598 | PPL: 7.84 | LR: 2.191587e-04\n",
      "  Step 1300/4407 - Loss: 2.0623 | PPL: 7.86 | LR: 2.190533e-04\n",
      "  Step 1300/4407 - Loss: 2.0623 | PPL: 7.86 | LR: 2.190533e-04\n",
      "  Step 1400/4407 - Loss: 2.0629 | PPL: 7.87 | LR: 2.189478e-04\n",
      "  Step 1400/4407 - Loss: 2.0629 | PPL: 7.87 | LR: 2.189478e-04\n",
      "  Step 1500/4407 - Loss: 2.0627 | PPL: 7.87 | LR: 2.188422e-04\n",
      "  Step 1500/4407 - Loss: 2.0627 | PPL: 7.87 | LR: 2.188422e-04\n",
      "  Step 1600/4407 - Loss: 2.0624 | PPL: 7.87 | LR: 2.187367e-04\n",
      "  Step 1600/4407 - Loss: 2.0624 | PPL: 7.87 | LR: 2.187367e-04\n",
      "  Step 1700/4407 - Loss: 2.0605 | PPL: 7.85 | LR: 2.186310e-04\n",
      "  Step 1700/4407 - Loss: 2.0605 | PPL: 7.85 | LR: 2.186310e-04\n",
      "  Step 1800/4407 - Loss: 2.0631 | PPL: 7.87 | LR: 2.185254e-04\n",
      "  Step 1800/4407 - Loss: 2.0631 | PPL: 7.87 | LR: 2.185254e-04\n",
      "  Step 1900/4407 - Loss: 2.0632 | PPL: 7.87 | LR: 2.184197e-04\n",
      "  Step 1900/4407 - Loss: 2.0632 | PPL: 7.87 | LR: 2.184197e-04\n",
      "  Step 2000/4407 - Loss: 2.0619 | PPL: 7.86 | LR: 2.183139e-04\n",
      "  Step 2000/4407 - Loss: 2.0619 | PPL: 7.86 | LR: 2.183139e-04\n",
      "  Step 2100/4407 - Loss: 2.0631 | PPL: 7.87 | LR: 2.182081e-04\n",
      "  Step 2100/4407 - Loss: 2.0631 | PPL: 7.87 | LR: 2.182081e-04\n",
      "  Step 2200/4407 - Loss: 2.0637 | PPL: 7.87 | LR: 2.181023e-04\n",
      "  Step 2200/4407 - Loss: 2.0637 | PPL: 7.87 | LR: 2.181023e-04\n",
      "  Step 2300/4407 - Loss: 2.0665 | PPL: 7.90 | LR: 2.179964e-04\n",
      "  Step 2300/4407 - Loss: 2.0665 | PPL: 7.90 | LR: 2.179964e-04\n",
      "  Step 2400/4407 - Loss: 2.0678 | PPL: 7.91 | LR: 2.178905e-04\n",
      "  Step 2400/4407 - Loss: 2.0678 | PPL: 7.91 | LR: 2.178905e-04\n",
      "  Step 2500/4407 - Loss: 2.0705 | PPL: 7.93 | LR: 2.177845e-04\n",
      "  Step 2500/4407 - Loss: 2.0705 | PPL: 7.93 | LR: 2.177845e-04\n",
      "  Step 2600/4407 - Loss: 2.0730 | PPL: 7.95 | LR: 2.176785e-04\n",
      "  Step 2600/4407 - Loss: 2.0730 | PPL: 7.95 | LR: 2.176785e-04\n",
      "  Step 2700/4407 - Loss: 2.0736 | PPL: 7.95 | LR: 2.175724e-04\n",
      "  Step 2700/4407 - Loss: 2.0736 | PPL: 7.95 | LR: 2.175724e-04\n",
      "  Step 2800/4407 - Loss: 2.0732 | PPL: 7.95 | LR: 2.174664e-04\n",
      "  Step 2800/4407 - Loss: 2.0732 | PPL: 7.95 | LR: 2.174664e-04\n",
      "  Step 2900/4407 - Loss: 2.0742 | PPL: 7.96 | LR: 2.173602e-04\n",
      "  Step 2900/4407 - Loss: 2.0742 | PPL: 7.96 | LR: 2.173602e-04\n",
      "  Step 3000/4407 - Loss: 2.0756 | PPL: 7.97 | LR: 2.172540e-04\n",
      "  Step 3000/4407 - Loss: 2.0756 | PPL: 7.97 | LR: 2.172540e-04\n",
      "  Step 3100/4407 - Loss: 2.0770 | PPL: 7.98 | LR: 2.171478e-04\n",
      "  Step 3100/4407 - Loss: 2.0770 | PPL: 7.98 | LR: 2.171478e-04\n",
      "  Step 3200/4407 - Loss: 2.0759 | PPL: 7.97 | LR: 2.170416e-04\n",
      "  Step 3200/4407 - Loss: 2.0759 | PPL: 7.97 | LR: 2.170416e-04\n",
      "  Step 3300/4407 - Loss: 2.0766 | PPL: 7.98 | LR: 2.169353e-04\n",
      "  Step 3300/4407 - Loss: 2.0766 | PPL: 7.98 | LR: 2.169353e-04\n",
      "  Step 3400/4407 - Loss: 2.0770 | PPL: 7.98 | LR: 2.168289e-04\n",
      "  Step 3400/4407 - Loss: 2.0770 | PPL: 7.98 | LR: 2.168289e-04\n",
      "  Step 3500/4407 - Loss: 2.0782 | PPL: 7.99 | LR: 2.167225e-04\n",
      "  Step 3500/4407 - Loss: 2.0782 | PPL: 7.99 | LR: 2.167225e-04\n",
      "  Step 3600/4407 - Loss: 2.0780 | PPL: 7.99 | LR: 2.166161e-04\n",
      "  Step 3600/4407 - Loss: 2.0780 | PPL: 7.99 | LR: 2.166161e-04\n",
      "  Step 3700/4407 - Loss: 2.0788 | PPL: 7.99 | LR: 2.165096e-04\n",
      "  Step 3700/4407 - Loss: 2.0788 | PPL: 7.99 | LR: 2.165096e-04\n",
      "  Step 3800/4407 - Loss: 2.0798 | PPL: 8.00 | LR: 2.164031e-04\n",
      "  Step 3800/4407 - Loss: 2.0798 | PPL: 8.00 | LR: 2.164031e-04\n",
      "  Step 3900/4407 - Loss: 2.0797 | PPL: 8.00 | LR: 2.162965e-04\n",
      "  Step 3900/4407 - Loss: 2.0797 | PPL: 8.00 | LR: 2.162965e-04\n",
      "  Step 4000/4407 - Loss: 2.0810 | PPL: 8.01 | LR: 2.161900e-04\n",
      "  Step 4000/4407 - Loss: 2.0810 | PPL: 8.01 | LR: 2.161900e-04\n",
      "  Step 4100/4407 - Loss: 2.0812 | PPL: 8.01 | LR: 2.160833e-04\n",
      "  Step 4100/4407 - Loss: 2.0812 | PPL: 8.01 | LR: 2.160833e-04\n",
      "  Step 4200/4407 - Loss: 2.0817 | PPL: 8.02 | LR: 2.159766e-04\n",
      "  Step 4200/4407 - Loss: 2.0817 | PPL: 8.02 | LR: 2.159766e-04\n",
      "  Step 4300/4407 - Loss: 2.0823 | PPL: 8.02 | LR: 2.158699e-04\n",
      "  Step 4300/4407 - Loss: 2.0823 | PPL: 8.02 | LR: 2.158699e-04\n",
      "  Step 4400/4407 - Loss: 2.0835 | PPL: 8.03 | LR: 2.157631e-04\n",
      "  Step 4400/4407 - Loss: 2.0835 | PPL: 8.03 | LR: 2.157631e-04\n",
      "Epoch 42/100 | Train Loss: 2.0831 | Train PPL: 8.03 | Val Loss: 2.6117 | Val PPL: 13.62\n",
      "Epoch 42/100 | Train Loss: 2.0831 | Train PPL: 8.03 | Val Loss: 2.6117 | Val PPL: 13.62\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.03\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 8.03\n",
      "  Step 100/4407 - Loss: 2.0427 | PPL: 7.71 | LR: 2.156489e-04\n",
      "  Step 100/4407 - Loss: 2.0427 | PPL: 7.71 | LR: 2.156489e-04\n",
      "  Step 200/4407 - Loss: 2.0548 | PPL: 7.81 | LR: 2.155420e-04\n",
      "  Step 200/4407 - Loss: 2.0548 | PPL: 7.81 | LR: 2.155420e-04\n",
      "  Step 300/4407 - Loss: 2.0534 | PPL: 7.79 | LR: 2.154351e-04\n",
      "  Step 300/4407 - Loss: 2.0534 | PPL: 7.79 | LR: 2.154351e-04\n",
      "  Step 400/4407 - Loss: 2.0613 | PPL: 7.86 | LR: 2.153282e-04\n",
      "  Step 400/4407 - Loss: 2.0613 | PPL: 7.86 | LR: 2.153282e-04\n",
      "  Step 500/4407 - Loss: 2.0686 | PPL: 7.91 | LR: 2.152212e-04\n",
      "  Step 500/4407 - Loss: 2.0686 | PPL: 7.91 | LR: 2.152212e-04\n",
      "  Step 600/4407 - Loss: 2.0679 | PPL: 7.91 | LR: 2.151142e-04\n",
      "  Step 600/4407 - Loss: 2.0679 | PPL: 7.91 | LR: 2.151142e-04\n",
      "  Step 700/4407 - Loss: 2.0686 | PPL: 7.91 | LR: 2.150072e-04\n",
      "  Step 700/4407 - Loss: 2.0686 | PPL: 7.91 | LR: 2.150072e-04\n",
      "  Step 800/4407 - Loss: 2.0651 | PPL: 7.89 | LR: 2.149001e-04\n",
      "  Step 800/4407 - Loss: 2.0651 | PPL: 7.89 | LR: 2.149001e-04\n",
      "  Step 900/4407 - Loss: 2.0634 | PPL: 7.87 | LR: 2.147929e-04\n",
      "  Step 900/4407 - Loss: 2.0634 | PPL: 7.87 | LR: 2.147929e-04\n",
      "  Step 1000/4407 - Loss: 2.0608 | PPL: 7.85 | LR: 2.146858e-04\n",
      "  Step 1000/4407 - Loss: 2.0608 | PPL: 7.85 | LR: 2.146858e-04\n",
      "  Step 1100/4407 - Loss: 2.0616 | PPL: 7.86 | LR: 2.145785e-04\n",
      "  Step 1100/4407 - Loss: 2.0616 | PPL: 7.86 | LR: 2.145785e-04\n",
      "  Step 1200/4407 - Loss: 2.0604 | PPL: 7.85 | LR: 2.144713e-04\n",
      "  Step 1200/4407 - Loss: 2.0604 | PPL: 7.85 | LR: 2.144713e-04\n",
      "  Step 1300/4407 - Loss: 2.0602 | PPL: 7.85 | LR: 2.143640e-04\n",
      "  Step 1300/4407 - Loss: 2.0602 | PPL: 7.85 | LR: 2.143640e-04\n",
      "  Step 1400/4407 - Loss: 2.0591 | PPL: 7.84 | LR: 2.142567e-04\n",
      "  Step 1400/4407 - Loss: 2.0591 | PPL: 7.84 | LR: 2.142567e-04\n",
      "  Step 1500/4407 - Loss: 2.0628 | PPL: 7.87 | LR: 2.141493e-04\n",
      "  Step 1500/4407 - Loss: 2.0628 | PPL: 7.87 | LR: 2.141493e-04\n",
      "  Step 1600/4407 - Loss: 2.0642 | PPL: 7.88 | LR: 2.140419e-04\n",
      "  Step 1600/4407 - Loss: 2.0642 | PPL: 7.88 | LR: 2.140419e-04\n",
      "  Step 1700/4407 - Loss: 2.0638 | PPL: 7.88 | LR: 2.139344e-04\n",
      "  Step 1700/4407 - Loss: 2.0638 | PPL: 7.88 | LR: 2.139344e-04\n",
      "  Step 1800/4407 - Loss: 2.0659 | PPL: 7.89 | LR: 2.138269e-04\n",
      "  Step 1800/4407 - Loss: 2.0659 | PPL: 7.89 | LR: 2.138269e-04\n",
      "  Step 1900/4407 - Loss: 2.0658 | PPL: 7.89 | LR: 2.137194e-04\n",
      "  Step 1900/4407 - Loss: 2.0658 | PPL: 7.89 | LR: 2.137194e-04\n",
      "  Step 2000/4407 - Loss: 2.0657 | PPL: 7.89 | LR: 2.136118e-04\n",
      "  Step 2000/4407 - Loss: 2.0657 | PPL: 7.89 | LR: 2.136118e-04\n",
      "  Step 2100/4407 - Loss: 2.0680 | PPL: 7.91 | LR: 2.135042e-04\n",
      "  Step 2100/4407 - Loss: 2.0680 | PPL: 7.91 | LR: 2.135042e-04\n",
      "  Step 2200/4407 - Loss: 2.0681 | PPL: 7.91 | LR: 2.133965e-04\n",
      "  Step 2200/4407 - Loss: 2.0681 | PPL: 7.91 | LR: 2.133965e-04\n",
      "  Step 2300/4407 - Loss: 2.0709 | PPL: 7.93 | LR: 2.132888e-04\n",
      "  Step 2300/4407 - Loss: 2.0709 | PPL: 7.93 | LR: 2.132888e-04\n",
      "  Step 2400/4407 - Loss: 2.0718 | PPL: 7.94 | LR: 2.131811e-04\n",
      "  Step 2400/4407 - Loss: 2.0718 | PPL: 7.94 | LR: 2.131811e-04\n",
      "  Step 2500/4407 - Loss: 2.0720 | PPL: 7.94 | LR: 2.130733e-04\n",
      "  Step 2500/4407 - Loss: 2.0720 | PPL: 7.94 | LR: 2.130733e-04\n",
      "  Step 2600/4407 - Loss: 2.0724 | PPL: 7.94 | LR: 2.129655e-04\n",
      "  Step 2600/4407 - Loss: 2.0724 | PPL: 7.94 | LR: 2.129655e-04\n",
      "  Step 2700/4407 - Loss: 2.0725 | PPL: 7.94 | LR: 2.128576e-04\n",
      "  Step 2700/4407 - Loss: 2.0725 | PPL: 7.94 | LR: 2.128576e-04\n",
      "  Step 2800/4407 - Loss: 2.0717 | PPL: 7.94 | LR: 2.127497e-04\n",
      "  Step 2800/4407 - Loss: 2.0717 | PPL: 7.94 | LR: 2.127497e-04\n",
      "  Step 2900/4407 - Loss: 2.0724 | PPL: 7.94 | LR: 2.126418e-04\n",
      "  Step 2900/4407 - Loss: 2.0724 | PPL: 7.94 | LR: 2.126418e-04\n",
      "  Step 3000/4407 - Loss: 2.0734 | PPL: 7.95 | LR: 2.125338e-04\n",
      "  Step 3000/4407 - Loss: 2.0734 | PPL: 7.95 | LR: 2.125338e-04\n",
      "  Step 3100/4407 - Loss: 2.0730 | PPL: 7.95 | LR: 2.124258e-04\n",
      "  Step 3100/4407 - Loss: 2.0730 | PPL: 7.95 | LR: 2.124258e-04\n",
      "  Step 3200/4407 - Loss: 2.0735 | PPL: 7.95 | LR: 2.123178e-04\n",
      "  Step 3200/4407 - Loss: 2.0735 | PPL: 7.95 | LR: 2.123178e-04\n",
      "  Step 3300/4407 - Loss: 2.0747 | PPL: 7.96 | LR: 2.122097e-04\n",
      "  Step 3300/4407 - Loss: 2.0747 | PPL: 7.96 | LR: 2.122097e-04\n",
      "  Step 3400/4407 - Loss: 2.0747 | PPL: 7.96 | LR: 2.121015e-04\n",
      "  Step 3400/4407 - Loss: 2.0747 | PPL: 7.96 | LR: 2.121015e-04\n",
      "  Step 3500/4407 - Loss: 2.0746 | PPL: 7.96 | LR: 2.119934e-04\n",
      "  Step 3500/4407 - Loss: 2.0746 | PPL: 7.96 | LR: 2.119934e-04\n",
      "  Step 3600/4407 - Loss: 2.0741 | PPL: 7.96 | LR: 2.118852e-04\n",
      "  Step 3600/4407 - Loss: 2.0741 | PPL: 7.96 | LR: 2.118852e-04\n",
      "  Step 3700/4407 - Loss: 2.0749 | PPL: 7.96 | LR: 2.117769e-04\n",
      "  Step 3700/4407 - Loss: 2.0749 | PPL: 7.96 | LR: 2.117769e-04\n",
      "  Step 3800/4407 - Loss: 2.0753 | PPL: 7.97 | LR: 2.116686e-04\n",
      "  Step 3800/4407 - Loss: 2.0753 | PPL: 7.97 | LR: 2.116686e-04\n",
      "  Step 3900/4407 - Loss: 2.0754 | PPL: 7.97 | LR: 2.115603e-04\n",
      "  Step 3900/4407 - Loss: 2.0754 | PPL: 7.97 | LR: 2.115603e-04\n",
      "  Step 4000/4407 - Loss: 2.0760 | PPL: 7.97 | LR: 2.114519e-04\n",
      "  Step 4000/4407 - Loss: 2.0760 | PPL: 7.97 | LR: 2.114519e-04\n",
      "  Step 4100/4407 - Loss: 2.0767 | PPL: 7.98 | LR: 2.113435e-04\n",
      "  Step 4100/4407 - Loss: 2.0767 | PPL: 7.98 | LR: 2.113435e-04\n",
      "  Step 4200/4407 - Loss: 2.0776 | PPL: 7.99 | LR: 2.112351e-04\n",
      "  Step 4200/4407 - Loss: 2.0776 | PPL: 7.99 | LR: 2.112351e-04\n",
      "  Step 4300/4407 - Loss: 2.0778 | PPL: 7.99 | LR: 2.111266e-04\n",
      "  Step 4300/4407 - Loss: 2.0778 | PPL: 7.99 | LR: 2.111266e-04\n",
      "  Step 4400/4407 - Loss: 2.0786 | PPL: 7.99 | LR: 2.110181e-04\n",
      "  Step 4400/4407 - Loss: 2.0786 | PPL: 7.99 | LR: 2.110181e-04\n",
      "Epoch 43/100 | Train Loss: 2.0782 | Train PPL: 7.99 | Val Loss: 2.6150 | Val PPL: 13.67\n",
      "Epoch 43/100 | Train Loss: 2.0782 | Train PPL: 7.99 | Val Loss: 2.6150 | Val PPL: 13.67\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.99\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.99\n",
      "  Step 100/4407 - Loss: 2.0513 | PPL: 7.78 | LR: 2.109019e-04\n",
      "  Step 100/4407 - Loss: 2.0513 | PPL: 7.78 | LR: 2.109019e-04\n",
      "  Step 200/4407 - Loss: 2.0564 | PPL: 7.82 | LR: 2.107933e-04\n",
      "  Step 200/4407 - Loss: 2.0564 | PPL: 7.82 | LR: 2.107933e-04\n",
      "  Step 300/4407 - Loss: 2.0519 | PPL: 7.78 | LR: 2.106847e-04\n",
      "  Step 300/4407 - Loss: 2.0519 | PPL: 7.78 | LR: 2.106847e-04\n",
      "  Step 400/4407 - Loss: 2.0528 | PPL: 7.79 | LR: 2.105760e-04\n",
      "  Step 400/4407 - Loss: 2.0528 | PPL: 7.79 | LR: 2.105760e-04\n",
      "  Step 500/4407 - Loss: 2.0497 | PPL: 7.77 | LR: 2.104673e-04\n",
      "  Step 500/4407 - Loss: 2.0497 | PPL: 7.77 | LR: 2.104673e-04\n",
      "  Step 600/4407 - Loss: 2.0523 | PPL: 7.79 | LR: 2.103586e-04\n",
      "  Step 600/4407 - Loss: 2.0523 | PPL: 7.79 | LR: 2.103586e-04\n",
      "  Step 700/4407 - Loss: 2.0541 | PPL: 7.80 | LR: 2.102498e-04\n",
      "  Step 700/4407 - Loss: 2.0541 | PPL: 7.80 | LR: 2.102498e-04\n",
      "  Step 800/4407 - Loss: 2.0488 | PPL: 7.76 | LR: 2.101410e-04\n",
      "  Step 800/4407 - Loss: 2.0488 | PPL: 7.76 | LR: 2.101410e-04\n",
      "  Step 900/4407 - Loss: 2.0455 | PPL: 7.73 | LR: 2.100321e-04\n",
      "  Step 900/4407 - Loss: 2.0455 | PPL: 7.73 | LR: 2.100321e-04\n",
      "  Step 1000/4407 - Loss: 2.0506 | PPL: 7.77 | LR: 2.099232e-04\n",
      "  Step 1000/4407 - Loss: 2.0506 | PPL: 7.77 | LR: 2.099232e-04\n",
      "  Step 1100/4407 - Loss: 2.0526 | PPL: 7.79 | LR: 2.098143e-04\n",
      "  Step 1100/4407 - Loss: 2.0526 | PPL: 7.79 | LR: 2.098143e-04\n",
      "  Step 1200/4407 - Loss: 2.0573 | PPL: 7.82 | LR: 2.097053e-04\n",
      "  Step 1200/4407 - Loss: 2.0573 | PPL: 7.82 | LR: 2.097053e-04\n",
      "  Step 1300/4407 - Loss: 2.0562 | PPL: 7.82 | LR: 2.095963e-04\n",
      "  Step 1300/4407 - Loss: 2.0562 | PPL: 7.82 | LR: 2.095963e-04\n",
      "  Step 1400/4407 - Loss: 2.0563 | PPL: 7.82 | LR: 2.094872e-04\n",
      "  Step 1400/4407 - Loss: 2.0563 | PPL: 7.82 | LR: 2.094872e-04\n",
      "  Step 1500/4407 - Loss: 2.0562 | PPL: 7.82 | LR: 2.093781e-04\n",
      "  Step 1500/4407 - Loss: 2.0562 | PPL: 7.82 | LR: 2.093781e-04\n",
      "  Step 1600/4407 - Loss: 2.0541 | PPL: 7.80 | LR: 2.092690e-04\n",
      "  Step 1600/4407 - Loss: 2.0541 | PPL: 7.80 | LR: 2.092690e-04\n",
      "  Step 1700/4407 - Loss: 2.0545 | PPL: 7.80 | LR: 2.091599e-04\n",
      "  Step 1700/4407 - Loss: 2.0545 | PPL: 7.80 | LR: 2.091599e-04\n",
      "  Step 1800/4407 - Loss: 2.0553 | PPL: 7.81 | LR: 2.090507e-04\n",
      "  Step 1800/4407 - Loss: 2.0553 | PPL: 7.81 | LR: 2.090507e-04\n",
      "  Step 1900/4407 - Loss: 2.0569 | PPL: 7.82 | LR: 2.089414e-04\n",
      "  Step 1900/4407 - Loss: 2.0569 | PPL: 7.82 | LR: 2.089414e-04\n",
      "  Step 2000/4407 - Loss: 2.0611 | PPL: 7.85 | LR: 2.088322e-04\n",
      "  Step 2000/4407 - Loss: 2.0611 | PPL: 7.85 | LR: 2.088322e-04\n",
      "  Step 2100/4407 - Loss: 2.0618 | PPL: 7.86 | LR: 2.087228e-04\n",
      "  Step 2100/4407 - Loss: 2.0618 | PPL: 7.86 | LR: 2.087228e-04\n",
      "  Step 2200/4407 - Loss: 2.0616 | PPL: 7.86 | LR: 2.086135e-04\n",
      "  Step 2200/4407 - Loss: 2.0616 | PPL: 7.86 | LR: 2.086135e-04\n",
      "  Step 2300/4407 - Loss: 2.0637 | PPL: 7.88 | LR: 2.085041e-04\n",
      "  Step 2300/4407 - Loss: 2.0637 | PPL: 7.88 | LR: 2.085041e-04\n",
      "  Step 2400/4407 - Loss: 2.0657 | PPL: 7.89 | LR: 2.083947e-04\n",
      "  Step 2400/4407 - Loss: 2.0657 | PPL: 7.89 | LR: 2.083947e-04\n",
      "  Step 2500/4407 - Loss: 2.0664 | PPL: 7.90 | LR: 2.082852e-04\n",
      "  Step 2500/4407 - Loss: 2.0664 | PPL: 7.90 | LR: 2.082852e-04\n",
      "  Step 2600/4407 - Loss: 2.0675 | PPL: 7.91 | LR: 2.081757e-04\n",
      "  Step 2600/4407 - Loss: 2.0675 | PPL: 7.91 | LR: 2.081757e-04\n",
      "  Step 2700/4407 - Loss: 2.0677 | PPL: 7.91 | LR: 2.080662e-04\n",
      "  Step 2700/4407 - Loss: 2.0677 | PPL: 7.91 | LR: 2.080662e-04\n",
      "  Step 2800/4407 - Loss: 2.0689 | PPL: 7.92 | LR: 2.079567e-04\n",
      "  Step 2800/4407 - Loss: 2.0689 | PPL: 7.92 | LR: 2.079567e-04\n",
      "  Step 2900/4407 - Loss: 2.0695 | PPL: 7.92 | LR: 2.078470e-04\n",
      "  Step 2900/4407 - Loss: 2.0695 | PPL: 7.92 | LR: 2.078470e-04\n",
      "  Step 3000/4407 - Loss: 2.0692 | PPL: 7.92 | LR: 2.077374e-04\n",
      "  Step 3000/4407 - Loss: 2.0692 | PPL: 7.92 | LR: 2.077374e-04\n",
      "  Step 3100/4407 - Loss: 2.0697 | PPL: 7.92 | LR: 2.076277e-04\n",
      "  Step 3100/4407 - Loss: 2.0697 | PPL: 7.92 | LR: 2.076277e-04\n",
      "  Step 3200/4407 - Loss: 2.0697 | PPL: 7.92 | LR: 2.075180e-04\n",
      "  Step 3200/4407 - Loss: 2.0697 | PPL: 7.92 | LR: 2.075180e-04\n",
      "  Step 3300/4407 - Loss: 2.0690 | PPL: 7.92 | LR: 2.074083e-04\n",
      "  Step 3300/4407 - Loss: 2.0690 | PPL: 7.92 | LR: 2.074083e-04\n",
      "  Step 3400/4407 - Loss: 2.0698 | PPL: 7.92 | LR: 2.072985e-04\n",
      "  Step 3400/4407 - Loss: 2.0698 | PPL: 7.92 | LR: 2.072985e-04\n",
      "  Step 3500/4407 - Loss: 2.0709 | PPL: 7.93 | LR: 2.071887e-04\n",
      "  Step 3500/4407 - Loss: 2.0709 | PPL: 7.93 | LR: 2.071887e-04\n",
      "  Step 3600/4407 - Loss: 2.0715 | PPL: 7.94 | LR: 2.070788e-04\n",
      "  Step 3600/4407 - Loss: 2.0715 | PPL: 7.94 | LR: 2.070788e-04\n",
      "  Step 3700/4407 - Loss: 2.0714 | PPL: 7.94 | LR: 2.069689e-04\n",
      "  Step 3700/4407 - Loss: 2.0714 | PPL: 7.94 | LR: 2.069689e-04\n",
      "  Step 3800/4407 - Loss: 2.0717 | PPL: 7.94 | LR: 2.068590e-04\n",
      "  Step 3800/4407 - Loss: 2.0717 | PPL: 7.94 | LR: 2.068590e-04\n",
      "  Step 3900/4407 - Loss: 2.0719 | PPL: 7.94 | LR: 2.067490e-04\n",
      "  Step 3900/4407 - Loss: 2.0719 | PPL: 7.94 | LR: 2.067490e-04\n",
      "  Step 4000/4407 - Loss: 2.0729 | PPL: 7.95 | LR: 2.066390e-04\n",
      "  Step 4000/4407 - Loss: 2.0729 | PPL: 7.95 | LR: 2.066390e-04\n",
      "  Step 4100/4407 - Loss: 2.0721 | PPL: 7.94 | LR: 2.065290e-04\n",
      "  Step 4100/4407 - Loss: 2.0721 | PPL: 7.94 | LR: 2.065290e-04\n",
      "  Step 4200/4407 - Loss: 2.0714 | PPL: 7.94 | LR: 2.064189e-04\n",
      "  Step 4200/4407 - Loss: 2.0714 | PPL: 7.94 | LR: 2.064189e-04\n",
      "  Step 4300/4407 - Loss: 2.0724 | PPL: 7.94 | LR: 2.063088e-04\n",
      "  Step 4300/4407 - Loss: 2.0724 | PPL: 7.94 | LR: 2.063088e-04\n",
      "  Step 4400/4407 - Loss: 2.0736 | PPL: 7.95 | LR: 2.061987e-04\n",
      "  Step 4400/4407 - Loss: 2.0736 | PPL: 7.95 | LR: 2.061987e-04\n",
      "Epoch 44/100 | Train Loss: 2.0738 | Train PPL: 7.95 | Val Loss: 2.6098 | Val PPL: 13.60\n",
      "Epoch 44/100 | Train Loss: 2.0738 | Train PPL: 7.95 | Val Loss: 2.6098 | Val PPL: 13.60\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.95\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.95\n",
      "  Step 100/4407 - Loss: 2.0299 | PPL: 7.61 | LR: 2.060808e-04\n",
      "  Step 100/4407 - Loss: 2.0299 | PPL: 7.61 | LR: 2.060808e-04\n",
      "  Step 200/4407 - Loss: 2.0213 | PPL: 7.55 | LR: 2.059706e-04\n",
      "  Step 200/4407 - Loss: 2.0213 | PPL: 7.55 | LR: 2.059706e-04\n",
      "  Step 300/4407 - Loss: 2.0336 | PPL: 7.64 | LR: 2.058604e-04\n",
      "  Step 300/4407 - Loss: 2.0336 | PPL: 7.64 | LR: 2.058604e-04\n",
      "  Step 400/4407 - Loss: 2.0293 | PPL: 7.61 | LR: 2.057501e-04\n",
      "  Step 400/4407 - Loss: 2.0293 | PPL: 7.61 | LR: 2.057501e-04\n",
      "  Step 500/4407 - Loss: 2.0383 | PPL: 7.68 | LR: 2.056398e-04\n",
      "  Step 500/4407 - Loss: 2.0383 | PPL: 7.68 | LR: 2.056398e-04\n",
      "  Step 600/4407 - Loss: 2.0376 | PPL: 7.67 | LR: 2.055294e-04\n",
      "  Step 600/4407 - Loss: 2.0376 | PPL: 7.67 | LR: 2.055294e-04\n",
      "  Step 700/4407 - Loss: 2.0408 | PPL: 7.70 | LR: 2.054190e-04\n",
      "  Step 700/4407 - Loss: 2.0408 | PPL: 7.70 | LR: 2.054190e-04\n",
      "  Step 800/4407 - Loss: 2.0421 | PPL: 7.71 | LR: 2.053086e-04\n",
      "  Step 800/4407 - Loss: 2.0421 | PPL: 7.71 | LR: 2.053086e-04\n",
      "  Step 900/4407 - Loss: 2.0465 | PPL: 7.74 | LR: 2.051981e-04\n",
      "  Step 900/4407 - Loss: 2.0465 | PPL: 7.74 | LR: 2.051981e-04\n",
      "  Step 1000/4407 - Loss: 2.0482 | PPL: 7.75 | LR: 2.050876e-04\n",
      "  Step 1000/4407 - Loss: 2.0482 | PPL: 7.75 | LR: 2.050876e-04\n",
      "  Step 1100/4407 - Loss: 2.0495 | PPL: 7.76 | LR: 2.049771e-04\n",
      "  Step 1100/4407 - Loss: 2.0495 | PPL: 7.76 | LR: 2.049771e-04\n",
      "  Step 1200/4407 - Loss: 2.0494 | PPL: 7.76 | LR: 2.048666e-04\n",
      "  Step 1200/4407 - Loss: 2.0494 | PPL: 7.76 | LR: 2.048666e-04\n",
      "  Step 1300/4407 - Loss: 2.0491 | PPL: 7.76 | LR: 2.047560e-04\n",
      "  Step 1300/4407 - Loss: 2.0491 | PPL: 7.76 | LR: 2.047560e-04\n",
      "  Step 1400/4407 - Loss: 2.0509 | PPL: 7.77 | LR: 2.046453e-04\n",
      "  Step 1400/4407 - Loss: 2.0509 | PPL: 7.77 | LR: 2.046453e-04\n",
      "  Step 1500/4407 - Loss: 2.0516 | PPL: 7.78 | LR: 2.045347e-04\n",
      "  Step 1500/4407 - Loss: 2.0516 | PPL: 7.78 | LR: 2.045347e-04\n",
      "  Step 1600/4407 - Loss: 2.0549 | PPL: 7.81 | LR: 2.044240e-04\n",
      "  Step 1600/4407 - Loss: 2.0549 | PPL: 7.81 | LR: 2.044240e-04\n",
      "  Step 1700/4407 - Loss: 2.0538 | PPL: 7.80 | LR: 2.043132e-04\n",
      "  Step 1700/4407 - Loss: 2.0538 | PPL: 7.80 | LR: 2.043132e-04\n",
      "  Step 1800/4407 - Loss: 2.0551 | PPL: 7.81 | LR: 2.042025e-04\n",
      "  Step 1800/4407 - Loss: 2.0551 | PPL: 7.81 | LR: 2.042025e-04\n",
      "  Step 1900/4407 - Loss: 2.0552 | PPL: 7.81 | LR: 2.040917e-04\n",
      "  Step 1900/4407 - Loss: 2.0552 | PPL: 7.81 | LR: 2.040917e-04\n",
      "  Step 2000/4407 - Loss: 2.0575 | PPL: 7.83 | LR: 2.039808e-04\n",
      "  Step 2000/4407 - Loss: 2.0575 | PPL: 7.83 | LR: 2.039808e-04\n",
      "  Step 2100/4407 - Loss: 2.0587 | PPL: 7.84 | LR: 2.038700e-04\n",
      "  Step 2100/4407 - Loss: 2.0587 | PPL: 7.84 | LR: 2.038700e-04\n",
      "  Step 2200/4407 - Loss: 2.0585 | PPL: 7.83 | LR: 2.037591e-04\n",
      "  Step 2200/4407 - Loss: 2.0585 | PPL: 7.83 | LR: 2.037591e-04\n",
      "  Step 2300/4407 - Loss: 2.0578 | PPL: 7.83 | LR: 2.036481e-04\n",
      "  Step 2300/4407 - Loss: 2.0578 | PPL: 7.83 | LR: 2.036481e-04\n",
      "  Step 2400/4407 - Loss: 2.0595 | PPL: 7.84 | LR: 2.035372e-04\n",
      "  Step 2400/4407 - Loss: 2.0595 | PPL: 7.84 | LR: 2.035372e-04\n",
      "  Step 2500/4407 - Loss: 2.0617 | PPL: 7.86 | LR: 2.034262e-04\n",
      "  Step 2500/4407 - Loss: 2.0617 | PPL: 7.86 | LR: 2.034262e-04\n",
      "  Step 2600/4407 - Loss: 2.0617 | PPL: 7.86 | LR: 2.033151e-04\n",
      "  Step 2600/4407 - Loss: 2.0617 | PPL: 7.86 | LR: 2.033151e-04\n",
      "  Step 2700/4407 - Loss: 2.0638 | PPL: 7.88 | LR: 2.032041e-04\n",
      "  Step 2700/4407 - Loss: 2.0638 | PPL: 7.88 | LR: 2.032041e-04\n",
      "  Step 2800/4407 - Loss: 2.0629 | PPL: 7.87 | LR: 2.030930e-04\n",
      "  Step 2800/4407 - Loss: 2.0629 | PPL: 7.87 | LR: 2.030930e-04\n",
      "  Step 2900/4407 - Loss: 2.0626 | PPL: 7.87 | LR: 2.029818e-04\n",
      "  Step 2900/4407 - Loss: 2.0626 | PPL: 7.87 | LR: 2.029818e-04\n",
      "  Step 3000/4407 - Loss: 2.0643 | PPL: 7.88 | LR: 2.028707e-04\n",
      "  Step 3000/4407 - Loss: 2.0643 | PPL: 7.88 | LR: 2.028707e-04\n",
      "  Step 3100/4407 - Loss: 2.0661 | PPL: 7.89 | LR: 2.027595e-04\n",
      "  Step 3100/4407 - Loss: 2.0661 | PPL: 7.89 | LR: 2.027595e-04\n",
      "  Step 3200/4407 - Loss: 2.0660 | PPL: 7.89 | LR: 2.026482e-04\n",
      "  Step 3200/4407 - Loss: 2.0660 | PPL: 7.89 | LR: 2.026482e-04\n",
      "  Step 3300/4407 - Loss: 2.0665 | PPL: 7.90 | LR: 2.025369e-04\n",
      "  Step 3300/4407 - Loss: 2.0665 | PPL: 7.90 | LR: 2.025369e-04\n",
      "  Step 3400/4407 - Loss: 2.0672 | PPL: 7.90 | LR: 2.024256e-04\n",
      "  Step 3400/4407 - Loss: 2.0672 | PPL: 7.90 | LR: 2.024256e-04\n",
      "  Step 3500/4407 - Loss: 2.0680 | PPL: 7.91 | LR: 2.023143e-04\n",
      "  Step 3500/4407 - Loss: 2.0680 | PPL: 7.91 | LR: 2.023143e-04\n",
      "  Step 3600/4407 - Loss: 2.0678 | PPL: 7.91 | LR: 2.022029e-04\n",
      "  Step 3600/4407 - Loss: 2.0678 | PPL: 7.91 | LR: 2.022029e-04\n",
      "  Step 3700/4407 - Loss: 2.0670 | PPL: 7.90 | LR: 2.020915e-04\n",
      "  Step 3700/4407 - Loss: 2.0670 | PPL: 7.90 | LR: 2.020915e-04\n",
      "  Step 3800/4407 - Loss: 2.0659 | PPL: 7.89 | LR: 2.019801e-04\n",
      "  Step 3800/4407 - Loss: 2.0659 | PPL: 7.89 | LR: 2.019801e-04\n",
      "  Step 3900/4407 - Loss: 2.0664 | PPL: 7.90 | LR: 2.018686e-04\n",
      "  Step 3900/4407 - Loss: 2.0664 | PPL: 7.90 | LR: 2.018686e-04\n",
      "  Step 4000/4407 - Loss: 2.0663 | PPL: 7.90 | LR: 2.017572e-04\n",
      "  Step 4000/4407 - Loss: 2.0663 | PPL: 7.90 | LR: 2.017572e-04\n",
      "  Step 4100/4407 - Loss: 2.0657 | PPL: 7.89 | LR: 2.016456e-04\n",
      "  Step 4100/4407 - Loss: 2.0657 | PPL: 7.89 | LR: 2.016456e-04\n",
      "  Step 4200/4407 - Loss: 2.0668 | PPL: 7.90 | LR: 2.015341e-04\n",
      "  Step 4200/4407 - Loss: 2.0668 | PPL: 7.90 | LR: 2.015341e-04\n",
      "  Step 4300/4407 - Loss: 2.0683 | PPL: 7.91 | LR: 2.014225e-04\n",
      "  Step 4300/4407 - Loss: 2.0683 | PPL: 7.91 | LR: 2.014225e-04\n",
      "  Step 4400/4407 - Loss: 2.0695 | PPL: 7.92 | LR: 2.013108e-04\n",
      "  Step 4400/4407 - Loss: 2.0695 | PPL: 7.92 | LR: 2.013108e-04\n",
      "Epoch 45/100 | Train Loss: 2.0692 | Train PPL: 7.92 | Val Loss: 2.6120 | Val PPL: 13.63\n",
      "Epoch 45/100 | Train Loss: 2.0692 | Train PPL: 7.92 | Val Loss: 2.6120 | Val PPL: 13.63\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.92\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.92\n",
      "  Step 100/4407 - Loss: 2.0414 | PPL: 7.70 | LR: 2.011914e-04\n",
      "  Step 100/4407 - Loss: 2.0414 | PPL: 7.70 | LR: 2.011914e-04\n",
      "  Step 200/4407 - Loss: 2.0447 | PPL: 7.73 | LR: 2.010797e-04\n",
      "  Step 200/4407 - Loss: 2.0447 | PPL: 7.73 | LR: 2.010797e-04\n",
      "  Step 300/4407 - Loss: 2.0349 | PPL: 7.65 | LR: 2.009679e-04\n",
      "  Step 300/4407 - Loss: 2.0349 | PPL: 7.65 | LR: 2.009679e-04\n",
      "  Step 400/4407 - Loss: 2.0418 | PPL: 7.70 | LR: 2.008562e-04\n",
      "  Step 400/4407 - Loss: 2.0418 | PPL: 7.70 | LR: 2.008562e-04\n",
      "  Step 500/4407 - Loss: 2.0398 | PPL: 7.69 | LR: 2.007444e-04\n",
      "  Step 500/4407 - Loss: 2.0398 | PPL: 7.69 | LR: 2.007444e-04\n",
      "  Step 600/4407 - Loss: 2.0374 | PPL: 7.67 | LR: 2.006326e-04\n",
      "  Step 600/4407 - Loss: 2.0374 | PPL: 7.67 | LR: 2.006326e-04\n",
      "  Step 700/4407 - Loss: 2.0397 | PPL: 7.69 | LR: 2.005207e-04\n",
      "  Step 700/4407 - Loss: 2.0397 | PPL: 7.69 | LR: 2.005207e-04\n",
      "  Step 800/4407 - Loss: 2.0410 | PPL: 7.70 | LR: 2.004088e-04\n",
      "  Step 800/4407 - Loss: 2.0410 | PPL: 7.70 | LR: 2.004088e-04\n",
      "  Step 900/4407 - Loss: 2.0420 | PPL: 7.71 | LR: 2.002969e-04\n",
      "  Step 900/4407 - Loss: 2.0420 | PPL: 7.71 | LR: 2.002969e-04\n",
      "  Step 1000/4407 - Loss: 2.0423 | PPL: 7.71 | LR: 2.001850e-04\n",
      "  Step 1000/4407 - Loss: 2.0423 | PPL: 7.71 | LR: 2.001850e-04\n",
      "  Step 1100/4407 - Loss: 2.0474 | PPL: 7.75 | LR: 2.000730e-04\n",
      "  Step 1100/4407 - Loss: 2.0474 | PPL: 7.75 | LR: 2.000730e-04\n",
      "  Step 1200/4407 - Loss: 2.0508 | PPL: 7.77 | LR: 1.999610e-04\n",
      "  Step 1200/4407 - Loss: 2.0508 | PPL: 7.77 | LR: 1.999610e-04\n",
      "  Step 1300/4407 - Loss: 2.0469 | PPL: 7.74 | LR: 1.998489e-04\n",
      "  Step 1300/4407 - Loss: 2.0469 | PPL: 7.74 | LR: 1.998489e-04\n",
      "  Step 1400/4407 - Loss: 2.0483 | PPL: 7.75 | LR: 1.997369e-04\n",
      "  Step 1400/4407 - Loss: 2.0483 | PPL: 7.75 | LR: 1.997369e-04\n",
      "  Step 1500/4407 - Loss: 2.0487 | PPL: 7.76 | LR: 1.996248e-04\n",
      "  Step 1500/4407 - Loss: 2.0487 | PPL: 7.76 | LR: 1.996248e-04\n",
      "  Step 1600/4407 - Loss: 2.0491 | PPL: 7.76 | LR: 1.995126e-04\n",
      "  Step 1600/4407 - Loss: 2.0491 | PPL: 7.76 | LR: 1.995126e-04\n",
      "  Step 1700/4407 - Loss: 2.0483 | PPL: 7.75 | LR: 1.994005e-04\n",
      "  Step 1700/4407 - Loss: 2.0483 | PPL: 7.75 | LR: 1.994005e-04\n",
      "  Step 1800/4407 - Loss: 2.0489 | PPL: 7.76 | LR: 1.992883e-04\n",
      "  Step 1800/4407 - Loss: 2.0489 | PPL: 7.76 | LR: 1.992883e-04\n",
      "  Step 1900/4407 - Loss: 2.0500 | PPL: 7.77 | LR: 1.991760e-04\n",
      "  Step 1900/4407 - Loss: 2.0500 | PPL: 7.77 | LR: 1.991760e-04\n",
      "  Step 2000/4407 - Loss: 2.0497 | PPL: 7.77 | LR: 1.990638e-04\n",
      "  Step 2000/4407 - Loss: 2.0497 | PPL: 7.77 | LR: 1.990638e-04\n",
      "  Step 2100/4407 - Loss: 2.0502 | PPL: 7.77 | LR: 1.989515e-04\n",
      "  Step 2100/4407 - Loss: 2.0502 | PPL: 7.77 | LR: 1.989515e-04\n",
      "  Step 2200/4407 - Loss: 2.0508 | PPL: 7.77 | LR: 1.988392e-04\n",
      "  Step 2200/4407 - Loss: 2.0508 | PPL: 7.77 | LR: 1.988392e-04\n",
      "  Step 2300/4407 - Loss: 2.0521 | PPL: 7.78 | LR: 1.987268e-04\n",
      "  Step 2300/4407 - Loss: 2.0521 | PPL: 7.78 | LR: 1.987268e-04\n",
      "  Step 2400/4407 - Loss: 2.0523 | PPL: 7.79 | LR: 1.986144e-04\n",
      "  Step 2400/4407 - Loss: 2.0523 | PPL: 7.79 | LR: 1.986144e-04\n",
      "  Step 2500/4407 - Loss: 2.0521 | PPL: 7.78 | LR: 1.985020e-04\n",
      "  Step 2500/4407 - Loss: 2.0521 | PPL: 7.78 | LR: 1.985020e-04\n",
      "  Step 2600/4407 - Loss: 2.0535 | PPL: 7.79 | LR: 1.983896e-04\n",
      "  Step 2600/4407 - Loss: 2.0535 | PPL: 7.79 | LR: 1.983896e-04\n",
      "  Step 2700/4407 - Loss: 2.0549 | PPL: 7.81 | LR: 1.982771e-04\n",
      "  Step 2700/4407 - Loss: 2.0549 | PPL: 7.81 | LR: 1.982771e-04\n",
      "  Step 2800/4407 - Loss: 2.0555 | PPL: 7.81 | LR: 1.981646e-04\n",
      "  Step 2800/4407 - Loss: 2.0555 | PPL: 7.81 | LR: 1.981646e-04\n",
      "  Step 2900/4407 - Loss: 2.0568 | PPL: 7.82 | LR: 1.980521e-04\n",
      "  Step 2900/4407 - Loss: 2.0568 | PPL: 7.82 | LR: 1.980521e-04\n",
      "  Step 3000/4407 - Loss: 2.0568 | PPL: 7.82 | LR: 1.979395e-04\n",
      "  Step 3000/4407 - Loss: 2.0568 | PPL: 7.82 | LR: 1.979395e-04\n",
      "  Step 3100/4407 - Loss: 2.0589 | PPL: 7.84 | LR: 1.978269e-04\n",
      "  Step 3100/4407 - Loss: 2.0589 | PPL: 7.84 | LR: 1.978269e-04\n",
      "  Step 3200/4407 - Loss: 2.0590 | PPL: 7.84 | LR: 1.977143e-04\n",
      "  Step 3200/4407 - Loss: 2.0590 | PPL: 7.84 | LR: 1.977143e-04\n",
      "  Step 3300/4407 - Loss: 2.0595 | PPL: 7.84 | LR: 1.976016e-04\n",
      "  Step 3300/4407 - Loss: 2.0595 | PPL: 7.84 | LR: 1.976016e-04\n",
      "  Step 3400/4407 - Loss: 2.0599 | PPL: 7.84 | LR: 1.974889e-04\n",
      "  Step 3400/4407 - Loss: 2.0599 | PPL: 7.84 | LR: 1.974889e-04\n",
      "  Step 3500/4407 - Loss: 2.0602 | PPL: 7.85 | LR: 1.973762e-04\n",
      "  Step 3500/4407 - Loss: 2.0602 | PPL: 7.85 | LR: 1.973762e-04\n",
      "  Step 3600/4407 - Loss: 2.0602 | PPL: 7.85 | LR: 1.972635e-04\n",
      "  Step 3600/4407 - Loss: 2.0602 | PPL: 7.85 | LR: 1.972635e-04\n",
      "  Step 3700/4407 - Loss: 2.0614 | PPL: 7.86 | LR: 1.971507e-04\n",
      "  Step 3700/4407 - Loss: 2.0614 | PPL: 7.86 | LR: 1.971507e-04\n",
      "  Step 3800/4407 - Loss: 2.0624 | PPL: 7.87 | LR: 1.970379e-04\n",
      "  Step 3800/4407 - Loss: 2.0624 | PPL: 7.87 | LR: 1.970379e-04\n",
      "  Step 3900/4407 - Loss: 2.0633 | PPL: 7.87 | LR: 1.969251e-04\n",
      "  Step 3900/4407 - Loss: 2.0633 | PPL: 7.87 | LR: 1.969251e-04\n",
      "  Step 4000/4407 - Loss: 2.0630 | PPL: 7.87 | LR: 1.968122e-04\n",
      "  Step 4000/4407 - Loss: 2.0630 | PPL: 7.87 | LR: 1.968122e-04\n",
      "  Step 4100/4407 - Loss: 2.0633 | PPL: 7.87 | LR: 1.966993e-04\n",
      "  Step 4100/4407 - Loss: 2.0633 | PPL: 7.87 | LR: 1.966993e-04\n",
      "  Step 4200/4407 - Loss: 2.0635 | PPL: 7.87 | LR: 1.965864e-04\n",
      "  Step 4200/4407 - Loss: 2.0635 | PPL: 7.87 | LR: 1.965864e-04\n",
      "  Step 4300/4407 - Loss: 2.0639 | PPL: 7.88 | LR: 1.964734e-04\n",
      "  Step 4300/4407 - Loss: 2.0639 | PPL: 7.88 | LR: 1.964734e-04\n",
      "  Step 4400/4407 - Loss: 2.0647 | PPL: 7.88 | LR: 1.963605e-04\n",
      "  Step 4400/4407 - Loss: 2.0647 | PPL: 7.88 | LR: 1.963605e-04\n",
      "Epoch 46/100 | Train Loss: 2.0648 | Train PPL: 7.88 | Val Loss: 2.6033 | Val PPL: 13.51\n",
      "Epoch 46/100 | Train Loss: 2.0648 | Train PPL: 7.88 | Val Loss: 2.6033 | Val PPL: 13.51\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.88\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.88\n",
      "  Step 100/4407 - Loss: 2.0337 | PPL: 7.64 | LR: 1.962395e-04\n",
      "  Step 100/4407 - Loss: 2.0337 | PPL: 7.64 | LR: 1.962395e-04\n",
      "  Step 200/4407 - Loss: 2.0173 | PPL: 7.52 | LR: 1.961265e-04\n",
      "  Step 200/4407 - Loss: 2.0173 | PPL: 7.52 | LR: 1.961265e-04\n",
      "  Step 300/4407 - Loss: 2.0146 | PPL: 7.50 | LR: 1.960134e-04\n",
      "  Step 300/4407 - Loss: 2.0146 | PPL: 7.50 | LR: 1.960134e-04\n",
      "  Step 400/4407 - Loss: 2.0246 | PPL: 7.57 | LR: 1.959003e-04\n",
      "  Step 400/4407 - Loss: 2.0246 | PPL: 7.57 | LR: 1.959003e-04\n",
      "  Step 500/4407 - Loss: 2.0217 | PPL: 7.55 | LR: 1.957872e-04\n",
      "  Step 500/4407 - Loss: 2.0217 | PPL: 7.55 | LR: 1.957872e-04\n",
      "  Step 600/4407 - Loss: 2.0180 | PPL: 7.52 | LR: 1.956741e-04\n",
      "  Step 600/4407 - Loss: 2.0180 | PPL: 7.52 | LR: 1.956741e-04\n",
      "  Step 700/4407 - Loss: 2.0262 | PPL: 7.59 | LR: 1.955609e-04\n",
      "  Step 700/4407 - Loss: 2.0262 | PPL: 7.59 | LR: 1.955609e-04\n",
      "  Step 800/4407 - Loss: 2.0310 | PPL: 7.62 | LR: 1.954477e-04\n",
      "  Step 800/4407 - Loss: 2.0310 | PPL: 7.62 | LR: 1.954477e-04\n",
      "  Step 900/4407 - Loss: 2.0281 | PPL: 7.60 | LR: 1.953344e-04\n",
      "  Step 900/4407 - Loss: 2.0281 | PPL: 7.60 | LR: 1.953344e-04\n",
      "  Step 1000/4407 - Loss: 2.0260 | PPL: 7.58 | LR: 1.952212e-04\n",
      "  Step 1000/4407 - Loss: 2.0260 | PPL: 7.58 | LR: 1.952212e-04\n",
      "  Step 1100/4407 - Loss: 2.0302 | PPL: 7.62 | LR: 1.951079e-04\n",
      "  Step 1100/4407 - Loss: 2.0302 | PPL: 7.62 | LR: 1.951079e-04\n",
      "  Step 1200/4407 - Loss: 2.0345 | PPL: 7.65 | LR: 1.949945e-04\n",
      "  Step 1200/4407 - Loss: 2.0345 | PPL: 7.65 | LR: 1.949945e-04\n",
      "  Step 1300/4407 - Loss: 2.0387 | PPL: 7.68 | LR: 1.948812e-04\n",
      "  Step 1300/4407 - Loss: 2.0387 | PPL: 7.68 | LR: 1.948812e-04\n",
      "  Step 1400/4407 - Loss: 2.0402 | PPL: 7.69 | LR: 1.947678e-04\n",
      "  Step 1400/4407 - Loss: 2.0402 | PPL: 7.69 | LR: 1.947678e-04\n",
      "  Step 1500/4407 - Loss: 2.0404 | PPL: 7.69 | LR: 1.946544e-04\n",
      "  Step 1500/4407 - Loss: 2.0404 | PPL: 7.69 | LR: 1.946544e-04\n",
      "  Step 1600/4407 - Loss: 2.0419 | PPL: 7.71 | LR: 1.945409e-04\n",
      "  Step 1600/4407 - Loss: 2.0419 | PPL: 7.71 | LR: 1.945409e-04\n",
      "  Step 1700/4407 - Loss: 2.0426 | PPL: 7.71 | LR: 1.944275e-04\n",
      "  Step 1700/4407 - Loss: 2.0426 | PPL: 7.71 | LR: 1.944275e-04\n",
      "  Step 1800/4407 - Loss: 2.0420 | PPL: 7.71 | LR: 1.943140e-04\n",
      "  Step 1800/4407 - Loss: 2.0420 | PPL: 7.71 | LR: 1.943140e-04\n",
      "  Step 1900/4407 - Loss: 2.0428 | PPL: 7.71 | LR: 1.942005e-04\n",
      "  Step 1900/4407 - Loss: 2.0428 | PPL: 7.71 | LR: 1.942005e-04\n",
      "  Step 2000/4407 - Loss: 2.0421 | PPL: 7.71 | LR: 1.940869e-04\n",
      "  Step 2000/4407 - Loss: 2.0421 | PPL: 7.71 | LR: 1.940869e-04\n",
      "  Step 2100/4407 - Loss: 2.0436 | PPL: 7.72 | LR: 1.939733e-04\n",
      "  Step 2100/4407 - Loss: 2.0436 | PPL: 7.72 | LR: 1.939733e-04\n",
      "  Step 2200/4407 - Loss: 2.0457 | PPL: 7.73 | LR: 1.938597e-04\n",
      "  Step 2200/4407 - Loss: 2.0457 | PPL: 7.73 | LR: 1.938597e-04\n",
      "  Step 2300/4407 - Loss: 2.0466 | PPL: 7.74 | LR: 1.937461e-04\n",
      "  Step 2300/4407 - Loss: 2.0466 | PPL: 7.74 | LR: 1.937461e-04\n",
      "  Step 2400/4407 - Loss: 2.0458 | PPL: 7.74 | LR: 1.936324e-04\n",
      "  Step 2400/4407 - Loss: 2.0458 | PPL: 7.74 | LR: 1.936324e-04\n",
      "  Step 2500/4407 - Loss: 2.0475 | PPL: 7.75 | LR: 1.935188e-04\n",
      "  Step 2500/4407 - Loss: 2.0475 | PPL: 7.75 | LR: 1.935188e-04\n",
      "  Step 2600/4407 - Loss: 2.0496 | PPL: 7.76 | LR: 1.934050e-04\n",
      "  Step 2600/4407 - Loss: 2.0496 | PPL: 7.76 | LR: 1.934050e-04\n",
      "  Step 2700/4407 - Loss: 2.0521 | PPL: 7.78 | LR: 1.932913e-04\n",
      "  Step 2700/4407 - Loss: 2.0521 | PPL: 7.78 | LR: 1.932913e-04\n",
      "  Step 2800/4407 - Loss: 2.0538 | PPL: 7.80 | LR: 1.931775e-04\n",
      "  Step 2800/4407 - Loss: 2.0538 | PPL: 7.80 | LR: 1.931775e-04\n",
      "  Step 2900/4407 - Loss: 2.0556 | PPL: 7.81 | LR: 1.930637e-04\n",
      "  Step 2900/4407 - Loss: 2.0556 | PPL: 7.81 | LR: 1.930637e-04\n",
      "  Step 3000/4407 - Loss: 2.0561 | PPL: 7.82 | LR: 1.929499e-04\n",
      "  Step 3000/4407 - Loss: 2.0561 | PPL: 7.82 | LR: 1.929499e-04\n",
      "  Step 3100/4407 - Loss: 2.0574 | PPL: 7.83 | LR: 1.928361e-04\n",
      "  Step 3100/4407 - Loss: 2.0574 | PPL: 7.83 | LR: 1.928361e-04\n",
      "  Step 3200/4407 - Loss: 2.0573 | PPL: 7.82 | LR: 1.927222e-04\n",
      "  Step 3200/4407 - Loss: 2.0573 | PPL: 7.82 | LR: 1.927222e-04\n",
      "  Step 3300/4407 - Loss: 2.0574 | PPL: 7.83 | LR: 1.926083e-04\n",
      "  Step 3300/4407 - Loss: 2.0574 | PPL: 7.83 | LR: 1.926083e-04\n",
      "  Step 3400/4407 - Loss: 2.0571 | PPL: 7.82 | LR: 1.924944e-04\n",
      "  Step 3400/4407 - Loss: 2.0571 | PPL: 7.82 | LR: 1.924944e-04\n",
      "  Step 3500/4407 - Loss: 2.0576 | PPL: 7.83 | LR: 1.923804e-04\n",
      "  Step 3500/4407 - Loss: 2.0576 | PPL: 7.83 | LR: 1.923804e-04\n",
      "  Step 3600/4407 - Loss: 2.0579 | PPL: 7.83 | LR: 1.922664e-04\n",
      "  Step 3600/4407 - Loss: 2.0579 | PPL: 7.83 | LR: 1.922664e-04\n",
      "  Step 3700/4407 - Loss: 2.0588 | PPL: 7.84 | LR: 1.921524e-04\n",
      "  Step 3700/4407 - Loss: 2.0588 | PPL: 7.84 | LR: 1.921524e-04\n",
      "  Step 3800/4407 - Loss: 2.0593 | PPL: 7.84 | LR: 1.920384e-04\n",
      "  Step 3800/4407 - Loss: 2.0593 | PPL: 7.84 | LR: 1.920384e-04\n",
      "  Step 3900/4407 - Loss: 2.0584 | PPL: 7.83 | LR: 1.919243e-04\n",
      "  Step 3900/4407 - Loss: 2.0584 | PPL: 7.83 | LR: 1.919243e-04\n",
      "  Step 4000/4407 - Loss: 2.0585 | PPL: 7.83 | LR: 1.918102e-04\n",
      "  Step 4000/4407 - Loss: 2.0585 | PPL: 7.83 | LR: 1.918102e-04\n",
      "  Step 4100/4407 - Loss: 2.0592 | PPL: 7.84 | LR: 1.916961e-04\n",
      "  Step 4100/4407 - Loss: 2.0592 | PPL: 7.84 | LR: 1.916961e-04\n",
      "  Step 4200/4407 - Loss: 2.0593 | PPL: 7.84 | LR: 1.915820e-04\n",
      "  Step 4200/4407 - Loss: 2.0593 | PPL: 7.84 | LR: 1.915820e-04\n",
      "  Step 4300/4407 - Loss: 2.0590 | PPL: 7.84 | LR: 1.914678e-04\n",
      "  Step 4300/4407 - Loss: 2.0590 | PPL: 7.84 | LR: 1.914678e-04\n",
      "  Step 4400/4407 - Loss: 2.0599 | PPL: 7.84 | LR: 1.913536e-04\n",
      "  Step 4400/4407 - Loss: 2.0599 | PPL: 7.84 | LR: 1.913536e-04\n",
      "Epoch 47/100 | Train Loss: 2.0602 | Train PPL: 7.85 | Val Loss: 2.6088 | Val PPL: 13.58\n",
      "Epoch 47/100 | Train Loss: 2.0602 | Train PPL: 7.85 | Val Loss: 2.6088 | Val PPL: 13.58\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.85\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.85\n",
      "  Step 100/4407 - Loss: 2.0275 | PPL: 7.60 | LR: 1.912314e-04\n",
      "  Step 100/4407 - Loss: 2.0275 | PPL: 7.60 | LR: 1.912314e-04\n",
      "  Step 200/4407 - Loss: 2.0021 | PPL: 7.40 | LR: 1.911171e-04\n",
      "  Step 200/4407 - Loss: 2.0021 | PPL: 7.40 | LR: 1.911171e-04\n",
      "  Step 300/4407 - Loss: 2.0157 | PPL: 7.51 | LR: 1.910029e-04\n",
      "  Step 300/4407 - Loss: 2.0157 | PPL: 7.51 | LR: 1.910029e-04\n",
      "  Step 400/4407 - Loss: 2.0212 | PPL: 7.55 | LR: 1.908886e-04\n",
      "  Step 400/4407 - Loss: 2.0212 | PPL: 7.55 | LR: 1.908886e-04\n",
      "  Step 500/4407 - Loss: 2.0157 | PPL: 7.51 | LR: 1.907742e-04\n",
      "  Step 500/4407 - Loss: 2.0157 | PPL: 7.51 | LR: 1.907742e-04\n",
      "  Step 600/4407 - Loss: 2.0200 | PPL: 7.54 | LR: 1.906599e-04\n",
      "  Step 600/4407 - Loss: 2.0200 | PPL: 7.54 | LR: 1.906599e-04\n",
      "  Step 700/4407 - Loss: 2.0296 | PPL: 7.61 | LR: 1.905455e-04\n",
      "  Step 700/4407 - Loss: 2.0296 | PPL: 7.61 | LR: 1.905455e-04\n",
      "  Step 800/4407 - Loss: 2.0312 | PPL: 7.62 | LR: 1.904311e-04\n",
      "  Step 800/4407 - Loss: 2.0312 | PPL: 7.62 | LR: 1.904311e-04\n",
      "  Step 900/4407 - Loss: 2.0267 | PPL: 7.59 | LR: 1.903167e-04\n",
      "  Step 900/4407 - Loss: 2.0267 | PPL: 7.59 | LR: 1.903167e-04\n",
      "  Step 1000/4407 - Loss: 2.0318 | PPL: 7.63 | LR: 1.902022e-04\n",
      "  Step 1000/4407 - Loss: 2.0318 | PPL: 7.63 | LR: 1.902022e-04\n",
      "  Step 1100/4407 - Loss: 2.0370 | PPL: 7.67 | LR: 1.900878e-04\n",
      "  Step 1100/4407 - Loss: 2.0370 | PPL: 7.67 | LR: 1.900878e-04\n",
      "  Step 1200/4407 - Loss: 2.0390 | PPL: 7.68 | LR: 1.899733e-04\n",
      "  Step 1200/4407 - Loss: 2.0390 | PPL: 7.68 | LR: 1.899733e-04\n",
      "  Step 1300/4407 - Loss: 2.0407 | PPL: 7.70 | LR: 1.898587e-04\n",
      "  Step 1300/4407 - Loss: 2.0407 | PPL: 7.70 | LR: 1.898587e-04\n",
      "  Step 1400/4407 - Loss: 2.0425 | PPL: 7.71 | LR: 1.897442e-04\n",
      "  Step 1400/4407 - Loss: 2.0425 | PPL: 7.71 | LR: 1.897442e-04\n",
      "  Step 1500/4407 - Loss: 2.0429 | PPL: 7.71 | LR: 1.896296e-04\n",
      "  Step 1500/4407 - Loss: 2.0429 | PPL: 7.71 | LR: 1.896296e-04\n",
      "  Step 1600/4407 - Loss: 2.0435 | PPL: 7.72 | LR: 1.895150e-04\n",
      "  Step 1600/4407 - Loss: 2.0435 | PPL: 7.72 | LR: 1.895150e-04\n",
      "  Step 1700/4407 - Loss: 2.0450 | PPL: 7.73 | LR: 1.894004e-04\n",
      "  Step 1700/4407 - Loss: 2.0450 | PPL: 7.73 | LR: 1.894004e-04\n",
      "  Step 1800/4407 - Loss: 2.0461 | PPL: 7.74 | LR: 1.892857e-04\n",
      "  Step 1800/4407 - Loss: 2.0461 | PPL: 7.74 | LR: 1.892857e-04\n",
      "  Step 1900/4407 - Loss: 2.0486 | PPL: 7.76 | LR: 1.891710e-04\n",
      "  Step 1900/4407 - Loss: 2.0486 | PPL: 7.76 | LR: 1.891710e-04\n",
      "  Step 2000/4407 - Loss: 2.0472 | PPL: 7.75 | LR: 1.890563e-04\n",
      "  Step 2000/4407 - Loss: 2.0472 | PPL: 7.75 | LR: 1.890563e-04\n",
      "  Step 2100/4407 - Loss: 2.0496 | PPL: 7.76 | LR: 1.889416e-04\n",
      "  Step 2100/4407 - Loss: 2.0496 | PPL: 7.76 | LR: 1.889416e-04\n",
      "  Step 2200/4407 - Loss: 2.0492 | PPL: 7.76 | LR: 1.888269e-04\n",
      "  Step 2200/4407 - Loss: 2.0492 | PPL: 7.76 | LR: 1.888269e-04\n",
      "  Step 2300/4407 - Loss: 2.0478 | PPL: 7.75 | LR: 1.887121e-04\n",
      "  Step 2300/4407 - Loss: 2.0478 | PPL: 7.75 | LR: 1.887121e-04\n",
      "  Step 2400/4407 - Loss: 2.0474 | PPL: 7.75 | LR: 1.885973e-04\n",
      "  Step 2400/4407 - Loss: 2.0474 | PPL: 7.75 | LR: 1.885973e-04\n",
      "  Step 2500/4407 - Loss: 2.0477 | PPL: 7.75 | LR: 1.884825e-04\n",
      "  Step 2500/4407 - Loss: 2.0477 | PPL: 7.75 | LR: 1.884825e-04\n",
      "  Step 2600/4407 - Loss: 2.0472 | PPL: 7.75 | LR: 1.883676e-04\n",
      "  Step 2600/4407 - Loss: 2.0472 | PPL: 7.75 | LR: 1.883676e-04\n",
      "  Step 2700/4407 - Loss: 2.0477 | PPL: 7.75 | LR: 1.882528e-04\n",
      "  Step 2700/4407 - Loss: 2.0477 | PPL: 7.75 | LR: 1.882528e-04\n",
      "  Step 2800/4407 - Loss: 2.0471 | PPL: 7.75 | LR: 1.881379e-04\n",
      "  Step 2800/4407 - Loss: 2.0471 | PPL: 7.75 | LR: 1.881379e-04\n",
      "  Step 2900/4407 - Loss: 2.0470 | PPL: 7.74 | LR: 1.880230e-04\n",
      "  Step 2900/4407 - Loss: 2.0470 | PPL: 7.74 | LR: 1.880230e-04\n",
      "  Step 3000/4407 - Loss: 2.0481 | PPL: 7.75 | LR: 1.879080e-04\n",
      "  Step 3000/4407 - Loss: 2.0481 | PPL: 7.75 | LR: 1.879080e-04\n",
      "  Step 3100/4407 - Loss: 2.0481 | PPL: 7.75 | LR: 1.877930e-04\n",
      "  Step 3100/4407 - Loss: 2.0481 | PPL: 7.75 | LR: 1.877930e-04\n",
      "  Step 3200/4407 - Loss: 2.0494 | PPL: 7.76 | LR: 1.876781e-04\n",
      "  Step 3200/4407 - Loss: 2.0494 | PPL: 7.76 | LR: 1.876781e-04\n",
      "  Step 3300/4407 - Loss: 2.0513 | PPL: 7.78 | LR: 1.875630e-04\n",
      "  Step 3300/4407 - Loss: 2.0513 | PPL: 7.78 | LR: 1.875630e-04\n",
      "  Step 3400/4407 - Loss: 2.0516 | PPL: 7.78 | LR: 1.874480e-04\n",
      "  Step 3400/4407 - Loss: 2.0516 | PPL: 7.78 | LR: 1.874480e-04\n",
      "  Step 3500/4407 - Loss: 2.0523 | PPL: 7.79 | LR: 1.873329e-04\n",
      "  Step 3500/4407 - Loss: 2.0523 | PPL: 7.79 | LR: 1.873329e-04\n",
      "  Step 3600/4407 - Loss: 2.0527 | PPL: 7.79 | LR: 1.872179e-04\n",
      "  Step 3600/4407 - Loss: 2.0527 | PPL: 7.79 | LR: 1.872179e-04\n",
      "  Step 3700/4407 - Loss: 2.0529 | PPL: 7.79 | LR: 1.871028e-04\n",
      "  Step 3700/4407 - Loss: 2.0529 | PPL: 7.79 | LR: 1.871028e-04\n",
      "  Step 3800/4407 - Loss: 2.0540 | PPL: 7.80 | LR: 1.869876e-04\n",
      "  Step 3800/4407 - Loss: 2.0540 | PPL: 7.80 | LR: 1.869876e-04\n",
      "  Step 3900/4407 - Loss: 2.0543 | PPL: 7.80 | LR: 1.868725e-04\n",
      "  Step 3900/4407 - Loss: 2.0543 | PPL: 7.80 | LR: 1.868725e-04\n",
      "  Step 4000/4407 - Loss: 2.0547 | PPL: 7.80 | LR: 1.867573e-04\n",
      "  Step 4000/4407 - Loss: 2.0547 | PPL: 7.80 | LR: 1.867573e-04\n",
      "  Step 4100/4407 - Loss: 2.0551 | PPL: 7.81 | LR: 1.866421e-04\n",
      "  Step 4100/4407 - Loss: 2.0551 | PPL: 7.81 | LR: 1.866421e-04\n",
      "  Step 4200/4407 - Loss: 2.0561 | PPL: 7.82 | LR: 1.865269e-04\n",
      "  Step 4200/4407 - Loss: 2.0561 | PPL: 7.82 | LR: 1.865269e-04\n",
      "  Step 4300/4407 - Loss: 2.0564 | PPL: 7.82 | LR: 1.864116e-04\n",
      "  Step 4300/4407 - Loss: 2.0564 | PPL: 7.82 | LR: 1.864116e-04\n",
      "  Step 4400/4407 - Loss: 2.0562 | PPL: 7.82 | LR: 1.862964e-04\n",
      "  Step 4400/4407 - Loss: 2.0562 | PPL: 7.82 | LR: 1.862964e-04\n",
      "Epoch 48/100 | Train Loss: 2.0560 | Train PPL: 7.81 | Val Loss: 2.6054 | Val PPL: 13.54\n",
      "Epoch 48/100 | Train Loss: 2.0560 | Train PPL: 7.81 | Val Loss: 2.6054 | Val PPL: 13.54\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.81\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.81\n",
      "  Step 100/4407 - Loss: 2.0047 | PPL: 7.42 | LR: 1.861730e-04\n",
      "  Step 100/4407 - Loss: 2.0047 | PPL: 7.42 | LR: 1.861730e-04\n",
      "  Step 200/4407 - Loss: 2.0128 | PPL: 7.48 | LR: 1.860577e-04\n",
      "  Step 200/4407 - Loss: 2.0128 | PPL: 7.48 | LR: 1.860577e-04\n",
      "  Step 300/4407 - Loss: 2.0262 | PPL: 7.59 | LR: 1.859423e-04\n",
      "  Step 300/4407 - Loss: 2.0262 | PPL: 7.59 | LR: 1.859423e-04\n",
      "  Step 400/4407 - Loss: 2.0333 | PPL: 7.64 | LR: 1.858270e-04\n",
      "  Step 400/4407 - Loss: 2.0333 | PPL: 7.64 | LR: 1.858270e-04\n",
      "  Step 500/4407 - Loss: 2.0289 | PPL: 7.61 | LR: 1.857116e-04\n",
      "  Step 500/4407 - Loss: 2.0289 | PPL: 7.61 | LR: 1.857116e-04\n",
      "  Step 600/4407 - Loss: 2.0274 | PPL: 7.59 | LR: 1.855962e-04\n",
      "  Step 600/4407 - Loss: 2.0274 | PPL: 7.59 | LR: 1.855962e-04\n",
      "  Step 700/4407 - Loss: 2.0308 | PPL: 7.62 | LR: 1.854808e-04\n",
      "  Step 700/4407 - Loss: 2.0308 | PPL: 7.62 | LR: 1.854808e-04\n",
      "  Step 800/4407 - Loss: 2.0331 | PPL: 7.64 | LR: 1.853653e-04\n",
      "  Step 800/4407 - Loss: 2.0331 | PPL: 7.64 | LR: 1.853653e-04\n",
      "  Step 900/4407 - Loss: 2.0362 | PPL: 7.66 | LR: 1.852498e-04\n",
      "  Step 900/4407 - Loss: 2.0362 | PPL: 7.66 | LR: 1.852498e-04\n",
      "  Step 1000/4407 - Loss: 2.0392 | PPL: 7.68 | LR: 1.851343e-04\n",
      "  Step 1000/4407 - Loss: 2.0392 | PPL: 7.68 | LR: 1.851343e-04\n",
      "  Step 1100/4407 - Loss: 2.0396 | PPL: 7.69 | LR: 1.850188e-04\n",
      "  Step 1100/4407 - Loss: 2.0396 | PPL: 7.69 | LR: 1.850188e-04\n",
      "  Step 1200/4407 - Loss: 2.0396 | PPL: 7.69 | LR: 1.849033e-04\n",
      "  Step 1200/4407 - Loss: 2.0396 | PPL: 7.69 | LR: 1.849033e-04\n",
      "  Step 1300/4407 - Loss: 2.0396 | PPL: 7.69 | LR: 1.847877e-04\n",
      "  Step 1300/4407 - Loss: 2.0396 | PPL: 7.69 | LR: 1.847877e-04\n",
      "  Step 1400/4407 - Loss: 2.0418 | PPL: 7.70 | LR: 1.846721e-04\n",
      "  Step 1400/4407 - Loss: 2.0418 | PPL: 7.70 | LR: 1.846721e-04\n",
      "  Step 1500/4407 - Loss: 2.0404 | PPL: 7.69 | LR: 1.845565e-04\n",
      "  Step 1500/4407 - Loss: 2.0404 | PPL: 7.69 | LR: 1.845565e-04\n",
      "  Step 1600/4407 - Loss: 2.0393 | PPL: 7.69 | LR: 1.844409e-04\n",
      "  Step 1600/4407 - Loss: 2.0393 | PPL: 7.69 | LR: 1.844409e-04\n",
      "  Step 1700/4407 - Loss: 2.0390 | PPL: 7.68 | LR: 1.843253e-04\n",
      "  Step 1700/4407 - Loss: 2.0390 | PPL: 7.68 | LR: 1.843253e-04\n",
      "  Step 1800/4407 - Loss: 2.0380 | PPL: 7.68 | LR: 1.842096e-04\n",
      "  Step 1800/4407 - Loss: 2.0380 | PPL: 7.68 | LR: 1.842096e-04\n",
      "  Step 1900/4407 - Loss: 2.0388 | PPL: 7.68 | LR: 1.840939e-04\n",
      "  Step 1900/4407 - Loss: 2.0388 | PPL: 7.68 | LR: 1.840939e-04\n",
      "  Step 2000/4407 - Loss: 2.0392 | PPL: 7.68 | LR: 1.839782e-04\n",
      "  Step 2000/4407 - Loss: 2.0392 | PPL: 7.68 | LR: 1.839782e-04\n",
      "  Step 2100/4407 - Loss: 2.0402 | PPL: 7.69 | LR: 1.838625e-04\n",
      "  Step 2100/4407 - Loss: 2.0402 | PPL: 7.69 | LR: 1.838625e-04\n",
      "  Step 2200/4407 - Loss: 2.0407 | PPL: 7.70 | LR: 1.837467e-04\n",
      "  Step 2200/4407 - Loss: 2.0407 | PPL: 7.70 | LR: 1.837467e-04\n",
      "  Step 2300/4407 - Loss: 2.0423 | PPL: 7.71 | LR: 1.836309e-04\n",
      "  Step 2300/4407 - Loss: 2.0423 | PPL: 7.71 | LR: 1.836309e-04\n",
      "  Step 2400/4407 - Loss: 2.0427 | PPL: 7.71 | LR: 1.835151e-04\n",
      "  Step 2400/4407 - Loss: 2.0427 | PPL: 7.71 | LR: 1.835151e-04\n",
      "  Step 2500/4407 - Loss: 2.0411 | PPL: 7.70 | LR: 1.833993e-04\n",
      "  Step 2500/4407 - Loss: 2.0411 | PPL: 7.70 | LR: 1.833993e-04\n",
      "  Step 2600/4407 - Loss: 2.0419 | PPL: 7.71 | LR: 1.832835e-04\n",
      "  Step 2600/4407 - Loss: 2.0419 | PPL: 7.71 | LR: 1.832835e-04\n",
      "  Step 2700/4407 - Loss: 2.0434 | PPL: 7.72 | LR: 1.831676e-04\n",
      "  Step 2700/4407 - Loss: 2.0434 | PPL: 7.72 | LR: 1.831676e-04\n",
      "  Step 2800/4407 - Loss: 2.0449 | PPL: 7.73 | LR: 1.830517e-04\n",
      "  Step 2800/4407 - Loss: 2.0449 | PPL: 7.73 | LR: 1.830517e-04\n",
      "  Step 2900/4407 - Loss: 2.0445 | PPL: 7.72 | LR: 1.829358e-04\n",
      "  Step 2900/4407 - Loss: 2.0445 | PPL: 7.72 | LR: 1.829358e-04\n",
      "  Step 3000/4407 - Loss: 2.0447 | PPL: 7.73 | LR: 1.828199e-04\n",
      "  Step 3000/4407 - Loss: 2.0447 | PPL: 7.73 | LR: 1.828199e-04\n",
      "  Step 3100/4407 - Loss: 2.0441 | PPL: 7.72 | LR: 1.827040e-04\n",
      "  Step 3100/4407 - Loss: 2.0441 | PPL: 7.72 | LR: 1.827040e-04\n",
      "  Step 3200/4407 - Loss: 2.0435 | PPL: 7.72 | LR: 1.825880e-04\n",
      "  Step 3200/4407 - Loss: 2.0435 | PPL: 7.72 | LR: 1.825880e-04\n",
      "  Step 3300/4407 - Loss: 2.0445 | PPL: 7.73 | LR: 1.824720e-04\n",
      "  Step 3300/4407 - Loss: 2.0445 | PPL: 7.73 | LR: 1.824720e-04\n",
      "  Step 3400/4407 - Loss: 2.0450 | PPL: 7.73 | LR: 1.823560e-04\n",
      "  Step 3400/4407 - Loss: 2.0450 | PPL: 7.73 | LR: 1.823560e-04\n",
      "  Step 3500/4407 - Loss: 2.0460 | PPL: 7.74 | LR: 1.822400e-04\n",
      "  Step 3500/4407 - Loss: 2.0460 | PPL: 7.74 | LR: 1.822400e-04\n",
      "  Step 3600/4407 - Loss: 2.0471 | PPL: 7.75 | LR: 1.821240e-04\n",
      "  Step 3600/4407 - Loss: 2.0471 | PPL: 7.75 | LR: 1.821240e-04\n",
      "  Step 3700/4407 - Loss: 2.0472 | PPL: 7.75 | LR: 1.820079e-04\n",
      "  Step 3700/4407 - Loss: 2.0472 | PPL: 7.75 | LR: 1.820079e-04\n",
      "  Step 3800/4407 - Loss: 2.0488 | PPL: 7.76 | LR: 1.818918e-04\n",
      "  Step 3800/4407 - Loss: 2.0488 | PPL: 7.76 | LR: 1.818918e-04\n",
      "  Step 3900/4407 - Loss: 2.0487 | PPL: 7.76 | LR: 1.817757e-04\n",
      "  Step 3900/4407 - Loss: 2.0487 | PPL: 7.76 | LR: 1.817757e-04\n",
      "  Step 4000/4407 - Loss: 2.0495 | PPL: 7.76 | LR: 1.816596e-04\n",
      "  Step 4000/4407 - Loss: 2.0495 | PPL: 7.76 | LR: 1.816596e-04\n",
      "  Step 4100/4407 - Loss: 2.0494 | PPL: 7.76 | LR: 1.815434e-04\n",
      "  Step 4100/4407 - Loss: 2.0494 | PPL: 7.76 | LR: 1.815434e-04\n",
      "  Step 4200/4407 - Loss: 2.0505 | PPL: 7.77 | LR: 1.814273e-04\n",
      "  Step 4200/4407 - Loss: 2.0505 | PPL: 7.77 | LR: 1.814273e-04\n",
      "  Step 4300/4407 - Loss: 2.0510 | PPL: 7.78 | LR: 1.813111e-04\n",
      "  Step 4300/4407 - Loss: 2.0510 | PPL: 7.78 | LR: 1.813111e-04\n",
      "  Step 4400/4407 - Loss: 2.0519 | PPL: 7.78 | LR: 1.811949e-04\n",
      "  Step 4400/4407 - Loss: 2.0519 | PPL: 7.78 | LR: 1.811949e-04\n",
      "Epoch 49/100 | Train Loss: 2.0518 | Train PPL: 7.78 | Val Loss: 2.6041 | Val PPL: 13.52\n",
      "Epoch 49/100 | Train Loss: 2.0518 | Train PPL: 7.78 | Val Loss: 2.6041 | Val PPL: 13.52\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.78\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.78\n",
      "  Step 100/4407 - Loss: 2.0314 | PPL: 7.63 | LR: 1.810705e-04\n",
      "  Step 100/4407 - Loss: 2.0314 | PPL: 7.63 | LR: 1.810705e-04\n",
      "  Step 200/4407 - Loss: 2.0227 | PPL: 7.56 | LR: 1.809543e-04\n",
      "  Step 200/4407 - Loss: 2.0227 | PPL: 7.56 | LR: 1.809543e-04\n",
      "  Step 300/4407 - Loss: 2.0308 | PPL: 7.62 | LR: 1.808380e-04\n",
      "  Step 300/4407 - Loss: 2.0308 | PPL: 7.62 | LR: 1.808380e-04\n",
      "  Step 400/4407 - Loss: 2.0354 | PPL: 7.66 | LR: 1.807217e-04\n",
      "  Step 400/4407 - Loss: 2.0354 | PPL: 7.66 | LR: 1.807217e-04\n",
      "  Step 500/4407 - Loss: 2.0320 | PPL: 7.63 | LR: 1.806054e-04\n",
      "  Step 500/4407 - Loss: 2.0320 | PPL: 7.63 | LR: 1.806054e-04\n",
      "  Step 600/4407 - Loss: 2.0347 | PPL: 7.65 | LR: 1.804891e-04\n",
      "  Step 600/4407 - Loss: 2.0347 | PPL: 7.65 | LR: 1.804891e-04\n",
      "  Step 700/4407 - Loss: 2.0365 | PPL: 7.66 | LR: 1.803728e-04\n",
      "  Step 700/4407 - Loss: 2.0365 | PPL: 7.66 | LR: 1.803728e-04\n",
      "  Step 800/4407 - Loss: 2.0392 | PPL: 7.68 | LR: 1.802564e-04\n",
      "  Step 800/4407 - Loss: 2.0392 | PPL: 7.68 | LR: 1.802564e-04\n",
      "  Step 900/4407 - Loss: 2.0367 | PPL: 7.67 | LR: 1.801400e-04\n",
      "  Step 900/4407 - Loss: 2.0367 | PPL: 7.67 | LR: 1.801400e-04\n",
      "  Step 1000/4407 - Loss: 2.0329 | PPL: 7.64 | LR: 1.800236e-04\n",
      "  Step 1000/4407 - Loss: 2.0329 | PPL: 7.64 | LR: 1.800236e-04\n",
      "  Step 1100/4407 - Loss: 2.0342 | PPL: 7.65 | LR: 1.799072e-04\n",
      "  Step 1100/4407 - Loss: 2.0342 | PPL: 7.65 | LR: 1.799072e-04\n",
      "  Step 1200/4407 - Loss: 2.0346 | PPL: 7.65 | LR: 1.797908e-04\n",
      "  Step 1200/4407 - Loss: 2.0346 | PPL: 7.65 | LR: 1.797908e-04\n",
      "  Step 1300/4407 - Loss: 2.0329 | PPL: 7.64 | LR: 1.796743e-04\n",
      "  Step 1300/4407 - Loss: 2.0329 | PPL: 7.64 | LR: 1.796743e-04\n",
      "  Step 1400/4407 - Loss: 2.0316 | PPL: 7.63 | LR: 1.795579e-04\n",
      "  Step 1400/4407 - Loss: 2.0316 | PPL: 7.63 | LR: 1.795579e-04\n",
      "  Step 1500/4407 - Loss: 2.0346 | PPL: 7.65 | LR: 1.794414e-04\n",
      "  Step 1500/4407 - Loss: 2.0346 | PPL: 7.65 | LR: 1.794414e-04\n",
      "  Step 1600/4407 - Loss: 2.0316 | PPL: 7.63 | LR: 1.793249e-04\n",
      "  Step 1600/4407 - Loss: 2.0316 | PPL: 7.63 | LR: 1.793249e-04\n",
      "  Step 1700/4407 - Loss: 2.0343 | PPL: 7.65 | LR: 1.792083e-04\n",
      "  Step 1700/4407 - Loss: 2.0343 | PPL: 7.65 | LR: 1.792083e-04\n",
      "  Step 1800/4407 - Loss: 2.0355 | PPL: 7.66 | LR: 1.790918e-04\n",
      "  Step 1800/4407 - Loss: 2.0355 | PPL: 7.66 | LR: 1.790918e-04\n",
      "  Step 1900/4407 - Loss: 2.0369 | PPL: 7.67 | LR: 1.789752e-04\n",
      "  Step 1900/4407 - Loss: 2.0369 | PPL: 7.67 | LR: 1.789752e-04\n",
      "  Step 2000/4407 - Loss: 2.0387 | PPL: 7.68 | LR: 1.788586e-04\n",
      "  Step 2000/4407 - Loss: 2.0387 | PPL: 7.68 | LR: 1.788586e-04\n",
      "  Step 2100/4407 - Loss: 2.0392 | PPL: 7.68 | LR: 1.787420e-04\n",
      "  Step 2100/4407 - Loss: 2.0392 | PPL: 7.68 | LR: 1.787420e-04\n",
      "  Step 2200/4407 - Loss: 2.0387 | PPL: 7.68 | LR: 1.786254e-04\n",
      "  Step 2200/4407 - Loss: 2.0387 | PPL: 7.68 | LR: 1.786254e-04\n",
      "  Step 2300/4407 - Loss: 2.0377 | PPL: 7.67 | LR: 1.785088e-04\n",
      "  Step 2300/4407 - Loss: 2.0377 | PPL: 7.67 | LR: 1.785088e-04\n",
      "  Step 2400/4407 - Loss: 2.0388 | PPL: 7.68 | LR: 1.783921e-04\n",
      "  Step 2400/4407 - Loss: 2.0388 | PPL: 7.68 | LR: 1.783921e-04\n",
      "  Step 2500/4407 - Loss: 2.0385 | PPL: 7.68 | LR: 1.782755e-04\n",
      "  Step 2500/4407 - Loss: 2.0385 | PPL: 7.68 | LR: 1.782755e-04\n",
      "  Step 2600/4407 - Loss: 2.0399 | PPL: 7.69 | LR: 1.781588e-04\n",
      "  Step 2600/4407 - Loss: 2.0399 | PPL: 7.69 | LR: 1.781588e-04\n",
      "  Step 2700/4407 - Loss: 2.0405 | PPL: 7.69 | LR: 1.780421e-04\n",
      "  Step 2700/4407 - Loss: 2.0405 | PPL: 7.69 | LR: 1.780421e-04\n",
      "  Step 2800/4407 - Loss: 2.0420 | PPL: 7.71 | LR: 1.779253e-04\n",
      "  Step 2800/4407 - Loss: 2.0420 | PPL: 7.71 | LR: 1.779253e-04\n",
      "  Step 2900/4407 - Loss: 2.0410 | PPL: 7.70 | LR: 1.778086e-04\n",
      "  Step 2900/4407 - Loss: 2.0410 | PPL: 7.70 | LR: 1.778086e-04\n",
      "  Step 3000/4407 - Loss: 2.0436 | PPL: 7.72 | LR: 1.776918e-04\n",
      "  Step 3000/4407 - Loss: 2.0436 | PPL: 7.72 | LR: 1.776918e-04\n",
      "  Step 3100/4407 - Loss: 2.0444 | PPL: 7.72 | LR: 1.775751e-04\n",
      "  Step 3100/4407 - Loss: 2.0444 | PPL: 7.72 | LR: 1.775751e-04\n",
      "  Step 3200/4407 - Loss: 2.0445 | PPL: 7.73 | LR: 1.774583e-04\n",
      "  Step 3200/4407 - Loss: 2.0445 | PPL: 7.73 | LR: 1.774583e-04\n",
      "  Step 3300/4407 - Loss: 2.0439 | PPL: 7.72 | LR: 1.773415e-04\n",
      "  Step 3300/4407 - Loss: 2.0439 | PPL: 7.72 | LR: 1.773415e-04\n",
      "  Step 3400/4407 - Loss: 2.0451 | PPL: 7.73 | LR: 1.772246e-04\n",
      "  Step 3400/4407 - Loss: 2.0451 | PPL: 7.73 | LR: 1.772246e-04\n",
      "  Step 3500/4407 - Loss: 2.0452 | PPL: 7.73 | LR: 1.771078e-04\n",
      "  Step 3500/4407 - Loss: 2.0452 | PPL: 7.73 | LR: 1.771078e-04\n",
      "  Step 3600/4407 - Loss: 2.0468 | PPL: 7.74 | LR: 1.769909e-04\n",
      "  Step 3600/4407 - Loss: 2.0468 | PPL: 7.74 | LR: 1.769909e-04\n",
      "  Step 3700/4407 - Loss: 2.0460 | PPL: 7.74 | LR: 1.768740e-04\n",
      "  Step 3700/4407 - Loss: 2.0460 | PPL: 7.74 | LR: 1.768740e-04\n",
      "  Step 3800/4407 - Loss: 2.0451 | PPL: 7.73 | LR: 1.767571e-04\n",
      "  Step 3800/4407 - Loss: 2.0451 | PPL: 7.73 | LR: 1.767571e-04\n",
      "  Step 3900/4407 - Loss: 2.0453 | PPL: 7.73 | LR: 1.766402e-04\n",
      "  Step 3900/4407 - Loss: 2.0453 | PPL: 7.73 | LR: 1.766402e-04\n",
      "  Step 4000/4407 - Loss: 2.0458 | PPL: 7.74 | LR: 1.765233e-04\n",
      "  Step 4000/4407 - Loss: 2.0458 | PPL: 7.74 | LR: 1.765233e-04\n",
      "  Step 4100/4407 - Loss: 2.0471 | PPL: 7.75 | LR: 1.764064e-04\n",
      "  Step 4100/4407 - Loss: 2.0471 | PPL: 7.75 | LR: 1.764064e-04\n",
      "  Step 4200/4407 - Loss: 2.0476 | PPL: 7.75 | LR: 1.762894e-04\n",
      "  Step 4200/4407 - Loss: 2.0476 | PPL: 7.75 | LR: 1.762894e-04\n",
      "  Step 4300/4407 - Loss: 2.0475 | PPL: 7.75 | LR: 1.761724e-04\n",
      "  Step 4300/4407 - Loss: 2.0475 | PPL: 7.75 | LR: 1.761724e-04\n",
      "  Step 4400/4407 - Loss: 2.0479 | PPL: 7.75 | LR: 1.760554e-04\n",
      "  Step 4400/4407 - Loss: 2.0479 | PPL: 7.75 | LR: 1.760554e-04\n",
      "Epoch 50/100 | Train Loss: 2.0479 | Train PPL: 7.75 | Val Loss: 2.6033 | Val PPL: 13.51\n",
      "Epoch 50/100 | Train Loss: 2.0479 | Train PPL: 7.75 | Val Loss: 2.6033 | Val PPL: 13.51\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.75\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.75\n",
      "  Step 100/4407 - Loss: 2.0187 | PPL: 7.53 | LR: 1.759302e-04\n",
      "  Step 100/4407 - Loss: 2.0187 | PPL: 7.53 | LR: 1.759302e-04\n",
      "  Step 200/4407 - Loss: 2.0200 | PPL: 7.54 | LR: 1.758132e-04\n",
      "  Step 200/4407 - Loss: 2.0200 | PPL: 7.54 | LR: 1.758132e-04\n",
      "  Step 300/4407 - Loss: 2.0287 | PPL: 7.60 | LR: 1.756961e-04\n",
      "  Step 300/4407 - Loss: 2.0287 | PPL: 7.60 | LR: 1.756961e-04\n",
      "  Step 400/4407 - Loss: 2.0289 | PPL: 7.61 | LR: 1.755791e-04\n",
      "  Step 400/4407 - Loss: 2.0289 | PPL: 7.61 | LR: 1.755791e-04\n",
      "  Step 500/4407 - Loss: 2.0239 | PPL: 7.57 | LR: 1.754620e-04\n",
      "  Step 500/4407 - Loss: 2.0239 | PPL: 7.57 | LR: 1.754620e-04\n",
      "  Step 600/4407 - Loss: 2.0193 | PPL: 7.53 | LR: 1.753449e-04\n",
      "  Step 600/4407 - Loss: 2.0193 | PPL: 7.53 | LR: 1.753449e-04\n",
      "  Step 700/4407 - Loss: 2.0197 | PPL: 7.54 | LR: 1.752278e-04\n",
      "  Step 700/4407 - Loss: 2.0197 | PPL: 7.54 | LR: 1.752278e-04\n",
      "  Step 800/4407 - Loss: 2.0204 | PPL: 7.54 | LR: 1.751107e-04\n",
      "  Step 800/4407 - Loss: 2.0204 | PPL: 7.54 | LR: 1.751107e-04\n",
      "  Step 900/4407 - Loss: 2.0235 | PPL: 7.56 | LR: 1.749935e-04\n",
      "  Step 900/4407 - Loss: 2.0235 | PPL: 7.56 | LR: 1.749935e-04\n",
      "  Step 1000/4407 - Loss: 2.0204 | PPL: 7.54 | LR: 1.748764e-04\n",
      "  Step 1000/4407 - Loss: 2.0204 | PPL: 7.54 | LR: 1.748764e-04\n",
      "  Step 1100/4407 - Loss: 2.0166 | PPL: 7.51 | LR: 1.747592e-04\n",
      "  Step 1100/4407 - Loss: 2.0166 | PPL: 7.51 | LR: 1.747592e-04\n",
      "  Step 1200/4407 - Loss: 2.0156 | PPL: 7.51 | LR: 1.746420e-04\n",
      "  Step 1200/4407 - Loss: 2.0156 | PPL: 7.51 | LR: 1.746420e-04\n",
      "  Step 1300/4407 - Loss: 2.0208 | PPL: 7.54 | LR: 1.745248e-04\n",
      "  Step 1300/4407 - Loss: 2.0208 | PPL: 7.54 | LR: 1.745248e-04\n",
      "  Step 1400/4407 - Loss: 2.0268 | PPL: 7.59 | LR: 1.744076e-04\n",
      "  Step 1400/4407 - Loss: 2.0268 | PPL: 7.59 | LR: 1.744076e-04\n",
      "  Step 1500/4407 - Loss: 2.0278 | PPL: 7.60 | LR: 1.742903e-04\n",
      "  Step 1500/4407 - Loss: 2.0278 | PPL: 7.60 | LR: 1.742903e-04\n",
      "  Step 1600/4407 - Loss: 2.0272 | PPL: 7.59 | LR: 1.741731e-04\n",
      "  Step 1600/4407 - Loss: 2.0272 | PPL: 7.59 | LR: 1.741731e-04\n",
      "  Step 1700/4407 - Loss: 2.0281 | PPL: 7.60 | LR: 1.740558e-04\n",
      "  Step 1700/4407 - Loss: 2.0281 | PPL: 7.60 | LR: 1.740558e-04\n",
      "  Step 1800/4407 - Loss: 2.0287 | PPL: 7.60 | LR: 1.739385e-04\n",
      "  Step 1800/4407 - Loss: 2.0287 | PPL: 7.60 | LR: 1.739385e-04\n",
      "  Step 1900/4407 - Loss: 2.0298 | PPL: 7.61 | LR: 1.738213e-04\n",
      "  Step 1900/4407 - Loss: 2.0298 | PPL: 7.61 | LR: 1.738213e-04\n",
      "  Step 2000/4407 - Loss: 2.0294 | PPL: 7.61 | LR: 1.737039e-04\n",
      "  Step 2000/4407 - Loss: 2.0294 | PPL: 7.61 | LR: 1.737039e-04\n",
      "  Step 2100/4407 - Loss: 2.0309 | PPL: 7.62 | LR: 1.735866e-04\n",
      "  Step 2100/4407 - Loss: 2.0309 | PPL: 7.62 | LR: 1.735866e-04\n",
      "  Step 2200/4407 - Loss: 2.0305 | PPL: 7.62 | LR: 1.734693e-04\n",
      "  Step 2200/4407 - Loss: 2.0305 | PPL: 7.62 | LR: 1.734693e-04\n",
      "  Step 2300/4407 - Loss: 2.0322 | PPL: 7.63 | LR: 1.733519e-04\n",
      "  Step 2300/4407 - Loss: 2.0322 | PPL: 7.63 | LR: 1.733519e-04\n",
      "  Step 2400/4407 - Loss: 2.0316 | PPL: 7.63 | LR: 1.732346e-04\n",
      "  Step 2400/4407 - Loss: 2.0316 | PPL: 7.63 | LR: 1.732346e-04\n",
      "  Step 2500/4407 - Loss: 2.0296 | PPL: 7.61 | LR: 1.731172e-04\n",
      "  Step 2500/4407 - Loss: 2.0296 | PPL: 7.61 | LR: 1.731172e-04\n",
      "  Step 2600/4407 - Loss: 2.0301 | PPL: 7.61 | LR: 1.729998e-04\n",
      "  Step 2600/4407 - Loss: 2.0301 | PPL: 7.61 | LR: 1.729998e-04\n",
      "  Step 2700/4407 - Loss: 2.0319 | PPL: 7.63 | LR: 1.728824e-04\n",
      "  Step 2700/4407 - Loss: 2.0319 | PPL: 7.63 | LR: 1.728824e-04\n",
      "  Step 2800/4407 - Loss: 2.0335 | PPL: 7.64 | LR: 1.727649e-04\n",
      "  Step 2800/4407 - Loss: 2.0335 | PPL: 7.64 | LR: 1.727649e-04\n",
      "  Step 2900/4407 - Loss: 2.0352 | PPL: 7.65 | LR: 1.726475e-04\n",
      "  Step 2900/4407 - Loss: 2.0352 | PPL: 7.65 | LR: 1.726475e-04\n",
      "  Step 3000/4407 - Loss: 2.0357 | PPL: 7.66 | LR: 1.725300e-04\n",
      "  Step 3000/4407 - Loss: 2.0357 | PPL: 7.66 | LR: 1.725300e-04\n",
      "  Step 3100/4407 - Loss: 2.0369 | PPL: 7.67 | LR: 1.724126e-04\n",
      "  Step 3100/4407 - Loss: 2.0369 | PPL: 7.67 | LR: 1.724126e-04\n",
      "  Step 3200/4407 - Loss: 2.0373 | PPL: 7.67 | LR: 1.722951e-04\n",
      "  Step 3200/4407 - Loss: 2.0373 | PPL: 7.67 | LR: 1.722951e-04\n",
      "  Step 3300/4407 - Loss: 2.0390 | PPL: 7.68 | LR: 1.721776e-04\n",
      "  Step 3300/4407 - Loss: 2.0390 | PPL: 7.68 | LR: 1.721776e-04\n",
      "  Step 3400/4407 - Loss: 2.0390 | PPL: 7.68 | LR: 1.720601e-04\n",
      "  Step 3400/4407 - Loss: 2.0390 | PPL: 7.68 | LR: 1.720601e-04\n",
      "  Step 3500/4407 - Loss: 2.0400 | PPL: 7.69 | LR: 1.719425e-04\n",
      "  Step 3500/4407 - Loss: 2.0400 | PPL: 7.69 | LR: 1.719425e-04\n",
      "  Step 3600/4407 - Loss: 2.0410 | PPL: 7.70 | LR: 1.718250e-04\n",
      "  Step 3600/4407 - Loss: 2.0410 | PPL: 7.70 | LR: 1.718250e-04\n",
      "  Step 3700/4407 - Loss: 2.0402 | PPL: 7.69 | LR: 1.717075e-04\n",
      "  Step 3700/4407 - Loss: 2.0402 | PPL: 7.69 | LR: 1.717075e-04\n",
      "  Step 3800/4407 - Loss: 2.0399 | PPL: 7.69 | LR: 1.715899e-04\n",
      "  Step 3800/4407 - Loss: 2.0399 | PPL: 7.69 | LR: 1.715899e-04\n",
      "  Step 3900/4407 - Loss: 2.0398 | PPL: 7.69 | LR: 1.714723e-04\n",
      "  Step 3900/4407 - Loss: 2.0398 | PPL: 7.69 | LR: 1.714723e-04\n",
      "  Step 4000/4407 - Loss: 2.0409 | PPL: 7.70 | LR: 1.713547e-04\n",
      "  Step 4000/4407 - Loss: 2.0409 | PPL: 7.70 | LR: 1.713547e-04\n",
      "  Step 4100/4407 - Loss: 2.0414 | PPL: 7.70 | LR: 1.712371e-04\n",
      "  Step 4100/4407 - Loss: 2.0414 | PPL: 7.70 | LR: 1.712371e-04\n",
      "  Step 4200/4407 - Loss: 2.0420 | PPL: 7.71 | LR: 1.711195e-04\n",
      "  Step 4200/4407 - Loss: 2.0420 | PPL: 7.71 | LR: 1.711195e-04\n",
      "  Step 4300/4407 - Loss: 2.0425 | PPL: 7.71 | LR: 1.710018e-04\n",
      "  Step 4300/4407 - Loss: 2.0425 | PPL: 7.71 | LR: 1.710018e-04\n",
      "  Step 4400/4407 - Loss: 2.0436 | PPL: 7.72 | LR: 1.708842e-04\n",
      "  Step 4400/4407 - Loss: 2.0436 | PPL: 7.72 | LR: 1.708842e-04\n",
      "Epoch 51/100 | Train Loss: 2.0438 | Train PPL: 7.72 | Val Loss: 2.6032 | Val PPL: 13.51\n",
      "Epoch 51/100 | Train Loss: 2.0438 | Train PPL: 7.72 | Val Loss: 2.6032 | Val PPL: 13.51\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.72\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.72\n",
      "  Step 100/4407 - Loss: 1.9754 | PPL: 7.21 | LR: 1.707583e-04\n",
      "  Step 100/4407 - Loss: 1.9754 | PPL: 7.21 | LR: 1.707583e-04\n",
      "  Step 200/4407 - Loss: 2.0057 | PPL: 7.43 | LR: 1.706406e-04\n",
      "  Step 200/4407 - Loss: 2.0057 | PPL: 7.43 | LR: 1.706406e-04\n",
      "  Step 300/4407 - Loss: 2.0134 | PPL: 7.49 | LR: 1.705229e-04\n",
      "  Step 300/4407 - Loss: 2.0134 | PPL: 7.49 | LR: 1.705229e-04\n",
      "  Step 400/4407 - Loss: 2.0174 | PPL: 7.52 | LR: 1.704052e-04\n",
      "  Step 400/4407 - Loss: 2.0174 | PPL: 7.52 | LR: 1.704052e-04\n",
      "  Step 500/4407 - Loss: 2.0109 | PPL: 7.47 | LR: 1.702875e-04\n",
      "  Step 500/4407 - Loss: 2.0109 | PPL: 7.47 | LR: 1.702875e-04\n",
      "  Step 600/4407 - Loss: 2.0146 | PPL: 7.50 | LR: 1.701698e-04\n",
      "  Step 600/4407 - Loss: 2.0146 | PPL: 7.50 | LR: 1.701698e-04\n",
      "  Step 700/4407 - Loss: 2.0131 | PPL: 7.49 | LR: 1.700521e-04\n",
      "  Step 700/4407 - Loss: 2.0131 | PPL: 7.49 | LR: 1.700521e-04\n",
      "  Step 800/4407 - Loss: 2.0156 | PPL: 7.51 | LR: 1.699343e-04\n",
      "  Step 800/4407 - Loss: 2.0156 | PPL: 7.51 | LR: 1.699343e-04\n",
      "  Step 900/4407 - Loss: 2.0184 | PPL: 7.53 | LR: 1.698166e-04\n",
      "  Step 900/4407 - Loss: 2.0184 | PPL: 7.53 | LR: 1.698166e-04\n",
      "  Step 1000/4407 - Loss: 2.0209 | PPL: 7.54 | LR: 1.696988e-04\n",
      "  Step 1000/4407 - Loss: 2.0209 | PPL: 7.54 | LR: 1.696988e-04\n",
      "  Step 1100/4407 - Loss: 2.0193 | PPL: 7.53 | LR: 1.695810e-04\n",
      "  Step 1100/4407 - Loss: 2.0193 | PPL: 7.53 | LR: 1.695810e-04\n",
      "  Step 1200/4407 - Loss: 2.0213 | PPL: 7.55 | LR: 1.694632e-04\n",
      "  Step 1200/4407 - Loss: 2.0213 | PPL: 7.55 | LR: 1.694632e-04\n",
      "  Step 1300/4407 - Loss: 2.0235 | PPL: 7.56 | LR: 1.693454e-04\n",
      "  Step 1300/4407 - Loss: 2.0235 | PPL: 7.56 | LR: 1.693454e-04\n",
      "  Step 1400/4407 - Loss: 2.0239 | PPL: 7.57 | LR: 1.692276e-04\n",
      "  Step 1400/4407 - Loss: 2.0239 | PPL: 7.57 | LR: 1.692276e-04\n",
      "  Step 1500/4407 - Loss: 2.0231 | PPL: 7.56 | LR: 1.691097e-04\n",
      "  Step 1500/4407 - Loss: 2.0231 | PPL: 7.56 | LR: 1.691097e-04\n",
      "  Step 1600/4407 - Loss: 2.0227 | PPL: 7.56 | LR: 1.689919e-04\n",
      "  Step 1600/4407 - Loss: 2.0227 | PPL: 7.56 | LR: 1.689919e-04\n",
      "  Step 1700/4407 - Loss: 2.0252 | PPL: 7.58 | LR: 1.688740e-04\n",
      "  Step 1700/4407 - Loss: 2.0252 | PPL: 7.58 | LR: 1.688740e-04\n",
      "  Step 1800/4407 - Loss: 2.0253 | PPL: 7.58 | LR: 1.687561e-04\n",
      "  Step 1800/4407 - Loss: 2.0253 | PPL: 7.58 | LR: 1.687561e-04\n",
      "  Step 1900/4407 - Loss: 2.0275 | PPL: 7.59 | LR: 1.686383e-04\n",
      "  Step 1900/4407 - Loss: 2.0275 | PPL: 7.59 | LR: 1.686383e-04\n",
      "  Step 2000/4407 - Loss: 2.0273 | PPL: 7.59 | LR: 1.685204e-04\n",
      "  Step 2000/4407 - Loss: 2.0273 | PPL: 7.59 | LR: 1.685204e-04\n",
      "  Step 2100/4407 - Loss: 2.0271 | PPL: 7.59 | LR: 1.684024e-04\n",
      "  Step 2100/4407 - Loss: 2.0271 | PPL: 7.59 | LR: 1.684024e-04\n",
      "  Step 2200/4407 - Loss: 2.0266 | PPL: 7.59 | LR: 1.682845e-04\n",
      "  Step 2200/4407 - Loss: 2.0266 | PPL: 7.59 | LR: 1.682845e-04\n",
      "  Step 2300/4407 - Loss: 2.0269 | PPL: 7.59 | LR: 1.681666e-04\n",
      "  Step 2300/4407 - Loss: 2.0269 | PPL: 7.59 | LR: 1.681666e-04\n",
      "  Step 2400/4407 - Loss: 2.0277 | PPL: 7.60 | LR: 1.680487e-04\n",
      "  Step 2400/4407 - Loss: 2.0277 | PPL: 7.60 | LR: 1.680487e-04\n",
      "  Step 2500/4407 - Loss: 2.0284 | PPL: 7.60 | LR: 1.679307e-04\n",
      "  Step 2500/4407 - Loss: 2.0284 | PPL: 7.60 | LR: 1.679307e-04\n",
      "  Step 2600/4407 - Loss: 2.0285 | PPL: 7.60 | LR: 1.678127e-04\n",
      "  Step 2600/4407 - Loss: 2.0285 | PPL: 7.60 | LR: 1.678127e-04\n",
      "  Step 2700/4407 - Loss: 2.0294 | PPL: 7.61 | LR: 1.676948e-04\n",
      "  Step 2700/4407 - Loss: 2.0294 | PPL: 7.61 | LR: 1.676948e-04\n",
      "  Step 2800/4407 - Loss: 2.0306 | PPL: 7.62 | LR: 1.675768e-04\n",
      "  Step 2800/4407 - Loss: 2.0306 | PPL: 7.62 | LR: 1.675768e-04\n",
      "  Step 2900/4407 - Loss: 2.0312 | PPL: 7.62 | LR: 1.674588e-04\n",
      "  Step 2900/4407 - Loss: 2.0312 | PPL: 7.62 | LR: 1.674588e-04\n",
      "  Step 3000/4407 - Loss: 2.0321 | PPL: 7.63 | LR: 1.673408e-04\n",
      "  Step 3000/4407 - Loss: 2.0321 | PPL: 7.63 | LR: 1.673408e-04\n",
      "  Step 3100/4407 - Loss: 2.0340 | PPL: 7.64 | LR: 1.672228e-04\n",
      "  Step 3100/4407 - Loss: 2.0340 | PPL: 7.64 | LR: 1.672228e-04\n",
      "  Step 3200/4407 - Loss: 2.0341 | PPL: 7.65 | LR: 1.671047e-04\n",
      "  Step 3200/4407 - Loss: 2.0341 | PPL: 7.65 | LR: 1.671047e-04\n",
      "  Step 3300/4407 - Loss: 2.0343 | PPL: 7.65 | LR: 1.669867e-04\n",
      "  Step 3300/4407 - Loss: 2.0343 | PPL: 7.65 | LR: 1.669867e-04\n",
      "  Step 3400/4407 - Loss: 2.0351 | PPL: 7.65 | LR: 1.668686e-04\n",
      "  Step 3400/4407 - Loss: 2.0351 | PPL: 7.65 | LR: 1.668686e-04\n",
      "  Step 3500/4407 - Loss: 2.0368 | PPL: 7.67 | LR: 1.667506e-04\n",
      "  Step 3500/4407 - Loss: 2.0368 | PPL: 7.67 | LR: 1.667506e-04\n",
      "  Step 3600/4407 - Loss: 2.0363 | PPL: 7.66 | LR: 1.666325e-04\n",
      "  Step 3600/4407 - Loss: 2.0363 | PPL: 7.66 | LR: 1.666325e-04\n",
      "  Step 3700/4407 - Loss: 2.0369 | PPL: 7.67 | LR: 1.665144e-04\n",
      "  Step 3700/4407 - Loss: 2.0369 | PPL: 7.67 | LR: 1.665144e-04\n",
      "  Step 3800/4407 - Loss: 2.0378 | PPL: 7.67 | LR: 1.663963e-04\n",
      "  Step 3800/4407 - Loss: 2.0378 | PPL: 7.67 | LR: 1.663963e-04\n",
      "  Step 3900/4407 - Loss: 2.0383 | PPL: 7.68 | LR: 1.662782e-04\n",
      "  Step 3900/4407 - Loss: 2.0383 | PPL: 7.68 | LR: 1.662782e-04\n",
      "  Step 4000/4407 - Loss: 2.0387 | PPL: 7.68 | LR: 1.661601e-04\n",
      "  Step 4000/4407 - Loss: 2.0387 | PPL: 7.68 | LR: 1.661601e-04\n",
      "  Step 4100/4407 - Loss: 2.0394 | PPL: 7.69 | LR: 1.660420e-04\n",
      "  Step 4100/4407 - Loss: 2.0394 | PPL: 7.69 | LR: 1.660420e-04\n",
      "  Step 4200/4407 - Loss: 2.0389 | PPL: 7.68 | LR: 1.659238e-04\n",
      "  Step 4200/4407 - Loss: 2.0389 | PPL: 7.68 | LR: 1.659238e-04\n",
      "  Step 4300/4407 - Loss: 2.0395 | PPL: 7.69 | LR: 1.658057e-04\n",
      "  Step 4300/4407 - Loss: 2.0395 | PPL: 7.69 | LR: 1.658057e-04\n",
      "  Step 4400/4407 - Loss: 2.0397 | PPL: 7.69 | LR: 1.656875e-04\n",
      "  Step 4400/4407 - Loss: 2.0397 | PPL: 7.69 | LR: 1.656875e-04\n",
      "Epoch 52/100 | Train Loss: 2.0396 | Train PPL: 7.69 | Val Loss: 2.5982 | Val PPL: 13.44\n",
      "Epoch 52/100 | Train Loss: 2.0396 | Train PPL: 7.69 | Val Loss: 2.5982 | Val PPL: 13.44\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.69\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.69\n",
      "  Step 100/4407 - Loss: 1.9989 | PPL: 7.38 | LR: 1.655611e-04\n",
      "  Step 100/4407 - Loss: 1.9989 | PPL: 7.38 | LR: 1.655611e-04\n",
      "  Step 200/4407 - Loss: 1.9922 | PPL: 7.33 | LR: 1.654429e-04\n",
      "  Step 200/4407 - Loss: 1.9922 | PPL: 7.33 | LR: 1.654429e-04\n",
      "  Step 300/4407 - Loss: 1.9868 | PPL: 7.29 | LR: 1.653247e-04\n",
      "  Step 300/4407 - Loss: 1.9868 | PPL: 7.29 | LR: 1.653247e-04\n",
      "  Step 400/4407 - Loss: 1.9887 | PPL: 7.31 | LR: 1.652066e-04\n",
      "  Step 400/4407 - Loss: 1.9887 | PPL: 7.31 | LR: 1.652066e-04\n",
      "  Step 500/4407 - Loss: 1.9892 | PPL: 7.31 | LR: 1.650883e-04\n",
      "  Step 500/4407 - Loss: 1.9892 | PPL: 7.31 | LR: 1.650883e-04\n",
      "  Step 600/4407 - Loss: 1.9885 | PPL: 7.30 | LR: 1.649701e-04\n",
      "  Step 600/4407 - Loss: 1.9885 | PPL: 7.30 | LR: 1.649701e-04\n",
      "  Step 700/4407 - Loss: 1.9932 | PPL: 7.34 | LR: 1.648519e-04\n",
      "  Step 700/4407 - Loss: 1.9932 | PPL: 7.34 | LR: 1.648519e-04\n",
      "  Step 800/4407 - Loss: 1.9957 | PPL: 7.36 | LR: 1.647337e-04\n",
      "  Step 800/4407 - Loss: 1.9957 | PPL: 7.36 | LR: 1.647337e-04\n",
      "  Step 900/4407 - Loss: 1.9973 | PPL: 7.37 | LR: 1.646154e-04\n",
      "  Step 900/4407 - Loss: 1.9973 | PPL: 7.37 | LR: 1.646154e-04\n",
      "  Step 1000/4407 - Loss: 1.9994 | PPL: 7.38 | LR: 1.644972e-04\n",
      "  Step 1000/4407 - Loss: 1.9994 | PPL: 7.38 | LR: 1.644972e-04\n",
      "  Step 1100/4407 - Loss: 2.0014 | PPL: 7.40 | LR: 1.643789e-04\n",
      "  Step 1100/4407 - Loss: 2.0014 | PPL: 7.40 | LR: 1.643789e-04\n",
      "  Step 1200/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.642607e-04\n",
      "  Step 1200/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.642607e-04\n",
      "  Step 1300/4407 - Loss: 2.0039 | PPL: 7.42 | LR: 1.641424e-04\n",
      "  Step 1300/4407 - Loss: 2.0039 | PPL: 7.42 | LR: 1.641424e-04\n",
      "  Step 1400/4407 - Loss: 2.0071 | PPL: 7.44 | LR: 1.640241e-04\n",
      "  Step 1400/4407 - Loss: 2.0071 | PPL: 7.44 | LR: 1.640241e-04\n",
      "  Step 1500/4407 - Loss: 2.0053 | PPL: 7.43 | LR: 1.639058e-04\n",
      "  Step 1500/4407 - Loss: 2.0053 | PPL: 7.43 | LR: 1.639058e-04\n",
      "  Step 1600/4407 - Loss: 2.0073 | PPL: 7.44 | LR: 1.637875e-04\n",
      "  Step 1600/4407 - Loss: 2.0073 | PPL: 7.44 | LR: 1.637875e-04\n",
      "  Step 1700/4407 - Loss: 2.0064 | PPL: 7.44 | LR: 1.636692e-04\n",
      "  Step 1700/4407 - Loss: 2.0064 | PPL: 7.44 | LR: 1.636692e-04\n",
      "  Step 1800/4407 - Loss: 2.0090 | PPL: 7.46 | LR: 1.635509e-04\n",
      "  Step 1800/4407 - Loss: 2.0090 | PPL: 7.46 | LR: 1.635509e-04\n",
      "  Step 1900/4407 - Loss: 2.0110 | PPL: 7.47 | LR: 1.634325e-04\n",
      "  Step 1900/4407 - Loss: 2.0110 | PPL: 7.47 | LR: 1.634325e-04\n",
      "  Step 2000/4407 - Loss: 2.0130 | PPL: 7.49 | LR: 1.633142e-04\n",
      "  Step 2000/4407 - Loss: 2.0130 | PPL: 7.49 | LR: 1.633142e-04\n",
      "  Step 2100/4407 - Loss: 2.0152 | PPL: 7.50 | LR: 1.631959e-04\n",
      "  Step 2100/4407 - Loss: 2.0152 | PPL: 7.50 | LR: 1.631959e-04\n",
      "  Step 2200/4407 - Loss: 2.0155 | PPL: 7.50 | LR: 1.630775e-04\n",
      "  Step 2200/4407 - Loss: 2.0155 | PPL: 7.50 | LR: 1.630775e-04\n",
      "  Step 2300/4407 - Loss: 2.0178 | PPL: 7.52 | LR: 1.629591e-04\n",
      "  Step 2300/4407 - Loss: 2.0178 | PPL: 7.52 | LR: 1.629591e-04\n",
      "  Step 2400/4407 - Loss: 2.0184 | PPL: 7.53 | LR: 1.628408e-04\n",
      "  Step 2400/4407 - Loss: 2.0184 | PPL: 7.53 | LR: 1.628408e-04\n",
      "  Step 2500/4407 - Loss: 2.0200 | PPL: 7.54 | LR: 1.627224e-04\n",
      "  Step 2500/4407 - Loss: 2.0200 | PPL: 7.54 | LR: 1.627224e-04\n",
      "  Step 2600/4407 - Loss: 2.0214 | PPL: 7.55 | LR: 1.626040e-04\n",
      "  Step 2600/4407 - Loss: 2.0214 | PPL: 7.55 | LR: 1.626040e-04\n",
      "  Step 2700/4407 - Loss: 2.0217 | PPL: 7.55 | LR: 1.624856e-04\n",
      "  Step 2700/4407 - Loss: 2.0217 | PPL: 7.55 | LR: 1.624856e-04\n",
      "  Step 2800/4407 - Loss: 2.0228 | PPL: 7.56 | LR: 1.623672e-04\n",
      "  Step 2800/4407 - Loss: 2.0228 | PPL: 7.56 | LR: 1.623672e-04\n",
      "  Step 2900/4407 - Loss: 2.0244 | PPL: 7.57 | LR: 1.622488e-04\n",
      "  Step 2900/4407 - Loss: 2.0244 | PPL: 7.57 | LR: 1.622488e-04\n",
      "  Step 3000/4407 - Loss: 2.0244 | PPL: 7.57 | LR: 1.621304e-04\n",
      "  Step 3000/4407 - Loss: 2.0244 | PPL: 7.57 | LR: 1.621304e-04\n",
      "  Step 3100/4407 - Loss: 2.0253 | PPL: 7.58 | LR: 1.620120e-04\n",
      "  Step 3100/4407 - Loss: 2.0253 | PPL: 7.58 | LR: 1.620120e-04\n",
      "  Step 3200/4407 - Loss: 2.0259 | PPL: 7.58 | LR: 1.618935e-04\n",
      "  Step 3200/4407 - Loss: 2.0259 | PPL: 7.58 | LR: 1.618935e-04\n",
      "  Step 3300/4407 - Loss: 2.0273 | PPL: 7.59 | LR: 1.617751e-04\n",
      "  Step 3300/4407 - Loss: 2.0273 | PPL: 7.59 | LR: 1.617751e-04\n",
      "  Step 3400/4407 - Loss: 2.0291 | PPL: 7.61 | LR: 1.616566e-04\n",
      "  Step 3400/4407 - Loss: 2.0291 | PPL: 7.61 | LR: 1.616566e-04\n",
      "  Step 3500/4407 - Loss: 2.0304 | PPL: 7.62 | LR: 1.615382e-04\n",
      "  Step 3500/4407 - Loss: 2.0304 | PPL: 7.62 | LR: 1.615382e-04\n",
      "  Step 3600/4407 - Loss: 2.0310 | PPL: 7.62 | LR: 1.614197e-04\n",
      "  Step 3600/4407 - Loss: 2.0310 | PPL: 7.62 | LR: 1.614197e-04\n",
      "  Step 3700/4407 - Loss: 2.0311 | PPL: 7.62 | LR: 1.613012e-04\n",
      "  Step 3700/4407 - Loss: 2.0311 | PPL: 7.62 | LR: 1.613012e-04\n",
      "  Step 3800/4407 - Loss: 2.0318 | PPL: 7.63 | LR: 1.611828e-04\n",
      "  Step 3800/4407 - Loss: 2.0318 | PPL: 7.63 | LR: 1.611828e-04\n",
      "  Step 3900/4407 - Loss: 2.0326 | PPL: 7.63 | LR: 1.610643e-04\n",
      "  Step 3900/4407 - Loss: 2.0326 | PPL: 7.63 | LR: 1.610643e-04\n",
      "  Step 4000/4407 - Loss: 2.0328 | PPL: 7.64 | LR: 1.609458e-04\n",
      "  Step 4000/4407 - Loss: 2.0328 | PPL: 7.64 | LR: 1.609458e-04\n",
      "  Step 4100/4407 - Loss: 2.0327 | PPL: 7.63 | LR: 1.608273e-04\n",
      "  Step 4100/4407 - Loss: 2.0327 | PPL: 7.63 | LR: 1.608273e-04\n",
      "  Step 4200/4407 - Loss: 2.0337 | PPL: 7.64 | LR: 1.607088e-04\n",
      "  Step 4200/4407 - Loss: 2.0337 | PPL: 7.64 | LR: 1.607088e-04\n",
      "  Step 4300/4407 - Loss: 2.0349 | PPL: 7.65 | LR: 1.605903e-04\n",
      "  Step 4300/4407 - Loss: 2.0349 | PPL: 7.65 | LR: 1.605903e-04\n",
      "  Step 4400/4407 - Loss: 2.0358 | PPL: 7.66 | LR: 1.604718e-04\n",
      "  Step 4400/4407 - Loss: 2.0358 | PPL: 7.66 | LR: 1.604718e-04\n",
      "Epoch 53/100 | Train Loss: 2.0357 | Train PPL: 7.66 | Val Loss: 2.6087 | Val PPL: 13.58\n",
      "Epoch 53/100 | Train Loss: 2.0357 | Train PPL: 7.66 | Val Loss: 2.6087 | Val PPL: 13.58\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.66\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.66\n",
      "  Step 100/4407 - Loss: 2.0153 | PPL: 7.50 | LR: 1.603449e-04\n",
      "  Step 100/4407 - Loss: 2.0153 | PPL: 7.50 | LR: 1.603449e-04\n",
      "  Step 200/4407 - Loss: 2.0265 | PPL: 7.59 | LR: 1.602264e-04\n",
      "  Step 200/4407 - Loss: 2.0265 | PPL: 7.59 | LR: 1.602264e-04\n",
      "  Step 300/4407 - Loss: 2.0312 | PPL: 7.62 | LR: 1.601079e-04\n",
      "  Step 300/4407 - Loss: 2.0312 | PPL: 7.62 | LR: 1.601079e-04\n",
      "  Step 400/4407 - Loss: 2.0183 | PPL: 7.53 | LR: 1.599893e-04\n",
      "  Step 400/4407 - Loss: 2.0183 | PPL: 7.53 | LR: 1.599893e-04\n",
      "  Step 500/4407 - Loss: 2.0184 | PPL: 7.53 | LR: 1.598708e-04\n",
      "  Step 500/4407 - Loss: 2.0184 | PPL: 7.53 | LR: 1.598708e-04\n",
      "  Step 600/4407 - Loss: 2.0152 | PPL: 7.50 | LR: 1.597522e-04\n",
      "  Step 600/4407 - Loss: 2.0152 | PPL: 7.50 | LR: 1.597522e-04\n",
      "  Step 700/4407 - Loss: 2.0106 | PPL: 7.47 | LR: 1.596337e-04\n",
      "  Step 700/4407 - Loss: 2.0106 | PPL: 7.47 | LR: 1.596337e-04\n",
      "  Step 800/4407 - Loss: 2.0116 | PPL: 7.48 | LR: 1.595151e-04\n",
      "  Step 800/4407 - Loss: 2.0116 | PPL: 7.48 | LR: 1.595151e-04\n",
      "  Step 900/4407 - Loss: 2.0089 | PPL: 7.45 | LR: 1.593965e-04\n",
      "  Step 900/4407 - Loss: 2.0089 | PPL: 7.45 | LR: 1.593965e-04\n",
      "  Step 1000/4407 - Loss: 2.0111 | PPL: 7.47 | LR: 1.592779e-04\n",
      "  Step 1000/4407 - Loss: 2.0111 | PPL: 7.47 | LR: 1.592779e-04\n",
      "  Step 1100/4407 - Loss: 2.0102 | PPL: 7.46 | LR: 1.591594e-04\n",
      "  Step 1100/4407 - Loss: 2.0102 | PPL: 7.46 | LR: 1.591594e-04\n",
      "  Step 1200/4407 - Loss: 2.0120 | PPL: 7.48 | LR: 1.590408e-04\n",
      "  Step 1200/4407 - Loss: 2.0120 | PPL: 7.48 | LR: 1.590408e-04\n",
      "  Step 1300/4407 - Loss: 2.0109 | PPL: 7.47 | LR: 1.589222e-04\n",
      "  Step 1300/4407 - Loss: 2.0109 | PPL: 7.47 | LR: 1.589222e-04\n",
      "  Step 1400/4407 - Loss: 2.0128 | PPL: 7.48 | LR: 1.588036e-04\n",
      "  Step 1400/4407 - Loss: 2.0128 | PPL: 7.48 | LR: 1.588036e-04\n",
      "  Step 1500/4407 - Loss: 2.0110 | PPL: 7.47 | LR: 1.586850e-04\n",
      "  Step 1500/4407 - Loss: 2.0110 | PPL: 7.47 | LR: 1.586850e-04\n",
      "  Step 1600/4407 - Loss: 2.0131 | PPL: 7.49 | LR: 1.585663e-04\n",
      "  Step 1600/4407 - Loss: 2.0131 | PPL: 7.49 | LR: 1.585663e-04\n",
      "  Step 1700/4407 - Loss: 2.0141 | PPL: 7.49 | LR: 1.584477e-04\n",
      "  Step 1700/4407 - Loss: 2.0141 | PPL: 7.49 | LR: 1.584477e-04\n",
      "  Step 1800/4407 - Loss: 2.0123 | PPL: 7.48 | LR: 1.583291e-04\n",
      "  Step 1800/4407 - Loss: 2.0123 | PPL: 7.48 | LR: 1.583291e-04\n",
      "  Step 1900/4407 - Loss: 2.0130 | PPL: 7.49 | LR: 1.582105e-04\n",
      "  Step 1900/4407 - Loss: 2.0130 | PPL: 7.49 | LR: 1.582105e-04\n",
      "  Step 2000/4407 - Loss: 2.0137 | PPL: 7.49 | LR: 1.580918e-04\n",
      "  Step 2000/4407 - Loss: 2.0137 | PPL: 7.49 | LR: 1.580918e-04\n",
      "  Step 2100/4407 - Loss: 2.0149 | PPL: 7.50 | LR: 1.579732e-04\n",
      "  Step 2100/4407 - Loss: 2.0149 | PPL: 7.50 | LR: 1.579732e-04\n",
      "  Step 2200/4407 - Loss: 2.0164 | PPL: 7.51 | LR: 1.578545e-04\n",
      "  Step 2200/4407 - Loss: 2.0164 | PPL: 7.51 | LR: 1.578545e-04\n",
      "  Step 2300/4407 - Loss: 2.0168 | PPL: 7.51 | LR: 1.577359e-04\n",
      "  Step 2300/4407 - Loss: 2.0168 | PPL: 7.51 | LR: 1.577359e-04\n",
      "  Step 2400/4407 - Loss: 2.0165 | PPL: 7.51 | LR: 1.576172e-04\n",
      "  Step 2400/4407 - Loss: 2.0165 | PPL: 7.51 | LR: 1.576172e-04\n",
      "  Step 2500/4407 - Loss: 2.0167 | PPL: 7.51 | LR: 1.574986e-04\n",
      "  Step 2500/4407 - Loss: 2.0167 | PPL: 7.51 | LR: 1.574986e-04\n",
      "  Step 2600/4407 - Loss: 2.0180 | PPL: 7.52 | LR: 1.573799e-04\n",
      "  Step 2600/4407 - Loss: 2.0180 | PPL: 7.52 | LR: 1.573799e-04\n",
      "  Step 2700/4407 - Loss: 2.0181 | PPL: 7.52 | LR: 1.572612e-04\n",
      "  Step 2700/4407 - Loss: 2.0181 | PPL: 7.52 | LR: 1.572612e-04\n",
      "  Step 2800/4407 - Loss: 2.0185 | PPL: 7.53 | LR: 1.571426e-04\n",
      "  Step 2800/4407 - Loss: 2.0185 | PPL: 7.53 | LR: 1.571426e-04\n",
      "  Step 2900/4407 - Loss: 2.0209 | PPL: 7.54 | LR: 1.570239e-04\n",
      "  Step 2900/4407 - Loss: 2.0209 | PPL: 7.54 | LR: 1.570239e-04\n",
      "  Step 3000/4407 - Loss: 2.0224 | PPL: 7.56 | LR: 1.569052e-04\n",
      "  Step 3000/4407 - Loss: 2.0224 | PPL: 7.56 | LR: 1.569052e-04\n",
      "  Step 3100/4407 - Loss: 2.0233 | PPL: 7.56 | LR: 1.567865e-04\n",
      "  Step 3100/4407 - Loss: 2.0233 | PPL: 7.56 | LR: 1.567865e-04\n",
      "  Step 3200/4407 - Loss: 2.0233 | PPL: 7.56 | LR: 1.566678e-04\n",
      "  Step 3200/4407 - Loss: 2.0233 | PPL: 7.56 | LR: 1.566678e-04\n",
      "  Step 3300/4407 - Loss: 2.0228 | PPL: 7.56 | LR: 1.565491e-04\n",
      "  Step 3300/4407 - Loss: 2.0228 | PPL: 7.56 | LR: 1.565491e-04\n",
      "  Step 3400/4407 - Loss: 2.0241 | PPL: 7.57 | LR: 1.564304e-04\n",
      "  Step 3400/4407 - Loss: 2.0241 | PPL: 7.57 | LR: 1.564304e-04\n",
      "  Step 3500/4407 - Loss: 2.0253 | PPL: 7.58 | LR: 1.563117e-04\n",
      "  Step 3500/4407 - Loss: 2.0253 | PPL: 7.58 | LR: 1.563117e-04\n",
      "  Step 3600/4407 - Loss: 2.0257 | PPL: 7.58 | LR: 1.561930e-04\n",
      "  Step 3600/4407 - Loss: 2.0257 | PPL: 7.58 | LR: 1.561930e-04\n",
      "  Step 3700/4407 - Loss: 2.0266 | PPL: 7.59 | LR: 1.560743e-04\n",
      "  Step 3700/4407 - Loss: 2.0266 | PPL: 7.59 | LR: 1.560743e-04\n",
      "  Step 3800/4407 - Loss: 2.0274 | PPL: 7.59 | LR: 1.559556e-04\n",
      "  Step 3800/4407 - Loss: 2.0274 | PPL: 7.59 | LR: 1.559556e-04\n",
      "  Step 3900/4407 - Loss: 2.0264 | PPL: 7.59 | LR: 1.558369e-04\n",
      "  Step 3900/4407 - Loss: 2.0264 | PPL: 7.59 | LR: 1.558369e-04\n",
      "  Step 4000/4407 - Loss: 2.0271 | PPL: 7.59 | LR: 1.557182e-04\n",
      "  Step 4000/4407 - Loss: 2.0271 | PPL: 7.59 | LR: 1.557182e-04\n",
      "  Step 4100/4407 - Loss: 2.0280 | PPL: 7.60 | LR: 1.555994e-04\n",
      "  Step 4100/4407 - Loss: 2.0280 | PPL: 7.60 | LR: 1.555994e-04\n",
      "  Step 4200/4407 - Loss: 2.0289 | PPL: 7.61 | LR: 1.554807e-04\n",
      "  Step 4200/4407 - Loss: 2.0289 | PPL: 7.61 | LR: 1.554807e-04\n",
      "  Step 4300/4407 - Loss: 2.0311 | PPL: 7.62 | LR: 1.553620e-04\n",
      "  Step 4300/4407 - Loss: 2.0311 | PPL: 7.62 | LR: 1.553620e-04\n",
      "  Step 4400/4407 - Loss: 2.0316 | PPL: 7.63 | LR: 1.552432e-04\n",
      "  Step 4400/4407 - Loss: 2.0316 | PPL: 7.63 | LR: 1.552432e-04\n",
      "Epoch 54/100 | Train Loss: 2.0319 | Train PPL: 7.63 | Val Loss: 2.5996 | Val PPL: 13.46\n",
      "Epoch 54/100 | Train Loss: 2.0319 | Train PPL: 7.63 | Val Loss: 2.5996 | Val PPL: 13.46\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.63\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.63\n",
      "  Step 100/4407 - Loss: 1.9892 | PPL: 7.31 | LR: 1.551162e-04\n",
      "  Step 100/4407 - Loss: 1.9892 | PPL: 7.31 | LR: 1.551162e-04\n",
      "  Step 200/4407 - Loss: 1.9885 | PPL: 7.30 | LR: 1.549974e-04\n",
      "  Step 200/4407 - Loss: 1.9885 | PPL: 7.30 | LR: 1.549974e-04\n",
      "  Step 300/4407 - Loss: 1.9975 | PPL: 7.37 | LR: 1.548787e-04\n",
      "  Step 300/4407 - Loss: 1.9975 | PPL: 7.37 | LR: 1.548787e-04\n",
      "  Step 400/4407 - Loss: 2.0081 | PPL: 7.45 | LR: 1.547599e-04\n",
      "  Step 400/4407 - Loss: 2.0081 | PPL: 7.45 | LR: 1.547599e-04\n",
      "  Step 500/4407 - Loss: 2.0160 | PPL: 7.51 | LR: 1.546412e-04\n",
      "  Step 500/4407 - Loss: 2.0160 | PPL: 7.51 | LR: 1.546412e-04\n",
      "  Step 600/4407 - Loss: 2.0120 | PPL: 7.48 | LR: 1.545224e-04\n",
      "  Step 600/4407 - Loss: 2.0120 | PPL: 7.48 | LR: 1.545224e-04\n",
      "  Step 700/4407 - Loss: 2.0088 | PPL: 7.45 | LR: 1.544037e-04\n",
      "  Step 700/4407 - Loss: 2.0088 | PPL: 7.45 | LR: 1.544037e-04\n",
      "  Step 800/4407 - Loss: 2.0092 | PPL: 7.46 | LR: 1.542849e-04\n",
      "  Step 800/4407 - Loss: 2.0092 | PPL: 7.46 | LR: 1.542849e-04\n",
      "  Step 900/4407 - Loss: 2.0128 | PPL: 7.48 | LR: 1.541662e-04\n",
      "  Step 900/4407 - Loss: 2.0128 | PPL: 7.48 | LR: 1.541662e-04\n",
      "  Step 1000/4407 - Loss: 2.0132 | PPL: 7.49 | LR: 1.540474e-04\n",
      "  Step 1000/4407 - Loss: 2.0132 | PPL: 7.49 | LR: 1.540474e-04\n",
      "  Step 1100/4407 - Loss: 2.0133 | PPL: 7.49 | LR: 1.539286e-04\n",
      "  Step 1100/4407 - Loss: 2.0133 | PPL: 7.49 | LR: 1.539286e-04\n",
      "  Step 1200/4407 - Loss: 2.0104 | PPL: 7.47 | LR: 1.538098e-04\n",
      "  Step 1200/4407 - Loss: 2.0104 | PPL: 7.47 | LR: 1.538098e-04\n",
      "  Step 1300/4407 - Loss: 2.0118 | PPL: 7.48 | LR: 1.536911e-04\n",
      "  Step 1300/4407 - Loss: 2.0118 | PPL: 7.48 | LR: 1.536911e-04\n",
      "  Step 1400/4407 - Loss: 2.0146 | PPL: 7.50 | LR: 1.535723e-04\n",
      "  Step 1400/4407 - Loss: 2.0146 | PPL: 7.50 | LR: 1.535723e-04\n",
      "  Step 1500/4407 - Loss: 2.0149 | PPL: 7.50 | LR: 1.534535e-04\n",
      "  Step 1500/4407 - Loss: 2.0149 | PPL: 7.50 | LR: 1.534535e-04\n",
      "  Step 1600/4407 - Loss: 2.0157 | PPL: 7.51 | LR: 1.533347e-04\n",
      "  Step 1600/4407 - Loss: 2.0157 | PPL: 7.51 | LR: 1.533347e-04\n",
      "  Step 1700/4407 - Loss: 2.0152 | PPL: 7.50 | LR: 1.532160e-04\n",
      "  Step 1700/4407 - Loss: 2.0152 | PPL: 7.50 | LR: 1.532160e-04\n",
      "  Step 1800/4407 - Loss: 2.0168 | PPL: 7.51 | LR: 1.530972e-04\n",
      "  Step 1800/4407 - Loss: 2.0168 | PPL: 7.51 | LR: 1.530972e-04\n",
      "  Step 1900/4407 - Loss: 2.0172 | PPL: 7.52 | LR: 1.529784e-04\n",
      "  Step 1900/4407 - Loss: 2.0172 | PPL: 7.52 | LR: 1.529784e-04\n",
      "  Step 2000/4407 - Loss: 2.0187 | PPL: 7.53 | LR: 1.528596e-04\n",
      "  Step 2000/4407 - Loss: 2.0187 | PPL: 7.53 | LR: 1.528596e-04\n",
      "  Step 2100/4407 - Loss: 2.0200 | PPL: 7.54 | LR: 1.527408e-04\n",
      "  Step 2100/4407 - Loss: 2.0200 | PPL: 7.54 | LR: 1.527408e-04\n",
      "  Step 2200/4407 - Loss: 2.0206 | PPL: 7.54 | LR: 1.526220e-04\n",
      "  Step 2200/4407 - Loss: 2.0206 | PPL: 7.54 | LR: 1.526220e-04\n",
      "  Step 2300/4407 - Loss: 2.0202 | PPL: 7.54 | LR: 1.525032e-04\n",
      "  Step 2300/4407 - Loss: 2.0202 | PPL: 7.54 | LR: 1.525032e-04\n",
      "  Step 2400/4407 - Loss: 2.0208 | PPL: 7.54 | LR: 1.523844e-04\n",
      "  Step 2400/4407 - Loss: 2.0208 | PPL: 7.54 | LR: 1.523844e-04\n",
      "  Step 2500/4407 - Loss: 2.0200 | PPL: 7.54 | LR: 1.522656e-04\n",
      "  Step 2500/4407 - Loss: 2.0200 | PPL: 7.54 | LR: 1.522656e-04\n",
      "  Step 2600/4407 - Loss: 2.0204 | PPL: 7.54 | LR: 1.521468e-04\n",
      "  Step 2600/4407 - Loss: 2.0204 | PPL: 7.54 | LR: 1.521468e-04\n",
      "  Step 2700/4407 - Loss: 2.0198 | PPL: 7.54 | LR: 1.520280e-04\n",
      "  Step 2700/4407 - Loss: 2.0198 | PPL: 7.54 | LR: 1.520280e-04\n",
      "  Step 2800/4407 - Loss: 2.0206 | PPL: 7.54 | LR: 1.519092e-04\n",
      "  Step 2800/4407 - Loss: 2.0206 | PPL: 7.54 | LR: 1.519092e-04\n",
      "  Step 2900/4407 - Loss: 2.0212 | PPL: 7.55 | LR: 1.517904e-04\n",
      "  Step 2900/4407 - Loss: 2.0212 | PPL: 7.55 | LR: 1.517904e-04\n",
      "  Step 3000/4407 - Loss: 2.0217 | PPL: 7.55 | LR: 1.516716e-04\n",
      "  Step 3000/4407 - Loss: 2.0217 | PPL: 7.55 | LR: 1.516716e-04\n",
      "  Step 3100/4407 - Loss: 2.0208 | PPL: 7.54 | LR: 1.515528e-04\n",
      "  Step 3100/4407 - Loss: 2.0208 | PPL: 7.54 | LR: 1.515528e-04\n",
      "  Step 3200/4407 - Loss: 2.0220 | PPL: 7.55 | LR: 1.514340e-04\n",
      "  Step 3200/4407 - Loss: 2.0220 | PPL: 7.55 | LR: 1.514340e-04\n",
      "  Step 3300/4407 - Loss: 2.0224 | PPL: 7.56 | LR: 1.513152e-04\n",
      "  Step 3300/4407 - Loss: 2.0224 | PPL: 7.56 | LR: 1.513152e-04\n",
      "  Step 3400/4407 - Loss: 2.0234 | PPL: 7.56 | LR: 1.511964e-04\n",
      "  Step 3400/4407 - Loss: 2.0234 | PPL: 7.56 | LR: 1.511964e-04\n",
      "  Step 3500/4407 - Loss: 2.0242 | PPL: 7.57 | LR: 1.510776e-04\n",
      "  Step 3500/4407 - Loss: 2.0242 | PPL: 7.57 | LR: 1.510776e-04\n",
      "  Step 3600/4407 - Loss: 2.0233 | PPL: 7.56 | LR: 1.509588e-04\n",
      "  Step 3600/4407 - Loss: 2.0233 | PPL: 7.56 | LR: 1.509588e-04\n",
      "  Step 3700/4407 - Loss: 2.0239 | PPL: 7.57 | LR: 1.508400e-04\n",
      "  Step 3700/4407 - Loss: 2.0239 | PPL: 7.57 | LR: 1.508400e-04\n",
      "  Step 3800/4407 - Loss: 2.0246 | PPL: 7.57 | LR: 1.507212e-04\n",
      "  Step 3800/4407 - Loss: 2.0246 | PPL: 7.57 | LR: 1.507212e-04\n",
      "  Step 3900/4407 - Loss: 2.0259 | PPL: 7.58 | LR: 1.506024e-04\n",
      "  Step 3900/4407 - Loss: 2.0259 | PPL: 7.58 | LR: 1.506024e-04\n",
      "  Step 4000/4407 - Loss: 2.0260 | PPL: 7.58 | LR: 1.504836e-04\n",
      "  Step 4000/4407 - Loss: 2.0260 | PPL: 7.58 | LR: 1.504836e-04\n",
      "  Step 4100/4407 - Loss: 2.0257 | PPL: 7.58 | LR: 1.503647e-04\n",
      "  Step 4100/4407 - Loss: 2.0257 | PPL: 7.58 | LR: 1.503647e-04\n",
      "  Step 4200/4407 - Loss: 2.0266 | PPL: 7.59 | LR: 1.502459e-04\n",
      "  Step 4200/4407 - Loss: 2.0266 | PPL: 7.59 | LR: 1.502459e-04\n",
      "  Step 4300/4407 - Loss: 2.0274 | PPL: 7.59 | LR: 1.501271e-04\n",
      "  Step 4300/4407 - Loss: 2.0274 | PPL: 7.59 | LR: 1.501271e-04\n",
      "  Step 4400/4407 - Loss: 2.0278 | PPL: 7.60 | LR: 1.500083e-04\n",
      "  Step 4400/4407 - Loss: 2.0278 | PPL: 7.60 | LR: 1.500083e-04\n",
      "Epoch 55/100 | Train Loss: 2.0278 | Train PPL: 7.60 | Val Loss: 2.6012 | Val PPL: 13.48\n",
      "Epoch 55/100 | Train Loss: 2.0278 | Train PPL: 7.60 | Val Loss: 2.6012 | Val PPL: 13.48\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.60\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.60\n",
      "  Step 100/4407 - Loss: 1.9876 | PPL: 7.30 | LR: 1.498812e-04\n",
      "  Step 100/4407 - Loss: 1.9876 | PPL: 7.30 | LR: 1.498812e-04\n",
      "  Step 200/4407 - Loss: 2.0032 | PPL: 7.41 | LR: 1.497624e-04\n",
      "  Step 200/4407 - Loss: 2.0032 | PPL: 7.41 | LR: 1.497624e-04\n",
      "  Step 300/4407 - Loss: 1.9899 | PPL: 7.31 | LR: 1.496436e-04\n",
      "  Step 300/4407 - Loss: 1.9899 | PPL: 7.31 | LR: 1.496436e-04\n",
      "  Step 400/4407 - Loss: 2.0043 | PPL: 7.42 | LR: 1.495248e-04\n",
      "  Step 400/4407 - Loss: 2.0043 | PPL: 7.42 | LR: 1.495248e-04\n",
      "  Step 500/4407 - Loss: 2.0094 | PPL: 7.46 | LR: 1.494059e-04\n",
      "  Step 500/4407 - Loss: 2.0094 | PPL: 7.46 | LR: 1.494059e-04\n",
      "  Step 600/4407 - Loss: 2.0089 | PPL: 7.46 | LR: 1.492871e-04\n",
      "  Step 600/4407 - Loss: 2.0089 | PPL: 7.46 | LR: 1.492871e-04\n",
      "  Step 700/4407 - Loss: 2.0145 | PPL: 7.50 | LR: 1.491683e-04\n",
      "  Step 700/4407 - Loss: 2.0145 | PPL: 7.50 | LR: 1.491683e-04\n",
      "  Step 800/4407 - Loss: 2.0127 | PPL: 7.48 | LR: 1.490495e-04\n",
      "  Step 800/4407 - Loss: 2.0127 | PPL: 7.48 | LR: 1.490495e-04\n",
      "  Step 900/4407 - Loss: 2.0113 | PPL: 7.47 | LR: 1.489307e-04\n",
      "  Step 900/4407 - Loss: 2.0113 | PPL: 7.47 | LR: 1.489307e-04\n",
      "  Step 1000/4407 - Loss: 2.0134 | PPL: 7.49 | LR: 1.488119e-04\n",
      "  Step 1000/4407 - Loss: 2.0134 | PPL: 7.49 | LR: 1.488119e-04\n",
      "  Step 1100/4407 - Loss: 2.0152 | PPL: 7.50 | LR: 1.486931e-04\n",
      "  Step 1100/4407 - Loss: 2.0152 | PPL: 7.50 | LR: 1.486931e-04\n",
      "  Step 1200/4407 - Loss: 2.0174 | PPL: 7.52 | LR: 1.485743e-04\n",
      "  Step 1200/4407 - Loss: 2.0174 | PPL: 7.52 | LR: 1.485743e-04\n",
      "  Step 1300/4407 - Loss: 2.0181 | PPL: 7.52 | LR: 1.484555e-04\n",
      "  Step 1300/4407 - Loss: 2.0181 | PPL: 7.52 | LR: 1.484555e-04\n",
      "  Step 1400/4407 - Loss: 2.0185 | PPL: 7.53 | LR: 1.483367e-04\n",
      "  Step 1400/4407 - Loss: 2.0185 | PPL: 7.53 | LR: 1.483367e-04\n",
      "  Step 1500/4407 - Loss: 2.0164 | PPL: 7.51 | LR: 1.482179e-04\n",
      "  Step 1500/4407 - Loss: 2.0164 | PPL: 7.51 | LR: 1.482179e-04\n",
      "  Step 1600/4407 - Loss: 2.0150 | PPL: 7.50 | LR: 1.480991e-04\n",
      "  Step 1600/4407 - Loss: 2.0150 | PPL: 7.50 | LR: 1.480991e-04\n",
      "  Step 1700/4407 - Loss: 2.0158 | PPL: 7.51 | LR: 1.479803e-04\n",
      "  Step 1700/4407 - Loss: 2.0158 | PPL: 7.51 | LR: 1.479803e-04\n",
      "  Step 1800/4407 - Loss: 2.0157 | PPL: 7.51 | LR: 1.478615e-04\n",
      "  Step 1800/4407 - Loss: 2.0157 | PPL: 7.51 | LR: 1.478615e-04\n",
      "  Step 1900/4407 - Loss: 2.0173 | PPL: 7.52 | LR: 1.477427e-04\n",
      "  Step 1900/4407 - Loss: 2.0173 | PPL: 7.52 | LR: 1.477427e-04\n",
      "  Step 2000/4407 - Loss: 2.0159 | PPL: 7.51 | LR: 1.476239e-04\n",
      "  Step 2000/4407 - Loss: 2.0159 | PPL: 7.51 | LR: 1.476239e-04\n",
      "  Step 2100/4407 - Loss: 2.0159 | PPL: 7.51 | LR: 1.475051e-04\n",
      "  Step 2100/4407 - Loss: 2.0159 | PPL: 7.51 | LR: 1.475051e-04\n",
      "  Step 2200/4407 - Loss: 2.0155 | PPL: 7.50 | LR: 1.473863e-04\n",
      "  Step 2200/4407 - Loss: 2.0155 | PPL: 7.50 | LR: 1.473863e-04\n",
      "  Step 2300/4407 - Loss: 2.0173 | PPL: 7.52 | LR: 1.472675e-04\n",
      "  Step 2300/4407 - Loss: 2.0173 | PPL: 7.52 | LR: 1.472675e-04\n",
      "  Step 2400/4407 - Loss: 2.0175 | PPL: 7.52 | LR: 1.471487e-04\n",
      "  Step 2400/4407 - Loss: 2.0175 | PPL: 7.52 | LR: 1.471487e-04\n",
      "  Step 2500/4407 - Loss: 2.0169 | PPL: 7.51 | LR: 1.470299e-04\n",
      "  Step 2500/4407 - Loss: 2.0169 | PPL: 7.51 | LR: 1.470299e-04\n",
      "  Step 2600/4407 - Loss: 2.0169 | PPL: 7.51 | LR: 1.469111e-04\n",
      "  Step 2600/4407 - Loss: 2.0169 | PPL: 7.51 | LR: 1.469111e-04\n",
      "  Step 2700/4407 - Loss: 2.0162 | PPL: 7.51 | LR: 1.467924e-04\n",
      "  Step 2700/4407 - Loss: 2.0162 | PPL: 7.51 | LR: 1.467924e-04\n",
      "  Step 2800/4407 - Loss: 2.0173 | PPL: 7.52 | LR: 1.466736e-04\n",
      "  Step 2800/4407 - Loss: 2.0173 | PPL: 7.52 | LR: 1.466736e-04\n",
      "  Step 2900/4407 - Loss: 2.0181 | PPL: 7.52 | LR: 1.465548e-04\n",
      "  Step 2900/4407 - Loss: 2.0181 | PPL: 7.52 | LR: 1.465548e-04\n",
      "  Step 3000/4407 - Loss: 2.0186 | PPL: 7.53 | LR: 1.464360e-04\n",
      "  Step 3000/4407 - Loss: 2.0186 | PPL: 7.53 | LR: 1.464360e-04\n",
      "  Step 3100/4407 - Loss: 2.0195 | PPL: 7.53 | LR: 1.463172e-04\n",
      "  Step 3100/4407 - Loss: 2.0195 | PPL: 7.53 | LR: 1.463172e-04\n",
      "  Step 3200/4407 - Loss: 2.0202 | PPL: 7.54 | LR: 1.461985e-04\n",
      "  Step 3200/4407 - Loss: 2.0202 | PPL: 7.54 | LR: 1.461985e-04\n",
      "  Step 3300/4407 - Loss: 2.0201 | PPL: 7.54 | LR: 1.460797e-04\n",
      "  Step 3300/4407 - Loss: 2.0201 | PPL: 7.54 | LR: 1.460797e-04\n",
      "  Step 3400/4407 - Loss: 2.0208 | PPL: 7.54 | LR: 1.459609e-04\n",
      "  Step 3400/4407 - Loss: 2.0208 | PPL: 7.54 | LR: 1.459609e-04\n",
      "  Step 3500/4407 - Loss: 2.0216 | PPL: 7.55 | LR: 1.458422e-04\n",
      "  Step 3500/4407 - Loss: 2.0216 | PPL: 7.55 | LR: 1.458422e-04\n",
      "  Step 3600/4407 - Loss: 2.0210 | PPL: 7.55 | LR: 1.457234e-04\n",
      "  Step 3600/4407 - Loss: 2.0210 | PPL: 7.55 | LR: 1.457234e-04\n",
      "  Step 3700/4407 - Loss: 2.0202 | PPL: 7.54 | LR: 1.456046e-04\n",
      "  Step 3700/4407 - Loss: 2.0202 | PPL: 7.54 | LR: 1.456046e-04\n",
      "  Step 3800/4407 - Loss: 2.0211 | PPL: 7.55 | LR: 1.454859e-04\n",
      "  Step 3800/4407 - Loss: 2.0211 | PPL: 7.55 | LR: 1.454859e-04\n",
      "  Step 3900/4407 - Loss: 2.0221 | PPL: 7.55 | LR: 1.453671e-04\n",
      "  Step 3900/4407 - Loss: 2.0221 | PPL: 7.55 | LR: 1.453671e-04\n",
      "  Step 4000/4407 - Loss: 2.0229 | PPL: 7.56 | LR: 1.452484e-04\n",
      "  Step 4000/4407 - Loss: 2.0229 | PPL: 7.56 | LR: 1.452484e-04\n",
      "  Step 4100/4407 - Loss: 2.0228 | PPL: 7.56 | LR: 1.451296e-04\n",
      "  Step 4100/4407 - Loss: 2.0228 | PPL: 7.56 | LR: 1.451296e-04\n",
      "  Step 4200/4407 - Loss: 2.0236 | PPL: 7.57 | LR: 1.450109e-04\n",
      "  Step 4200/4407 - Loss: 2.0236 | PPL: 7.57 | LR: 1.450109e-04\n",
      "  Step 4300/4407 - Loss: 2.0236 | PPL: 7.57 | LR: 1.448921e-04\n",
      "  Step 4300/4407 - Loss: 2.0236 | PPL: 7.57 | LR: 1.448921e-04\n",
      "  Step 4400/4407 - Loss: 2.0239 | PPL: 7.57 | LR: 1.447734e-04\n",
      "  Step 4400/4407 - Loss: 2.0239 | PPL: 7.57 | LR: 1.447734e-04\n",
      "Epoch 56/100 | Train Loss: 2.0243 | Train PPL: 7.57 | Val Loss: 2.5994 | Val PPL: 13.46\n",
      "Epoch 56/100 | Train Loss: 2.0243 | Train PPL: 7.57 | Val Loss: 2.5994 | Val PPL: 13.46\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.57\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.57\n",
      "  Step 100/4407 - Loss: 2.0271 | PPL: 7.59 | LR: 1.446463e-04\n",
      "  Step 100/4407 - Loss: 2.0271 | PPL: 7.59 | LR: 1.446463e-04\n",
      "  Step 200/4407 - Loss: 2.0206 | PPL: 7.54 | LR: 1.445276e-04\n",
      "  Step 200/4407 - Loss: 2.0206 | PPL: 7.54 | LR: 1.445276e-04\n",
      "  Step 300/4407 - Loss: 2.0028 | PPL: 7.41 | LR: 1.444089e-04\n",
      "  Step 300/4407 - Loss: 2.0028 | PPL: 7.41 | LR: 1.444089e-04\n",
      "  Step 400/4407 - Loss: 1.9942 | PPL: 7.35 | LR: 1.442901e-04\n",
      "  Step 400/4407 - Loss: 1.9942 | PPL: 7.35 | LR: 1.442901e-04\n",
      "  Step 500/4407 - Loss: 2.0023 | PPL: 7.41 | LR: 1.441714e-04\n",
      "  Step 500/4407 - Loss: 2.0023 | PPL: 7.41 | LR: 1.441714e-04\n",
      "  Step 600/4407 - Loss: 2.0119 | PPL: 7.48 | LR: 1.440527e-04\n",
      "  Step 600/4407 - Loss: 2.0119 | PPL: 7.48 | LR: 1.440527e-04\n",
      "  Step 700/4407 - Loss: 2.0112 | PPL: 7.47 | LR: 1.439340e-04\n",
      "  Step 700/4407 - Loss: 2.0112 | PPL: 7.47 | LR: 1.439340e-04\n",
      "  Step 800/4407 - Loss: 2.0161 | PPL: 7.51 | LR: 1.438153e-04\n",
      "  Step 800/4407 - Loss: 2.0161 | PPL: 7.51 | LR: 1.438153e-04\n",
      "  Step 900/4407 - Loss: 2.0152 | PPL: 7.50 | LR: 1.436966e-04\n",
      "  Step 900/4407 - Loss: 2.0152 | PPL: 7.50 | LR: 1.436966e-04\n",
      "  Step 1000/4407 - Loss: 2.0148 | PPL: 7.50 | LR: 1.435779e-04\n",
      "  Step 1000/4407 - Loss: 2.0148 | PPL: 7.50 | LR: 1.435779e-04\n",
      "  Step 1100/4407 - Loss: 2.0136 | PPL: 7.49 | LR: 1.434592e-04\n",
      "  Step 1100/4407 - Loss: 2.0136 | PPL: 7.49 | LR: 1.434592e-04\n",
      "  Step 1200/4407 - Loss: 2.0142 | PPL: 7.49 | LR: 1.433405e-04\n",
      "  Step 1200/4407 - Loss: 2.0142 | PPL: 7.49 | LR: 1.433405e-04\n",
      "  Step 1300/4407 - Loss: 2.0147 | PPL: 7.50 | LR: 1.432218e-04\n",
      "  Step 1300/4407 - Loss: 2.0147 | PPL: 7.50 | LR: 1.432218e-04\n",
      "  Step 1400/4407 - Loss: 2.0190 | PPL: 7.53 | LR: 1.431031e-04\n",
      "  Step 1400/4407 - Loss: 2.0190 | PPL: 7.53 | LR: 1.431031e-04\n",
      "  Step 1500/4407 - Loss: 2.0135 | PPL: 7.49 | LR: 1.429844e-04\n",
      "  Step 1500/4407 - Loss: 2.0135 | PPL: 7.49 | LR: 1.429844e-04\n",
      "  Step 1600/4407 - Loss: 2.0112 | PPL: 7.47 | LR: 1.428657e-04\n",
      "  Step 1600/4407 - Loss: 2.0112 | PPL: 7.47 | LR: 1.428657e-04\n",
      "  Step 1700/4407 - Loss: 2.0113 | PPL: 7.47 | LR: 1.427471e-04\n",
      "  Step 1700/4407 - Loss: 2.0113 | PPL: 7.47 | LR: 1.427471e-04\n",
      "  Step 1800/4407 - Loss: 2.0138 | PPL: 7.49 | LR: 1.426284e-04\n",
      "  Step 1800/4407 - Loss: 2.0138 | PPL: 7.49 | LR: 1.426284e-04\n",
      "  Step 1900/4407 - Loss: 2.0146 | PPL: 7.50 | LR: 1.425097e-04\n",
      "  Step 1900/4407 - Loss: 2.0146 | PPL: 7.50 | LR: 1.425097e-04\n",
      "  Step 2000/4407 - Loss: 2.0138 | PPL: 7.49 | LR: 1.423911e-04\n",
      "  Step 2000/4407 - Loss: 2.0138 | PPL: 7.49 | LR: 1.423911e-04\n",
      "  Step 2100/4407 - Loss: 2.0141 | PPL: 7.49 | LR: 1.422724e-04\n",
      "  Step 2100/4407 - Loss: 2.0141 | PPL: 7.49 | LR: 1.422724e-04\n",
      "  Step 2200/4407 - Loss: 2.0140 | PPL: 7.49 | LR: 1.421538e-04\n",
      "  Step 2200/4407 - Loss: 2.0140 | PPL: 7.49 | LR: 1.421538e-04\n",
      "  Step 2300/4407 - Loss: 2.0137 | PPL: 7.49 | LR: 1.420351e-04\n",
      "  Step 2300/4407 - Loss: 2.0137 | PPL: 7.49 | LR: 1.420351e-04\n",
      "  Step 2400/4407 - Loss: 2.0145 | PPL: 7.50 | LR: 1.419165e-04\n",
      "  Step 2400/4407 - Loss: 2.0145 | PPL: 7.50 | LR: 1.419165e-04\n",
      "  Step 2500/4407 - Loss: 2.0145 | PPL: 7.50 | LR: 1.417978e-04\n",
      "  Step 2500/4407 - Loss: 2.0145 | PPL: 7.50 | LR: 1.417978e-04\n",
      "  Step 2600/4407 - Loss: 2.0147 | PPL: 7.50 | LR: 1.416792e-04\n",
      "  Step 2600/4407 - Loss: 2.0147 | PPL: 7.50 | LR: 1.416792e-04\n",
      "  Step 2700/4407 - Loss: 2.0145 | PPL: 7.50 | LR: 1.415606e-04\n",
      "  Step 2700/4407 - Loss: 2.0145 | PPL: 7.50 | LR: 1.415606e-04\n",
      "  Step 2800/4407 - Loss: 2.0153 | PPL: 7.50 | LR: 1.414420e-04\n",
      "  Step 2800/4407 - Loss: 2.0153 | PPL: 7.50 | LR: 1.414420e-04\n",
      "  Step 2900/4407 - Loss: 2.0142 | PPL: 7.49 | LR: 1.413233e-04\n",
      "  Step 2900/4407 - Loss: 2.0142 | PPL: 7.49 | LR: 1.413233e-04\n",
      "  Step 3000/4407 - Loss: 2.0150 | PPL: 7.50 | LR: 1.412047e-04\n",
      "  Step 3000/4407 - Loss: 2.0150 | PPL: 7.50 | LR: 1.412047e-04\n",
      "  Step 3100/4407 - Loss: 2.0153 | PPL: 7.50 | LR: 1.410861e-04\n",
      "  Step 3100/4407 - Loss: 2.0153 | PPL: 7.50 | LR: 1.410861e-04\n",
      "  Step 3200/4407 - Loss: 2.0154 | PPL: 7.50 | LR: 1.409675e-04\n",
      "  Step 3200/4407 - Loss: 2.0154 | PPL: 7.50 | LR: 1.409675e-04\n",
      "  Step 3300/4407 - Loss: 2.0168 | PPL: 7.51 | LR: 1.408489e-04\n",
      "  Step 3300/4407 - Loss: 2.0168 | PPL: 7.51 | LR: 1.408489e-04\n",
      "  Step 3400/4407 - Loss: 2.0162 | PPL: 7.51 | LR: 1.407304e-04\n",
      "  Step 3400/4407 - Loss: 2.0162 | PPL: 7.51 | LR: 1.407304e-04\n",
      "  Step 3500/4407 - Loss: 2.0162 | PPL: 7.51 | LR: 1.406118e-04\n",
      "  Step 3500/4407 - Loss: 2.0162 | PPL: 7.51 | LR: 1.406118e-04\n",
      "  Step 3600/4407 - Loss: 2.0171 | PPL: 7.52 | LR: 1.404932e-04\n",
      "  Step 3600/4407 - Loss: 2.0171 | PPL: 7.52 | LR: 1.404932e-04\n",
      "  Step 3700/4407 - Loss: 2.0173 | PPL: 7.52 | LR: 1.403746e-04\n",
      "  Step 3700/4407 - Loss: 2.0173 | PPL: 7.52 | LR: 1.403746e-04\n",
      "  Step 3800/4407 - Loss: 2.0186 | PPL: 7.53 | LR: 1.402561e-04\n",
      "  Step 3800/4407 - Loss: 2.0186 | PPL: 7.53 | LR: 1.402561e-04\n",
      "  Step 3900/4407 - Loss: 2.0192 | PPL: 7.53 | LR: 1.401375e-04\n",
      "  Step 3900/4407 - Loss: 2.0192 | PPL: 7.53 | LR: 1.401375e-04\n",
      "  Step 4000/4407 - Loss: 2.0202 | PPL: 7.54 | LR: 1.400190e-04\n",
      "  Step 4000/4407 - Loss: 2.0202 | PPL: 7.54 | LR: 1.400190e-04\n",
      "  Step 4100/4407 - Loss: 2.0196 | PPL: 7.54 | LR: 1.399004e-04\n",
      "  Step 4100/4407 - Loss: 2.0196 | PPL: 7.54 | LR: 1.399004e-04\n",
      "  Step 4200/4407 - Loss: 2.0204 | PPL: 7.54 | LR: 1.397819e-04\n",
      "  Step 4200/4407 - Loss: 2.0204 | PPL: 7.54 | LR: 1.397819e-04\n",
      "  Step 4300/4407 - Loss: 2.0212 | PPL: 7.55 | LR: 1.396634e-04\n",
      "  Step 4300/4407 - Loss: 2.0212 | PPL: 7.55 | LR: 1.396634e-04\n",
      "  Step 4400/4407 - Loss: 2.0201 | PPL: 7.54 | LR: 1.395448e-04\n",
      "  Step 4400/4407 - Loss: 2.0201 | PPL: 7.54 | LR: 1.395448e-04\n",
      "Epoch 57/100 | Train Loss: 2.0202 | Train PPL: 7.54 | Val Loss: 2.6039 | Val PPL: 13.52\n",
      "Epoch 57/100 | Train Loss: 2.0202 | Train PPL: 7.54 | Val Loss: 2.6039 | Val PPL: 13.52\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.54\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.54\n",
      "  Step 100/4407 - Loss: 2.0501 | PPL: 7.77 | LR: 1.394180e-04\n",
      "  Step 100/4407 - Loss: 2.0501 | PPL: 7.77 | LR: 1.394180e-04\n",
      "  Step 200/4407 - Loss: 2.0187 | PPL: 7.53 | LR: 1.392995e-04\n",
      "  Step 200/4407 - Loss: 2.0187 | PPL: 7.53 | LR: 1.392995e-04\n",
      "  Step 300/4407 - Loss: 2.0091 | PPL: 7.46 | LR: 1.391810e-04\n",
      "  Step 300/4407 - Loss: 2.0091 | PPL: 7.46 | LR: 1.391810e-04\n",
      "  Step 400/4407 - Loss: 1.9962 | PPL: 7.36 | LR: 1.390625e-04\n",
      "  Step 400/4407 - Loss: 1.9962 | PPL: 7.36 | LR: 1.390625e-04\n",
      "  Step 500/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.389440e-04\n",
      "  Step 500/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.389440e-04\n",
      "  Step 600/4407 - Loss: 2.0041 | PPL: 7.42 | LR: 1.388255e-04\n",
      "  Step 600/4407 - Loss: 2.0041 | PPL: 7.42 | LR: 1.388255e-04\n",
      "  Step 700/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.387070e-04\n",
      "  Step 700/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.387070e-04\n",
      "  Step 800/4407 - Loss: 2.0063 | PPL: 7.44 | LR: 1.385886e-04\n",
      "  Step 800/4407 - Loss: 2.0063 | PPL: 7.44 | LR: 1.385886e-04\n",
      "  Step 900/4407 - Loss: 2.0040 | PPL: 7.42 | LR: 1.384701e-04\n",
      "  Step 900/4407 - Loss: 2.0040 | PPL: 7.42 | LR: 1.384701e-04\n",
      "  Step 1000/4407 - Loss: 2.0034 | PPL: 7.41 | LR: 1.383517e-04\n",
      "  Step 1000/4407 - Loss: 2.0034 | PPL: 7.41 | LR: 1.383517e-04\n",
      "  Step 1100/4407 - Loss: 2.0019 | PPL: 7.40 | LR: 1.382332e-04\n",
      "  Step 1100/4407 - Loss: 2.0019 | PPL: 7.40 | LR: 1.382332e-04\n",
      "  Step 1200/4407 - Loss: 2.0008 | PPL: 7.40 | LR: 1.381148e-04\n",
      "  Step 1200/4407 - Loss: 2.0008 | PPL: 7.40 | LR: 1.381148e-04\n",
      "  Step 1300/4407 - Loss: 1.9962 | PPL: 7.36 | LR: 1.379963e-04\n",
      "  Step 1300/4407 - Loss: 1.9962 | PPL: 7.36 | LR: 1.379963e-04\n",
      "  Step 1400/4407 - Loss: 1.9974 | PPL: 7.37 | LR: 1.378779e-04\n",
      "  Step 1400/4407 - Loss: 1.9974 | PPL: 7.37 | LR: 1.378779e-04\n",
      "  Step 1500/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.377595e-04\n",
      "  Step 1500/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.377595e-04\n",
      "  Step 1600/4407 - Loss: 2.0051 | PPL: 7.43 | LR: 1.376411e-04\n",
      "  Step 1600/4407 - Loss: 2.0051 | PPL: 7.43 | LR: 1.376411e-04\n",
      "  Step 1700/4407 - Loss: 2.0057 | PPL: 7.43 | LR: 1.375227e-04\n",
      "  Step 1700/4407 - Loss: 2.0057 | PPL: 7.43 | LR: 1.375227e-04\n",
      "  Step 1800/4407 - Loss: 2.0049 | PPL: 7.43 | LR: 1.374043e-04\n",
      "  Step 1800/4407 - Loss: 2.0049 | PPL: 7.43 | LR: 1.374043e-04\n",
      "  Step 1900/4407 - Loss: 2.0061 | PPL: 7.43 | LR: 1.372859e-04\n",
      "  Step 1900/4407 - Loss: 2.0061 | PPL: 7.43 | LR: 1.372859e-04\n",
      "  Step 2000/4407 - Loss: 2.0066 | PPL: 7.44 | LR: 1.371675e-04\n",
      "  Step 2000/4407 - Loss: 2.0066 | PPL: 7.44 | LR: 1.371675e-04\n",
      "  Step 2100/4407 - Loss: 2.0085 | PPL: 7.45 | LR: 1.370491e-04\n",
      "  Step 2100/4407 - Loss: 2.0085 | PPL: 7.45 | LR: 1.370491e-04\n",
      "  Step 2200/4407 - Loss: 2.0089 | PPL: 7.46 | LR: 1.369308e-04\n",
      "  Step 2200/4407 - Loss: 2.0089 | PPL: 7.46 | LR: 1.369308e-04\n",
      "  Step 2300/4407 - Loss: 2.0119 | PPL: 7.48 | LR: 1.368124e-04\n",
      "  Step 2300/4407 - Loss: 2.0119 | PPL: 7.48 | LR: 1.368124e-04\n",
      "  Step 2400/4407 - Loss: 2.0097 | PPL: 7.46 | LR: 1.366941e-04\n",
      "  Step 2400/4407 - Loss: 2.0097 | PPL: 7.46 | LR: 1.366941e-04\n",
      "  Step 2500/4407 - Loss: 2.0107 | PPL: 7.47 | LR: 1.365757e-04\n",
      "  Step 2500/4407 - Loss: 2.0107 | PPL: 7.47 | LR: 1.365757e-04\n",
      "  Step 2600/4407 - Loss: 2.0107 | PPL: 7.47 | LR: 1.364574e-04\n",
      "  Step 2600/4407 - Loss: 2.0107 | PPL: 7.47 | LR: 1.364574e-04\n",
      "  Step 2700/4407 - Loss: 2.0119 | PPL: 7.48 | LR: 1.363391e-04\n",
      "  Step 2700/4407 - Loss: 2.0119 | PPL: 7.48 | LR: 1.363391e-04\n",
      "  Step 2800/4407 - Loss: 2.0111 | PPL: 7.47 | LR: 1.362208e-04\n",
      "  Step 2800/4407 - Loss: 2.0111 | PPL: 7.47 | LR: 1.362208e-04\n",
      "  Step 2900/4407 - Loss: 2.0129 | PPL: 7.49 | LR: 1.361025e-04\n",
      "  Step 2900/4407 - Loss: 2.0129 | PPL: 7.49 | LR: 1.361025e-04\n",
      "  Step 3000/4407 - Loss: 2.0134 | PPL: 7.49 | LR: 1.359842e-04\n",
      "  Step 3000/4407 - Loss: 2.0134 | PPL: 7.49 | LR: 1.359842e-04\n",
      "  Step 3100/4407 - Loss: 2.0132 | PPL: 7.49 | LR: 1.358659e-04\n",
      "  Step 3100/4407 - Loss: 2.0132 | PPL: 7.49 | LR: 1.358659e-04\n",
      "  Step 3200/4407 - Loss: 2.0135 | PPL: 7.49 | LR: 1.357476e-04\n",
      "  Step 3200/4407 - Loss: 2.0135 | PPL: 7.49 | LR: 1.357476e-04\n",
      "  Step 3300/4407 - Loss: 2.0137 | PPL: 7.49 | LR: 1.356293e-04\n",
      "  Step 3300/4407 - Loss: 2.0137 | PPL: 7.49 | LR: 1.356293e-04\n",
      "  Step 3400/4407 - Loss: 2.0133 | PPL: 7.49 | LR: 1.355111e-04\n",
      "  Step 3400/4407 - Loss: 2.0133 | PPL: 7.49 | LR: 1.355111e-04\n",
      "  Step 3500/4407 - Loss: 2.0141 | PPL: 7.49 | LR: 1.353928e-04\n",
      "  Step 3500/4407 - Loss: 2.0141 | PPL: 7.49 | LR: 1.353928e-04\n",
      "  Step 3600/4407 - Loss: 2.0140 | PPL: 7.49 | LR: 1.352746e-04\n",
      "  Step 3600/4407 - Loss: 2.0140 | PPL: 7.49 | LR: 1.352746e-04\n",
      "  Step 3700/4407 - Loss: 2.0158 | PPL: 7.51 | LR: 1.351564e-04\n",
      "  Step 3700/4407 - Loss: 2.0158 | PPL: 7.51 | LR: 1.351564e-04\n",
      "  Step 3800/4407 - Loss: 2.0155 | PPL: 7.50 | LR: 1.350381e-04\n",
      "  Step 3800/4407 - Loss: 2.0155 | PPL: 7.50 | LR: 1.350381e-04\n",
      "  Step 3900/4407 - Loss: 2.0156 | PPL: 7.50 | LR: 1.349199e-04\n",
      "  Step 3900/4407 - Loss: 2.0156 | PPL: 7.50 | LR: 1.349199e-04\n",
      "  Step 4000/4407 - Loss: 2.0154 | PPL: 7.50 | LR: 1.348017e-04\n",
      "  Step 4000/4407 - Loss: 2.0154 | PPL: 7.50 | LR: 1.348017e-04\n",
      "  Step 4100/4407 - Loss: 2.0154 | PPL: 7.50 | LR: 1.346835e-04\n",
      "  Step 4100/4407 - Loss: 2.0154 | PPL: 7.50 | LR: 1.346835e-04\n",
      "  Step 4200/4407 - Loss: 2.0151 | PPL: 7.50 | LR: 1.345653e-04\n",
      "  Step 4200/4407 - Loss: 2.0151 | PPL: 7.50 | LR: 1.345653e-04\n",
      "  Step 4300/4407 - Loss: 2.0157 | PPL: 7.51 | LR: 1.344472e-04\n",
      "  Step 4300/4407 - Loss: 2.0157 | PPL: 7.51 | LR: 1.344472e-04\n",
      "  Step 4400/4407 - Loss: 2.0166 | PPL: 7.51 | LR: 1.343290e-04\n",
      "  Step 4400/4407 - Loss: 2.0166 | PPL: 7.51 | LR: 1.343290e-04\n",
      "Epoch 58/100 | Train Loss: 2.0168 | Train PPL: 7.51 | Val Loss: 2.6013 | Val PPL: 13.48\n",
      "Epoch 58/100 | Train Loss: 2.0168 | Train PPL: 7.51 | Val Loss: 2.6013 | Val PPL: 13.48\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.51\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.51\n",
      "  Step 100/4407 - Loss: 2.0030 | PPL: 7.41 | LR: 1.342026e-04\n",
      "  Step 100/4407 - Loss: 2.0030 | PPL: 7.41 | LR: 1.342026e-04\n",
      "  Step 200/4407 - Loss: 2.0028 | PPL: 7.41 | LR: 1.340844e-04\n",
      "  Step 200/4407 - Loss: 2.0028 | PPL: 7.41 | LR: 1.340844e-04\n",
      "  Step 300/4407 - Loss: 1.9959 | PPL: 7.36 | LR: 1.339663e-04\n",
      "  Step 300/4407 - Loss: 1.9959 | PPL: 7.36 | LR: 1.339663e-04\n",
      "  Step 400/4407 - Loss: 1.9886 | PPL: 7.31 | LR: 1.338482e-04\n",
      "  Step 400/4407 - Loss: 1.9886 | PPL: 7.31 | LR: 1.338482e-04\n",
      "  Step 500/4407 - Loss: 1.9850 | PPL: 7.28 | LR: 1.337301e-04\n",
      "  Step 500/4407 - Loss: 1.9850 | PPL: 7.28 | LR: 1.337301e-04\n",
      "  Step 600/4407 - Loss: 1.9897 | PPL: 7.31 | LR: 1.336120e-04\n",
      "  Step 600/4407 - Loss: 1.9897 | PPL: 7.31 | LR: 1.336120e-04\n",
      "  Step 700/4407 - Loss: 1.9960 | PPL: 7.36 | LR: 1.334939e-04\n",
      "  Step 700/4407 - Loss: 1.9960 | PPL: 7.36 | LR: 1.334939e-04\n",
      "  Step 800/4407 - Loss: 1.9974 | PPL: 7.37 | LR: 1.333758e-04\n",
      "  Step 800/4407 - Loss: 1.9974 | PPL: 7.37 | LR: 1.333758e-04\n",
      "  Step 900/4407 - Loss: 1.9969 | PPL: 7.37 | LR: 1.332577e-04\n",
      "  Step 900/4407 - Loss: 1.9969 | PPL: 7.37 | LR: 1.332577e-04\n",
      "  Step 1000/4407 - Loss: 1.9948 | PPL: 7.35 | LR: 1.331396e-04\n",
      "  Step 1000/4407 - Loss: 1.9948 | PPL: 7.35 | LR: 1.331396e-04\n",
      "  Step 1100/4407 - Loss: 1.9987 | PPL: 7.38 | LR: 1.330216e-04\n",
      "  Step 1100/4407 - Loss: 1.9987 | PPL: 7.38 | LR: 1.330216e-04\n",
      "  Step 1200/4407 - Loss: 1.9998 | PPL: 7.39 | LR: 1.329035e-04\n",
      "  Step 1200/4407 - Loss: 1.9998 | PPL: 7.39 | LR: 1.329035e-04\n",
      "  Step 1300/4407 - Loss: 1.9990 | PPL: 7.38 | LR: 1.327855e-04\n",
      "  Step 1300/4407 - Loss: 1.9990 | PPL: 7.38 | LR: 1.327855e-04\n",
      "  Step 1400/4407 - Loss: 1.9976 | PPL: 7.37 | LR: 1.326675e-04\n",
      "  Step 1400/4407 - Loss: 1.9976 | PPL: 7.37 | LR: 1.326675e-04\n",
      "  Step 1500/4407 - Loss: 2.0002 | PPL: 7.39 | LR: 1.325495e-04\n",
      "  Step 1500/4407 - Loss: 2.0002 | PPL: 7.39 | LR: 1.325495e-04\n",
      "  Step 1600/4407 - Loss: 2.0003 | PPL: 7.39 | LR: 1.324315e-04\n",
      "  Step 1600/4407 - Loss: 2.0003 | PPL: 7.39 | LR: 1.324315e-04\n",
      "  Step 1700/4407 - Loss: 1.9995 | PPL: 7.39 | LR: 1.323135e-04\n",
      "  Step 1700/4407 - Loss: 1.9995 | PPL: 7.39 | LR: 1.323135e-04\n",
      "  Step 1800/4407 - Loss: 1.9988 | PPL: 7.38 | LR: 1.321955e-04\n",
      "  Step 1800/4407 - Loss: 1.9988 | PPL: 7.38 | LR: 1.321955e-04\n",
      "  Step 1900/4407 - Loss: 1.9995 | PPL: 7.39 | LR: 1.320776e-04\n",
      "  Step 1900/4407 - Loss: 1.9995 | PPL: 7.39 | LR: 1.320776e-04\n",
      "  Step 2000/4407 - Loss: 2.0007 | PPL: 7.39 | LR: 1.319596e-04\n",
      "  Step 2000/4407 - Loss: 2.0007 | PPL: 7.39 | LR: 1.319596e-04\n",
      "  Step 2100/4407 - Loss: 2.0012 | PPL: 7.40 | LR: 1.318417e-04\n",
      "  Step 2100/4407 - Loss: 2.0012 | PPL: 7.40 | LR: 1.318417e-04\n",
      "  Step 2200/4407 - Loss: 2.0008 | PPL: 7.39 | LR: 1.317237e-04\n",
      "  Step 2200/4407 - Loss: 2.0008 | PPL: 7.39 | LR: 1.317237e-04\n",
      "  Step 2300/4407 - Loss: 1.9999 | PPL: 7.39 | LR: 1.316058e-04\n",
      "  Step 2300/4407 - Loss: 1.9999 | PPL: 7.39 | LR: 1.316058e-04\n",
      "  Step 2400/4407 - Loss: 2.0020 | PPL: 7.40 | LR: 1.314879e-04\n",
      "  Step 2400/4407 - Loss: 2.0020 | PPL: 7.40 | LR: 1.314879e-04\n",
      "  Step 2500/4407 - Loss: 2.0043 | PPL: 7.42 | LR: 1.313700e-04\n",
      "  Step 2500/4407 - Loss: 2.0043 | PPL: 7.42 | LR: 1.313700e-04\n",
      "  Step 2600/4407 - Loss: 2.0040 | PPL: 7.42 | LR: 1.312521e-04\n",
      "  Step 2600/4407 - Loss: 2.0040 | PPL: 7.42 | LR: 1.312521e-04\n",
      "  Step 2700/4407 - Loss: 2.0041 | PPL: 7.42 | LR: 1.311342e-04\n",
      "  Step 2700/4407 - Loss: 2.0041 | PPL: 7.42 | LR: 1.311342e-04\n",
      "  Step 2800/4407 - Loss: 2.0059 | PPL: 7.43 | LR: 1.310164e-04\n",
      "  Step 2800/4407 - Loss: 2.0059 | PPL: 7.43 | LR: 1.310164e-04\n",
      "  Step 2900/4407 - Loss: 2.0062 | PPL: 7.44 | LR: 1.308985e-04\n",
      "  Step 2900/4407 - Loss: 2.0062 | PPL: 7.44 | LR: 1.308985e-04\n",
      "  Step 3000/4407 - Loss: 2.0076 | PPL: 7.45 | LR: 1.307807e-04\n",
      "  Step 3000/4407 - Loss: 2.0076 | PPL: 7.45 | LR: 1.307807e-04\n",
      "  Step 3100/4407 - Loss: 2.0090 | PPL: 7.46 | LR: 1.306629e-04\n",
      "  Step 3100/4407 - Loss: 2.0090 | PPL: 7.46 | LR: 1.306629e-04\n",
      "  Step 3200/4407 - Loss: 2.0106 | PPL: 7.47 | LR: 1.305451e-04\n",
      "  Step 3200/4407 - Loss: 2.0106 | PPL: 7.47 | LR: 1.305451e-04\n",
      "  Step 3300/4407 - Loss: 2.0098 | PPL: 7.46 | LR: 1.304273e-04\n",
      "  Step 3300/4407 - Loss: 2.0098 | PPL: 7.46 | LR: 1.304273e-04\n",
      "  Step 3400/4407 - Loss: 2.0098 | PPL: 7.46 | LR: 1.303095e-04\n",
      "  Step 3400/4407 - Loss: 2.0098 | PPL: 7.46 | LR: 1.303095e-04\n",
      "  Step 3500/4407 - Loss: 2.0106 | PPL: 7.47 | LR: 1.301917e-04\n",
      "  Step 3500/4407 - Loss: 2.0106 | PPL: 7.47 | LR: 1.301917e-04\n",
      "  Step 3600/4407 - Loss: 2.0114 | PPL: 7.47 | LR: 1.300739e-04\n",
      "  Step 3600/4407 - Loss: 2.0114 | PPL: 7.47 | LR: 1.300739e-04\n",
      "  Step 3700/4407 - Loss: 2.0131 | PPL: 7.49 | LR: 1.299562e-04\n",
      "  Step 3700/4407 - Loss: 2.0131 | PPL: 7.49 | LR: 1.299562e-04\n",
      "  Step 3800/4407 - Loss: 2.0141 | PPL: 7.49 | LR: 1.298384e-04\n",
      "  Step 3800/4407 - Loss: 2.0141 | PPL: 7.49 | LR: 1.298384e-04\n",
      "  Step 3900/4407 - Loss: 2.0137 | PPL: 7.49 | LR: 1.297207e-04\n",
      "  Step 3900/4407 - Loss: 2.0137 | PPL: 7.49 | LR: 1.297207e-04\n",
      "  Step 4000/4407 - Loss: 2.0149 | PPL: 7.50 | LR: 1.296030e-04\n",
      "  Step 4000/4407 - Loss: 2.0149 | PPL: 7.50 | LR: 1.296030e-04\n",
      "  Step 4100/4407 - Loss: 2.0160 | PPL: 7.51 | LR: 1.294853e-04\n",
      "  Step 4100/4407 - Loss: 2.0160 | PPL: 7.51 | LR: 1.294853e-04\n",
      "  Step 4200/4407 - Loss: 2.0154 | PPL: 7.50 | LR: 1.293676e-04\n",
      "  Step 4200/4407 - Loss: 2.0154 | PPL: 7.50 | LR: 1.293676e-04\n",
      "  Step 4300/4407 - Loss: 2.0142 | PPL: 7.49 | LR: 1.292499e-04\n",
      "  Step 4300/4407 - Loss: 2.0142 | PPL: 7.49 | LR: 1.292499e-04\n",
      "  Step 4400/4407 - Loss: 2.0133 | PPL: 7.49 | LR: 1.291323e-04\n",
      "  Step 4400/4407 - Loss: 2.0133 | PPL: 7.49 | LR: 1.291323e-04\n",
      "Epoch 59/100 | Train Loss: 2.0133 | Train PPL: 7.49 | Val Loss: 2.5975 | Val PPL: 13.43\n",
      "Epoch 59/100 | Train Loss: 2.0133 | Train PPL: 7.49 | Val Loss: 2.5975 | Val PPL: 13.43\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.49\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.49\n",
      "  Step 100/4407 - Loss: 1.9907 | PPL: 7.32 | LR: 1.290064e-04\n",
      "  Step 100/4407 - Loss: 1.9907 | PPL: 7.32 | LR: 1.290064e-04\n",
      "  Step 200/4407 - Loss: 1.9864 | PPL: 7.29 | LR: 1.288888e-04\n",
      "  Step 200/4407 - Loss: 1.9864 | PPL: 7.29 | LR: 1.288888e-04\n",
      "  Step 300/4407 - Loss: 1.9876 | PPL: 7.30 | LR: 1.287711e-04\n",
      "  Step 300/4407 - Loss: 1.9876 | PPL: 7.30 | LR: 1.287711e-04\n",
      "  Step 400/4407 - Loss: 1.9995 | PPL: 7.39 | LR: 1.286535e-04\n",
      "  Step 400/4407 - Loss: 1.9995 | PPL: 7.39 | LR: 1.286535e-04\n",
      "  Step 500/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.285359e-04\n",
      "  Step 500/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.285359e-04\n",
      "  Step 600/4407 - Loss: 2.0000 | PPL: 7.39 | LR: 1.284183e-04\n",
      "  Step 600/4407 - Loss: 2.0000 | PPL: 7.39 | LR: 1.284183e-04\n",
      "  Step 700/4407 - Loss: 2.0001 | PPL: 7.39 | LR: 1.283008e-04\n",
      "  Step 700/4407 - Loss: 2.0001 | PPL: 7.39 | LR: 1.283008e-04\n",
      "  Step 800/4407 - Loss: 1.9980 | PPL: 7.37 | LR: 1.281832e-04\n",
      "  Step 800/4407 - Loss: 1.9980 | PPL: 7.37 | LR: 1.281832e-04\n",
      "  Step 900/4407 - Loss: 1.9964 | PPL: 7.36 | LR: 1.280657e-04\n",
      "  Step 900/4407 - Loss: 1.9964 | PPL: 7.36 | LR: 1.280657e-04\n",
      "  Step 1000/4407 - Loss: 1.9948 | PPL: 7.35 | LR: 1.279482e-04\n",
      "  Step 1000/4407 - Loss: 1.9948 | PPL: 7.35 | LR: 1.279482e-04\n",
      "  Step 1100/4407 - Loss: 1.9974 | PPL: 7.37 | LR: 1.278306e-04\n",
      "  Step 1100/4407 - Loss: 1.9974 | PPL: 7.37 | LR: 1.278306e-04\n",
      "  Step 1200/4407 - Loss: 1.9968 | PPL: 7.37 | LR: 1.277131e-04\n",
      "  Step 1200/4407 - Loss: 1.9968 | PPL: 7.37 | LR: 1.277131e-04\n",
      "  Step 1300/4407 - Loss: 1.9985 | PPL: 7.38 | LR: 1.275957e-04\n",
      "  Step 1300/4407 - Loss: 1.9985 | PPL: 7.38 | LR: 1.275957e-04\n",
      "  Step 1400/4407 - Loss: 1.9992 | PPL: 7.38 | LR: 1.274782e-04\n",
      "  Step 1400/4407 - Loss: 1.9992 | PPL: 7.38 | LR: 1.274782e-04\n",
      "  Step 1500/4407 - Loss: 1.9984 | PPL: 7.38 | LR: 1.273607e-04\n",
      "  Step 1500/4407 - Loss: 1.9984 | PPL: 7.38 | LR: 1.273607e-04\n",
      "  Step 1600/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.272433e-04\n",
      "  Step 1600/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.272433e-04\n",
      "  Step 1700/4407 - Loss: 2.0010 | PPL: 7.40 | LR: 1.271259e-04\n",
      "  Step 1700/4407 - Loss: 2.0010 | PPL: 7.40 | LR: 1.271259e-04\n",
      "  Step 1800/4407 - Loss: 2.0027 | PPL: 7.41 | LR: 1.270084e-04\n",
      "  Step 1800/4407 - Loss: 2.0027 | PPL: 7.41 | LR: 1.270084e-04\n",
      "  Step 1900/4407 - Loss: 2.0032 | PPL: 7.41 | LR: 1.268910e-04\n",
      "  Step 1900/4407 - Loss: 2.0032 | PPL: 7.41 | LR: 1.268910e-04\n",
      "  Step 2000/4407 - Loss: 2.0056 | PPL: 7.43 | LR: 1.267737e-04\n",
      "  Step 2000/4407 - Loss: 2.0056 | PPL: 7.43 | LR: 1.267737e-04\n",
      "  Step 2100/4407 - Loss: 2.0058 | PPL: 7.43 | LR: 1.266563e-04\n",
      "  Step 2100/4407 - Loss: 2.0058 | PPL: 7.43 | LR: 1.266563e-04\n",
      "  Step 2200/4407 - Loss: 2.0064 | PPL: 7.44 | LR: 1.265389e-04\n",
      "  Step 2200/4407 - Loss: 2.0064 | PPL: 7.44 | LR: 1.265389e-04\n",
      "  Step 2300/4407 - Loss: 2.0068 | PPL: 7.44 | LR: 1.264216e-04\n",
      "  Step 2300/4407 - Loss: 2.0068 | PPL: 7.44 | LR: 1.264216e-04\n",
      "  Step 2400/4407 - Loss: 2.0067 | PPL: 7.44 | LR: 1.263043e-04\n",
      "  Step 2400/4407 - Loss: 2.0067 | PPL: 7.44 | LR: 1.263043e-04\n",
      "  Step 2500/4407 - Loss: 2.0049 | PPL: 7.43 | LR: 1.261870e-04\n",
      "  Step 2500/4407 - Loss: 2.0049 | PPL: 7.43 | LR: 1.261870e-04\n",
      "  Step 2600/4407 - Loss: 2.0050 | PPL: 7.43 | LR: 1.260697e-04\n",
      "  Step 2600/4407 - Loss: 2.0050 | PPL: 7.43 | LR: 1.260697e-04\n",
      "  Step 2700/4407 - Loss: 2.0053 | PPL: 7.43 | LR: 1.259524e-04\n",
      "  Step 2700/4407 - Loss: 2.0053 | PPL: 7.43 | LR: 1.259524e-04\n",
      "  Step 2800/4407 - Loss: 2.0041 | PPL: 7.42 | LR: 1.258351e-04\n",
      "  Step 2800/4407 - Loss: 2.0041 | PPL: 7.42 | LR: 1.258351e-04\n",
      "  Step 2900/4407 - Loss: 2.0030 | PPL: 7.41 | LR: 1.257179e-04\n",
      "  Step 2900/4407 - Loss: 2.0030 | PPL: 7.41 | LR: 1.257179e-04\n",
      "  Step 3000/4407 - Loss: 2.0030 | PPL: 7.41 | LR: 1.256006e-04\n",
      "  Step 3000/4407 - Loss: 2.0030 | PPL: 7.41 | LR: 1.256006e-04\n",
      "  Step 3100/4407 - Loss: 2.0031 | PPL: 7.41 | LR: 1.254834e-04\n",
      "  Step 3100/4407 - Loss: 2.0031 | PPL: 7.41 | LR: 1.254834e-04\n",
      "  Step 3200/4407 - Loss: 2.0043 | PPL: 7.42 | LR: 1.253662e-04\n",
      "  Step 3200/4407 - Loss: 2.0043 | PPL: 7.42 | LR: 1.253662e-04\n",
      "  Step 3300/4407 - Loss: 2.0044 | PPL: 7.42 | LR: 1.252490e-04\n",
      "  Step 3300/4407 - Loss: 2.0044 | PPL: 7.42 | LR: 1.252490e-04\n",
      "  Step 3400/4407 - Loss: 2.0052 | PPL: 7.43 | LR: 1.251318e-04\n",
      "  Step 3400/4407 - Loss: 2.0052 | PPL: 7.43 | LR: 1.251318e-04\n",
      "  Step 3500/4407 - Loss: 2.0054 | PPL: 7.43 | LR: 1.250147e-04\n",
      "  Step 3500/4407 - Loss: 2.0054 | PPL: 7.43 | LR: 1.250147e-04\n",
      "  Step 3600/4407 - Loss: 2.0061 | PPL: 7.43 | LR: 1.248975e-04\n",
      "  Step 3600/4407 - Loss: 2.0061 | PPL: 7.43 | LR: 1.248975e-04\n",
      "  Step 3700/4407 - Loss: 2.0078 | PPL: 7.45 | LR: 1.247804e-04\n",
      "  Step 3700/4407 - Loss: 2.0078 | PPL: 7.45 | LR: 1.247804e-04\n",
      "  Step 3800/4407 - Loss: 2.0076 | PPL: 7.45 | LR: 1.246633e-04\n",
      "  Step 3800/4407 - Loss: 2.0076 | PPL: 7.45 | LR: 1.246633e-04\n",
      "  Step 3900/4407 - Loss: 2.0074 | PPL: 7.44 | LR: 1.245462e-04\n",
      "  Step 3900/4407 - Loss: 2.0074 | PPL: 7.44 | LR: 1.245462e-04\n",
      "  Step 4000/4407 - Loss: 2.0084 | PPL: 7.45 | LR: 1.244291e-04\n",
      "  Step 4000/4407 - Loss: 2.0084 | PPL: 7.45 | LR: 1.244291e-04\n",
      "  Step 4100/4407 - Loss: 2.0088 | PPL: 7.45 | LR: 1.243121e-04\n",
      "  Step 4100/4407 - Loss: 2.0088 | PPL: 7.45 | LR: 1.243121e-04\n",
      "  Step 4200/4407 - Loss: 2.0083 | PPL: 7.45 | LR: 1.241950e-04\n",
      "  Step 4200/4407 - Loss: 2.0083 | PPL: 7.45 | LR: 1.241950e-04\n",
      "  Step 4300/4407 - Loss: 2.0090 | PPL: 7.46 | LR: 1.240780e-04\n",
      "  Step 4300/4407 - Loss: 2.0090 | PPL: 7.46 | LR: 1.240780e-04\n",
      "  Step 4400/4407 - Loss: 2.0093 | PPL: 7.46 | LR: 1.239610e-04\n",
      "  Step 4400/4407 - Loss: 2.0093 | PPL: 7.46 | LR: 1.239610e-04\n",
      "Epoch 60/100 | Train Loss: 2.0094 | Train PPL: 7.46 | Val Loss: 2.5987 | Val PPL: 13.45\n",
      "Epoch 60/100 | Train Loss: 2.0094 | Train PPL: 7.46 | Val Loss: 2.5987 | Val PPL: 13.45\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.46\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.46\n",
      "  Step 100/4407 - Loss: 1.9734 | PPL: 7.19 | LR: 1.238358e-04\n",
      "  Step 100/4407 - Loss: 1.9734 | PPL: 7.19 | LR: 1.238358e-04\n",
      "  Step 200/4407 - Loss: 1.9671 | PPL: 7.15 | LR: 1.237188e-04\n",
      "  Step 200/4407 - Loss: 1.9671 | PPL: 7.15 | LR: 1.237188e-04\n",
      "  Step 300/4407 - Loss: 1.9947 | PPL: 7.35 | LR: 1.236018e-04\n",
      "  Step 300/4407 - Loss: 1.9947 | PPL: 7.35 | LR: 1.236018e-04\n",
      "  Step 400/4407 - Loss: 1.9864 | PPL: 7.29 | LR: 1.234849e-04\n",
      "  Step 400/4407 - Loss: 1.9864 | PPL: 7.29 | LR: 1.234849e-04\n",
      "  Step 500/4407 - Loss: 1.9877 | PPL: 7.30 | LR: 1.233680e-04\n",
      "  Step 500/4407 - Loss: 1.9877 | PPL: 7.30 | LR: 1.233680e-04\n",
      "  Step 600/4407 - Loss: 1.9976 | PPL: 7.37 | LR: 1.232510e-04\n",
      "  Step 600/4407 - Loss: 1.9976 | PPL: 7.37 | LR: 1.232510e-04\n",
      "  Step 700/4407 - Loss: 1.9927 | PPL: 7.34 | LR: 1.231341e-04\n",
      "  Step 700/4407 - Loss: 1.9927 | PPL: 7.34 | LR: 1.231341e-04\n",
      "  Step 800/4407 - Loss: 2.0006 | PPL: 7.39 | LR: 1.230173e-04\n",
      "  Step 800/4407 - Loss: 2.0006 | PPL: 7.39 | LR: 1.230173e-04\n",
      "  Step 900/4407 - Loss: 1.9989 | PPL: 7.38 | LR: 1.229004e-04\n",
      "  Step 900/4407 - Loss: 1.9989 | PPL: 7.38 | LR: 1.229004e-04\n",
      "  Step 1000/4407 - Loss: 1.9942 | PPL: 7.35 | LR: 1.227835e-04\n",
      "  Step 1000/4407 - Loss: 1.9942 | PPL: 7.35 | LR: 1.227835e-04\n",
      "  Step 1100/4407 - Loss: 1.9968 | PPL: 7.37 | LR: 1.226667e-04\n",
      "  Step 1100/4407 - Loss: 1.9968 | PPL: 7.37 | LR: 1.226667e-04\n",
      "  Step 1200/4407 - Loss: 1.9988 | PPL: 7.38 | LR: 1.225499e-04\n",
      "  Step 1200/4407 - Loss: 1.9988 | PPL: 7.38 | LR: 1.225499e-04\n",
      "  Step 1300/4407 - Loss: 1.9999 | PPL: 7.39 | LR: 1.224331e-04\n",
      "  Step 1300/4407 - Loss: 1.9999 | PPL: 7.39 | LR: 1.224331e-04\n",
      "  Step 1400/4407 - Loss: 1.9970 | PPL: 7.37 | LR: 1.223163e-04\n",
      "  Step 1400/4407 - Loss: 1.9970 | PPL: 7.37 | LR: 1.223163e-04\n",
      "  Step 1500/4407 - Loss: 1.9972 | PPL: 7.37 | LR: 1.221996e-04\n",
      "  Step 1500/4407 - Loss: 1.9972 | PPL: 7.37 | LR: 1.221996e-04\n",
      "  Step 1600/4407 - Loss: 1.9971 | PPL: 7.37 | LR: 1.220828e-04\n",
      "  Step 1600/4407 - Loss: 1.9971 | PPL: 7.37 | LR: 1.220828e-04\n",
      "  Step 1700/4407 - Loss: 1.9963 | PPL: 7.36 | LR: 1.219661e-04\n",
      "  Step 1700/4407 - Loss: 1.9963 | PPL: 7.36 | LR: 1.219661e-04\n",
      "  Step 1800/4407 - Loss: 1.9971 | PPL: 7.37 | LR: 1.218494e-04\n",
      "  Step 1800/4407 - Loss: 1.9971 | PPL: 7.37 | LR: 1.218494e-04\n",
      "  Step 1900/4407 - Loss: 1.9979 | PPL: 7.37 | LR: 1.217327e-04\n",
      "  Step 1900/4407 - Loss: 1.9979 | PPL: 7.37 | LR: 1.217327e-04\n",
      "  Step 2000/4407 - Loss: 1.9980 | PPL: 7.37 | LR: 1.216160e-04\n",
      "  Step 2000/4407 - Loss: 1.9980 | PPL: 7.37 | LR: 1.216160e-04\n",
      "  Step 2100/4407 - Loss: 1.9984 | PPL: 7.38 | LR: 1.214994e-04\n",
      "  Step 2100/4407 - Loss: 1.9984 | PPL: 7.38 | LR: 1.214994e-04\n",
      "  Step 2200/4407 - Loss: 1.9982 | PPL: 7.38 | LR: 1.213827e-04\n",
      "  Step 2200/4407 - Loss: 1.9982 | PPL: 7.38 | LR: 1.213827e-04\n",
      "  Step 2300/4407 - Loss: 1.9983 | PPL: 7.38 | LR: 1.212661e-04\n",
      "  Step 2300/4407 - Loss: 1.9983 | PPL: 7.38 | LR: 1.212661e-04\n",
      "  Step 2400/4407 - Loss: 2.0000 | PPL: 7.39 | LR: 1.211495e-04\n",
      "  Step 2400/4407 - Loss: 2.0000 | PPL: 7.39 | LR: 1.211495e-04\n",
      "  Step 2500/4407 - Loss: 2.0025 | PPL: 7.41 | LR: 1.210329e-04\n",
      "  Step 2500/4407 - Loss: 2.0025 | PPL: 7.41 | LR: 1.210329e-04\n",
      "  Step 2600/4407 - Loss: 2.0012 | PPL: 7.40 | LR: 1.209164e-04\n",
      "  Step 2600/4407 - Loss: 2.0012 | PPL: 7.40 | LR: 1.209164e-04\n",
      "  Step 2700/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.207998e-04\n",
      "  Step 2700/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.207998e-04\n",
      "  Step 2800/4407 - Loss: 2.0010 | PPL: 7.40 | LR: 1.206833e-04\n",
      "  Step 2800/4407 - Loss: 2.0010 | PPL: 7.40 | LR: 1.206833e-04\n",
      "  Step 2900/4407 - Loss: 2.0016 | PPL: 7.40 | LR: 1.205668e-04\n",
      "  Step 2900/4407 - Loss: 2.0016 | PPL: 7.40 | LR: 1.205668e-04\n",
      "  Step 3000/4407 - Loss: 2.0010 | PPL: 7.40 | LR: 1.204503e-04\n",
      "  Step 3000/4407 - Loss: 2.0010 | PPL: 7.40 | LR: 1.204503e-04\n",
      "  Step 3100/4407 - Loss: 2.0020 | PPL: 7.40 | LR: 1.203338e-04\n",
      "  Step 3100/4407 - Loss: 2.0020 | PPL: 7.40 | LR: 1.203338e-04\n",
      "  Step 3200/4407 - Loss: 2.0031 | PPL: 7.41 | LR: 1.202174e-04\n",
      "  Step 3200/4407 - Loss: 2.0031 | PPL: 7.41 | LR: 1.202174e-04\n",
      "  Step 3300/4407 - Loss: 2.0037 | PPL: 7.42 | LR: 1.201009e-04\n",
      "  Step 3300/4407 - Loss: 2.0037 | PPL: 7.42 | LR: 1.201009e-04\n",
      "  Step 3400/4407 - Loss: 2.0031 | PPL: 7.41 | LR: 1.199845e-04\n",
      "  Step 3400/4407 - Loss: 2.0031 | PPL: 7.41 | LR: 1.199845e-04\n",
      "  Step 3500/4407 - Loss: 2.0038 | PPL: 7.42 | LR: 1.198681e-04\n",
      "  Step 3500/4407 - Loss: 2.0038 | PPL: 7.42 | LR: 1.198681e-04\n",
      "  Step 3600/4407 - Loss: 2.0037 | PPL: 7.42 | LR: 1.197517e-04\n",
      "  Step 3600/4407 - Loss: 2.0037 | PPL: 7.42 | LR: 1.197517e-04\n",
      "  Step 3700/4407 - Loss: 2.0039 | PPL: 7.42 | LR: 1.196354e-04\n",
      "  Step 3700/4407 - Loss: 2.0039 | PPL: 7.42 | LR: 1.196354e-04\n",
      "  Step 3800/4407 - Loss: 2.0044 | PPL: 7.42 | LR: 1.195190e-04\n",
      "  Step 3800/4407 - Loss: 2.0044 | PPL: 7.42 | LR: 1.195190e-04\n",
      "  Step 3900/4407 - Loss: 2.0027 | PPL: 7.41 | LR: 1.194027e-04\n",
      "  Step 3900/4407 - Loss: 2.0027 | PPL: 7.41 | LR: 1.194027e-04\n",
      "  Step 4000/4407 - Loss: 2.0032 | PPL: 7.41 | LR: 1.192864e-04\n",
      "  Step 4000/4407 - Loss: 2.0032 | PPL: 7.41 | LR: 1.192864e-04\n",
      "  Step 4100/4407 - Loss: 2.0039 | PPL: 7.42 | LR: 1.191701e-04\n",
      "  Step 4100/4407 - Loss: 2.0039 | PPL: 7.42 | LR: 1.191701e-04\n",
      "  Step 4200/4407 - Loss: 2.0057 | PPL: 7.43 | LR: 1.190539e-04\n",
      "  Step 4200/4407 - Loss: 2.0057 | PPL: 7.43 | LR: 1.190539e-04\n",
      "  Step 4300/4407 - Loss: 2.0063 | PPL: 7.44 | LR: 1.189376e-04\n",
      "  Step 4300/4407 - Loss: 2.0063 | PPL: 7.44 | LR: 1.189376e-04\n",
      "  Step 4400/4407 - Loss: 2.0059 | PPL: 7.43 | LR: 1.188214e-04\n",
      "  Step 4400/4407 - Loss: 2.0059 | PPL: 7.43 | LR: 1.188214e-04\n",
      "Epoch 61/100 | Train Loss: 2.0059 | Train PPL: 7.43 | Val Loss: 2.5918 | Val PPL: 13.35\n",
      "Epoch 61/100 | Train Loss: 2.0059 | Train PPL: 7.43 | Val Loss: 2.5918 | Val PPL: 13.35\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.43\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.43\n",
      "  Step 100/4407 - Loss: 1.9641 | PPL: 7.13 | LR: 1.186970e-04\n",
      "  Step 100/4407 - Loss: 1.9641 | PPL: 7.13 | LR: 1.186970e-04\n",
      "  Step 200/4407 - Loss: 1.9595 | PPL: 7.10 | LR: 1.185809e-04\n",
      "  Step 200/4407 - Loss: 1.9595 | PPL: 7.10 | LR: 1.185809e-04\n",
      "  Step 300/4407 - Loss: 1.9765 | PPL: 7.22 | LR: 1.184647e-04\n",
      "  Step 300/4407 - Loss: 1.9765 | PPL: 7.22 | LR: 1.184647e-04\n",
      "  Step 400/4407 - Loss: 1.9846 | PPL: 7.28 | LR: 1.183485e-04\n",
      "  Step 400/4407 - Loss: 1.9846 | PPL: 7.28 | LR: 1.183485e-04\n",
      "  Step 500/4407 - Loss: 1.9784 | PPL: 7.23 | LR: 1.182324e-04\n",
      "  Step 500/4407 - Loss: 1.9784 | PPL: 7.23 | LR: 1.182324e-04\n",
      "  Step 600/4407 - Loss: 1.9866 | PPL: 7.29 | LR: 1.181163e-04\n",
      "  Step 600/4407 - Loss: 1.9866 | PPL: 7.29 | LR: 1.181163e-04\n",
      "  Step 700/4407 - Loss: 1.9837 | PPL: 7.27 | LR: 1.180002e-04\n",
      "  Step 700/4407 - Loss: 1.9837 | PPL: 7.27 | LR: 1.180002e-04\n",
      "  Step 800/4407 - Loss: 1.9863 | PPL: 7.29 | LR: 1.178842e-04\n",
      "  Step 800/4407 - Loss: 1.9863 | PPL: 7.29 | LR: 1.178842e-04\n",
      "  Step 900/4407 - Loss: 1.9853 | PPL: 7.28 | LR: 1.177681e-04\n",
      "  Step 900/4407 - Loss: 1.9853 | PPL: 7.28 | LR: 1.177681e-04\n",
      "  Step 1000/4407 - Loss: 1.9865 | PPL: 7.29 | LR: 1.176521e-04\n",
      "  Step 1000/4407 - Loss: 1.9865 | PPL: 7.29 | LR: 1.176521e-04\n",
      "  Step 1100/4407 - Loss: 1.9828 | PPL: 7.26 | LR: 1.175361e-04\n",
      "  Step 1100/4407 - Loss: 1.9828 | PPL: 7.26 | LR: 1.175361e-04\n",
      "  Step 1200/4407 - Loss: 1.9838 | PPL: 7.27 | LR: 1.174201e-04\n",
      "  Step 1200/4407 - Loss: 1.9838 | PPL: 7.27 | LR: 1.174201e-04\n",
      "  Step 1300/4407 - Loss: 1.9847 | PPL: 7.28 | LR: 1.173041e-04\n",
      "  Step 1300/4407 - Loss: 1.9847 | PPL: 7.28 | LR: 1.173041e-04\n",
      "  Step 1400/4407 - Loss: 1.9864 | PPL: 7.29 | LR: 1.171882e-04\n",
      "  Step 1400/4407 - Loss: 1.9864 | PPL: 7.29 | LR: 1.171882e-04\n",
      "  Step 1500/4407 - Loss: 1.9873 | PPL: 7.30 | LR: 1.170723e-04\n",
      "  Step 1500/4407 - Loss: 1.9873 | PPL: 7.30 | LR: 1.170723e-04\n",
      "  Step 1600/4407 - Loss: 1.9909 | PPL: 7.32 | LR: 1.169564e-04\n",
      "  Step 1600/4407 - Loss: 1.9909 | PPL: 7.32 | LR: 1.169564e-04\n",
      "  Step 1700/4407 - Loss: 1.9894 | PPL: 7.31 | LR: 1.168405e-04\n",
      "  Step 1700/4407 - Loss: 1.9894 | PPL: 7.31 | LR: 1.168405e-04\n",
      "  Step 1800/4407 - Loss: 1.9900 | PPL: 7.32 | LR: 1.167246e-04\n",
      "  Step 1800/4407 - Loss: 1.9900 | PPL: 7.32 | LR: 1.167246e-04\n",
      "  Step 1900/4407 - Loss: 1.9919 | PPL: 7.33 | LR: 1.166088e-04\n",
      "  Step 1900/4407 - Loss: 1.9919 | PPL: 7.33 | LR: 1.166088e-04\n",
      "  Step 2000/4407 - Loss: 1.9939 | PPL: 7.34 | LR: 1.164930e-04\n",
      "  Step 2000/4407 - Loss: 1.9939 | PPL: 7.34 | LR: 1.164930e-04\n",
      "  Step 2100/4407 - Loss: 1.9942 | PPL: 7.35 | LR: 1.163772e-04\n",
      "  Step 2100/4407 - Loss: 1.9942 | PPL: 7.35 | LR: 1.163772e-04\n",
      "  Step 2200/4407 - Loss: 1.9949 | PPL: 7.35 | LR: 1.162614e-04\n",
      "  Step 2200/4407 - Loss: 1.9949 | PPL: 7.35 | LR: 1.162614e-04\n",
      "  Step 2300/4407 - Loss: 1.9956 | PPL: 7.36 | LR: 1.161456e-04\n",
      "  Step 2300/4407 - Loss: 1.9956 | PPL: 7.36 | LR: 1.161456e-04\n",
      "  Step 2400/4407 - Loss: 1.9958 | PPL: 7.36 | LR: 1.160299e-04\n",
      "  Step 2400/4407 - Loss: 1.9958 | PPL: 7.36 | LR: 1.160299e-04\n",
      "  Step 2500/4407 - Loss: 1.9972 | PPL: 7.37 | LR: 1.159142e-04\n",
      "  Step 2500/4407 - Loss: 1.9972 | PPL: 7.37 | LR: 1.159142e-04\n",
      "  Step 2600/4407 - Loss: 1.9987 | PPL: 7.38 | LR: 1.157985e-04\n",
      "  Step 2600/4407 - Loss: 1.9987 | PPL: 7.38 | LR: 1.157985e-04\n",
      "  Step 2700/4407 - Loss: 1.9996 | PPL: 7.39 | LR: 1.156828e-04\n",
      "  Step 2700/4407 - Loss: 1.9996 | PPL: 7.39 | LR: 1.156828e-04\n",
      "  Step 2800/4407 - Loss: 1.9995 | PPL: 7.39 | LR: 1.155672e-04\n",
      "  Step 2800/4407 - Loss: 1.9995 | PPL: 7.39 | LR: 1.155672e-04\n",
      "  Step 2900/4407 - Loss: 2.0009 | PPL: 7.40 | LR: 1.154516e-04\n",
      "  Step 2900/4407 - Loss: 2.0009 | PPL: 7.40 | LR: 1.154516e-04\n",
      "  Step 3000/4407 - Loss: 2.0014 | PPL: 7.40 | LR: 1.153359e-04\n",
      "  Step 3000/4407 - Loss: 2.0014 | PPL: 7.40 | LR: 1.153359e-04\n",
      "  Step 3100/4407 - Loss: 2.0009 | PPL: 7.40 | LR: 1.152204e-04\n",
      "  Step 3100/4407 - Loss: 2.0009 | PPL: 7.40 | LR: 1.152204e-04\n",
      "  Step 3200/4407 - Loss: 2.0003 | PPL: 7.39 | LR: 1.151048e-04\n",
      "  Step 3200/4407 - Loss: 2.0003 | PPL: 7.39 | LR: 1.151048e-04\n",
      "  Step 3300/4407 - Loss: 2.0002 | PPL: 7.39 | LR: 1.149893e-04\n",
      "  Step 3300/4407 - Loss: 2.0002 | PPL: 7.39 | LR: 1.149893e-04\n",
      "  Step 3400/4407 - Loss: 2.0000 | PPL: 7.39 | LR: 1.148737e-04\n",
      "  Step 3400/4407 - Loss: 2.0000 | PPL: 7.39 | LR: 1.148737e-04\n",
      "  Step 3500/4407 - Loss: 2.0003 | PPL: 7.39 | LR: 1.147582e-04\n",
      "  Step 3500/4407 - Loss: 2.0003 | PPL: 7.39 | LR: 1.147582e-04\n",
      "  Step 3600/4407 - Loss: 2.0008 | PPL: 7.40 | LR: 1.146428e-04\n",
      "  Step 3600/4407 - Loss: 2.0008 | PPL: 7.40 | LR: 1.146428e-04\n",
      "  Step 3700/4407 - Loss: 2.0007 | PPL: 7.39 | LR: 1.145273e-04\n",
      "  Step 3700/4407 - Loss: 2.0007 | PPL: 7.39 | LR: 1.145273e-04\n",
      "  Step 3800/4407 - Loss: 2.0005 | PPL: 7.39 | LR: 1.144119e-04\n",
      "  Step 3800/4407 - Loss: 2.0005 | PPL: 7.39 | LR: 1.144119e-04\n",
      "  Step 3900/4407 - Loss: 2.0003 | PPL: 7.39 | LR: 1.142965e-04\n",
      "  Step 3900/4407 - Loss: 2.0003 | PPL: 7.39 | LR: 1.142965e-04\n",
      "  Step 4000/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.141811e-04\n",
      "  Step 4000/4407 - Loss: 2.0011 | PPL: 7.40 | LR: 1.141811e-04\n",
      "  Step 4100/4407 - Loss: 2.0029 | PPL: 7.41 | LR: 1.140657e-04\n",
      "  Step 4100/4407 - Loss: 2.0029 | PPL: 7.41 | LR: 1.140657e-04\n",
      "  Step 4200/4407 - Loss: 2.0030 | PPL: 7.41 | LR: 1.139504e-04\n",
      "  Step 4200/4407 - Loss: 2.0030 | PPL: 7.41 | LR: 1.139504e-04\n",
      "  Step 4300/4407 - Loss: 2.0034 | PPL: 7.41 | LR: 1.138351e-04\n",
      "  Step 4300/4407 - Loss: 2.0034 | PPL: 7.41 | LR: 1.138351e-04\n",
      "  Step 4400/4407 - Loss: 2.0025 | PPL: 7.41 | LR: 1.137198e-04\n",
      "  Step 4400/4407 - Loss: 2.0025 | PPL: 7.41 | LR: 1.137198e-04\n",
      "Epoch 62/100 | Train Loss: 2.0025 | Train PPL: 7.41 | Val Loss: 2.5955 | Val PPL: 13.40\n",
      "Epoch 62/100 | Train Loss: 2.0025 | Train PPL: 7.41 | Val Loss: 2.5955 | Val PPL: 13.40\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.41\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.41\n",
      "  Step 100/4407 - Loss: 2.0380 | PPL: 7.68 | LR: 1.135964e-04\n",
      "  Step 100/4407 - Loss: 2.0380 | PPL: 7.68 | LR: 1.135964e-04\n",
      "  Step 200/4407 - Loss: 1.9860 | PPL: 7.29 | LR: 1.134812e-04\n",
      "  Step 200/4407 - Loss: 1.9860 | PPL: 7.29 | LR: 1.134812e-04\n",
      "  Step 300/4407 - Loss: 1.9849 | PPL: 7.28 | LR: 1.133660e-04\n",
      "  Step 300/4407 - Loss: 1.9849 | PPL: 7.28 | LR: 1.133660e-04\n",
      "  Step 400/4407 - Loss: 1.9724 | PPL: 7.19 | LR: 1.132508e-04\n",
      "  Step 400/4407 - Loss: 1.9724 | PPL: 7.19 | LR: 1.132508e-04\n",
      "  Step 500/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 1.131356e-04\n",
      "  Step 500/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 1.131356e-04\n",
      "  Step 600/4407 - Loss: 1.9749 | PPL: 7.21 | LR: 1.130204e-04\n",
      "  Step 600/4407 - Loss: 1.9749 | PPL: 7.21 | LR: 1.130204e-04\n",
      "  Step 700/4407 - Loss: 1.9776 | PPL: 7.23 | LR: 1.129053e-04\n",
      "  Step 700/4407 - Loss: 1.9776 | PPL: 7.23 | LR: 1.129053e-04\n",
      "  Step 800/4407 - Loss: 1.9731 | PPL: 7.19 | LR: 1.127902e-04\n",
      "  Step 800/4407 - Loss: 1.9731 | PPL: 7.19 | LR: 1.127902e-04\n",
      "  Step 900/4407 - Loss: 1.9763 | PPL: 7.22 | LR: 1.126751e-04\n",
      "  Step 900/4407 - Loss: 1.9763 | PPL: 7.22 | LR: 1.126751e-04\n",
      "  Step 1000/4407 - Loss: 1.9788 | PPL: 7.23 | LR: 1.125601e-04\n",
      "  Step 1000/4407 - Loss: 1.9788 | PPL: 7.23 | LR: 1.125601e-04\n",
      "  Step 1100/4407 - Loss: 1.9773 | PPL: 7.22 | LR: 1.124450e-04\n",
      "  Step 1100/4407 - Loss: 1.9773 | PPL: 7.22 | LR: 1.124450e-04\n",
      "  Step 1200/4407 - Loss: 1.9759 | PPL: 7.21 | LR: 1.123300e-04\n",
      "  Step 1200/4407 - Loss: 1.9759 | PPL: 7.21 | LR: 1.123300e-04\n",
      "  Step 1300/4407 - Loss: 1.9752 | PPL: 7.21 | LR: 1.122150e-04\n",
      "  Step 1300/4407 - Loss: 1.9752 | PPL: 7.21 | LR: 1.122150e-04\n",
      "  Step 1400/4407 - Loss: 1.9751 | PPL: 7.21 | LR: 1.121000e-04\n",
      "  Step 1400/4407 - Loss: 1.9751 | PPL: 7.21 | LR: 1.121000e-04\n",
      "  Step 1500/4407 - Loss: 1.9770 | PPL: 7.22 | LR: 1.119851e-04\n",
      "  Step 1500/4407 - Loss: 1.9770 | PPL: 7.22 | LR: 1.119851e-04\n",
      "  Step 1600/4407 - Loss: 1.9773 | PPL: 7.22 | LR: 1.118702e-04\n",
      "  Step 1600/4407 - Loss: 1.9773 | PPL: 7.22 | LR: 1.118702e-04\n",
      "  Step 1700/4407 - Loss: 1.9773 | PPL: 7.22 | LR: 1.117553e-04\n",
      "  Step 1700/4407 - Loss: 1.9773 | PPL: 7.22 | LR: 1.117553e-04\n",
      "  Step 1800/4407 - Loss: 1.9805 | PPL: 7.25 | LR: 1.116404e-04\n",
      "  Step 1800/4407 - Loss: 1.9805 | PPL: 7.25 | LR: 1.116404e-04\n",
      "  Step 1900/4407 - Loss: 1.9829 | PPL: 7.26 | LR: 1.115256e-04\n",
      "  Step 1900/4407 - Loss: 1.9829 | PPL: 7.26 | LR: 1.115256e-04\n",
      "  Step 2000/4407 - Loss: 1.9868 | PPL: 7.29 | LR: 1.114107e-04\n",
      "  Step 2000/4407 - Loss: 1.9868 | PPL: 7.29 | LR: 1.114107e-04\n",
      "  Step 2100/4407 - Loss: 1.9881 | PPL: 7.30 | LR: 1.112959e-04\n",
      "  Step 2100/4407 - Loss: 1.9881 | PPL: 7.30 | LR: 1.112959e-04\n",
      "  Step 2200/4407 - Loss: 1.9882 | PPL: 7.30 | LR: 1.111812e-04\n",
      "  Step 2200/4407 - Loss: 1.9882 | PPL: 7.30 | LR: 1.111812e-04\n",
      "  Step 2300/4407 - Loss: 1.9878 | PPL: 7.30 | LR: 1.110664e-04\n",
      "  Step 2300/4407 - Loss: 1.9878 | PPL: 7.30 | LR: 1.110664e-04\n",
      "  Step 2400/4407 - Loss: 1.9889 | PPL: 7.31 | LR: 1.109517e-04\n",
      "  Step 2400/4407 - Loss: 1.9889 | PPL: 7.31 | LR: 1.109517e-04\n",
      "  Step 2500/4407 - Loss: 1.9907 | PPL: 7.32 | LR: 1.108370e-04\n",
      "  Step 2500/4407 - Loss: 1.9907 | PPL: 7.32 | LR: 1.108370e-04\n",
      "  Step 2600/4407 - Loss: 1.9888 | PPL: 7.31 | LR: 1.107223e-04\n",
      "  Step 2600/4407 - Loss: 1.9888 | PPL: 7.31 | LR: 1.107223e-04\n",
      "  Step 2700/4407 - Loss: 1.9900 | PPL: 7.32 | LR: 1.106076e-04\n",
      "  Step 2700/4407 - Loss: 1.9900 | PPL: 7.32 | LR: 1.106076e-04\n",
      "  Step 2800/4407 - Loss: 1.9894 | PPL: 7.31 | LR: 1.104930e-04\n",
      "  Step 2800/4407 - Loss: 1.9894 | PPL: 7.31 | LR: 1.104930e-04\n",
      "  Step 2900/4407 - Loss: 1.9916 | PPL: 7.33 | LR: 1.103784e-04\n",
      "  Step 2900/4407 - Loss: 1.9916 | PPL: 7.33 | LR: 1.103784e-04\n",
      "  Step 3000/4407 - Loss: 1.9924 | PPL: 7.33 | LR: 1.102638e-04\n",
      "  Step 3000/4407 - Loss: 1.9924 | PPL: 7.33 | LR: 1.102638e-04\n",
      "  Step 3100/4407 - Loss: 1.9918 | PPL: 7.33 | LR: 1.101493e-04\n",
      "  Step 3100/4407 - Loss: 1.9918 | PPL: 7.33 | LR: 1.101493e-04\n",
      "  Step 3200/4407 - Loss: 1.9929 | PPL: 7.34 | LR: 1.100348e-04\n",
      "  Step 3200/4407 - Loss: 1.9929 | PPL: 7.34 | LR: 1.100348e-04\n",
      "  Step 3300/4407 - Loss: 1.9936 | PPL: 7.34 | LR: 1.099203e-04\n",
      "  Step 3300/4407 - Loss: 1.9936 | PPL: 7.34 | LR: 1.099203e-04\n",
      "  Step 3400/4407 - Loss: 1.9940 | PPL: 7.34 | LR: 1.098058e-04\n",
      "  Step 3400/4407 - Loss: 1.9940 | PPL: 7.34 | LR: 1.098058e-04\n",
      "  Step 3500/4407 - Loss: 1.9944 | PPL: 7.35 | LR: 1.096913e-04\n",
      "  Step 3500/4407 - Loss: 1.9944 | PPL: 7.35 | LR: 1.096913e-04\n",
      "  Step 3600/4407 - Loss: 1.9954 | PPL: 7.35 | LR: 1.095769e-04\n",
      "  Step 3600/4407 - Loss: 1.9954 | PPL: 7.35 | LR: 1.095769e-04\n",
      "  Step 3700/4407 - Loss: 1.9957 | PPL: 7.36 | LR: 1.094625e-04\n",
      "  Step 3700/4407 - Loss: 1.9957 | PPL: 7.36 | LR: 1.094625e-04\n",
      "  Step 3800/4407 - Loss: 1.9958 | PPL: 7.36 | LR: 1.093481e-04\n",
      "  Step 3800/4407 - Loss: 1.9958 | PPL: 7.36 | LR: 1.093481e-04\n",
      "  Step 3900/4407 - Loss: 1.9963 | PPL: 7.36 | LR: 1.092338e-04\n",
      "  Step 3900/4407 - Loss: 1.9963 | PPL: 7.36 | LR: 1.092338e-04\n",
      "  Step 4000/4407 - Loss: 1.9976 | PPL: 7.37 | LR: 1.091194e-04\n",
      "  Step 4000/4407 - Loss: 1.9976 | PPL: 7.37 | LR: 1.091194e-04\n",
      "  Step 4100/4407 - Loss: 1.9977 | PPL: 7.37 | LR: 1.090051e-04\n",
      "  Step 4100/4407 - Loss: 1.9977 | PPL: 7.37 | LR: 1.090051e-04\n",
      "  Step 4200/4407 - Loss: 1.9986 | PPL: 7.38 | LR: 1.088909e-04\n",
      "  Step 4200/4407 - Loss: 1.9986 | PPL: 7.38 | LR: 1.088909e-04\n",
      "  Step 4300/4407 - Loss: 1.9986 | PPL: 7.38 | LR: 1.087766e-04\n",
      "  Step 4300/4407 - Loss: 1.9986 | PPL: 7.38 | LR: 1.087766e-04\n",
      "  Step 4400/4407 - Loss: 1.9989 | PPL: 7.38 | LR: 1.086624e-04\n",
      "  Step 4400/4407 - Loss: 1.9989 | PPL: 7.38 | LR: 1.086624e-04\n",
      "Epoch 63/100 | Train Loss: 1.9991 | Train PPL: 7.38 | Val Loss: 2.5943 | Val PPL: 13.39\n",
      "Epoch 63/100 | Train Loss: 1.9991 | Train PPL: 7.38 | Val Loss: 2.5943 | Val PPL: 13.39\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.38\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.38\n",
      "  Step 100/4407 - Loss: 1.9562 | PPL: 7.07 | LR: 1.085402e-04\n",
      "  Step 100/4407 - Loss: 1.9562 | PPL: 7.07 | LR: 1.085402e-04\n",
      "  Step 200/4407 - Loss: 1.9964 | PPL: 7.36 | LR: 1.084260e-04\n",
      "  Step 200/4407 - Loss: 1.9964 | PPL: 7.36 | LR: 1.084260e-04\n",
      "  Step 300/4407 - Loss: 2.0014 | PPL: 7.40 | LR: 1.083119e-04\n",
      "  Step 300/4407 - Loss: 2.0014 | PPL: 7.40 | LR: 1.083119e-04\n",
      "  Step 400/4407 - Loss: 2.0046 | PPL: 7.42 | LR: 1.081978e-04\n",
      "  Step 400/4407 - Loss: 2.0046 | PPL: 7.42 | LR: 1.081978e-04\n",
      "  Step 500/4407 - Loss: 2.0005 | PPL: 7.39 | LR: 1.080837e-04\n",
      "  Step 500/4407 - Loss: 2.0005 | PPL: 7.39 | LR: 1.080837e-04\n",
      "  Step 600/4407 - Loss: 1.9912 | PPL: 7.32 | LR: 1.079696e-04\n",
      "  Step 600/4407 - Loss: 1.9912 | PPL: 7.32 | LR: 1.079696e-04\n",
      "  Step 700/4407 - Loss: 1.9921 | PPL: 7.33 | LR: 1.078556e-04\n",
      "  Step 700/4407 - Loss: 1.9921 | PPL: 7.33 | LR: 1.078556e-04\n",
      "  Step 800/4407 - Loss: 1.9915 | PPL: 7.33 | LR: 1.077416e-04\n",
      "  Step 800/4407 - Loss: 1.9915 | PPL: 7.33 | LR: 1.077416e-04\n",
      "  Step 900/4407 - Loss: 1.9906 | PPL: 7.32 | LR: 1.076276e-04\n",
      "  Step 900/4407 - Loss: 1.9906 | PPL: 7.32 | LR: 1.076276e-04\n",
      "  Step 1000/4407 - Loss: 1.9864 | PPL: 7.29 | LR: 1.075136e-04\n",
      "  Step 1000/4407 - Loss: 1.9864 | PPL: 7.29 | LR: 1.075136e-04\n",
      "  Step 1100/4407 - Loss: 1.9865 | PPL: 7.29 | LR: 1.073997e-04\n",
      "  Step 1100/4407 - Loss: 1.9865 | PPL: 7.29 | LR: 1.073997e-04\n",
      "  Step 1200/4407 - Loss: 1.9858 | PPL: 7.28 | LR: 1.072858e-04\n",
      "  Step 1200/4407 - Loss: 1.9858 | PPL: 7.28 | LR: 1.072858e-04\n",
      "  Step 1300/4407 - Loss: 1.9881 | PPL: 7.30 | LR: 1.071719e-04\n",
      "  Step 1300/4407 - Loss: 1.9881 | PPL: 7.30 | LR: 1.071719e-04\n",
      "  Step 1400/4407 - Loss: 1.9868 | PPL: 7.29 | LR: 1.070581e-04\n",
      "  Step 1400/4407 - Loss: 1.9868 | PPL: 7.29 | LR: 1.070581e-04\n",
      "  Step 1500/4407 - Loss: 1.9895 | PPL: 7.31 | LR: 1.069442e-04\n",
      "  Step 1500/4407 - Loss: 1.9895 | PPL: 7.31 | LR: 1.069442e-04\n",
      "  Step 1600/4407 - Loss: 1.9893 | PPL: 7.31 | LR: 1.068304e-04\n",
      "  Step 1600/4407 - Loss: 1.9893 | PPL: 7.31 | LR: 1.068304e-04\n",
      "  Step 1700/4407 - Loss: 1.9898 | PPL: 7.31 | LR: 1.067167e-04\n",
      "  Step 1700/4407 - Loss: 1.9898 | PPL: 7.31 | LR: 1.067167e-04\n",
      "  Step 1800/4407 - Loss: 1.9879 | PPL: 7.30 | LR: 1.066029e-04\n",
      "  Step 1800/4407 - Loss: 1.9879 | PPL: 7.30 | LR: 1.066029e-04\n",
      "  Step 1900/4407 - Loss: 1.9873 | PPL: 7.30 | LR: 1.064892e-04\n",
      "  Step 1900/4407 - Loss: 1.9873 | PPL: 7.30 | LR: 1.064892e-04\n",
      "  Step 2000/4407 - Loss: 1.9863 | PPL: 7.29 | LR: 1.063755e-04\n",
      "  Step 2000/4407 - Loss: 1.9863 | PPL: 7.29 | LR: 1.063755e-04\n",
      "  Step 2100/4407 - Loss: 1.9876 | PPL: 7.30 | LR: 1.062619e-04\n",
      "  Step 2100/4407 - Loss: 1.9876 | PPL: 7.30 | LR: 1.062619e-04\n",
      "  Step 2200/4407 - Loss: 1.9881 | PPL: 7.30 | LR: 1.061482e-04\n",
      "  Step 2200/4407 - Loss: 1.9881 | PPL: 7.30 | LR: 1.061482e-04\n",
      "  Step 2300/4407 - Loss: 1.9892 | PPL: 7.31 | LR: 1.060346e-04\n",
      "  Step 2300/4407 - Loss: 1.9892 | PPL: 7.31 | LR: 1.060346e-04\n",
      "  Step 2400/4407 - Loss: 1.9909 | PPL: 7.32 | LR: 1.059210e-04\n",
      "  Step 2400/4407 - Loss: 1.9909 | PPL: 7.32 | LR: 1.059210e-04\n",
      "  Step 2500/4407 - Loss: 1.9902 | PPL: 7.32 | LR: 1.058075e-04\n",
      "  Step 2500/4407 - Loss: 1.9902 | PPL: 7.32 | LR: 1.058075e-04\n",
      "  Step 2600/4407 - Loss: 1.9890 | PPL: 7.31 | LR: 1.056940e-04\n",
      "  Step 2600/4407 - Loss: 1.9890 | PPL: 7.31 | LR: 1.056940e-04\n",
      "  Step 2700/4407 - Loss: 1.9901 | PPL: 7.32 | LR: 1.055805e-04\n",
      "  Step 2700/4407 - Loss: 1.9901 | PPL: 7.32 | LR: 1.055805e-04\n",
      "  Step 2800/4407 - Loss: 1.9909 | PPL: 7.32 | LR: 1.054670e-04\n",
      "  Step 2800/4407 - Loss: 1.9909 | PPL: 7.32 | LR: 1.054670e-04\n",
      "  Step 2900/4407 - Loss: 1.9907 | PPL: 7.32 | LR: 1.053536e-04\n",
      "  Step 2900/4407 - Loss: 1.9907 | PPL: 7.32 | LR: 1.053536e-04\n",
      "  Step 3000/4407 - Loss: 1.9912 | PPL: 7.32 | LR: 1.052401e-04\n",
      "  Step 3000/4407 - Loss: 1.9912 | PPL: 7.32 | LR: 1.052401e-04\n",
      "  Step 3100/4407 - Loss: 1.9911 | PPL: 7.32 | LR: 1.051268e-04\n",
      "  Step 3100/4407 - Loss: 1.9911 | PPL: 7.32 | LR: 1.051268e-04\n",
      "  Step 3200/4407 - Loss: 1.9926 | PPL: 7.33 | LR: 1.050134e-04\n",
      "  Step 3200/4407 - Loss: 1.9926 | PPL: 7.33 | LR: 1.050134e-04\n",
      "  Step 3300/4407 - Loss: 1.9931 | PPL: 7.34 | LR: 1.049001e-04\n",
      "  Step 3300/4407 - Loss: 1.9931 | PPL: 7.34 | LR: 1.049001e-04\n",
      "  Step 3400/4407 - Loss: 1.9935 | PPL: 7.34 | LR: 1.047868e-04\n",
      "  Step 3400/4407 - Loss: 1.9935 | PPL: 7.34 | LR: 1.047868e-04\n",
      "  Step 3500/4407 - Loss: 1.9946 | PPL: 7.35 | LR: 1.046735e-04\n",
      "  Step 3500/4407 - Loss: 1.9946 | PPL: 7.35 | LR: 1.046735e-04\n",
      "  Step 3600/4407 - Loss: 1.9949 | PPL: 7.35 | LR: 1.045603e-04\n",
      "  Step 3600/4407 - Loss: 1.9949 | PPL: 7.35 | LR: 1.045603e-04\n",
      "  Step 3700/4407 - Loss: 1.9960 | PPL: 7.36 | LR: 1.044471e-04\n",
      "  Step 3700/4407 - Loss: 1.9960 | PPL: 7.36 | LR: 1.044471e-04\n",
      "  Step 3800/4407 - Loss: 1.9964 | PPL: 7.36 | LR: 1.043339e-04\n",
      "  Step 3800/4407 - Loss: 1.9964 | PPL: 7.36 | LR: 1.043339e-04\n",
      "  Step 3900/4407 - Loss: 1.9953 | PPL: 7.35 | LR: 1.042207e-04\n",
      "  Step 3900/4407 - Loss: 1.9953 | PPL: 7.35 | LR: 1.042207e-04\n",
      "  Step 4000/4407 - Loss: 1.9963 | PPL: 7.36 | LR: 1.041076e-04\n",
      "  Step 4000/4407 - Loss: 1.9963 | PPL: 7.36 | LR: 1.041076e-04\n",
      "  Step 4100/4407 - Loss: 1.9963 | PPL: 7.36 | LR: 1.039945e-04\n",
      "  Step 4100/4407 - Loss: 1.9963 | PPL: 7.36 | LR: 1.039945e-04\n",
      "  Step 4200/4407 - Loss: 1.9958 | PPL: 7.36 | LR: 1.038814e-04\n",
      "  Step 4200/4407 - Loss: 1.9958 | PPL: 7.36 | LR: 1.038814e-04\n",
      "  Step 4300/4407 - Loss: 1.9961 | PPL: 7.36 | LR: 1.037684e-04\n",
      "  Step 4300/4407 - Loss: 1.9961 | PPL: 7.36 | LR: 1.037684e-04\n",
      "  Step 4400/4407 - Loss: 1.9959 | PPL: 7.36 | LR: 1.036554e-04\n",
      "  Step 4400/4407 - Loss: 1.9959 | PPL: 7.36 | LR: 1.036554e-04\n",
      "Epoch 64/100 | Train Loss: 1.9958 | Train PPL: 7.36 | Val Loss: 2.5994 | Val PPL: 13.46\n",
      "Epoch 64/100 | Train Loss: 1.9958 | Train PPL: 7.36 | Val Loss: 2.5994 | Val PPL: 13.46\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.36\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.36\n",
      "  Step 100/4407 - Loss: 1.9412 | PPL: 6.97 | LR: 1.035345e-04\n",
      "  Step 100/4407 - Loss: 1.9412 | PPL: 6.97 | LR: 1.035345e-04\n",
      "  Step 200/4407 - Loss: 1.9601 | PPL: 7.10 | LR: 1.034215e-04\n",
      "  Step 200/4407 - Loss: 1.9601 | PPL: 7.10 | LR: 1.034215e-04\n",
      "  Step 300/4407 - Loss: 1.9608 | PPL: 7.11 | LR: 1.033086e-04\n",
      "  Step 300/4407 - Loss: 1.9608 | PPL: 7.11 | LR: 1.033086e-04\n",
      "  Step 400/4407 - Loss: 1.9459 | PPL: 7.00 | LR: 1.031957e-04\n",
      "  Step 400/4407 - Loss: 1.9459 | PPL: 7.00 | LR: 1.031957e-04\n",
      "  Step 500/4407 - Loss: 1.9523 | PPL: 7.05 | LR: 1.030828e-04\n",
      "  Step 500/4407 - Loss: 1.9523 | PPL: 7.05 | LR: 1.030828e-04\n",
      "  Step 600/4407 - Loss: 1.9541 | PPL: 7.06 | LR: 1.029700e-04\n",
      "  Step 600/4407 - Loss: 1.9541 | PPL: 7.06 | LR: 1.029700e-04\n",
      "  Step 700/4407 - Loss: 1.9604 | PPL: 7.10 | LR: 1.028572e-04\n",
      "  Step 700/4407 - Loss: 1.9604 | PPL: 7.10 | LR: 1.028572e-04\n",
      "  Step 800/4407 - Loss: 1.9593 | PPL: 7.09 | LR: 1.027444e-04\n",
      "  Step 800/4407 - Loss: 1.9593 | PPL: 7.09 | LR: 1.027444e-04\n",
      "  Step 900/4407 - Loss: 1.9675 | PPL: 7.15 | LR: 1.026317e-04\n",
      "  Step 900/4407 - Loss: 1.9675 | PPL: 7.15 | LR: 1.026317e-04\n",
      "  Step 1000/4407 - Loss: 1.9704 | PPL: 7.17 | LR: 1.025190e-04\n",
      "  Step 1000/4407 - Loss: 1.9704 | PPL: 7.17 | LR: 1.025190e-04\n",
      "  Step 1100/4407 - Loss: 1.9726 | PPL: 7.19 | LR: 1.024063e-04\n",
      "  Step 1100/4407 - Loss: 1.9726 | PPL: 7.19 | LR: 1.024063e-04\n",
      "  Step 1200/4407 - Loss: 1.9731 | PPL: 7.19 | LR: 1.022936e-04\n",
      "  Step 1200/4407 - Loss: 1.9731 | PPL: 7.19 | LR: 1.022936e-04\n",
      "  Step 1300/4407 - Loss: 1.9734 | PPL: 7.19 | LR: 1.021810e-04\n",
      "  Step 1300/4407 - Loss: 1.9734 | PPL: 7.19 | LR: 1.021810e-04\n",
      "  Step 1400/4407 - Loss: 1.9733 | PPL: 7.19 | LR: 1.020684e-04\n",
      "  Step 1400/4407 - Loss: 1.9733 | PPL: 7.19 | LR: 1.020684e-04\n",
      "  Step 1500/4407 - Loss: 1.9728 | PPL: 7.19 | LR: 1.019558e-04\n",
      "  Step 1500/4407 - Loss: 1.9728 | PPL: 7.19 | LR: 1.019558e-04\n",
      "  Step 1600/4407 - Loss: 1.9759 | PPL: 7.21 | LR: 1.018433e-04\n",
      "  Step 1600/4407 - Loss: 1.9759 | PPL: 7.21 | LR: 1.018433e-04\n",
      "  Step 1700/4407 - Loss: 1.9777 | PPL: 7.23 | LR: 1.017308e-04\n",
      "  Step 1700/4407 - Loss: 1.9777 | PPL: 7.23 | LR: 1.017308e-04\n",
      "  Step 1800/4407 - Loss: 1.9796 | PPL: 7.24 | LR: 1.016183e-04\n",
      "  Step 1800/4407 - Loss: 1.9796 | PPL: 7.24 | LR: 1.016183e-04\n",
      "  Step 1900/4407 - Loss: 1.9811 | PPL: 7.25 | LR: 1.015059e-04\n",
      "  Step 1900/4407 - Loss: 1.9811 | PPL: 7.25 | LR: 1.015059e-04\n",
      "  Step 2000/4407 - Loss: 1.9820 | PPL: 7.26 | LR: 1.013934e-04\n",
      "  Step 2000/4407 - Loss: 1.9820 | PPL: 7.26 | LR: 1.013934e-04\n",
      "  Step 2100/4407 - Loss: 1.9822 | PPL: 7.26 | LR: 1.012811e-04\n",
      "  Step 2100/4407 - Loss: 1.9822 | PPL: 7.26 | LR: 1.012811e-04\n",
      "  Step 2200/4407 - Loss: 1.9833 | PPL: 7.27 | LR: 1.011687e-04\n",
      "  Step 2200/4407 - Loss: 1.9833 | PPL: 7.27 | LR: 1.011687e-04\n",
      "  Step 2300/4407 - Loss: 1.9835 | PPL: 7.27 | LR: 1.010564e-04\n",
      "  Step 2300/4407 - Loss: 1.9835 | PPL: 7.27 | LR: 1.010564e-04\n",
      "  Step 2400/4407 - Loss: 1.9844 | PPL: 7.27 | LR: 1.009441e-04\n",
      "  Step 2400/4407 - Loss: 1.9844 | PPL: 7.27 | LR: 1.009441e-04\n",
      "  Step 2500/4407 - Loss: 1.9861 | PPL: 7.29 | LR: 1.008318e-04\n",
      "  Step 2500/4407 - Loss: 1.9861 | PPL: 7.29 | LR: 1.008318e-04\n",
      "  Step 2600/4407 - Loss: 1.9876 | PPL: 7.30 | LR: 1.007196e-04\n",
      "  Step 2600/4407 - Loss: 1.9876 | PPL: 7.30 | LR: 1.007196e-04\n",
      "  Step 2700/4407 - Loss: 1.9872 | PPL: 7.29 | LR: 1.006074e-04\n",
      "  Step 2700/4407 - Loss: 1.9872 | PPL: 7.29 | LR: 1.006074e-04\n",
      "  Step 2800/4407 - Loss: 1.9877 | PPL: 7.30 | LR: 1.004952e-04\n",
      "  Step 2800/4407 - Loss: 1.9877 | PPL: 7.30 | LR: 1.004952e-04\n",
      "  Step 2900/4407 - Loss: 1.9884 | PPL: 7.30 | LR: 1.003831e-04\n",
      "  Step 2900/4407 - Loss: 1.9884 | PPL: 7.30 | LR: 1.003831e-04\n",
      "  Step 3000/4407 - Loss: 1.9882 | PPL: 7.30 | LR: 1.002710e-04\n",
      "  Step 3000/4407 - Loss: 1.9882 | PPL: 7.30 | LR: 1.002710e-04\n",
      "  Step 3100/4407 - Loss: 1.9889 | PPL: 7.31 | LR: 1.001589e-04\n",
      "  Step 3100/4407 - Loss: 1.9889 | PPL: 7.31 | LR: 1.001589e-04\n",
      "  Step 3200/4407 - Loss: 1.9872 | PPL: 7.30 | LR: 1.000469e-04\n",
      "  Step 3200/4407 - Loss: 1.9872 | PPL: 7.30 | LR: 1.000469e-04\n",
      "  Step 3300/4407 - Loss: 1.9882 | PPL: 7.30 | LR: 9.993485e-05\n",
      "  Step 3300/4407 - Loss: 1.9882 | PPL: 7.30 | LR: 9.993485e-05\n",
      "  Step 3400/4407 - Loss: 1.9886 | PPL: 7.31 | LR: 9.982287e-05\n",
      "  Step 3400/4407 - Loss: 1.9886 | PPL: 7.31 | LR: 9.982287e-05\n",
      "  Step 3500/4407 - Loss: 1.9903 | PPL: 7.32 | LR: 9.971092e-05\n",
      "  Step 3500/4407 - Loss: 1.9903 | PPL: 7.32 | LR: 9.971092e-05\n",
      "  Step 3600/4407 - Loss: 1.9899 | PPL: 7.31 | LR: 9.959900e-05\n",
      "  Step 3600/4407 - Loss: 1.9899 | PPL: 7.31 | LR: 9.959900e-05\n",
      "  Step 3700/4407 - Loss: 1.9906 | PPL: 7.32 | LR: 9.948711e-05\n",
      "  Step 3700/4407 - Loss: 1.9906 | PPL: 7.32 | LR: 9.948711e-05\n",
      "  Step 3800/4407 - Loss: 1.9919 | PPL: 7.33 | LR: 9.937526e-05\n",
      "  Step 3800/4407 - Loss: 1.9919 | PPL: 7.33 | LR: 9.937526e-05\n",
      "  Step 3900/4407 - Loss: 1.9918 | PPL: 7.33 | LR: 9.926343e-05\n",
      "  Step 3900/4407 - Loss: 1.9918 | PPL: 7.33 | LR: 9.926343e-05\n",
      "  Step 4000/4407 - Loss: 1.9922 | PPL: 7.33 | LR: 9.915164e-05\n",
      "  Step 4000/4407 - Loss: 1.9922 | PPL: 7.33 | LR: 9.915164e-05\n",
      "  Step 4100/4407 - Loss: 1.9926 | PPL: 7.33 | LR: 9.903988e-05\n",
      "  Step 4100/4407 - Loss: 1.9926 | PPL: 7.33 | LR: 9.903988e-05\n",
      "  Step 4200/4407 - Loss: 1.9928 | PPL: 7.34 | LR: 9.892815e-05\n",
      "  Step 4200/4407 - Loss: 1.9928 | PPL: 7.34 | LR: 9.892815e-05\n",
      "  Step 4300/4407 - Loss: 1.9926 | PPL: 7.33 | LR: 9.881646e-05\n",
      "  Step 4300/4407 - Loss: 1.9926 | PPL: 7.33 | LR: 9.881646e-05\n",
      "  Step 4400/4407 - Loss: 1.9922 | PPL: 7.33 | LR: 9.870479e-05\n",
      "  Step 4400/4407 - Loss: 1.9922 | PPL: 7.33 | LR: 9.870479e-05\n",
      "Epoch 65/100 | Train Loss: 1.9923 | Train PPL: 7.33 | Val Loss: 2.5931 | Val PPL: 13.37\n",
      "Epoch 65/100 | Train Loss: 1.9923 | Train PPL: 7.33 | Val Loss: 2.5931 | Val PPL: 13.37\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.33\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.33\n",
      "  Step 100/4407 - Loss: 1.9645 | PPL: 7.13 | LR: 9.858535e-05\n",
      "  Step 100/4407 - Loss: 1.9645 | PPL: 7.13 | LR: 9.858535e-05\n",
      "  Step 200/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 9.847375e-05\n",
      "  Step 200/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 9.847375e-05\n",
      "  Step 300/4407 - Loss: 1.9638 | PPL: 7.13 | LR: 9.836219e-05\n",
      "  Step 300/4407 - Loss: 1.9638 | PPL: 7.13 | LR: 9.836219e-05\n",
      "  Step 400/4407 - Loss: 1.9825 | PPL: 7.26 | LR: 9.825065e-05\n",
      "  Step 400/4407 - Loss: 1.9825 | PPL: 7.26 | LR: 9.825065e-05\n",
      "  Step 500/4407 - Loss: 1.9834 | PPL: 7.27 | LR: 9.813915e-05\n",
      "  Step 500/4407 - Loss: 1.9834 | PPL: 7.27 | LR: 9.813915e-05\n",
      "  Step 600/4407 - Loss: 1.9843 | PPL: 7.27 | LR: 9.802769e-05\n",
      "  Step 600/4407 - Loss: 1.9843 | PPL: 7.27 | LR: 9.802769e-05\n",
      "  Step 700/4407 - Loss: 1.9847 | PPL: 7.28 | LR: 9.791625e-05\n",
      "  Step 700/4407 - Loss: 1.9847 | PPL: 7.28 | LR: 9.791625e-05\n",
      "  Step 800/4407 - Loss: 1.9813 | PPL: 7.25 | LR: 9.780485e-05\n",
      "  Step 800/4407 - Loss: 1.9813 | PPL: 7.25 | LR: 9.780485e-05\n",
      "  Step 900/4407 - Loss: 1.9775 | PPL: 7.22 | LR: 9.769348e-05\n",
      "  Step 900/4407 - Loss: 1.9775 | PPL: 7.22 | LR: 9.769348e-05\n",
      "  Step 1000/4407 - Loss: 1.9801 | PPL: 7.24 | LR: 9.758214e-05\n",
      "  Step 1000/4407 - Loss: 1.9801 | PPL: 7.24 | LR: 9.758214e-05\n",
      "  Step 1100/4407 - Loss: 1.9788 | PPL: 7.23 | LR: 9.747084e-05\n",
      "  Step 1100/4407 - Loss: 1.9788 | PPL: 7.23 | LR: 9.747084e-05\n",
      "  Step 1200/4407 - Loss: 1.9802 | PPL: 7.24 | LR: 9.735957e-05\n",
      "  Step 1200/4407 - Loss: 1.9802 | PPL: 7.24 | LR: 9.735957e-05\n",
      "  Step 1300/4407 - Loss: 1.9771 | PPL: 7.22 | LR: 9.724833e-05\n",
      "  Step 1300/4407 - Loss: 1.9771 | PPL: 7.22 | LR: 9.724833e-05\n",
      "  Step 1400/4407 - Loss: 1.9777 | PPL: 7.23 | LR: 9.713713e-05\n",
      "  Step 1400/4407 - Loss: 1.9777 | PPL: 7.23 | LR: 9.713713e-05\n",
      "  Step 1500/4407 - Loss: 1.9782 | PPL: 7.23 | LR: 9.702596e-05\n",
      "  Step 1500/4407 - Loss: 1.9782 | PPL: 7.23 | LR: 9.702596e-05\n",
      "  Step 1600/4407 - Loss: 1.9791 | PPL: 7.24 | LR: 9.691482e-05\n",
      "  Step 1600/4407 - Loss: 1.9791 | PPL: 7.24 | LR: 9.691482e-05\n",
      "  Step 1700/4407 - Loss: 1.9777 | PPL: 7.23 | LR: 9.680371e-05\n",
      "  Step 1700/4407 - Loss: 1.9777 | PPL: 7.23 | LR: 9.680371e-05\n",
      "  Step 1800/4407 - Loss: 1.9800 | PPL: 7.24 | LR: 9.669264e-05\n",
      "  Step 1800/4407 - Loss: 1.9800 | PPL: 7.24 | LR: 9.669264e-05\n",
      "  Step 1900/4407 - Loss: 1.9819 | PPL: 7.26 | LR: 9.658160e-05\n",
      "  Step 1900/4407 - Loss: 1.9819 | PPL: 7.26 | LR: 9.658160e-05\n",
      "  Step 2000/4407 - Loss: 1.9816 | PPL: 7.25 | LR: 9.647060e-05\n",
      "  Step 2000/4407 - Loss: 1.9816 | PPL: 7.25 | LR: 9.647060e-05\n",
      "  Step 2100/4407 - Loss: 1.9804 | PPL: 7.25 | LR: 9.635963e-05\n",
      "  Step 2100/4407 - Loss: 1.9804 | PPL: 7.25 | LR: 9.635963e-05\n",
      "  Step 2200/4407 - Loss: 1.9831 | PPL: 7.27 | LR: 9.624869e-05\n",
      "  Step 2200/4407 - Loss: 1.9831 | PPL: 7.27 | LR: 9.624869e-05\n",
      "  Step 2300/4407 - Loss: 1.9840 | PPL: 7.27 | LR: 9.613779e-05\n",
      "  Step 2300/4407 - Loss: 1.9840 | PPL: 7.27 | LR: 9.613779e-05\n",
      "  Step 2400/4407 - Loss: 1.9844 | PPL: 7.28 | LR: 9.602692e-05\n",
      "  Step 2400/4407 - Loss: 1.9844 | PPL: 7.28 | LR: 9.602692e-05\n",
      "  Step 2500/4407 - Loss: 1.9853 | PPL: 7.28 | LR: 9.591608e-05\n",
      "  Step 2500/4407 - Loss: 1.9853 | PPL: 7.28 | LR: 9.591608e-05\n",
      "  Step 2600/4407 - Loss: 1.9858 | PPL: 7.28 | LR: 9.580528e-05\n",
      "  Step 2600/4407 - Loss: 1.9858 | PPL: 7.28 | LR: 9.580528e-05\n",
      "  Step 2700/4407 - Loss: 1.9861 | PPL: 7.29 | LR: 9.569451e-05\n",
      "  Step 2700/4407 - Loss: 1.9861 | PPL: 7.29 | LR: 9.569451e-05\n",
      "  Step 2800/4407 - Loss: 1.9878 | PPL: 7.30 | LR: 9.558378e-05\n",
      "  Step 2800/4407 - Loss: 1.9878 | PPL: 7.30 | LR: 9.558378e-05\n",
      "  Step 2900/4407 - Loss: 1.9893 | PPL: 7.31 | LR: 9.547308e-05\n",
      "  Step 2900/4407 - Loss: 1.9893 | PPL: 7.31 | LR: 9.547308e-05\n",
      "  Step 3000/4407 - Loss: 1.9893 | PPL: 7.31 | LR: 9.536241e-05\n",
      "  Step 3000/4407 - Loss: 1.9893 | PPL: 7.31 | LR: 9.536241e-05\n",
      "  Step 3100/4407 - Loss: 1.9889 | PPL: 7.31 | LR: 9.525178e-05\n",
      "  Step 3100/4407 - Loss: 1.9889 | PPL: 7.31 | LR: 9.525178e-05\n",
      "  Step 3200/4407 - Loss: 1.9882 | PPL: 7.30 | LR: 9.514118e-05\n",
      "  Step 3200/4407 - Loss: 1.9882 | PPL: 7.30 | LR: 9.514118e-05\n",
      "  Step 3300/4407 - Loss: 1.9880 | PPL: 7.30 | LR: 9.503062e-05\n",
      "  Step 3300/4407 - Loss: 1.9880 | PPL: 7.30 | LR: 9.503062e-05\n",
      "  Step 3400/4407 - Loss: 1.9880 | PPL: 7.30 | LR: 9.492009e-05\n",
      "  Step 3400/4407 - Loss: 1.9880 | PPL: 7.30 | LR: 9.492009e-05\n",
      "  Step 3500/4407 - Loss: 1.9883 | PPL: 7.30 | LR: 9.480960e-05\n",
      "  Step 3500/4407 - Loss: 1.9883 | PPL: 7.30 | LR: 9.480960e-05\n",
      "  Step 3600/4407 - Loss: 1.9890 | PPL: 7.31 | LR: 9.469914e-05\n",
      "  Step 3600/4407 - Loss: 1.9890 | PPL: 7.31 | LR: 9.469914e-05\n",
      "  Step 3700/4407 - Loss: 1.9896 | PPL: 7.31 | LR: 9.458871e-05\n",
      "  Step 3700/4407 - Loss: 1.9896 | PPL: 7.31 | LR: 9.458871e-05\n",
      "  Step 3800/4407 - Loss: 1.9902 | PPL: 7.32 | LR: 9.447833e-05\n",
      "  Step 3800/4407 - Loss: 1.9902 | PPL: 7.32 | LR: 9.447833e-05\n",
      "  Step 3900/4407 - Loss: 1.9902 | PPL: 7.32 | LR: 9.436797e-05\n",
      "  Step 3900/4407 - Loss: 1.9902 | PPL: 7.32 | LR: 9.436797e-05\n",
      "  Step 4000/4407 - Loss: 1.9905 | PPL: 7.32 | LR: 9.425765e-05\n",
      "  Step 4000/4407 - Loss: 1.9905 | PPL: 7.32 | LR: 9.425765e-05\n",
      "  Step 4100/4407 - Loss: 1.9891 | PPL: 7.31 | LR: 9.414737e-05\n",
      "  Step 4100/4407 - Loss: 1.9891 | PPL: 7.31 | LR: 9.414737e-05\n",
      "  Step 4200/4407 - Loss: 1.9893 | PPL: 7.31 | LR: 9.403712e-05\n",
      "  Step 4200/4407 - Loss: 1.9893 | PPL: 7.31 | LR: 9.403712e-05\n",
      "  Step 4300/4407 - Loss: 1.9893 | PPL: 7.31 | LR: 9.392690e-05\n",
      "  Step 4300/4407 - Loss: 1.9893 | PPL: 7.31 | LR: 9.392690e-05\n",
      "  Step 4400/4407 - Loss: 1.9890 | PPL: 7.31 | LR: 9.381672e-05\n",
      "  Step 4400/4407 - Loss: 1.9890 | PPL: 7.31 | LR: 9.381672e-05\n",
      "Epoch 66/100 | Train Loss: 1.9891 | Train PPL: 7.31 | Val Loss: 2.5997 | Val PPL: 13.46\n",
      "Epoch 66/100 | Train Loss: 1.9891 | Train PPL: 7.31 | Val Loss: 2.5997 | Val PPL: 13.46\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.31\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.31\n",
      "  Step 100/4407 - Loss: 1.9773 | PPL: 7.22 | LR: 9.369887e-05\n",
      "  Step 100/4407 - Loss: 1.9773 | PPL: 7.22 | LR: 9.369887e-05\n",
      "  Step 200/4407 - Loss: 1.9938 | PPL: 7.34 | LR: 9.358876e-05\n",
      "  Step 200/4407 - Loss: 1.9938 | PPL: 7.34 | LR: 9.358876e-05\n",
      "  Step 300/4407 - Loss: 1.9868 | PPL: 7.29 | LR: 9.347869e-05\n",
      "  Step 300/4407 - Loss: 1.9868 | PPL: 7.29 | LR: 9.347869e-05\n",
      "  Step 400/4407 - Loss: 1.9860 | PPL: 7.29 | LR: 9.336866e-05\n",
      "  Step 400/4407 - Loss: 1.9860 | PPL: 7.29 | LR: 9.336866e-05\n",
      "  Step 500/4407 - Loss: 1.9831 | PPL: 7.27 | LR: 9.325866e-05\n",
      "  Step 500/4407 - Loss: 1.9831 | PPL: 7.27 | LR: 9.325866e-05\n",
      "  Step 600/4407 - Loss: 1.9762 | PPL: 7.22 | LR: 9.314869e-05\n",
      "  Step 600/4407 - Loss: 1.9762 | PPL: 7.22 | LR: 9.314869e-05\n",
      "  Step 700/4407 - Loss: 1.9712 | PPL: 7.18 | LR: 9.303876e-05\n",
      "  Step 700/4407 - Loss: 1.9712 | PPL: 7.18 | LR: 9.303876e-05\n",
      "  Step 800/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 9.292887e-05\n",
      "  Step 800/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 9.292887e-05\n",
      "  Step 900/4407 - Loss: 1.9709 | PPL: 7.18 | LR: 9.281901e-05\n",
      "  Step 900/4407 - Loss: 1.9709 | PPL: 7.18 | LR: 9.281901e-05\n",
      "  Step 1000/4407 - Loss: 1.9720 | PPL: 7.18 | LR: 9.270919e-05\n",
      "  Step 1000/4407 - Loss: 1.9720 | PPL: 7.18 | LR: 9.270919e-05\n",
      "  Step 1100/4407 - Loss: 1.9734 | PPL: 7.20 | LR: 9.259941e-05\n",
      "  Step 1100/4407 - Loss: 1.9734 | PPL: 7.20 | LR: 9.259941e-05\n",
      "  Step 1200/4407 - Loss: 1.9765 | PPL: 7.22 | LR: 9.248966e-05\n",
      "  Step 1200/4407 - Loss: 1.9765 | PPL: 7.22 | LR: 9.248966e-05\n",
      "  Step 1300/4407 - Loss: 1.9766 | PPL: 7.22 | LR: 9.237994e-05\n",
      "  Step 1300/4407 - Loss: 1.9766 | PPL: 7.22 | LR: 9.237994e-05\n",
      "  Step 1400/4407 - Loss: 1.9783 | PPL: 7.23 | LR: 9.227027e-05\n",
      "  Step 1400/4407 - Loss: 1.9783 | PPL: 7.23 | LR: 9.227027e-05\n",
      "  Step 1500/4407 - Loss: 1.9784 | PPL: 7.23 | LR: 9.216063e-05\n",
      "  Step 1500/4407 - Loss: 1.9784 | PPL: 7.23 | LR: 9.216063e-05\n",
      "  Step 1600/4407 - Loss: 1.9818 | PPL: 7.26 | LR: 9.205102e-05\n",
      "  Step 1600/4407 - Loss: 1.9818 | PPL: 7.26 | LR: 9.205102e-05\n",
      "  Step 1700/4407 - Loss: 1.9792 | PPL: 7.24 | LR: 9.194145e-05\n",
      "  Step 1700/4407 - Loss: 1.9792 | PPL: 7.24 | LR: 9.194145e-05\n",
      "  Step 1800/4407 - Loss: 1.9793 | PPL: 7.24 | LR: 9.183192e-05\n",
      "  Step 1800/4407 - Loss: 1.9793 | PPL: 7.24 | LR: 9.183192e-05\n",
      "  Step 1900/4407 - Loss: 1.9781 | PPL: 7.23 | LR: 9.172243e-05\n",
      "  Step 1900/4407 - Loss: 1.9781 | PPL: 7.23 | LR: 9.172243e-05\n",
      "  Step 2000/4407 - Loss: 1.9753 | PPL: 7.21 | LR: 9.161297e-05\n",
      "  Step 2000/4407 - Loss: 1.9753 | PPL: 7.21 | LR: 9.161297e-05\n",
      "  Step 2100/4407 - Loss: 1.9760 | PPL: 7.21 | LR: 9.150354e-05\n",
      "  Step 2100/4407 - Loss: 1.9760 | PPL: 7.21 | LR: 9.150354e-05\n",
      "  Step 2200/4407 - Loss: 1.9750 | PPL: 7.21 | LR: 9.139416e-05\n",
      "  Step 2200/4407 - Loss: 1.9750 | PPL: 7.21 | LR: 9.139416e-05\n",
      "  Step 2300/4407 - Loss: 1.9757 | PPL: 7.21 | LR: 9.128481e-05\n",
      "  Step 2300/4407 - Loss: 1.9757 | PPL: 7.21 | LR: 9.128481e-05\n",
      "  Step 2400/4407 - Loss: 1.9759 | PPL: 7.21 | LR: 9.117550e-05\n",
      "  Step 2400/4407 - Loss: 1.9759 | PPL: 7.21 | LR: 9.117550e-05\n",
      "  Step 2500/4407 - Loss: 1.9755 | PPL: 7.21 | LR: 9.106622e-05\n",
      "  Step 2500/4407 - Loss: 1.9755 | PPL: 7.21 | LR: 9.106622e-05\n",
      "  Step 2600/4407 - Loss: 1.9762 | PPL: 7.21 | LR: 9.095698e-05\n",
      "  Step 2600/4407 - Loss: 1.9762 | PPL: 7.21 | LR: 9.095698e-05\n",
      "  Step 2700/4407 - Loss: 1.9773 | PPL: 7.22 | LR: 9.084778e-05\n",
      "  Step 2700/4407 - Loss: 1.9773 | PPL: 7.22 | LR: 9.084778e-05\n",
      "  Step 2800/4407 - Loss: 1.9768 | PPL: 7.22 | LR: 9.073862e-05\n",
      "  Step 2800/4407 - Loss: 1.9768 | PPL: 7.22 | LR: 9.073862e-05\n",
      "  Step 2900/4407 - Loss: 1.9779 | PPL: 7.23 | LR: 9.062949e-05\n",
      "  Step 2900/4407 - Loss: 1.9779 | PPL: 7.23 | LR: 9.062949e-05\n",
      "  Step 3000/4407 - Loss: 1.9791 | PPL: 7.24 | LR: 9.052040e-05\n",
      "  Step 3000/4407 - Loss: 1.9791 | PPL: 7.24 | LR: 9.052040e-05\n",
      "  Step 3100/4407 - Loss: 1.9796 | PPL: 7.24 | LR: 9.041135e-05\n",
      "  Step 3100/4407 - Loss: 1.9796 | PPL: 7.24 | LR: 9.041135e-05\n",
      "  Step 3200/4407 - Loss: 1.9794 | PPL: 7.24 | LR: 9.030234e-05\n",
      "  Step 3200/4407 - Loss: 1.9794 | PPL: 7.24 | LR: 9.030234e-05\n",
      "  Step 3300/4407 - Loss: 1.9797 | PPL: 7.24 | LR: 9.019336e-05\n",
      "  Step 3300/4407 - Loss: 1.9797 | PPL: 7.24 | LR: 9.019336e-05\n",
      "  Step 3400/4407 - Loss: 1.9803 | PPL: 7.25 | LR: 9.008442e-05\n",
      "  Step 3400/4407 - Loss: 1.9803 | PPL: 7.25 | LR: 9.008442e-05\n",
      "  Step 3500/4407 - Loss: 1.9811 | PPL: 7.25 | LR: 8.997552e-05\n",
      "  Step 3500/4407 - Loss: 1.9811 | PPL: 7.25 | LR: 8.997552e-05\n",
      "  Step 3600/4407 - Loss: 1.9808 | PPL: 7.25 | LR: 8.986665e-05\n",
      "  Step 3600/4407 - Loss: 1.9808 | PPL: 7.25 | LR: 8.986665e-05\n",
      "  Step 3700/4407 - Loss: 1.9822 | PPL: 7.26 | LR: 8.975783e-05\n",
      "  Step 3700/4407 - Loss: 1.9822 | PPL: 7.26 | LR: 8.975783e-05\n",
      "  Step 3800/4407 - Loss: 1.9828 | PPL: 7.26 | LR: 8.964904e-05\n",
      "  Step 3800/4407 - Loss: 1.9828 | PPL: 7.26 | LR: 8.964904e-05\n",
      "  Step 3900/4407 - Loss: 1.9824 | PPL: 7.26 | LR: 8.954029e-05\n",
      "  Step 3900/4407 - Loss: 1.9824 | PPL: 7.26 | LR: 8.954029e-05\n",
      "  Step 4000/4407 - Loss: 1.9839 | PPL: 7.27 | LR: 8.943157e-05\n",
      "  Step 4000/4407 - Loss: 1.9839 | PPL: 7.27 | LR: 8.943157e-05\n",
      "  Step 4100/4407 - Loss: 1.9844 | PPL: 7.27 | LR: 8.932290e-05\n",
      "  Step 4100/4407 - Loss: 1.9844 | PPL: 7.27 | LR: 8.932290e-05\n",
      "  Step 4200/4407 - Loss: 1.9859 | PPL: 7.29 | LR: 8.921426e-05\n",
      "  Step 4200/4407 - Loss: 1.9859 | PPL: 7.29 | LR: 8.921426e-05\n",
      "  Step 4300/4407 - Loss: 1.9859 | PPL: 7.29 | LR: 8.910566e-05\n",
      "  Step 4300/4407 - Loss: 1.9859 | PPL: 7.29 | LR: 8.910566e-05\n",
      "  Step 4400/4407 - Loss: 1.9855 | PPL: 7.28 | LR: 8.899710e-05\n",
      "  Step 4400/4407 - Loss: 1.9855 | PPL: 7.28 | LR: 8.899710e-05\n",
      "Epoch 67/100 | Train Loss: 1.9858 | Train PPL: 7.29 | Val Loss: 2.5917 | Val PPL: 13.35\n",
      "Epoch 67/100 | Train Loss: 1.9858 | Train PPL: 7.29 | Val Loss: 2.5917 | Val PPL: 13.35\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.29\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.29\n",
      "  Step 100/4407 - Loss: 1.9582 | PPL: 7.09 | LR: 8.888098e-05\n",
      "  Step 100/4407 - Loss: 1.9582 | PPL: 7.09 | LR: 8.888098e-05\n",
      "  Step 200/4407 - Loss: 1.9613 | PPL: 7.11 | LR: 8.877250e-05\n",
      "  Step 200/4407 - Loss: 1.9613 | PPL: 7.11 | LR: 8.877250e-05\n",
      "  Step 300/4407 - Loss: 1.9691 | PPL: 7.16 | LR: 8.866406e-05\n",
      "  Step 300/4407 - Loss: 1.9691 | PPL: 7.16 | LR: 8.866406e-05\n",
      "  Step 400/4407 - Loss: 1.9654 | PPL: 7.14 | LR: 8.855565e-05\n",
      "  Step 400/4407 - Loss: 1.9654 | PPL: 7.14 | LR: 8.855565e-05\n",
      "  Step 500/4407 - Loss: 1.9631 | PPL: 7.12 | LR: 8.844729e-05\n",
      "  Step 500/4407 - Loss: 1.9631 | PPL: 7.12 | LR: 8.844729e-05\n",
      "  Step 600/4407 - Loss: 1.9608 | PPL: 7.10 | LR: 8.833896e-05\n",
      "  Step 600/4407 - Loss: 1.9608 | PPL: 7.10 | LR: 8.833896e-05\n",
      "  Step 700/4407 - Loss: 1.9603 | PPL: 7.10 | LR: 8.823067e-05\n",
      "  Step 700/4407 - Loss: 1.9603 | PPL: 7.10 | LR: 8.823067e-05\n",
      "  Step 800/4407 - Loss: 1.9603 | PPL: 7.10 | LR: 8.812242e-05\n",
      "  Step 800/4407 - Loss: 1.9603 | PPL: 7.10 | LR: 8.812242e-05\n",
      "  Step 900/4407 - Loss: 1.9638 | PPL: 7.13 | LR: 8.801421e-05\n",
      "  Step 900/4407 - Loss: 1.9638 | PPL: 7.13 | LR: 8.801421e-05\n",
      "  Step 1000/4407 - Loss: 1.9651 | PPL: 7.14 | LR: 8.790604e-05\n",
      "  Step 1000/4407 - Loss: 1.9651 | PPL: 7.14 | LR: 8.790604e-05\n",
      "  Step 1100/4407 - Loss: 1.9679 | PPL: 7.16 | LR: 8.779791e-05\n",
      "  Step 1100/4407 - Loss: 1.9679 | PPL: 7.16 | LR: 8.779791e-05\n",
      "  Step 1200/4407 - Loss: 1.9684 | PPL: 7.16 | LR: 8.768981e-05\n",
      "  Step 1200/4407 - Loss: 1.9684 | PPL: 7.16 | LR: 8.768981e-05\n",
      "  Step 1300/4407 - Loss: 1.9696 | PPL: 7.17 | LR: 8.758176e-05\n",
      "  Step 1300/4407 - Loss: 1.9696 | PPL: 7.17 | LR: 8.758176e-05\n",
      "  Step 1400/4407 - Loss: 1.9698 | PPL: 7.17 | LR: 8.747374e-05\n",
      "  Step 1400/4407 - Loss: 1.9698 | PPL: 7.17 | LR: 8.747374e-05\n",
      "  Step 1500/4407 - Loss: 1.9695 | PPL: 7.17 | LR: 8.736576e-05\n",
      "  Step 1500/4407 - Loss: 1.9695 | PPL: 7.17 | LR: 8.736576e-05\n",
      "  Step 1600/4407 - Loss: 1.9722 | PPL: 7.19 | LR: 8.725783e-05\n",
      "  Step 1600/4407 - Loss: 1.9722 | PPL: 7.19 | LR: 8.725783e-05\n",
      "  Step 1700/4407 - Loss: 1.9723 | PPL: 7.19 | LR: 8.714993e-05\n",
      "  Step 1700/4407 - Loss: 1.9723 | PPL: 7.19 | LR: 8.714993e-05\n",
      "  Step 1800/4407 - Loss: 1.9748 | PPL: 7.21 | LR: 8.704207e-05\n",
      "  Step 1800/4407 - Loss: 1.9748 | PPL: 7.21 | LR: 8.704207e-05\n",
      "  Step 1900/4407 - Loss: 1.9770 | PPL: 7.22 | LR: 8.693425e-05\n",
      "  Step 1900/4407 - Loss: 1.9770 | PPL: 7.22 | LR: 8.693425e-05\n",
      "  Step 2000/4407 - Loss: 1.9776 | PPL: 7.23 | LR: 8.682647e-05\n",
      "  Step 2000/4407 - Loss: 1.9776 | PPL: 7.23 | LR: 8.682647e-05\n",
      "  Step 2100/4407 - Loss: 1.9778 | PPL: 7.23 | LR: 8.671873e-05\n",
      "  Step 2100/4407 - Loss: 1.9778 | PPL: 7.23 | LR: 8.671873e-05\n",
      "  Step 2200/4407 - Loss: 1.9791 | PPL: 7.24 | LR: 8.661103e-05\n",
      "  Step 2200/4407 - Loss: 1.9791 | PPL: 7.24 | LR: 8.661103e-05\n",
      "  Step 2300/4407 - Loss: 1.9792 | PPL: 7.24 | LR: 8.650337e-05\n",
      "  Step 2300/4407 - Loss: 1.9792 | PPL: 7.24 | LR: 8.650337e-05\n",
      "  Step 2400/4407 - Loss: 1.9791 | PPL: 7.24 | LR: 8.639575e-05\n",
      "  Step 2400/4407 - Loss: 1.9791 | PPL: 7.24 | LR: 8.639575e-05\n",
      "  Step 2500/4407 - Loss: 1.9786 | PPL: 7.23 | LR: 8.628817e-05\n",
      "  Step 2500/4407 - Loss: 1.9786 | PPL: 7.23 | LR: 8.628817e-05\n",
      "  Step 2600/4407 - Loss: 1.9792 | PPL: 7.24 | LR: 8.618063e-05\n",
      "  Step 2600/4407 - Loss: 1.9792 | PPL: 7.24 | LR: 8.618063e-05\n",
      "  Step 2700/4407 - Loss: 1.9790 | PPL: 7.24 | LR: 8.607313e-05\n",
      "  Step 2700/4407 - Loss: 1.9790 | PPL: 7.24 | LR: 8.607313e-05\n",
      "  Step 2800/4407 - Loss: 1.9788 | PPL: 7.23 | LR: 8.596567e-05\n",
      "  Step 2800/4407 - Loss: 1.9788 | PPL: 7.23 | LR: 8.596567e-05\n",
      "  Step 2900/4407 - Loss: 1.9786 | PPL: 7.23 | LR: 8.585824e-05\n",
      "  Step 2900/4407 - Loss: 1.9786 | PPL: 7.23 | LR: 8.585824e-05\n",
      "  Step 3000/4407 - Loss: 1.9780 | PPL: 7.23 | LR: 8.575086e-05\n",
      "  Step 3000/4407 - Loss: 1.9780 | PPL: 7.23 | LR: 8.575086e-05\n",
      "  Step 3100/4407 - Loss: 1.9779 | PPL: 7.23 | LR: 8.564352e-05\n",
      "  Step 3100/4407 - Loss: 1.9779 | PPL: 7.23 | LR: 8.564352e-05\n",
      "  Step 3200/4407 - Loss: 1.9779 | PPL: 7.23 | LR: 8.553622e-05\n",
      "  Step 3200/4407 - Loss: 1.9779 | PPL: 7.23 | LR: 8.553622e-05\n",
      "  Step 3300/4407 - Loss: 1.9786 | PPL: 7.23 | LR: 8.542897e-05\n",
      "  Step 3300/4407 - Loss: 1.9786 | PPL: 7.23 | LR: 8.542897e-05\n",
      "  Step 3400/4407 - Loss: 1.9790 | PPL: 7.24 | LR: 8.532175e-05\n",
      "  Step 3400/4407 - Loss: 1.9790 | PPL: 7.24 | LR: 8.532175e-05\n",
      "  Step 3500/4407 - Loss: 1.9793 | PPL: 7.24 | LR: 8.521457e-05\n",
      "  Step 3500/4407 - Loss: 1.9793 | PPL: 7.24 | LR: 8.521457e-05\n",
      "  Step 3600/4407 - Loss: 1.9791 | PPL: 7.24 | LR: 8.510743e-05\n",
      "  Step 3600/4407 - Loss: 1.9791 | PPL: 7.24 | LR: 8.510743e-05\n",
      "  Step 3700/4407 - Loss: 1.9800 | PPL: 7.24 | LR: 8.500033e-05\n",
      "  Step 3700/4407 - Loss: 1.9800 | PPL: 7.24 | LR: 8.500033e-05\n",
      "  Step 3800/4407 - Loss: 1.9804 | PPL: 7.25 | LR: 8.489328e-05\n",
      "  Step 3800/4407 - Loss: 1.9804 | PPL: 7.25 | LR: 8.489328e-05\n",
      "  Step 3900/4407 - Loss: 1.9803 | PPL: 7.24 | LR: 8.478626e-05\n",
      "  Step 3900/4407 - Loss: 1.9803 | PPL: 7.24 | LR: 8.478626e-05\n",
      "  Step 4000/4407 - Loss: 1.9801 | PPL: 7.24 | LR: 8.467929e-05\n",
      "  Step 4000/4407 - Loss: 1.9801 | PPL: 7.24 | LR: 8.467929e-05\n",
      "  Step 4100/4407 - Loss: 1.9811 | PPL: 7.25 | LR: 8.457236e-05\n",
      "  Step 4100/4407 - Loss: 1.9811 | PPL: 7.25 | LR: 8.457236e-05\n",
      "  Step 4200/4407 - Loss: 1.9818 | PPL: 7.26 | LR: 8.446546e-05\n",
      "  Step 4200/4407 - Loss: 1.9818 | PPL: 7.26 | LR: 8.446546e-05\n",
      "  Step 4300/4407 - Loss: 1.9825 | PPL: 7.26 | LR: 8.435861e-05\n",
      "  Step 4300/4407 - Loss: 1.9825 | PPL: 7.26 | LR: 8.435861e-05\n",
      "  Step 4400/4407 - Loss: 1.9823 | PPL: 7.26 | LR: 8.425180e-05\n",
      "  Step 4400/4407 - Loss: 1.9823 | PPL: 7.26 | LR: 8.425180e-05\n",
      "Epoch 68/100 | Train Loss: 1.9825 | Train PPL: 7.26 | Val Loss: 2.5914 | Val PPL: 13.35\n",
      "Epoch 68/100 | Train Loss: 1.9825 | Train PPL: 7.26 | Val Loss: 2.5914 | Val PPL: 13.35\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.26\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.26\n",
      "  Step 100/4407 - Loss: 2.0167 | PPL: 7.51 | LR: 8.413756e-05\n",
      "  Step 100/4407 - Loss: 2.0167 | PPL: 7.51 | LR: 8.413756e-05\n",
      "  Step 200/4407 - Loss: 2.0121 | PPL: 7.48 | LR: 8.403084e-05\n",
      "  Step 200/4407 - Loss: 2.0121 | PPL: 7.48 | LR: 8.403084e-05\n",
      "  Step 300/4407 - Loss: 1.9923 | PPL: 7.33 | LR: 8.392415e-05\n",
      "  Step 300/4407 - Loss: 1.9923 | PPL: 7.33 | LR: 8.392415e-05\n",
      "  Step 400/4407 - Loss: 1.9823 | PPL: 7.26 | LR: 8.381751e-05\n",
      "  Step 400/4407 - Loss: 1.9823 | PPL: 7.26 | LR: 8.381751e-05\n",
      "  Step 500/4407 - Loss: 1.9791 | PPL: 7.24 | LR: 8.371091e-05\n",
      "  Step 500/4407 - Loss: 1.9791 | PPL: 7.24 | LR: 8.371091e-05\n",
      "  Step 600/4407 - Loss: 1.9725 | PPL: 7.19 | LR: 8.360435e-05\n",
      "  Step 600/4407 - Loss: 1.9725 | PPL: 7.19 | LR: 8.360435e-05\n",
      "  Step 700/4407 - Loss: 1.9755 | PPL: 7.21 | LR: 8.349784e-05\n",
      "  Step 700/4407 - Loss: 1.9755 | PPL: 7.21 | LR: 8.349784e-05\n",
      "  Step 800/4407 - Loss: 1.9728 | PPL: 7.19 | LR: 8.339136e-05\n",
      "  Step 800/4407 - Loss: 1.9728 | PPL: 7.19 | LR: 8.339136e-05\n",
      "  Step 900/4407 - Loss: 1.9750 | PPL: 7.21 | LR: 8.328493e-05\n",
      "  Step 900/4407 - Loss: 1.9750 | PPL: 7.21 | LR: 8.328493e-05\n",
      "  Step 1000/4407 - Loss: 1.9763 | PPL: 7.22 | LR: 8.317854e-05\n",
      "  Step 1000/4407 - Loss: 1.9763 | PPL: 7.22 | LR: 8.317854e-05\n",
      "  Step 1100/4407 - Loss: 1.9759 | PPL: 7.21 | LR: 8.307219e-05\n",
      "  Step 1100/4407 - Loss: 1.9759 | PPL: 7.21 | LR: 8.307219e-05\n",
      "  Step 1200/4407 - Loss: 1.9753 | PPL: 7.21 | LR: 8.296588e-05\n",
      "  Step 1200/4407 - Loss: 1.9753 | PPL: 7.21 | LR: 8.296588e-05\n",
      "  Step 1300/4407 - Loss: 1.9754 | PPL: 7.21 | LR: 8.285962e-05\n",
      "  Step 1300/4407 - Loss: 1.9754 | PPL: 7.21 | LR: 8.285962e-05\n",
      "  Step 1400/4407 - Loss: 1.9737 | PPL: 7.20 | LR: 8.275339e-05\n",
      "  Step 1400/4407 - Loss: 1.9737 | PPL: 7.20 | LR: 8.275339e-05\n",
      "  Step 1500/4407 - Loss: 1.9726 | PPL: 7.19 | LR: 8.264721e-05\n",
      "  Step 1500/4407 - Loss: 1.9726 | PPL: 7.19 | LR: 8.264721e-05\n",
      "  Step 1600/4407 - Loss: 1.9735 | PPL: 7.20 | LR: 8.254107e-05\n",
      "  Step 1600/4407 - Loss: 1.9735 | PPL: 7.20 | LR: 8.254107e-05\n",
      "  Step 1700/4407 - Loss: 1.9729 | PPL: 7.19 | LR: 8.243498e-05\n",
      "  Step 1700/4407 - Loss: 1.9729 | PPL: 7.19 | LR: 8.243498e-05\n",
      "  Step 1800/4407 - Loss: 1.9745 | PPL: 7.20 | LR: 8.232892e-05\n",
      "  Step 1800/4407 - Loss: 1.9745 | PPL: 7.20 | LR: 8.232892e-05\n",
      "  Step 1900/4407 - Loss: 1.9748 | PPL: 7.20 | LR: 8.222291e-05\n",
      "  Step 1900/4407 - Loss: 1.9748 | PPL: 7.20 | LR: 8.222291e-05\n",
      "  Step 2000/4407 - Loss: 1.9732 | PPL: 7.19 | LR: 8.211694e-05\n",
      "  Step 2000/4407 - Loss: 1.9732 | PPL: 7.19 | LR: 8.211694e-05\n",
      "  Step 2100/4407 - Loss: 1.9743 | PPL: 7.20 | LR: 8.201101e-05\n",
      "  Step 2100/4407 - Loss: 1.9743 | PPL: 7.20 | LR: 8.201101e-05\n",
      "  Step 2200/4407 - Loss: 1.9749 | PPL: 7.21 | LR: 8.190513e-05\n",
      "  Step 2200/4407 - Loss: 1.9749 | PPL: 7.21 | LR: 8.190513e-05\n",
      "  Step 2300/4407 - Loss: 1.9742 | PPL: 7.20 | LR: 8.179929e-05\n",
      "  Step 2300/4407 - Loss: 1.9742 | PPL: 7.20 | LR: 8.179929e-05\n",
      "  Step 2400/4407 - Loss: 1.9750 | PPL: 7.21 | LR: 8.169349e-05\n",
      "  Step 2400/4407 - Loss: 1.9750 | PPL: 7.21 | LR: 8.169349e-05\n",
      "  Step 2500/4407 - Loss: 1.9762 | PPL: 7.22 | LR: 8.158773e-05\n",
      "  Step 2500/4407 - Loss: 1.9762 | PPL: 7.22 | LR: 8.158773e-05\n",
      "  Step 2600/4407 - Loss: 1.9775 | PPL: 7.22 | LR: 8.148202e-05\n",
      "  Step 2600/4407 - Loss: 1.9775 | PPL: 7.22 | LR: 8.148202e-05\n",
      "  Step 2700/4407 - Loss: 1.9774 | PPL: 7.22 | LR: 8.137635e-05\n",
      "  Step 2700/4407 - Loss: 1.9774 | PPL: 7.22 | LR: 8.137635e-05\n",
      "  Step 2800/4407 - Loss: 1.9784 | PPL: 7.23 | LR: 8.127073e-05\n",
      "  Step 2800/4407 - Loss: 1.9784 | PPL: 7.23 | LR: 8.127073e-05\n",
      "  Step 2900/4407 - Loss: 1.9777 | PPL: 7.23 | LR: 8.116514e-05\n",
      "  Step 2900/4407 - Loss: 1.9777 | PPL: 7.23 | LR: 8.116514e-05\n",
      "  Step 3000/4407 - Loss: 1.9780 | PPL: 7.23 | LR: 8.105960e-05\n",
      "  Step 3000/4407 - Loss: 1.9780 | PPL: 7.23 | LR: 8.105960e-05\n",
      "  Step 3100/4407 - Loss: 1.9803 | PPL: 7.24 | LR: 8.095410e-05\n",
      "  Step 3100/4407 - Loss: 1.9803 | PPL: 7.24 | LR: 8.095410e-05\n",
      "  Step 3200/4407 - Loss: 1.9804 | PPL: 7.25 | LR: 8.084865e-05\n",
      "  Step 3200/4407 - Loss: 1.9804 | PPL: 7.25 | LR: 8.084865e-05\n",
      "  Step 3300/4407 - Loss: 1.9819 | PPL: 7.26 | LR: 8.074324e-05\n",
      "  Step 3300/4407 - Loss: 1.9819 | PPL: 7.26 | LR: 8.074324e-05\n",
      "  Step 3400/4407 - Loss: 1.9807 | PPL: 7.25 | LR: 8.063787e-05\n",
      "  Step 3400/4407 - Loss: 1.9807 | PPL: 7.25 | LR: 8.063787e-05\n",
      "  Step 3500/4407 - Loss: 1.9816 | PPL: 7.25 | LR: 8.053255e-05\n",
      "  Step 3500/4407 - Loss: 1.9816 | PPL: 7.25 | LR: 8.053255e-05\n",
      "  Step 3600/4407 - Loss: 1.9810 | PPL: 7.25 | LR: 8.042727e-05\n",
      "  Step 3600/4407 - Loss: 1.9810 | PPL: 7.25 | LR: 8.042727e-05\n",
      "  Step 3700/4407 - Loss: 1.9808 | PPL: 7.25 | LR: 8.032203e-05\n",
      "  Step 3700/4407 - Loss: 1.9808 | PPL: 7.25 | LR: 8.032203e-05\n",
      "  Step 3800/4407 - Loss: 1.9811 | PPL: 7.25 | LR: 8.021684e-05\n",
      "  Step 3800/4407 - Loss: 1.9811 | PPL: 7.25 | LR: 8.021684e-05\n",
      "  Step 3900/4407 - Loss: 1.9798 | PPL: 7.24 | LR: 8.011169e-05\n",
      "  Step 3900/4407 - Loss: 1.9798 | PPL: 7.24 | LR: 8.011169e-05\n",
      "  Step 4000/4407 - Loss: 1.9800 | PPL: 7.24 | LR: 8.000659e-05\n",
      "  Step 4000/4407 - Loss: 1.9800 | PPL: 7.24 | LR: 8.000659e-05\n",
      "  Step 4100/4407 - Loss: 1.9799 | PPL: 7.24 | LR: 7.990153e-05\n",
      "  Step 4100/4407 - Loss: 1.9799 | PPL: 7.24 | LR: 7.990153e-05\n",
      "  Step 4200/4407 - Loss: 1.9795 | PPL: 7.24 | LR: 7.979651e-05\n",
      "  Step 4200/4407 - Loss: 1.9795 | PPL: 7.24 | LR: 7.979651e-05\n",
      "  Step 4300/4407 - Loss: 1.9793 | PPL: 7.24 | LR: 7.969154e-05\n",
      "  Step 4300/4407 - Loss: 1.9793 | PPL: 7.24 | LR: 7.969154e-05\n",
      "  Step 4400/4407 - Loss: 1.9797 | PPL: 7.24 | LR: 7.958661e-05\n",
      "  Step 4400/4407 - Loss: 1.9797 | PPL: 7.24 | LR: 7.958661e-05\n",
      "Epoch 69/100 | Train Loss: 1.9796 | Train PPL: 7.24 | Val Loss: 2.5979 | Val PPL: 13.43\n",
      "Epoch 69/100 | Train Loss: 1.9796 | Train PPL: 7.24 | Val Loss: 2.5979 | Val PPL: 13.43\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.24\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.24\n",
      "  Step 100/4407 - Loss: 1.9858 | PPL: 7.28 | LR: 7.947438e-05\n",
      "  Step 100/4407 - Loss: 1.9858 | PPL: 7.28 | LR: 7.947438e-05\n",
      "  Step 200/4407 - Loss: 1.9848 | PPL: 7.28 | LR: 7.936955e-05\n",
      "  Step 200/4407 - Loss: 1.9848 | PPL: 7.28 | LR: 7.936955e-05\n",
      "  Step 300/4407 - Loss: 1.9711 | PPL: 7.18 | LR: 7.926475e-05\n",
      "  Step 300/4407 - Loss: 1.9711 | PPL: 7.18 | LR: 7.926475e-05\n",
      "  Step 400/4407 - Loss: 1.9675 | PPL: 7.15 | LR: 7.916001e-05\n",
      "  Step 400/4407 - Loss: 1.9675 | PPL: 7.15 | LR: 7.916001e-05\n",
      "  Step 500/4407 - Loss: 1.9629 | PPL: 7.12 | LR: 7.905530e-05\n",
      "  Step 500/4407 - Loss: 1.9629 | PPL: 7.12 | LR: 7.905530e-05\n",
      "  Step 600/4407 - Loss: 1.9630 | PPL: 7.12 | LR: 7.895064e-05\n",
      "  Step 600/4407 - Loss: 1.9630 | PPL: 7.12 | LR: 7.895064e-05\n",
      "  Step 700/4407 - Loss: 1.9658 | PPL: 7.14 | LR: 7.884603e-05\n",
      "  Step 700/4407 - Loss: 1.9658 | PPL: 7.14 | LR: 7.884603e-05\n",
      "  Step 800/4407 - Loss: 1.9690 | PPL: 7.16 | LR: 7.874146e-05\n",
      "  Step 800/4407 - Loss: 1.9690 | PPL: 7.16 | LR: 7.874146e-05\n",
      "  Step 900/4407 - Loss: 1.9704 | PPL: 7.17 | LR: 7.863693e-05\n",
      "  Step 900/4407 - Loss: 1.9704 | PPL: 7.17 | LR: 7.863693e-05\n",
      "  Step 1000/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 7.853245e-05\n",
      "  Step 1000/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 7.853245e-05\n",
      "  Step 1100/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 7.842801e-05\n",
      "  Step 1100/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 7.842801e-05\n",
      "  Step 1200/4407 - Loss: 1.9727 | PPL: 7.19 | LR: 7.832362e-05\n",
      "  Step 1200/4407 - Loss: 1.9727 | PPL: 7.19 | LR: 7.832362e-05\n",
      "  Step 1300/4407 - Loss: 1.9724 | PPL: 7.19 | LR: 7.821928e-05\n",
      "  Step 1300/4407 - Loss: 1.9724 | PPL: 7.19 | LR: 7.821928e-05\n",
      "  Step 1400/4407 - Loss: 1.9718 | PPL: 7.18 | LR: 7.811497e-05\n",
      "  Step 1400/4407 - Loss: 1.9718 | PPL: 7.18 | LR: 7.811497e-05\n",
      "  Step 1500/4407 - Loss: 1.9733 | PPL: 7.19 | LR: 7.801072e-05\n",
      "  Step 1500/4407 - Loss: 1.9733 | PPL: 7.19 | LR: 7.801072e-05\n",
      "  Step 1600/4407 - Loss: 1.9717 | PPL: 7.18 | LR: 7.790651e-05\n",
      "  Step 1600/4407 - Loss: 1.9717 | PPL: 7.18 | LR: 7.790651e-05\n",
      "  Step 1700/4407 - Loss: 1.9708 | PPL: 7.18 | LR: 7.780234e-05\n",
      "  Step 1700/4407 - Loss: 1.9708 | PPL: 7.18 | LR: 7.780234e-05\n",
      "  Step 1800/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 7.769822e-05\n",
      "  Step 1800/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 7.769822e-05\n",
      "  Step 1900/4407 - Loss: 1.9715 | PPL: 7.18 | LR: 7.759415e-05\n",
      "  Step 1900/4407 - Loss: 1.9715 | PPL: 7.18 | LR: 7.759415e-05\n",
      "  Step 2000/4407 - Loss: 1.9712 | PPL: 7.18 | LR: 7.749012e-05\n",
      "  Step 2000/4407 - Loss: 1.9712 | PPL: 7.18 | LR: 7.749012e-05\n",
      "  Step 2100/4407 - Loss: 1.9717 | PPL: 7.18 | LR: 7.738613e-05\n",
      "  Step 2100/4407 - Loss: 1.9717 | PPL: 7.18 | LR: 7.738613e-05\n",
      "  Step 2200/4407 - Loss: 1.9722 | PPL: 7.19 | LR: 7.728219e-05\n",
      "  Step 2200/4407 - Loss: 1.9722 | PPL: 7.19 | LR: 7.728219e-05\n",
      "  Step 2300/4407 - Loss: 1.9724 | PPL: 7.19 | LR: 7.717830e-05\n",
      "  Step 2300/4407 - Loss: 1.9724 | PPL: 7.19 | LR: 7.717830e-05\n",
      "  Step 2400/4407 - Loss: 1.9723 | PPL: 7.19 | LR: 7.707445e-05\n",
      "  Step 2400/4407 - Loss: 1.9723 | PPL: 7.19 | LR: 7.707445e-05\n",
      "  Step 2500/4407 - Loss: 1.9733 | PPL: 7.19 | LR: 7.697065e-05\n",
      "  Step 2500/4407 - Loss: 1.9733 | PPL: 7.19 | LR: 7.697065e-05\n",
      "  Step 2600/4407 - Loss: 1.9723 | PPL: 7.19 | LR: 7.686690e-05\n",
      "  Step 2600/4407 - Loss: 1.9723 | PPL: 7.19 | LR: 7.686690e-05\n",
      "  Step 2700/4407 - Loss: 1.9734 | PPL: 7.19 | LR: 7.676319e-05\n",
      "  Step 2700/4407 - Loss: 1.9734 | PPL: 7.19 | LR: 7.676319e-05\n",
      "  Step 2800/4407 - Loss: 1.9718 | PPL: 7.18 | LR: 7.665952e-05\n",
      "  Step 2800/4407 - Loss: 1.9718 | PPL: 7.18 | LR: 7.665952e-05\n",
      "  Step 2900/4407 - Loss: 1.9723 | PPL: 7.19 | LR: 7.655590e-05\n",
      "  Step 2900/4407 - Loss: 1.9723 | PPL: 7.19 | LR: 7.655590e-05\n",
      "  Step 3000/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 7.645233e-05\n",
      "  Step 3000/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 7.645233e-05\n",
      "  Step 3100/4407 - Loss: 1.9728 | PPL: 7.19 | LR: 7.634881e-05\n",
      "  Step 3100/4407 - Loss: 1.9728 | PPL: 7.19 | LR: 7.634881e-05\n",
      "  Step 3200/4407 - Loss: 1.9726 | PPL: 7.19 | LR: 7.624533e-05\n",
      "  Step 3200/4407 - Loss: 1.9726 | PPL: 7.19 | LR: 7.624533e-05\n",
      "  Step 3300/4407 - Loss: 1.9736 | PPL: 7.20 | LR: 7.614189e-05\n",
      "  Step 3300/4407 - Loss: 1.9736 | PPL: 7.20 | LR: 7.614189e-05\n",
      "  Step 3400/4407 - Loss: 1.9748 | PPL: 7.21 | LR: 7.603851e-05\n",
      "  Step 3400/4407 - Loss: 1.9748 | PPL: 7.21 | LR: 7.603851e-05\n",
      "  Step 3500/4407 - Loss: 1.9760 | PPL: 7.21 | LR: 7.593517e-05\n",
      "  Step 3500/4407 - Loss: 1.9760 | PPL: 7.21 | LR: 7.593517e-05\n",
      "  Step 3600/4407 - Loss: 1.9755 | PPL: 7.21 | LR: 7.583187e-05\n",
      "  Step 3600/4407 - Loss: 1.9755 | PPL: 7.21 | LR: 7.583187e-05\n",
      "  Step 3700/4407 - Loss: 1.9752 | PPL: 7.21 | LR: 7.572863e-05\n",
      "  Step 3700/4407 - Loss: 1.9752 | PPL: 7.21 | LR: 7.572863e-05\n",
      "  Step 3800/4407 - Loss: 1.9756 | PPL: 7.21 | LR: 7.562543e-05\n",
      "  Step 3800/4407 - Loss: 1.9756 | PPL: 7.21 | LR: 7.562543e-05\n",
      "  Step 3900/4407 - Loss: 1.9772 | PPL: 7.22 | LR: 7.552227e-05\n",
      "  Step 3900/4407 - Loss: 1.9772 | PPL: 7.22 | LR: 7.552227e-05\n",
      "  Step 4000/4407 - Loss: 1.9765 | PPL: 7.22 | LR: 7.541916e-05\n",
      "  Step 4000/4407 - Loss: 1.9765 | PPL: 7.22 | LR: 7.541916e-05\n",
      "  Step 4100/4407 - Loss: 1.9767 | PPL: 7.22 | LR: 7.531610e-05\n",
      "  Step 4100/4407 - Loss: 1.9767 | PPL: 7.22 | LR: 7.531610e-05\n",
      "  Step 4200/4407 - Loss: 1.9764 | PPL: 7.22 | LR: 7.521309e-05\n",
      "  Step 4200/4407 - Loss: 1.9764 | PPL: 7.22 | LR: 7.521309e-05\n",
      "  Step 4300/4407 - Loss: 1.9763 | PPL: 7.22 | LR: 7.511012e-05\n",
      "  Step 4300/4407 - Loss: 1.9763 | PPL: 7.22 | LR: 7.511012e-05\n",
      "  Step 4400/4407 - Loss: 1.9765 | PPL: 7.22 | LR: 7.500720e-05\n",
      "  Step 4400/4407 - Loss: 1.9765 | PPL: 7.22 | LR: 7.500720e-05\n",
      "Epoch 70/100 | Train Loss: 1.9767 | Train PPL: 7.22 | Val Loss: 2.5920 | Val PPL: 13.36\n",
      "Epoch 70/100 | Train Loss: 1.9767 | Train PPL: 7.22 | Val Loss: 2.5920 | Val PPL: 13.36\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.22\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.22\n",
      "  Step 100/4407 - Loss: 1.9484 | PPL: 7.02 | LR: 7.489713e-05\n",
      "  Step 100/4407 - Loss: 1.9484 | PPL: 7.02 | LR: 7.489713e-05\n",
      "  Step 200/4407 - Loss: 1.9568 | PPL: 7.08 | LR: 7.479431e-05\n",
      "  Step 200/4407 - Loss: 1.9568 | PPL: 7.08 | LR: 7.479431e-05\n",
      "  Step 300/4407 - Loss: 1.9557 | PPL: 7.07 | LR: 7.469153e-05\n",
      "  Step 300/4407 - Loss: 1.9557 | PPL: 7.07 | LR: 7.469153e-05\n",
      "  Step 400/4407 - Loss: 1.9611 | PPL: 7.11 | LR: 7.458880e-05\n",
      "  Step 400/4407 - Loss: 1.9611 | PPL: 7.11 | LR: 7.458880e-05\n",
      "  Step 500/4407 - Loss: 1.9618 | PPL: 7.11 | LR: 7.448612e-05\n",
      "  Step 500/4407 - Loss: 1.9618 | PPL: 7.11 | LR: 7.448612e-05\n",
      "  Step 600/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 7.438349e-05\n",
      "  Step 600/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 7.438349e-05\n",
      "  Step 700/4407 - Loss: 1.9599 | PPL: 7.10 | LR: 7.428090e-05\n",
      "  Step 700/4407 - Loss: 1.9599 | PPL: 7.10 | LR: 7.428090e-05\n",
      "  Step 800/4407 - Loss: 1.9633 | PPL: 7.12 | LR: 7.417837e-05\n",
      "  Step 800/4407 - Loss: 1.9633 | PPL: 7.12 | LR: 7.417837e-05\n",
      "  Step 900/4407 - Loss: 1.9651 | PPL: 7.14 | LR: 7.407588e-05\n",
      "  Step 900/4407 - Loss: 1.9651 | PPL: 7.14 | LR: 7.407588e-05\n",
      "  Step 1000/4407 - Loss: 1.9639 | PPL: 7.13 | LR: 7.397343e-05\n",
      "  Step 1000/4407 - Loss: 1.9639 | PPL: 7.13 | LR: 7.397343e-05\n",
      "  Step 1100/4407 - Loss: 1.9647 | PPL: 7.13 | LR: 7.387104e-05\n",
      "  Step 1100/4407 - Loss: 1.9647 | PPL: 7.13 | LR: 7.387104e-05\n",
      "  Step 1200/4407 - Loss: 1.9649 | PPL: 7.13 | LR: 7.376869e-05\n",
      "  Step 1200/4407 - Loss: 1.9649 | PPL: 7.13 | LR: 7.376869e-05\n",
      "  Step 1300/4407 - Loss: 1.9667 | PPL: 7.15 | LR: 7.366639e-05\n",
      "  Step 1300/4407 - Loss: 1.9667 | PPL: 7.15 | LR: 7.366639e-05\n",
      "  Step 1400/4407 - Loss: 1.9684 | PPL: 7.16 | LR: 7.356414e-05\n",
      "  Step 1400/4407 - Loss: 1.9684 | PPL: 7.16 | LR: 7.356414e-05\n",
      "  Step 1500/4407 - Loss: 1.9680 | PPL: 7.16 | LR: 7.346193e-05\n",
      "  Step 1500/4407 - Loss: 1.9680 | PPL: 7.16 | LR: 7.346193e-05\n",
      "  Step 1600/4407 - Loss: 1.9688 | PPL: 7.16 | LR: 7.335978e-05\n",
      "  Step 1600/4407 - Loss: 1.9688 | PPL: 7.16 | LR: 7.335978e-05\n",
      "  Step 1700/4407 - Loss: 1.9685 | PPL: 7.16 | LR: 7.325767e-05\n",
      "  Step 1700/4407 - Loss: 1.9685 | PPL: 7.16 | LR: 7.325767e-05\n",
      "  Step 1800/4407 - Loss: 1.9706 | PPL: 7.17 | LR: 7.315561e-05\n",
      "  Step 1800/4407 - Loss: 1.9706 | PPL: 7.17 | LR: 7.315561e-05\n",
      "  Step 1900/4407 - Loss: 1.9705 | PPL: 7.17 | LR: 7.305360e-05\n",
      "  Step 1900/4407 - Loss: 1.9705 | PPL: 7.17 | LR: 7.305360e-05\n",
      "  Step 2000/4407 - Loss: 1.9726 | PPL: 7.19 | LR: 7.295163e-05\n",
      "  Step 2000/4407 - Loss: 1.9726 | PPL: 7.19 | LR: 7.295163e-05\n",
      "  Step 2100/4407 - Loss: 1.9716 | PPL: 7.18 | LR: 7.284972e-05\n",
      "  Step 2100/4407 - Loss: 1.9716 | PPL: 7.18 | LR: 7.284972e-05\n",
      "  Step 2200/4407 - Loss: 1.9699 | PPL: 7.17 | LR: 7.274785e-05\n",
      "  Step 2200/4407 - Loss: 1.9699 | PPL: 7.17 | LR: 7.274785e-05\n",
      "  Step 2300/4407 - Loss: 1.9714 | PPL: 7.18 | LR: 7.264604e-05\n",
      "  Step 2300/4407 - Loss: 1.9714 | PPL: 7.18 | LR: 7.264604e-05\n",
      "  Step 2400/4407 - Loss: 1.9702 | PPL: 7.17 | LR: 7.254427e-05\n",
      "  Step 2400/4407 - Loss: 1.9702 | PPL: 7.17 | LR: 7.254427e-05\n",
      "  Step 2500/4407 - Loss: 1.9697 | PPL: 7.17 | LR: 7.244254e-05\n",
      "  Step 2500/4407 - Loss: 1.9697 | PPL: 7.17 | LR: 7.244254e-05\n",
      "  Step 2600/4407 - Loss: 1.9711 | PPL: 7.18 | LR: 7.234087e-05\n",
      "  Step 2600/4407 - Loss: 1.9711 | PPL: 7.18 | LR: 7.234087e-05\n",
      "  Step 2700/4407 - Loss: 1.9709 | PPL: 7.18 | LR: 7.223925e-05\n",
      "  Step 2700/4407 - Loss: 1.9709 | PPL: 7.18 | LR: 7.223925e-05\n",
      "  Step 2800/4407 - Loss: 1.9710 | PPL: 7.18 | LR: 7.213767e-05\n",
      "  Step 2800/4407 - Loss: 1.9710 | PPL: 7.18 | LR: 7.213767e-05\n",
      "  Step 2900/4407 - Loss: 1.9713 | PPL: 7.18 | LR: 7.203615e-05\n",
      "  Step 2900/4407 - Loss: 1.9713 | PPL: 7.18 | LR: 7.203615e-05\n",
      "  Step 3000/4407 - Loss: 1.9717 | PPL: 7.18 | LR: 7.193467e-05\n",
      "  Step 3000/4407 - Loss: 1.9717 | PPL: 7.18 | LR: 7.193467e-05\n",
      "  Step 3100/4407 - Loss: 1.9725 | PPL: 7.19 | LR: 7.183324e-05\n",
      "  Step 3100/4407 - Loss: 1.9725 | PPL: 7.19 | LR: 7.183324e-05\n",
      "  Step 3200/4407 - Loss: 1.9722 | PPL: 7.19 | LR: 7.173186e-05\n",
      "  Step 3200/4407 - Loss: 1.9722 | PPL: 7.19 | LR: 7.173186e-05\n",
      "  Step 3300/4407 - Loss: 1.9717 | PPL: 7.18 | LR: 7.163053e-05\n",
      "  Step 3300/4407 - Loss: 1.9717 | PPL: 7.18 | LR: 7.163053e-05\n",
      "  Step 3400/4407 - Loss: 1.9733 | PPL: 7.19 | LR: 7.152925e-05\n",
      "  Step 3400/4407 - Loss: 1.9733 | PPL: 7.19 | LR: 7.152925e-05\n",
      "  Step 3500/4407 - Loss: 1.9733 | PPL: 7.19 | LR: 7.142802e-05\n",
      "  Step 3500/4407 - Loss: 1.9733 | PPL: 7.19 | LR: 7.142802e-05\n",
      "  Step 3600/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 7.132684e-05\n",
      "  Step 3600/4407 - Loss: 1.9730 | PPL: 7.19 | LR: 7.132684e-05\n",
      "  Step 3700/4407 - Loss: 1.9724 | PPL: 7.19 | LR: 7.122571e-05\n",
      "  Step 3700/4407 - Loss: 1.9724 | PPL: 7.19 | LR: 7.122571e-05\n",
      "  Step 3800/4407 - Loss: 1.9709 | PPL: 7.18 | LR: 7.112462e-05\n",
      "  Step 3800/4407 - Loss: 1.9709 | PPL: 7.18 | LR: 7.112462e-05\n",
      "  Step 3900/4407 - Loss: 1.9718 | PPL: 7.18 | LR: 7.102359e-05\n",
      "  Step 3900/4407 - Loss: 1.9718 | PPL: 7.18 | LR: 7.102359e-05\n",
      "  Step 4000/4407 - Loss: 1.9729 | PPL: 7.19 | LR: 7.092260e-05\n",
      "  Step 4000/4407 - Loss: 1.9729 | PPL: 7.19 | LR: 7.092260e-05\n",
      "  Step 4100/4407 - Loss: 1.9735 | PPL: 7.20 | LR: 7.082167e-05\n",
      "  Step 4100/4407 - Loss: 1.9735 | PPL: 7.20 | LR: 7.082167e-05\n",
      "  Step 4200/4407 - Loss: 1.9733 | PPL: 7.19 | LR: 7.072078e-05\n",
      "  Step 4200/4407 - Loss: 1.9733 | PPL: 7.19 | LR: 7.072078e-05\n",
      "  Step 4300/4407 - Loss: 1.9738 | PPL: 7.20 | LR: 7.061995e-05\n",
      "  Step 4300/4407 - Loss: 1.9738 | PPL: 7.20 | LR: 7.061995e-05\n",
      "  Step 4400/4407 - Loss: 1.9737 | PPL: 7.20 | LR: 7.051916e-05\n",
      "  Step 4400/4407 - Loss: 1.9737 | PPL: 7.20 | LR: 7.051916e-05\n",
      "Epoch 71/100 | Train Loss: 1.9738 | Train PPL: 7.20 | Val Loss: 2.5924 | Val PPL: 13.36\n",
      "Epoch 71/100 | Train Loss: 1.9738 | Train PPL: 7.20 | Val Loss: 2.5924 | Val PPL: 13.36\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.20\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.20\n",
      "  Step 100/4407 - Loss: 1.9739 | PPL: 7.20 | LR: 7.041138e-05\n",
      "  Step 100/4407 - Loss: 1.9739 | PPL: 7.20 | LR: 7.041138e-05\n",
      "  Step 200/4407 - Loss: 1.9577 | PPL: 7.08 | LR: 7.031070e-05\n",
      "  Step 200/4407 - Loss: 1.9577 | PPL: 7.08 | LR: 7.031070e-05\n",
      "  Step 300/4407 - Loss: 1.9470 | PPL: 7.01 | LR: 7.021006e-05\n",
      "  Step 300/4407 - Loss: 1.9470 | PPL: 7.01 | LR: 7.021006e-05\n",
      "  Step 400/4407 - Loss: 1.9528 | PPL: 7.05 | LR: 7.010948e-05\n",
      "  Step 400/4407 - Loss: 1.9528 | PPL: 7.05 | LR: 7.010948e-05\n",
      "  Step 500/4407 - Loss: 1.9503 | PPL: 7.03 | LR: 7.000895e-05\n",
      "  Step 500/4407 - Loss: 1.9503 | PPL: 7.03 | LR: 7.000895e-05\n",
      "  Step 600/4407 - Loss: 1.9565 | PPL: 7.07 | LR: 6.990847e-05\n",
      "  Step 600/4407 - Loss: 1.9565 | PPL: 7.07 | LR: 6.990847e-05\n",
      "  Step 700/4407 - Loss: 1.9569 | PPL: 7.08 | LR: 6.980804e-05\n",
      "  Step 700/4407 - Loss: 1.9569 | PPL: 7.08 | LR: 6.980804e-05\n",
      "  Step 800/4407 - Loss: 1.9570 | PPL: 7.08 | LR: 6.970765e-05\n",
      "  Step 800/4407 - Loss: 1.9570 | PPL: 7.08 | LR: 6.970765e-05\n",
      "  Step 900/4407 - Loss: 1.9638 | PPL: 7.13 | LR: 6.960732e-05\n",
      "  Step 900/4407 - Loss: 1.9638 | PPL: 7.13 | LR: 6.960732e-05\n",
      "  Step 1000/4407 - Loss: 1.9670 | PPL: 7.15 | LR: 6.950704e-05\n",
      "  Step 1000/4407 - Loss: 1.9670 | PPL: 7.15 | LR: 6.950704e-05\n",
      "  Step 1100/4407 - Loss: 1.9692 | PPL: 7.16 | LR: 6.940681e-05\n",
      "  Step 1100/4407 - Loss: 1.9692 | PPL: 7.16 | LR: 6.940681e-05\n",
      "  Step 1200/4407 - Loss: 1.9658 | PPL: 7.14 | LR: 6.930663e-05\n",
      "  Step 1200/4407 - Loss: 1.9658 | PPL: 7.14 | LR: 6.930663e-05\n",
      "  Step 1300/4407 - Loss: 1.9653 | PPL: 7.14 | LR: 6.920650e-05\n",
      "  Step 1300/4407 - Loss: 1.9653 | PPL: 7.14 | LR: 6.920650e-05\n",
      "  Step 1400/4407 - Loss: 1.9670 | PPL: 7.15 | LR: 6.910643e-05\n",
      "  Step 1400/4407 - Loss: 1.9670 | PPL: 7.15 | LR: 6.910643e-05\n",
      "  Step 1500/4407 - Loss: 1.9632 | PPL: 7.12 | LR: 6.900640e-05\n",
      "  Step 1500/4407 - Loss: 1.9632 | PPL: 7.12 | LR: 6.900640e-05\n",
      "  Step 1600/4407 - Loss: 1.9607 | PPL: 7.10 | LR: 6.890642e-05\n",
      "  Step 1600/4407 - Loss: 1.9607 | PPL: 7.10 | LR: 6.890642e-05\n",
      "  Step 1700/4407 - Loss: 1.9603 | PPL: 7.10 | LR: 6.880650e-05\n",
      "  Step 1700/4407 - Loss: 1.9603 | PPL: 7.10 | LR: 6.880650e-05\n",
      "  Step 1800/4407 - Loss: 1.9634 | PPL: 7.12 | LR: 6.870662e-05\n",
      "  Step 1800/4407 - Loss: 1.9634 | PPL: 7.12 | LR: 6.870662e-05\n",
      "  Step 1900/4407 - Loss: 1.9630 | PPL: 7.12 | LR: 6.860680e-05\n",
      "  Step 1900/4407 - Loss: 1.9630 | PPL: 7.12 | LR: 6.860680e-05\n",
      "  Step 2000/4407 - Loss: 1.9637 | PPL: 7.13 | LR: 6.850702e-05\n",
      "  Step 2000/4407 - Loss: 1.9637 | PPL: 7.13 | LR: 6.850702e-05\n",
      "  Step 2100/4407 - Loss: 1.9640 | PPL: 7.13 | LR: 6.840730e-05\n",
      "  Step 2100/4407 - Loss: 1.9640 | PPL: 7.13 | LR: 6.840730e-05\n",
      "  Step 2200/4407 - Loss: 1.9655 | PPL: 7.14 | LR: 6.830763e-05\n",
      "  Step 2200/4407 - Loss: 1.9655 | PPL: 7.14 | LR: 6.830763e-05\n",
      "  Step 2300/4407 - Loss: 1.9660 | PPL: 7.14 | LR: 6.820801e-05\n",
      "  Step 2300/4407 - Loss: 1.9660 | PPL: 7.14 | LR: 6.820801e-05\n",
      "  Step 2400/4407 - Loss: 1.9672 | PPL: 7.15 | LR: 6.810845e-05\n",
      "  Step 2400/4407 - Loss: 1.9672 | PPL: 7.15 | LR: 6.810845e-05\n",
      "  Step 2500/4407 - Loss: 1.9670 | PPL: 7.15 | LR: 6.800893e-05\n",
      "  Step 2500/4407 - Loss: 1.9670 | PPL: 7.15 | LR: 6.800893e-05\n",
      "  Step 2600/4407 - Loss: 1.9670 | PPL: 7.15 | LR: 6.790946e-05\n",
      "  Step 2600/4407 - Loss: 1.9670 | PPL: 7.15 | LR: 6.790946e-05\n",
      "  Step 2700/4407 - Loss: 1.9679 | PPL: 7.16 | LR: 6.781005e-05\n",
      "  Step 2700/4407 - Loss: 1.9679 | PPL: 7.16 | LR: 6.781005e-05\n",
      "  Step 2800/4407 - Loss: 1.9678 | PPL: 7.15 | LR: 6.771069e-05\n",
      "  Step 2800/4407 - Loss: 1.9678 | PPL: 7.15 | LR: 6.771069e-05\n",
      "  Step 2900/4407 - Loss: 1.9684 | PPL: 7.16 | LR: 6.761138e-05\n",
      "  Step 2900/4407 - Loss: 1.9684 | PPL: 7.16 | LR: 6.761138e-05\n",
      "  Step 3000/4407 - Loss: 1.9691 | PPL: 7.16 | LR: 6.751212e-05\n",
      "  Step 3000/4407 - Loss: 1.9691 | PPL: 7.16 | LR: 6.751212e-05\n",
      "  Step 3100/4407 - Loss: 1.9689 | PPL: 7.16 | LR: 6.741291e-05\n",
      "  Step 3100/4407 - Loss: 1.9689 | PPL: 7.16 | LR: 6.741291e-05\n",
      "  Step 3200/4407 - Loss: 1.9692 | PPL: 7.17 | LR: 6.731376e-05\n",
      "  Step 3200/4407 - Loss: 1.9692 | PPL: 7.17 | LR: 6.731376e-05\n",
      "  Step 3300/4407 - Loss: 1.9692 | PPL: 7.16 | LR: 6.721465e-05\n",
      "  Step 3300/4407 - Loss: 1.9692 | PPL: 7.16 | LR: 6.721465e-05\n",
      "  Step 3400/4407 - Loss: 1.9691 | PPL: 7.16 | LR: 6.711560e-05\n",
      "  Step 3400/4407 - Loss: 1.9691 | PPL: 7.16 | LR: 6.711560e-05\n",
      "  Step 3500/4407 - Loss: 1.9697 | PPL: 7.17 | LR: 6.701660e-05\n",
      "  Step 3500/4407 - Loss: 1.9697 | PPL: 7.17 | LR: 6.701660e-05\n",
      "  Step 3600/4407 - Loss: 1.9689 | PPL: 7.16 | LR: 6.691766e-05\n",
      "  Step 3600/4407 - Loss: 1.9689 | PPL: 7.16 | LR: 6.691766e-05\n",
      "  Step 3700/4407 - Loss: 1.9691 | PPL: 7.16 | LR: 6.681876e-05\n",
      "  Step 3700/4407 - Loss: 1.9691 | PPL: 7.16 | LR: 6.681876e-05\n",
      "  Step 3800/4407 - Loss: 1.9693 | PPL: 7.17 | LR: 6.671992e-05\n",
      "  Step 3800/4407 - Loss: 1.9693 | PPL: 7.17 | LR: 6.671992e-05\n",
      "  Step 3900/4407 - Loss: 1.9699 | PPL: 7.17 | LR: 6.662113e-05\n",
      "  Step 3900/4407 - Loss: 1.9699 | PPL: 7.17 | LR: 6.662113e-05\n",
      "  Step 4000/4407 - Loss: 1.9708 | PPL: 7.18 | LR: 6.652239e-05\n",
      "  Step 4000/4407 - Loss: 1.9708 | PPL: 7.18 | LR: 6.652239e-05\n",
      "  Step 4100/4407 - Loss: 1.9701 | PPL: 7.17 | LR: 6.642370e-05\n",
      "  Step 4100/4407 - Loss: 1.9701 | PPL: 7.17 | LR: 6.642370e-05\n",
      "  Step 4200/4407 - Loss: 1.9700 | PPL: 7.17 | LR: 6.632507e-05\n",
      "  Step 4200/4407 - Loss: 1.9700 | PPL: 7.17 | LR: 6.632507e-05\n",
      "  Step 4300/4407 - Loss: 1.9693 | PPL: 7.17 | LR: 6.622649e-05\n",
      "  Step 4300/4407 - Loss: 1.9693 | PPL: 7.17 | LR: 6.622649e-05\n",
      "  Step 4400/4407 - Loss: 1.9706 | PPL: 7.17 | LR: 6.612796e-05\n",
      "  Step 4400/4407 - Loss: 1.9706 | PPL: 7.17 | LR: 6.612796e-05\n",
      "Epoch 72/100 | Train Loss: 1.9706 | Train PPL: 7.18 | Val Loss: 2.5929 | Val PPL: 13.37\n",
      "Epoch 72/100 | Train Loss: 1.9706 | Train PPL: 7.18 | Val Loss: 2.5929 | Val PPL: 13.37\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.18\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.18\n",
      "  Step 100/4407 - Loss: 2.0057 | PPL: 7.43 | LR: 6.602259e-05\n",
      "  Step 100/4407 - Loss: 2.0057 | PPL: 7.43 | LR: 6.602259e-05\n",
      "  Step 200/4407 - Loss: 1.9768 | PPL: 7.22 | LR: 6.592417e-05\n",
      "  Step 200/4407 - Loss: 1.9768 | PPL: 7.22 | LR: 6.592417e-05\n",
      "  Step 300/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 6.582581e-05\n",
      "  Step 300/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 6.582581e-05\n",
      "  Step 400/4407 - Loss: 1.9581 | PPL: 7.09 | LR: 6.572749e-05\n",
      "  Step 400/4407 - Loss: 1.9581 | PPL: 7.09 | LR: 6.572749e-05\n",
      "  Step 500/4407 - Loss: 1.9601 | PPL: 7.10 | LR: 6.562923e-05\n",
      "  Step 500/4407 - Loss: 1.9601 | PPL: 7.10 | LR: 6.562923e-05\n",
      "  Step 600/4407 - Loss: 1.9590 | PPL: 7.09 | LR: 6.553102e-05\n",
      "  Step 600/4407 - Loss: 1.9590 | PPL: 7.09 | LR: 6.553102e-05\n",
      "  Step 700/4407 - Loss: 1.9524 | PPL: 7.05 | LR: 6.543287e-05\n",
      "  Step 700/4407 - Loss: 1.9524 | PPL: 7.05 | LR: 6.543287e-05\n",
      "  Step 800/4407 - Loss: 1.9533 | PPL: 7.05 | LR: 6.533477e-05\n",
      "  Step 800/4407 - Loss: 1.9533 | PPL: 7.05 | LR: 6.533477e-05\n",
      "  Step 900/4407 - Loss: 1.9576 | PPL: 7.08 | LR: 6.523672e-05\n",
      "  Step 900/4407 - Loss: 1.9576 | PPL: 7.08 | LR: 6.523672e-05\n",
      "  Step 1000/4407 - Loss: 1.9544 | PPL: 7.06 | LR: 6.513872e-05\n",
      "  Step 1000/4407 - Loss: 1.9544 | PPL: 7.06 | LR: 6.513872e-05\n",
      "  Step 1100/4407 - Loss: 1.9526 | PPL: 7.05 | LR: 6.504078e-05\n",
      "  Step 1100/4407 - Loss: 1.9526 | PPL: 7.05 | LR: 6.504078e-05\n",
      "  Step 1200/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 6.494289e-05\n",
      "  Step 1200/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 6.494289e-05\n",
      "  Step 1300/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 6.484505e-05\n",
      "  Step 1300/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 6.484505e-05\n",
      "  Step 1400/4407 - Loss: 1.9492 | PPL: 7.02 | LR: 6.474727e-05\n",
      "  Step 1400/4407 - Loss: 1.9492 | PPL: 7.02 | LR: 6.474727e-05\n",
      "  Step 1500/4407 - Loss: 1.9529 | PPL: 7.05 | LR: 6.464954e-05\n",
      "  Step 1500/4407 - Loss: 1.9529 | PPL: 7.05 | LR: 6.464954e-05\n",
      "  Step 1600/4407 - Loss: 1.9531 | PPL: 7.05 | LR: 6.455187e-05\n",
      "  Step 1600/4407 - Loss: 1.9531 | PPL: 7.05 | LR: 6.455187e-05\n",
      "  Step 1700/4407 - Loss: 1.9527 | PPL: 7.05 | LR: 6.445424e-05\n",
      "  Step 1700/4407 - Loss: 1.9527 | PPL: 7.05 | LR: 6.445424e-05\n",
      "  Step 1800/4407 - Loss: 1.9541 | PPL: 7.06 | LR: 6.435668e-05\n",
      "  Step 1800/4407 - Loss: 1.9541 | PPL: 7.06 | LR: 6.435668e-05\n",
      "  Step 1900/4407 - Loss: 1.9527 | PPL: 7.05 | LR: 6.425916e-05\n",
      "  Step 1900/4407 - Loss: 1.9527 | PPL: 7.05 | LR: 6.425916e-05\n",
      "  Step 2000/4407 - Loss: 1.9535 | PPL: 7.05 | LR: 6.416170e-05\n",
      "  Step 2000/4407 - Loss: 1.9535 | PPL: 7.05 | LR: 6.416170e-05\n",
      "  Step 2100/4407 - Loss: 1.9533 | PPL: 7.05 | LR: 6.406429e-05\n",
      "  Step 2100/4407 - Loss: 1.9533 | PPL: 7.05 | LR: 6.406429e-05\n",
      "  Step 2200/4407 - Loss: 1.9546 | PPL: 7.06 | LR: 6.396694e-05\n",
      "  Step 2200/4407 - Loss: 1.9546 | PPL: 7.06 | LR: 6.396694e-05\n",
      "  Step 2300/4407 - Loss: 1.9543 | PPL: 7.06 | LR: 6.386964e-05\n",
      "  Step 2300/4407 - Loss: 1.9543 | PPL: 7.06 | LR: 6.386964e-05\n",
      "  Step 2400/4407 - Loss: 1.9570 | PPL: 7.08 | LR: 6.377240e-05\n",
      "  Step 2400/4407 - Loss: 1.9570 | PPL: 7.08 | LR: 6.377240e-05\n",
      "  Step 2500/4407 - Loss: 1.9567 | PPL: 7.08 | LR: 6.367521e-05\n",
      "  Step 2500/4407 - Loss: 1.9567 | PPL: 7.08 | LR: 6.367521e-05\n",
      "  Step 2600/4407 - Loss: 1.9574 | PPL: 7.08 | LR: 6.357807e-05\n",
      "  Step 2600/4407 - Loss: 1.9574 | PPL: 7.08 | LR: 6.357807e-05\n",
      "  Step 2700/4407 - Loss: 1.9588 | PPL: 7.09 | LR: 6.348099e-05\n",
      "  Step 2700/4407 - Loss: 1.9588 | PPL: 7.09 | LR: 6.348099e-05\n",
      "  Step 2800/4407 - Loss: 1.9593 | PPL: 7.09 | LR: 6.338396e-05\n",
      "  Step 2800/4407 - Loss: 1.9593 | PPL: 7.09 | LR: 6.338396e-05\n",
      "  Step 2900/4407 - Loss: 1.9621 | PPL: 7.11 | LR: 6.328699e-05\n",
      "  Step 2900/4407 - Loss: 1.9621 | PPL: 7.11 | LR: 6.328699e-05\n",
      "  Step 3000/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 6.319007e-05\n",
      "  Step 3000/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 6.319007e-05\n",
      "  Step 3100/4407 - Loss: 1.9632 | PPL: 7.12 | LR: 6.309320e-05\n",
      "  Step 3100/4407 - Loss: 1.9632 | PPL: 7.12 | LR: 6.309320e-05\n",
      "  Step 3200/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 6.299639e-05\n",
      "  Step 3200/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 6.299639e-05\n",
      "  Step 3300/4407 - Loss: 1.9615 | PPL: 7.11 | LR: 6.289963e-05\n",
      "  Step 3300/4407 - Loss: 1.9615 | PPL: 7.11 | LR: 6.289963e-05\n",
      "  Step 3400/4407 - Loss: 1.9625 | PPL: 7.12 | LR: 6.280293e-05\n",
      "  Step 3400/4407 - Loss: 1.9625 | PPL: 7.12 | LR: 6.280293e-05\n",
      "  Step 3500/4407 - Loss: 1.9635 | PPL: 7.12 | LR: 6.270629e-05\n",
      "  Step 3500/4407 - Loss: 1.9635 | PPL: 7.12 | LR: 6.270629e-05\n",
      "  Step 3600/4407 - Loss: 1.9631 | PPL: 7.12 | LR: 6.260970e-05\n",
      "  Step 3600/4407 - Loss: 1.9631 | PPL: 7.12 | LR: 6.260970e-05\n",
      "  Step 3700/4407 - Loss: 1.9631 | PPL: 7.12 | LR: 6.251316e-05\n",
      "  Step 3700/4407 - Loss: 1.9631 | PPL: 7.12 | LR: 6.251316e-05\n",
      "  Step 3800/4407 - Loss: 1.9646 | PPL: 7.13 | LR: 6.241668e-05\n",
      "  Step 3800/4407 - Loss: 1.9646 | PPL: 7.13 | LR: 6.241668e-05\n",
      "  Step 3900/4407 - Loss: 1.9651 | PPL: 7.14 | LR: 6.232025e-05\n",
      "  Step 3900/4407 - Loss: 1.9651 | PPL: 7.14 | LR: 6.232025e-05\n",
      "  Step 4000/4407 - Loss: 1.9654 | PPL: 7.14 | LR: 6.222388e-05\n",
      "  Step 4000/4407 - Loss: 1.9654 | PPL: 7.14 | LR: 6.222388e-05\n",
      "  Step 4100/4407 - Loss: 1.9659 | PPL: 7.14 | LR: 6.212756e-05\n",
      "  Step 4100/4407 - Loss: 1.9659 | PPL: 7.14 | LR: 6.212756e-05\n",
      "  Step 4200/4407 - Loss: 1.9662 | PPL: 7.14 | LR: 6.203130e-05\n",
      "  Step 4200/4407 - Loss: 1.9662 | PPL: 7.14 | LR: 6.203130e-05\n",
      "  Step 4300/4407 - Loss: 1.9667 | PPL: 7.15 | LR: 6.193509e-05\n",
      "  Step 4300/4407 - Loss: 1.9667 | PPL: 7.15 | LR: 6.193509e-05\n",
      "  Step 4400/4407 - Loss: 1.9681 | PPL: 7.16 | LR: 6.183894e-05\n",
      "  Step 4400/4407 - Loss: 1.9681 | PPL: 7.16 | LR: 6.183894e-05\n",
      "Epoch 73/100 | Train Loss: 1.9681 | Train PPL: 7.16 | Val Loss: 2.5899 | Val PPL: 13.33\n",
      "Epoch 73/100 | Train Loss: 1.9681 | Train PPL: 7.16 | Val Loss: 2.5899 | Val PPL: 13.33\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.16\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.16\n",
      "  Step 100/4407 - Loss: 1.9338 | PPL: 6.92 | LR: 6.173612e-05\n",
      "  Step 100/4407 - Loss: 1.9338 | PPL: 6.92 | LR: 6.173612e-05\n",
      "  Step 200/4407 - Loss: 1.9259 | PPL: 6.86 | LR: 6.164008e-05\n",
      "  Step 200/4407 - Loss: 1.9259 | PPL: 6.86 | LR: 6.164008e-05\n",
      "  Step 300/4407 - Loss: 1.9535 | PPL: 7.05 | LR: 6.154410e-05\n",
      "  Step 300/4407 - Loss: 1.9535 | PPL: 7.05 | LR: 6.154410e-05\n",
      "  Step 400/4407 - Loss: 1.9575 | PPL: 7.08 | LR: 6.144818e-05\n",
      "  Step 400/4407 - Loss: 1.9575 | PPL: 7.08 | LR: 6.144818e-05\n",
      "  Step 500/4407 - Loss: 1.9528 | PPL: 7.05 | LR: 6.135231e-05\n",
      "  Step 500/4407 - Loss: 1.9528 | PPL: 7.05 | LR: 6.135231e-05\n",
      "  Step 600/4407 - Loss: 1.9581 | PPL: 7.09 | LR: 6.125649e-05\n",
      "  Step 600/4407 - Loss: 1.9581 | PPL: 7.09 | LR: 6.125649e-05\n",
      "  Step 700/4407 - Loss: 1.9562 | PPL: 7.07 | LR: 6.116073e-05\n",
      "  Step 700/4407 - Loss: 1.9562 | PPL: 7.07 | LR: 6.116073e-05\n",
      "  Step 800/4407 - Loss: 1.9590 | PPL: 7.09 | LR: 6.106503e-05\n",
      "  Step 800/4407 - Loss: 1.9590 | PPL: 7.09 | LR: 6.106503e-05\n",
      "  Step 900/4407 - Loss: 1.9535 | PPL: 7.05 | LR: 6.096938e-05\n",
      "  Step 900/4407 - Loss: 1.9535 | PPL: 7.05 | LR: 6.096938e-05\n",
      "  Step 1000/4407 - Loss: 1.9539 | PPL: 7.06 | LR: 6.087379e-05\n",
      "  Step 1000/4407 - Loss: 1.9539 | PPL: 7.06 | LR: 6.087379e-05\n",
      "  Step 1100/4407 - Loss: 1.9526 | PPL: 7.05 | LR: 6.077825e-05\n",
      "  Step 1100/4407 - Loss: 1.9526 | PPL: 7.05 | LR: 6.077825e-05\n",
      "  Step 1200/4407 - Loss: 1.9539 | PPL: 7.06 | LR: 6.068277e-05\n",
      "  Step 1200/4407 - Loss: 1.9539 | PPL: 7.06 | LR: 6.068277e-05\n",
      "  Step 1300/4407 - Loss: 1.9555 | PPL: 7.07 | LR: 6.058735e-05\n",
      "  Step 1300/4407 - Loss: 1.9555 | PPL: 7.07 | LR: 6.058735e-05\n",
      "  Step 1400/4407 - Loss: 1.9581 | PPL: 7.09 | LR: 6.049198e-05\n",
      "  Step 1400/4407 - Loss: 1.9581 | PPL: 7.09 | LR: 6.049198e-05\n",
      "  Step 1500/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 6.039667e-05\n",
      "  Step 1500/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 6.039667e-05\n",
      "  Step 1600/4407 - Loss: 1.9596 | PPL: 7.10 | LR: 6.030142e-05\n",
      "  Step 1600/4407 - Loss: 1.9596 | PPL: 7.10 | LR: 6.030142e-05\n",
      "  Step 1700/4407 - Loss: 1.9596 | PPL: 7.10 | LR: 6.020622e-05\n",
      "  Step 1700/4407 - Loss: 1.9596 | PPL: 7.10 | LR: 6.020622e-05\n",
      "  Step 1800/4407 - Loss: 1.9592 | PPL: 7.09 | LR: 6.011107e-05\n",
      "  Step 1800/4407 - Loss: 1.9592 | PPL: 7.09 | LR: 6.011107e-05\n",
      "  Step 1900/4407 - Loss: 1.9614 | PPL: 7.11 | LR: 6.001599e-05\n",
      "  Step 1900/4407 - Loss: 1.9614 | PPL: 7.11 | LR: 6.001599e-05\n",
      "  Step 2000/4407 - Loss: 1.9608 | PPL: 7.10 | LR: 5.992096e-05\n",
      "  Step 2000/4407 - Loss: 1.9608 | PPL: 7.10 | LR: 5.992096e-05\n",
      "  Step 2100/4407 - Loss: 1.9611 | PPL: 7.11 | LR: 5.982598e-05\n",
      "  Step 2100/4407 - Loss: 1.9611 | PPL: 7.11 | LR: 5.982598e-05\n",
      "  Step 2200/4407 - Loss: 1.9617 | PPL: 7.11 | LR: 5.973107e-05\n",
      "  Step 2200/4407 - Loss: 1.9617 | PPL: 7.11 | LR: 5.973107e-05\n",
      "  Step 2300/4407 - Loss: 1.9614 | PPL: 7.11 | LR: 5.963621e-05\n",
      "  Step 2300/4407 - Loss: 1.9614 | PPL: 7.11 | LR: 5.963621e-05\n",
      "  Step 2400/4407 - Loss: 1.9616 | PPL: 7.11 | LR: 5.954140e-05\n",
      "  Step 2400/4407 - Loss: 1.9616 | PPL: 7.11 | LR: 5.954140e-05\n",
      "  Step 2500/4407 - Loss: 1.9613 | PPL: 7.11 | LR: 5.944666e-05\n",
      "  Step 2500/4407 - Loss: 1.9613 | PPL: 7.11 | LR: 5.944666e-05\n",
      "  Step 2600/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 5.935197e-05\n",
      "  Step 2600/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 5.935197e-05\n",
      "  Step 2700/4407 - Loss: 1.9621 | PPL: 7.11 | LR: 5.925733e-05\n",
      "  Step 2700/4407 - Loss: 1.9621 | PPL: 7.11 | LR: 5.925733e-05\n",
      "  Step 2800/4407 - Loss: 1.9618 | PPL: 7.11 | LR: 5.916276e-05\n",
      "  Step 2800/4407 - Loss: 1.9618 | PPL: 7.11 | LR: 5.916276e-05\n",
      "  Step 2900/4407 - Loss: 1.9605 | PPL: 7.10 | LR: 5.906824e-05\n",
      "  Step 2900/4407 - Loss: 1.9605 | PPL: 7.10 | LR: 5.906824e-05\n",
      "  Step 3000/4407 - Loss: 1.9591 | PPL: 7.09 | LR: 5.897378e-05\n",
      "  Step 3000/4407 - Loss: 1.9591 | PPL: 7.09 | LR: 5.897378e-05\n",
      "  Step 3100/4407 - Loss: 1.9601 | PPL: 7.10 | LR: 5.887937e-05\n",
      "  Step 3100/4407 - Loss: 1.9601 | PPL: 7.10 | LR: 5.887937e-05\n",
      "  Step 3200/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 5.878502e-05\n",
      "  Step 3200/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 5.878502e-05\n",
      "  Step 3300/4407 - Loss: 1.9605 | PPL: 7.10 | LR: 5.869073e-05\n",
      "  Step 3300/4407 - Loss: 1.9605 | PPL: 7.10 | LR: 5.869073e-05\n",
      "  Step 3400/4407 - Loss: 1.9621 | PPL: 7.11 | LR: 5.859650e-05\n",
      "  Step 3400/4407 - Loss: 1.9621 | PPL: 7.11 | LR: 5.859650e-05\n",
      "  Step 3500/4407 - Loss: 1.9619 | PPL: 7.11 | LR: 5.850233e-05\n",
      "  Step 3500/4407 - Loss: 1.9619 | PPL: 7.11 | LR: 5.850233e-05\n",
      "  Step 3600/4407 - Loss: 1.9627 | PPL: 7.12 | LR: 5.840821e-05\n",
      "  Step 3600/4407 - Loss: 1.9627 | PPL: 7.12 | LR: 5.840821e-05\n",
      "  Step 3700/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 5.831415e-05\n",
      "  Step 3700/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 5.831415e-05\n",
      "  Step 3800/4407 - Loss: 1.9629 | PPL: 7.12 | LR: 5.822014e-05\n",
      "  Step 3800/4407 - Loss: 1.9629 | PPL: 7.12 | LR: 5.822014e-05\n",
      "  Step 3900/4407 - Loss: 1.9635 | PPL: 7.12 | LR: 5.812620e-05\n",
      "  Step 3900/4407 - Loss: 1.9635 | PPL: 7.12 | LR: 5.812620e-05\n",
      "  Step 4000/4407 - Loss: 1.9640 | PPL: 7.13 | LR: 5.803231e-05\n",
      "  Step 4000/4407 - Loss: 1.9640 | PPL: 7.13 | LR: 5.803231e-05\n",
      "  Step 4100/4407 - Loss: 1.9638 | PPL: 7.13 | LR: 5.793848e-05\n",
      "  Step 4100/4407 - Loss: 1.9638 | PPL: 7.13 | LR: 5.793848e-05\n",
      "  Step 4200/4407 - Loss: 1.9636 | PPL: 7.12 | LR: 5.784470e-05\n",
      "  Step 4200/4407 - Loss: 1.9636 | PPL: 7.12 | LR: 5.784470e-05\n",
      "  Step 4300/4407 - Loss: 1.9645 | PPL: 7.13 | LR: 5.775099e-05\n",
      "  Step 4300/4407 - Loss: 1.9645 | PPL: 7.13 | LR: 5.775099e-05\n",
      "  Step 4400/4407 - Loss: 1.9654 | PPL: 7.14 | LR: 5.765733e-05\n",
      "  Step 4400/4407 - Loss: 1.9654 | PPL: 7.14 | LR: 5.765733e-05\n",
      "Epoch 74/100 | Train Loss: 1.9654 | Train PPL: 7.14 | Val Loss: 2.5952 | Val PPL: 13.40\n",
      "Epoch 74/100 | Train Loss: 1.9654 | Train PPL: 7.14 | Val Loss: 2.5952 | Val PPL: 13.40\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.14\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.14\n",
      "  Step 100/4407 - Loss: 1.9751 | PPL: 7.21 | LR: 5.755718e-05\n",
      "  Step 100/4407 - Loss: 1.9751 | PPL: 7.21 | LR: 5.755718e-05\n",
      "  Step 200/4407 - Loss: 1.9585 | PPL: 7.09 | LR: 5.746365e-05\n",
      "  Step 200/4407 - Loss: 1.9585 | PPL: 7.09 | LR: 5.746365e-05\n",
      "  Step 300/4407 - Loss: 1.9718 | PPL: 7.18 | LR: 5.737017e-05\n",
      "  Step 300/4407 - Loss: 1.9718 | PPL: 7.18 | LR: 5.737017e-05\n",
      "  Step 400/4407 - Loss: 1.9713 | PPL: 7.18 | LR: 5.727675e-05\n",
      "  Step 400/4407 - Loss: 1.9713 | PPL: 7.18 | LR: 5.727675e-05\n",
      "  Step 500/4407 - Loss: 1.9701 | PPL: 7.17 | LR: 5.718338e-05\n",
      "  Step 500/4407 - Loss: 1.9701 | PPL: 7.17 | LR: 5.718338e-05\n",
      "  Step 600/4407 - Loss: 1.9661 | PPL: 7.14 | LR: 5.709008e-05\n",
      "  Step 600/4407 - Loss: 1.9661 | PPL: 7.14 | LR: 5.709008e-05\n",
      "  Step 700/4407 - Loss: 1.9694 | PPL: 7.17 | LR: 5.699683e-05\n",
      "  Step 700/4407 - Loss: 1.9694 | PPL: 7.17 | LR: 5.699683e-05\n",
      "  Step 800/4407 - Loss: 1.9685 | PPL: 7.16 | LR: 5.690364e-05\n",
      "  Step 800/4407 - Loss: 1.9685 | PPL: 7.16 | LR: 5.690364e-05\n",
      "  Step 900/4407 - Loss: 1.9657 | PPL: 7.14 | LR: 5.681052e-05\n",
      "  Step 900/4407 - Loss: 1.9657 | PPL: 7.14 | LR: 5.681052e-05\n",
      "  Step 1000/4407 - Loss: 1.9671 | PPL: 7.15 | LR: 5.671744e-05\n",
      "  Step 1000/4407 - Loss: 1.9671 | PPL: 7.15 | LR: 5.671744e-05\n",
      "  Step 1100/4407 - Loss: 1.9690 | PPL: 7.16 | LR: 5.662443e-05\n",
      "  Step 1100/4407 - Loss: 1.9690 | PPL: 7.16 | LR: 5.662443e-05\n",
      "  Step 1200/4407 - Loss: 1.9659 | PPL: 7.14 | LR: 5.653148e-05\n",
      "  Step 1200/4407 - Loss: 1.9659 | PPL: 7.14 | LR: 5.653148e-05\n",
      "  Step 1300/4407 - Loss: 1.9655 | PPL: 7.14 | LR: 5.643858e-05\n",
      "  Step 1300/4407 - Loss: 1.9655 | PPL: 7.14 | LR: 5.643858e-05\n",
      "  Step 1400/4407 - Loss: 1.9662 | PPL: 7.14 | LR: 5.634575e-05\n",
      "  Step 1400/4407 - Loss: 1.9662 | PPL: 7.14 | LR: 5.634575e-05\n",
      "  Step 1500/4407 - Loss: 1.9662 | PPL: 7.14 | LR: 5.625297e-05\n",
      "  Step 1500/4407 - Loss: 1.9662 | PPL: 7.14 | LR: 5.625297e-05\n",
      "  Step 1600/4407 - Loss: 1.9679 | PPL: 7.16 | LR: 5.616025e-05\n",
      "  Step 1600/4407 - Loss: 1.9679 | PPL: 7.16 | LR: 5.616025e-05\n",
      "  Step 1700/4407 - Loss: 1.9671 | PPL: 7.15 | LR: 5.606759e-05\n",
      "  Step 1700/4407 - Loss: 1.9671 | PPL: 7.15 | LR: 5.606759e-05\n",
      "  Step 1800/4407 - Loss: 1.9654 | PPL: 7.14 | LR: 5.597499e-05\n",
      "  Step 1800/4407 - Loss: 1.9654 | PPL: 7.14 | LR: 5.597499e-05\n",
      "  Step 1900/4407 - Loss: 1.9633 | PPL: 7.12 | LR: 5.588245e-05\n",
      "  Step 1900/4407 - Loss: 1.9633 | PPL: 7.12 | LR: 5.588245e-05\n",
      "  Step 2000/4407 - Loss: 1.9629 | PPL: 7.12 | LR: 5.578996e-05\n",
      "  Step 2000/4407 - Loss: 1.9629 | PPL: 7.12 | LR: 5.578996e-05\n",
      "  Step 2100/4407 - Loss: 1.9612 | PPL: 7.11 | LR: 5.569754e-05\n",
      "  Step 2100/4407 - Loss: 1.9612 | PPL: 7.11 | LR: 5.569754e-05\n",
      "  Step 2200/4407 - Loss: 1.9588 | PPL: 7.09 | LR: 5.560517e-05\n",
      "  Step 2200/4407 - Loss: 1.9588 | PPL: 7.09 | LR: 5.560517e-05\n",
      "  Step 2300/4407 - Loss: 1.9594 | PPL: 7.10 | LR: 5.551287e-05\n",
      "  Step 2300/4407 - Loss: 1.9594 | PPL: 7.10 | LR: 5.551287e-05\n",
      "  Step 2400/4407 - Loss: 1.9600 | PPL: 7.10 | LR: 5.542062e-05\n",
      "  Step 2400/4407 - Loss: 1.9600 | PPL: 7.10 | LR: 5.542062e-05\n",
      "  Step 2500/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 5.532843e-05\n",
      "  Step 2500/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 5.532843e-05\n",
      "  Step 2600/4407 - Loss: 1.9612 | PPL: 7.11 | LR: 5.523631e-05\n",
      "  Step 2600/4407 - Loss: 1.9612 | PPL: 7.11 | LR: 5.523631e-05\n",
      "  Step 2700/4407 - Loss: 1.9609 | PPL: 7.11 | LR: 5.514424e-05\n",
      "  Step 2700/4407 - Loss: 1.9609 | PPL: 7.11 | LR: 5.514424e-05\n",
      "  Step 2800/4407 - Loss: 1.9614 | PPL: 7.11 | LR: 5.505223e-05\n",
      "  Step 2800/4407 - Loss: 1.9614 | PPL: 7.11 | LR: 5.505223e-05\n",
      "  Step 2900/4407 - Loss: 1.9608 | PPL: 7.10 | LR: 5.496028e-05\n",
      "  Step 2900/4407 - Loss: 1.9608 | PPL: 7.10 | LR: 5.496028e-05\n",
      "  Step 3000/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 5.486839e-05\n",
      "  Step 3000/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 5.486839e-05\n",
      "  Step 3100/4407 - Loss: 1.9609 | PPL: 7.11 | LR: 5.477656e-05\n",
      "  Step 3100/4407 - Loss: 1.9609 | PPL: 7.11 | LR: 5.477656e-05\n",
      "  Step 3200/4407 - Loss: 1.9614 | PPL: 7.11 | LR: 5.468479e-05\n",
      "  Step 3200/4407 - Loss: 1.9614 | PPL: 7.11 | LR: 5.468479e-05\n",
      "  Step 3300/4407 - Loss: 1.9619 | PPL: 7.11 | LR: 5.459308e-05\n",
      "  Step 3300/4407 - Loss: 1.9619 | PPL: 7.11 | LR: 5.459308e-05\n",
      "  Step 3400/4407 - Loss: 1.9629 | PPL: 7.12 | LR: 5.450143e-05\n",
      "  Step 3400/4407 - Loss: 1.9629 | PPL: 7.12 | LR: 5.450143e-05\n",
      "  Step 3500/4407 - Loss: 1.9622 | PPL: 7.11 | LR: 5.440984e-05\n",
      "  Step 3500/4407 - Loss: 1.9622 | PPL: 7.11 | LR: 5.440984e-05\n",
      "  Step 3600/4407 - Loss: 1.9624 | PPL: 7.12 | LR: 5.431831e-05\n",
      "  Step 3600/4407 - Loss: 1.9624 | PPL: 7.12 | LR: 5.431831e-05\n",
      "  Step 3700/4407 - Loss: 1.9621 | PPL: 7.11 | LR: 5.422684e-05\n",
      "  Step 3700/4407 - Loss: 1.9621 | PPL: 7.11 | LR: 5.422684e-05\n",
      "  Step 3800/4407 - Loss: 1.9624 | PPL: 7.12 | LR: 5.413543e-05\n",
      "  Step 3800/4407 - Loss: 1.9624 | PPL: 7.12 | LR: 5.413543e-05\n",
      "  Step 3900/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 5.404408e-05\n",
      "  Step 3900/4407 - Loss: 1.9623 | PPL: 7.12 | LR: 5.404408e-05\n",
      "  Step 4000/4407 - Loss: 1.9627 | PPL: 7.12 | LR: 5.395279e-05\n",
      "  Step 4000/4407 - Loss: 1.9627 | PPL: 7.12 | LR: 5.395279e-05\n",
      "  Step 4100/4407 - Loss: 1.9626 | PPL: 7.12 | LR: 5.386156e-05\n",
      "  Step 4100/4407 - Loss: 1.9626 | PPL: 7.12 | LR: 5.386156e-05\n",
      "  Step 4200/4407 - Loss: 1.9624 | PPL: 7.12 | LR: 5.377039e-05\n",
      "  Step 4200/4407 - Loss: 1.9624 | PPL: 7.12 | LR: 5.377039e-05\n",
      "  Step 4300/4407 - Loss: 1.9624 | PPL: 7.12 | LR: 5.367928e-05\n",
      "  Step 4300/4407 - Loss: 1.9624 | PPL: 7.12 | LR: 5.367928e-05\n",
      "  Step 4400/4407 - Loss: 1.9624 | PPL: 7.12 | LR: 5.358823e-05\n",
      "  Step 4400/4407 - Loss: 1.9624 | PPL: 7.12 | LR: 5.358823e-05\n",
      "Epoch 75/100 | Train Loss: 1.9626 | Train PPL: 7.12 | Val Loss: 2.5913 | Val PPL: 13.35\n",
      "Epoch 75/100 | Train Loss: 1.9626 | Train PPL: 7.12 | Val Loss: 2.5913 | Val PPL: 13.35\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.12\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.12\n",
      "  Step 100/4407 - Loss: 1.9801 | PPL: 7.24 | LR: 5.349087e-05\n",
      "  Step 100/4407 - Loss: 1.9801 | PPL: 7.24 | LR: 5.349087e-05\n",
      "  Step 200/4407 - Loss: 1.9475 | PPL: 7.01 | LR: 5.339995e-05\n",
      "  Step 200/4407 - Loss: 1.9475 | PPL: 7.01 | LR: 5.339995e-05\n",
      "  Step 300/4407 - Loss: 1.9472 | PPL: 7.01 | LR: 5.330909e-05\n",
      "  Step 300/4407 - Loss: 1.9472 | PPL: 7.01 | LR: 5.330909e-05\n",
      "  Step 400/4407 - Loss: 1.9530 | PPL: 7.05 | LR: 5.321829e-05\n",
      "  Step 400/4407 - Loss: 1.9530 | PPL: 7.05 | LR: 5.321829e-05\n",
      "  Step 500/4407 - Loss: 1.9494 | PPL: 7.02 | LR: 5.312754e-05\n",
      "  Step 500/4407 - Loss: 1.9494 | PPL: 7.02 | LR: 5.312754e-05\n",
      "  Step 600/4407 - Loss: 1.9541 | PPL: 7.06 | LR: 5.303686e-05\n",
      "  Step 600/4407 - Loss: 1.9541 | PPL: 7.06 | LR: 5.303686e-05\n",
      "  Step 700/4407 - Loss: 1.9576 | PPL: 7.08 | LR: 5.294624e-05\n",
      "  Step 700/4407 - Loss: 1.9576 | PPL: 7.08 | LR: 5.294624e-05\n",
      "  Step 800/4407 - Loss: 1.9587 | PPL: 7.09 | LR: 5.285568e-05\n",
      "  Step 800/4407 - Loss: 1.9587 | PPL: 7.09 | LR: 5.285568e-05\n",
      "  Step 900/4407 - Loss: 1.9557 | PPL: 7.07 | LR: 5.276519e-05\n",
      "  Step 900/4407 - Loss: 1.9557 | PPL: 7.07 | LR: 5.276519e-05\n",
      "  Step 1000/4407 - Loss: 1.9577 | PPL: 7.08 | LR: 5.267475e-05\n",
      "  Step 1000/4407 - Loss: 1.9577 | PPL: 7.08 | LR: 5.267475e-05\n",
      "  Step 1100/4407 - Loss: 1.9549 | PPL: 7.06 | LR: 5.258437e-05\n",
      "  Step 1100/4407 - Loss: 1.9549 | PPL: 7.06 | LR: 5.258437e-05\n",
      "  Step 1200/4407 - Loss: 1.9535 | PPL: 7.05 | LR: 5.249406e-05\n",
      "  Step 1200/4407 - Loss: 1.9535 | PPL: 7.05 | LR: 5.249406e-05\n",
      "  Step 1300/4407 - Loss: 1.9568 | PPL: 7.08 | LR: 5.240381e-05\n",
      "  Step 1300/4407 - Loss: 1.9568 | PPL: 7.08 | LR: 5.240381e-05\n",
      "  Step 1400/4407 - Loss: 1.9570 | PPL: 7.08 | LR: 5.231361e-05\n",
      "  Step 1400/4407 - Loss: 1.9570 | PPL: 7.08 | LR: 5.231361e-05\n",
      "  Step 1500/4407 - Loss: 1.9587 | PPL: 7.09 | LR: 5.222348e-05\n",
      "  Step 1500/4407 - Loss: 1.9587 | PPL: 7.09 | LR: 5.222348e-05\n",
      "  Step 1600/4407 - Loss: 1.9601 | PPL: 7.10 | LR: 5.213341e-05\n",
      "  Step 1600/4407 - Loss: 1.9601 | PPL: 7.10 | LR: 5.213341e-05\n",
      "  Step 1700/4407 - Loss: 1.9614 | PPL: 7.11 | LR: 5.204340e-05\n",
      "  Step 1700/4407 - Loss: 1.9614 | PPL: 7.11 | LR: 5.204340e-05\n",
      "  Step 1800/4407 - Loss: 1.9599 | PPL: 7.10 | LR: 5.195346e-05\n",
      "  Step 1800/4407 - Loss: 1.9599 | PPL: 7.10 | LR: 5.195346e-05\n",
      "  Step 1900/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 5.186357e-05\n",
      "  Step 1900/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 5.186357e-05\n",
      "  Step 2000/4407 - Loss: 1.9601 | PPL: 7.10 | LR: 5.177375e-05\n",
      "  Step 2000/4407 - Loss: 1.9601 | PPL: 7.10 | LR: 5.177375e-05\n",
      "  Step 2100/4407 - Loss: 1.9612 | PPL: 7.11 | LR: 5.168398e-05\n",
      "  Step 2100/4407 - Loss: 1.9612 | PPL: 7.11 | LR: 5.168398e-05\n",
      "  Step 2200/4407 - Loss: 1.9601 | PPL: 7.10 | LR: 5.159428e-05\n",
      "  Step 2200/4407 - Loss: 1.9601 | PPL: 7.10 | LR: 5.159428e-05\n",
      "  Step 2300/4407 - Loss: 1.9593 | PPL: 7.09 | LR: 5.150465e-05\n",
      "  Step 2300/4407 - Loss: 1.9593 | PPL: 7.09 | LR: 5.150465e-05\n",
      "  Step 2400/4407 - Loss: 1.9570 | PPL: 7.08 | LR: 5.141507e-05\n",
      "  Step 2400/4407 - Loss: 1.9570 | PPL: 7.08 | LR: 5.141507e-05\n",
      "  Step 2500/4407 - Loss: 1.9569 | PPL: 7.08 | LR: 5.132555e-05\n",
      "  Step 2500/4407 - Loss: 1.9569 | PPL: 7.08 | LR: 5.132555e-05\n",
      "  Step 2600/4407 - Loss: 1.9588 | PPL: 7.09 | LR: 5.123610e-05\n",
      "  Step 2600/4407 - Loss: 1.9588 | PPL: 7.09 | LR: 5.123610e-05\n",
      "  Step 2700/4407 - Loss: 1.9603 | PPL: 7.10 | LR: 5.114671e-05\n",
      "  Step 2700/4407 - Loss: 1.9603 | PPL: 7.10 | LR: 5.114671e-05\n",
      "  Step 2800/4407 - Loss: 1.9588 | PPL: 7.09 | LR: 5.105738e-05\n",
      "  Step 2800/4407 - Loss: 1.9588 | PPL: 7.09 | LR: 5.105738e-05\n",
      "  Step 2900/4407 - Loss: 1.9581 | PPL: 7.09 | LR: 5.096811e-05\n",
      "  Step 2900/4407 - Loss: 1.9581 | PPL: 7.09 | LR: 5.096811e-05\n",
      "  Step 3000/4407 - Loss: 1.9582 | PPL: 7.09 | LR: 5.087891e-05\n",
      "  Step 3000/4407 - Loss: 1.9582 | PPL: 7.09 | LR: 5.087891e-05\n",
      "  Step 3100/4407 - Loss: 1.9587 | PPL: 7.09 | LR: 5.078976e-05\n",
      "  Step 3100/4407 - Loss: 1.9587 | PPL: 7.09 | LR: 5.078976e-05\n",
      "  Step 3200/4407 - Loss: 1.9588 | PPL: 7.09 | LR: 5.070068e-05\n",
      "  Step 3200/4407 - Loss: 1.9588 | PPL: 7.09 | LR: 5.070068e-05\n",
      "  Step 3300/4407 - Loss: 1.9597 | PPL: 7.10 | LR: 5.061166e-05\n",
      "  Step 3300/4407 - Loss: 1.9597 | PPL: 7.10 | LR: 5.061166e-05\n",
      "  Step 3400/4407 - Loss: 1.9603 | PPL: 7.10 | LR: 5.052271e-05\n",
      "  Step 3400/4407 - Loss: 1.9603 | PPL: 7.10 | LR: 5.052271e-05\n",
      "  Step 3500/4407 - Loss: 1.9613 | PPL: 7.11 | LR: 5.043381e-05\n",
      "  Step 3500/4407 - Loss: 1.9613 | PPL: 7.11 | LR: 5.043381e-05\n",
      "  Step 3600/4407 - Loss: 1.9613 | PPL: 7.11 | LR: 5.034498e-05\n",
      "  Step 3600/4407 - Loss: 1.9613 | PPL: 7.11 | LR: 5.034498e-05\n",
      "  Step 3700/4407 - Loss: 1.9607 | PPL: 7.10 | LR: 5.025622e-05\n",
      "  Step 3700/4407 - Loss: 1.9607 | PPL: 7.10 | LR: 5.025622e-05\n",
      "  Step 3800/4407 - Loss: 1.9599 | PPL: 7.10 | LR: 5.016751e-05\n",
      "  Step 3800/4407 - Loss: 1.9599 | PPL: 7.10 | LR: 5.016751e-05\n",
      "  Step 3900/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 5.007887e-05\n",
      "  Step 3900/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 5.007887e-05\n",
      "  Step 4000/4407 - Loss: 1.9590 | PPL: 7.09 | LR: 4.999028e-05\n",
      "  Step 4000/4407 - Loss: 1.9590 | PPL: 7.09 | LR: 4.999028e-05\n",
      "  Step 4100/4407 - Loss: 1.9582 | PPL: 7.09 | LR: 4.990177e-05\n",
      "  Step 4100/4407 - Loss: 1.9582 | PPL: 7.09 | LR: 4.990177e-05\n",
      "  Step 4200/4407 - Loss: 1.9594 | PPL: 7.10 | LR: 4.981331e-05\n",
      "  Step 4200/4407 - Loss: 1.9594 | PPL: 7.10 | LR: 4.981331e-05\n",
      "  Step 4300/4407 - Loss: 1.9592 | PPL: 7.09 | LR: 4.972492e-05\n",
      "  Step 4300/4407 - Loss: 1.9592 | PPL: 7.09 | LR: 4.972492e-05\n",
      "  Step 4400/4407 - Loss: 1.9596 | PPL: 7.10 | LR: 4.963659e-05\n",
      "  Step 4400/4407 - Loss: 1.9596 | PPL: 7.10 | LR: 4.963659e-05\n",
      "Epoch 76/100 | Train Loss: 1.9598 | Train PPL: 7.10 | Val Loss: 2.5915 | Val PPL: 13.35\n",
      "Epoch 76/100 | Train Loss: 1.9598 | Train PPL: 7.10 | Val Loss: 2.5915 | Val PPL: 13.35\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.10\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.10\n",
      "  Step 100/4407 - Loss: 1.9185 | PPL: 6.81 | LR: 4.954215e-05\n",
      "  Step 100/4407 - Loss: 1.9185 | PPL: 6.81 | LR: 4.954215e-05\n",
      "  Step 200/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 4.945395e-05\n",
      "  Step 200/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 4.945395e-05\n",
      "  Step 300/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 4.936581e-05\n",
      "  Step 300/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 4.936581e-05\n",
      "  Step 400/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 4.927774e-05\n",
      "  Step 400/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 4.927774e-05\n",
      "  Step 500/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 4.918973e-05\n",
      "  Step 500/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 4.918973e-05\n",
      "  Step 600/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 4.910178e-05\n",
      "  Step 600/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 4.910178e-05\n",
      "  Step 700/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 4.901390e-05\n",
      "  Step 700/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 4.901390e-05\n",
      "  Step 800/4407 - Loss: 1.9389 | PPL: 6.95 | LR: 4.892608e-05\n",
      "  Step 800/4407 - Loss: 1.9389 | PPL: 6.95 | LR: 4.892608e-05\n",
      "  Step 900/4407 - Loss: 1.9401 | PPL: 6.96 | LR: 4.883832e-05\n",
      "  Step 900/4407 - Loss: 1.9401 | PPL: 6.96 | LR: 4.883832e-05\n",
      "  Step 1000/4407 - Loss: 1.9376 | PPL: 6.94 | LR: 4.875063e-05\n",
      "  Step 1000/4407 - Loss: 1.9376 | PPL: 6.94 | LR: 4.875063e-05\n",
      "  Step 1100/4407 - Loss: 1.9389 | PPL: 6.95 | LR: 4.866300e-05\n",
      "  Step 1100/4407 - Loss: 1.9389 | PPL: 6.95 | LR: 4.866300e-05\n",
      "  Step 1200/4407 - Loss: 1.9380 | PPL: 6.94 | LR: 4.857544e-05\n",
      "  Step 1200/4407 - Loss: 1.9380 | PPL: 6.94 | LR: 4.857544e-05\n",
      "  Step 1300/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 4.848793e-05\n",
      "  Step 1300/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 4.848793e-05\n",
      "  Step 1400/4407 - Loss: 1.9383 | PPL: 6.95 | LR: 4.840050e-05\n",
      "  Step 1400/4407 - Loss: 1.9383 | PPL: 6.95 | LR: 4.840050e-05\n",
      "  Step 1500/4407 - Loss: 1.9383 | PPL: 6.95 | LR: 4.831312e-05\n",
      "  Step 1500/4407 - Loss: 1.9383 | PPL: 6.95 | LR: 4.831312e-05\n",
      "  Step 1600/4407 - Loss: 1.9401 | PPL: 6.96 | LR: 4.822581e-05\n",
      "  Step 1600/4407 - Loss: 1.9401 | PPL: 6.96 | LR: 4.822581e-05\n",
      "  Step 1700/4407 - Loss: 1.9402 | PPL: 6.96 | LR: 4.813856e-05\n",
      "  Step 1700/4407 - Loss: 1.9402 | PPL: 6.96 | LR: 4.813856e-05\n",
      "  Step 1800/4407 - Loss: 1.9407 | PPL: 6.96 | LR: 4.805138e-05\n",
      "  Step 1800/4407 - Loss: 1.9407 | PPL: 6.96 | LR: 4.805138e-05\n",
      "  Step 1900/4407 - Loss: 1.9427 | PPL: 6.98 | LR: 4.796426e-05\n",
      "  Step 1900/4407 - Loss: 1.9427 | PPL: 6.98 | LR: 4.796426e-05\n",
      "  Step 2000/4407 - Loss: 1.9439 | PPL: 6.99 | LR: 4.787721e-05\n",
      "  Step 2000/4407 - Loss: 1.9439 | PPL: 6.99 | LR: 4.787721e-05\n",
      "  Step 2100/4407 - Loss: 1.9464 | PPL: 7.00 | LR: 4.779021e-05\n",
      "  Step 2100/4407 - Loss: 1.9464 | PPL: 7.00 | LR: 4.779021e-05\n",
      "  Step 2200/4407 - Loss: 1.9465 | PPL: 7.00 | LR: 4.770329e-05\n",
      "  Step 2200/4407 - Loss: 1.9465 | PPL: 7.00 | LR: 4.770329e-05\n",
      "  Step 2300/4407 - Loss: 1.9467 | PPL: 7.01 | LR: 4.761642e-05\n",
      "  Step 2300/4407 - Loss: 1.9467 | PPL: 7.01 | LR: 4.761642e-05\n",
      "  Step 2400/4407 - Loss: 1.9476 | PPL: 7.01 | LR: 4.752963e-05\n",
      "  Step 2400/4407 - Loss: 1.9476 | PPL: 7.01 | LR: 4.752963e-05\n",
      "  Step 2500/4407 - Loss: 1.9462 | PPL: 7.00 | LR: 4.744289e-05\n",
      "  Step 2500/4407 - Loss: 1.9462 | PPL: 7.00 | LR: 4.744289e-05\n",
      "  Step 2600/4407 - Loss: 1.9464 | PPL: 7.00 | LR: 4.735622e-05\n",
      "  Step 2600/4407 - Loss: 1.9464 | PPL: 7.00 | LR: 4.735622e-05\n",
      "  Step 2700/4407 - Loss: 1.9474 | PPL: 7.01 | LR: 4.726962e-05\n",
      "  Step 2700/4407 - Loss: 1.9474 | PPL: 7.01 | LR: 4.726962e-05\n",
      "  Step 2800/4407 - Loss: 1.9476 | PPL: 7.01 | LR: 4.718308e-05\n",
      "  Step 2800/4407 - Loss: 1.9476 | PPL: 7.01 | LR: 4.718308e-05\n",
      "  Step 2900/4407 - Loss: 1.9483 | PPL: 7.02 | LR: 4.709660e-05\n",
      "  Step 2900/4407 - Loss: 1.9483 | PPL: 7.02 | LR: 4.709660e-05\n",
      "  Step 3000/4407 - Loss: 1.9476 | PPL: 7.01 | LR: 4.701019e-05\n",
      "  Step 3000/4407 - Loss: 1.9476 | PPL: 7.01 | LR: 4.701019e-05\n",
      "  Step 3100/4407 - Loss: 1.9481 | PPL: 7.02 | LR: 4.692384e-05\n",
      "  Step 3100/4407 - Loss: 1.9481 | PPL: 7.02 | LR: 4.692384e-05\n",
      "  Step 3200/4407 - Loss: 1.9495 | PPL: 7.03 | LR: 4.683756e-05\n",
      "  Step 3200/4407 - Loss: 1.9495 | PPL: 7.03 | LR: 4.683756e-05\n",
      "  Step 3300/4407 - Loss: 1.9494 | PPL: 7.02 | LR: 4.675134e-05\n",
      "  Step 3300/4407 - Loss: 1.9494 | PPL: 7.02 | LR: 4.675134e-05\n",
      "  Step 3400/4407 - Loss: 1.9507 | PPL: 7.03 | LR: 4.666519e-05\n",
      "  Step 3400/4407 - Loss: 1.9507 | PPL: 7.03 | LR: 4.666519e-05\n",
      "  Step 3500/4407 - Loss: 1.9510 | PPL: 7.04 | LR: 4.657910e-05\n",
      "  Step 3500/4407 - Loss: 1.9510 | PPL: 7.04 | LR: 4.657910e-05\n",
      "  Step 3600/4407 - Loss: 1.9522 | PPL: 7.04 | LR: 4.649307e-05\n",
      "  Step 3600/4407 - Loss: 1.9522 | PPL: 7.04 | LR: 4.649307e-05\n",
      "  Step 3700/4407 - Loss: 1.9531 | PPL: 7.05 | LR: 4.640711e-05\n",
      "  Step 3700/4407 - Loss: 1.9531 | PPL: 7.05 | LR: 4.640711e-05\n",
      "  Step 3800/4407 - Loss: 1.9530 | PPL: 7.05 | LR: 4.632122e-05\n",
      "  Step 3800/4407 - Loss: 1.9530 | PPL: 7.05 | LR: 4.632122e-05\n",
      "  Step 3900/4407 - Loss: 1.9522 | PPL: 7.04 | LR: 4.623539e-05\n",
      "  Step 3900/4407 - Loss: 1.9522 | PPL: 7.04 | LR: 4.623539e-05\n",
      "  Step 4000/4407 - Loss: 1.9529 | PPL: 7.05 | LR: 4.614963e-05\n",
      "  Step 4000/4407 - Loss: 1.9529 | PPL: 7.05 | LR: 4.614963e-05\n",
      "  Step 4100/4407 - Loss: 1.9544 | PPL: 7.06 | LR: 4.606393e-05\n",
      "  Step 4100/4407 - Loss: 1.9544 | PPL: 7.06 | LR: 4.606393e-05\n",
      "  Step 4200/4407 - Loss: 1.9557 | PPL: 7.07 | LR: 4.597830e-05\n",
      "  Step 4200/4407 - Loss: 1.9557 | PPL: 7.07 | LR: 4.597830e-05\n",
      "  Step 4300/4407 - Loss: 1.9563 | PPL: 7.07 | LR: 4.589273e-05\n",
      "  Step 4300/4407 - Loss: 1.9563 | PPL: 7.07 | LR: 4.589273e-05\n",
      "  Step 4400/4407 - Loss: 1.9575 | PPL: 7.08 | LR: 4.580723e-05\n",
      "  Step 4400/4407 - Loss: 1.9575 | PPL: 7.08 | LR: 4.580723e-05\n",
      "Epoch 77/100 | Train Loss: 1.9575 | Train PPL: 7.08 | Val Loss: 2.5923 | Val PPL: 13.36\n",
      "Epoch 77/100 | Train Loss: 1.9575 | Train PPL: 7.08 | Val Loss: 2.5923 | Val PPL: 13.36\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.08\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.08\n",
      "  Step 100/4407 - Loss: 1.9136 | PPL: 6.78 | LR: 4.571581e-05\n",
      "  Step 100/4407 - Loss: 1.9136 | PPL: 6.78 | LR: 4.571581e-05\n",
      "  Step 200/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 4.563044e-05\n",
      "  Step 200/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 4.563044e-05\n",
      "  Step 300/4407 - Loss: 1.9325 | PPL: 6.91 | LR: 4.554514e-05\n",
      "  Step 300/4407 - Loss: 1.9325 | PPL: 6.91 | LR: 4.554514e-05\n",
      "  Step 400/4407 - Loss: 1.9211 | PPL: 6.83 | LR: 4.545991e-05\n",
      "  Step 400/4407 - Loss: 1.9211 | PPL: 6.83 | LR: 4.545991e-05\n",
      "  Step 500/4407 - Loss: 1.9298 | PPL: 6.89 | LR: 4.537474e-05\n",
      "  Step 500/4407 - Loss: 1.9298 | PPL: 6.89 | LR: 4.537474e-05\n",
      "  Step 600/4407 - Loss: 1.9301 | PPL: 6.89 | LR: 4.528963e-05\n",
      "  Step 600/4407 - Loss: 1.9301 | PPL: 6.89 | LR: 4.528963e-05\n",
      "  Step 700/4407 - Loss: 1.9385 | PPL: 6.95 | LR: 4.520459e-05\n",
      "  Step 700/4407 - Loss: 1.9385 | PPL: 6.95 | LR: 4.520459e-05\n",
      "  Step 800/4407 - Loss: 1.9410 | PPL: 6.97 | LR: 4.511962e-05\n",
      "  Step 800/4407 - Loss: 1.9410 | PPL: 6.97 | LR: 4.511962e-05\n",
      "  Step 900/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 4.503471e-05\n",
      "  Step 900/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 4.503471e-05\n",
      "  Step 1000/4407 - Loss: 1.9417 | PPL: 6.97 | LR: 4.494987e-05\n",
      "  Step 1000/4407 - Loss: 1.9417 | PPL: 6.97 | LR: 4.494987e-05\n",
      "  Step 1100/4407 - Loss: 1.9458 | PPL: 7.00 | LR: 4.486509e-05\n",
      "  Step 1100/4407 - Loss: 1.9458 | PPL: 7.00 | LR: 4.486509e-05\n",
      "  Step 1200/4407 - Loss: 1.9435 | PPL: 6.98 | LR: 4.478038e-05\n",
      "  Step 1200/4407 - Loss: 1.9435 | PPL: 6.98 | LR: 4.478038e-05\n",
      "  Step 1300/4407 - Loss: 1.9437 | PPL: 6.98 | LR: 4.469574e-05\n",
      "  Step 1300/4407 - Loss: 1.9437 | PPL: 6.98 | LR: 4.469574e-05\n",
      "  Step 1400/4407 - Loss: 1.9463 | PPL: 7.00 | LR: 4.461116e-05\n",
      "  Step 1400/4407 - Loss: 1.9463 | PPL: 7.00 | LR: 4.461116e-05\n",
      "  Step 1500/4407 - Loss: 1.9469 | PPL: 7.01 | LR: 4.452665e-05\n",
      "  Step 1500/4407 - Loss: 1.9469 | PPL: 7.01 | LR: 4.452665e-05\n",
      "  Step 1600/4407 - Loss: 1.9473 | PPL: 7.01 | LR: 4.444220e-05\n",
      "  Step 1600/4407 - Loss: 1.9473 | PPL: 7.01 | LR: 4.444220e-05\n",
      "  Step 1700/4407 - Loss: 1.9454 | PPL: 7.00 | LR: 4.435782e-05\n",
      "  Step 1700/4407 - Loss: 1.9454 | PPL: 7.00 | LR: 4.435782e-05\n",
      "  Step 1800/4407 - Loss: 1.9486 | PPL: 7.02 | LR: 4.427351e-05\n",
      "  Step 1800/4407 - Loss: 1.9486 | PPL: 7.02 | LR: 4.427351e-05\n",
      "  Step 1900/4407 - Loss: 1.9491 | PPL: 7.02 | LR: 4.418927e-05\n",
      "  Step 1900/4407 - Loss: 1.9491 | PPL: 7.02 | LR: 4.418927e-05\n",
      "  Step 2000/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 4.410508e-05\n",
      "  Step 2000/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 4.410508e-05\n",
      "  Step 2100/4407 - Loss: 1.9489 | PPL: 7.02 | LR: 4.402097e-05\n",
      "  Step 2100/4407 - Loss: 1.9489 | PPL: 7.02 | LR: 4.402097e-05\n",
      "  Step 2200/4407 - Loss: 1.9473 | PPL: 7.01 | LR: 4.393692e-05\n",
      "  Step 2200/4407 - Loss: 1.9473 | PPL: 7.01 | LR: 4.393692e-05\n",
      "  Step 2300/4407 - Loss: 1.9463 | PPL: 7.00 | LR: 4.385294e-05\n",
      "  Step 2300/4407 - Loss: 1.9463 | PPL: 7.00 | LR: 4.385294e-05\n",
      "  Step 2400/4407 - Loss: 1.9456 | PPL: 7.00 | LR: 4.376903e-05\n",
      "  Step 2400/4407 - Loss: 1.9456 | PPL: 7.00 | LR: 4.376903e-05\n",
      "  Step 2500/4407 - Loss: 1.9460 | PPL: 7.00 | LR: 4.368518e-05\n",
      "  Step 2500/4407 - Loss: 1.9460 | PPL: 7.00 | LR: 4.368518e-05\n",
      "  Step 2600/4407 - Loss: 1.9470 | PPL: 7.01 | LR: 4.360140e-05\n",
      "  Step 2600/4407 - Loss: 1.9470 | PPL: 7.01 | LR: 4.360140e-05\n",
      "  Step 2700/4407 - Loss: 1.9447 | PPL: 6.99 | LR: 4.351769e-05\n",
      "  Step 2700/4407 - Loss: 1.9447 | PPL: 6.99 | LR: 4.351769e-05\n",
      "  Step 2800/4407 - Loss: 1.9447 | PPL: 6.99 | LR: 4.343404e-05\n",
      "  Step 2800/4407 - Loss: 1.9447 | PPL: 6.99 | LR: 4.343404e-05\n",
      "  Step 2900/4407 - Loss: 1.9471 | PPL: 7.01 | LR: 4.335046e-05\n",
      "  Step 2900/4407 - Loss: 1.9471 | PPL: 7.01 | LR: 4.335046e-05\n",
      "  Step 3000/4407 - Loss: 1.9465 | PPL: 7.00 | LR: 4.326694e-05\n",
      "  Step 3000/4407 - Loss: 1.9465 | PPL: 7.00 | LR: 4.326694e-05\n",
      "  Step 3100/4407 - Loss: 1.9469 | PPL: 7.01 | LR: 4.318350e-05\n",
      "  Step 3100/4407 - Loss: 1.9469 | PPL: 7.01 | LR: 4.318350e-05\n",
      "  Step 3200/4407 - Loss: 1.9474 | PPL: 7.01 | LR: 4.310012e-05\n",
      "  Step 3200/4407 - Loss: 1.9474 | PPL: 7.01 | LR: 4.310012e-05\n",
      "  Step 3300/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 4.301680e-05\n",
      "  Step 3300/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 4.301680e-05\n",
      "  Step 3400/4407 - Loss: 1.9499 | PPL: 7.03 | LR: 4.293356e-05\n",
      "  Step 3400/4407 - Loss: 1.9499 | PPL: 7.03 | LR: 4.293356e-05\n",
      "  Step 3500/4407 - Loss: 1.9496 | PPL: 7.03 | LR: 4.285038e-05\n",
      "  Step 3500/4407 - Loss: 1.9496 | PPL: 7.03 | LR: 4.285038e-05\n",
      "  Step 3600/4407 - Loss: 1.9494 | PPL: 7.02 | LR: 4.276727e-05\n",
      "  Step 3600/4407 - Loss: 1.9494 | PPL: 7.02 | LR: 4.276727e-05\n",
      "  Step 3700/4407 - Loss: 1.9494 | PPL: 7.02 | LR: 4.268423e-05\n",
      "  Step 3700/4407 - Loss: 1.9494 | PPL: 7.02 | LR: 4.268423e-05\n",
      "  Step 3800/4407 - Loss: 1.9489 | PPL: 7.02 | LR: 4.260125e-05\n",
      "  Step 3800/4407 - Loss: 1.9489 | PPL: 7.02 | LR: 4.260125e-05\n",
      "  Step 3900/4407 - Loss: 1.9504 | PPL: 7.03 | LR: 4.251834e-05\n",
      "  Step 3900/4407 - Loss: 1.9504 | PPL: 7.03 | LR: 4.251834e-05\n",
      "  Step 4000/4407 - Loss: 1.9518 | PPL: 7.04 | LR: 4.243550e-05\n",
      "  Step 4000/4407 - Loss: 1.9518 | PPL: 7.04 | LR: 4.243550e-05\n",
      "  Step 4100/4407 - Loss: 1.9528 | PPL: 7.05 | LR: 4.235272e-05\n",
      "  Step 4100/4407 - Loss: 1.9528 | PPL: 7.05 | LR: 4.235272e-05\n",
      "  Step 4200/4407 - Loss: 1.9537 | PPL: 7.05 | LR: 4.227002e-05\n",
      "  Step 4200/4407 - Loss: 1.9537 | PPL: 7.05 | LR: 4.227002e-05\n",
      "  Step 4300/4407 - Loss: 1.9543 | PPL: 7.06 | LR: 4.218738e-05\n",
      "  Step 4300/4407 - Loss: 1.9543 | PPL: 7.06 | LR: 4.218738e-05\n",
      "  Step 4400/4407 - Loss: 1.9548 | PPL: 7.06 | LR: 4.210481e-05\n",
      "  Step 4400/4407 - Loss: 1.9548 | PPL: 7.06 | LR: 4.210481e-05\n",
      "Epoch 78/100 | Train Loss: 1.9550 | Train PPL: 7.06 | Val Loss: 2.5930 | Val PPL: 13.37\n",
      "Epoch 78/100 | Train Loss: 1.9550 | Train PPL: 7.06 | Val Loss: 2.5930 | Val PPL: 13.37\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.06\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.06\n",
      "  Step 100/4407 - Loss: 1.9377 | PPL: 6.94 | LR: 4.201653e-05\n",
      "  Step 100/4407 - Loss: 1.9377 | PPL: 6.94 | LR: 4.201653e-05\n",
      "  Step 200/4407 - Loss: 1.9100 | PPL: 6.75 | LR: 4.193410e-05\n",
      "  Step 200/4407 - Loss: 1.9100 | PPL: 6.75 | LR: 4.193410e-05\n",
      "  Step 300/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 4.185174e-05\n",
      "  Step 300/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 4.185174e-05\n",
      "  Step 400/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 4.176944e-05\n",
      "  Step 400/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 4.176944e-05\n",
      "  Step 500/4407 - Loss: 1.9440 | PPL: 6.99 | LR: 4.168721e-05\n",
      "  Step 500/4407 - Loss: 1.9440 | PPL: 6.99 | LR: 4.168721e-05\n",
      "  Step 600/4407 - Loss: 1.9436 | PPL: 6.98 | LR: 4.160505e-05\n",
      "  Step 600/4407 - Loss: 1.9436 | PPL: 6.98 | LR: 4.160505e-05\n",
      "  Step 700/4407 - Loss: 1.9442 | PPL: 6.99 | LR: 4.152296e-05\n",
      "  Step 700/4407 - Loss: 1.9442 | PPL: 6.99 | LR: 4.152296e-05\n",
      "  Step 800/4407 - Loss: 1.9442 | PPL: 6.99 | LR: 4.144094e-05\n",
      "  Step 800/4407 - Loss: 1.9442 | PPL: 6.99 | LR: 4.144094e-05\n",
      "  Step 900/4407 - Loss: 1.9497 | PPL: 7.03 | LR: 4.135898e-05\n",
      "  Step 900/4407 - Loss: 1.9497 | PPL: 7.03 | LR: 4.135898e-05\n",
      "  Step 1000/4407 - Loss: 1.9503 | PPL: 7.03 | LR: 4.127709e-05\n",
      "  Step 1000/4407 - Loss: 1.9503 | PPL: 7.03 | LR: 4.127709e-05\n",
      "  Step 1100/4407 - Loss: 1.9469 | PPL: 7.01 | LR: 4.119528e-05\n",
      "  Step 1100/4407 - Loss: 1.9469 | PPL: 7.01 | LR: 4.119528e-05\n",
      "  Step 1200/4407 - Loss: 1.9477 | PPL: 7.01 | LR: 4.111352e-05\n",
      "  Step 1200/4407 - Loss: 1.9477 | PPL: 7.01 | LR: 4.111352e-05\n",
      "  Step 1300/4407 - Loss: 1.9505 | PPL: 7.03 | LR: 4.103184e-05\n",
      "  Step 1300/4407 - Loss: 1.9505 | PPL: 7.03 | LR: 4.103184e-05\n",
      "  Step 1400/4407 - Loss: 1.9525 | PPL: 7.05 | LR: 4.095023e-05\n",
      "  Step 1400/4407 - Loss: 1.9525 | PPL: 7.05 | LR: 4.095023e-05\n",
      "  Step 1500/4407 - Loss: 1.9524 | PPL: 7.05 | LR: 4.086868e-05\n",
      "  Step 1500/4407 - Loss: 1.9524 | PPL: 7.05 | LR: 4.086868e-05\n",
      "  Step 1600/4407 - Loss: 1.9517 | PPL: 7.04 | LR: 4.078720e-05\n",
      "  Step 1600/4407 - Loss: 1.9517 | PPL: 7.04 | LR: 4.078720e-05\n",
      "  Step 1700/4407 - Loss: 1.9528 | PPL: 7.05 | LR: 4.070580e-05\n",
      "  Step 1700/4407 - Loss: 1.9528 | PPL: 7.05 | LR: 4.070580e-05\n",
      "  Step 1800/4407 - Loss: 1.9503 | PPL: 7.03 | LR: 4.062446e-05\n",
      "  Step 1800/4407 - Loss: 1.9503 | PPL: 7.03 | LR: 4.062446e-05\n",
      "  Step 1900/4407 - Loss: 1.9515 | PPL: 7.04 | LR: 4.054318e-05\n",
      "  Step 1900/4407 - Loss: 1.9515 | PPL: 7.04 | LR: 4.054318e-05\n",
      "  Step 2000/4407 - Loss: 1.9534 | PPL: 7.05 | LR: 4.046198e-05\n",
      "  Step 2000/4407 - Loss: 1.9534 | PPL: 7.05 | LR: 4.046198e-05\n",
      "  Step 2100/4407 - Loss: 1.9544 | PPL: 7.06 | LR: 4.038085e-05\n",
      "  Step 2100/4407 - Loss: 1.9544 | PPL: 7.06 | LR: 4.038085e-05\n",
      "  Step 2200/4407 - Loss: 1.9530 | PPL: 7.05 | LR: 4.029978e-05\n",
      "  Step 2200/4407 - Loss: 1.9530 | PPL: 7.05 | LR: 4.029978e-05\n",
      "  Step 2300/4407 - Loss: 1.9518 | PPL: 7.04 | LR: 4.021878e-05\n",
      "  Step 2300/4407 - Loss: 1.9518 | PPL: 7.04 | LR: 4.021878e-05\n",
      "  Step 2400/4407 - Loss: 1.9515 | PPL: 7.04 | LR: 4.013786e-05\n",
      "  Step 2400/4407 - Loss: 1.9515 | PPL: 7.04 | LR: 4.013786e-05\n",
      "  Step 2500/4407 - Loss: 1.9502 | PPL: 7.03 | LR: 4.005700e-05\n",
      "  Step 2500/4407 - Loss: 1.9502 | PPL: 7.03 | LR: 4.005700e-05\n",
      "  Step 2600/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 3.997621e-05\n",
      "  Step 2600/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 3.997621e-05\n",
      "  Step 2700/4407 - Loss: 1.9489 | PPL: 7.02 | LR: 3.989549e-05\n",
      "  Step 2700/4407 - Loss: 1.9489 | PPL: 7.02 | LR: 3.989549e-05\n",
      "  Step 2800/4407 - Loss: 1.9499 | PPL: 7.03 | LR: 3.981483e-05\n",
      "  Step 2800/4407 - Loss: 1.9499 | PPL: 7.03 | LR: 3.981483e-05\n",
      "  Step 2900/4407 - Loss: 1.9494 | PPL: 7.02 | LR: 3.973425e-05\n",
      "  Step 2900/4407 - Loss: 1.9494 | PPL: 7.02 | LR: 3.973425e-05\n",
      "  Step 3000/4407 - Loss: 1.9495 | PPL: 7.02 | LR: 3.965374e-05\n",
      "  Step 3000/4407 - Loss: 1.9495 | PPL: 7.02 | LR: 3.965374e-05\n",
      "  Step 3100/4407 - Loss: 1.9492 | PPL: 7.02 | LR: 3.957329e-05\n",
      "  Step 3100/4407 - Loss: 1.9492 | PPL: 7.02 | LR: 3.957329e-05\n",
      "  Step 3200/4407 - Loss: 1.9497 | PPL: 7.03 | LR: 3.949292e-05\n",
      "  Step 3200/4407 - Loss: 1.9497 | PPL: 7.03 | LR: 3.949292e-05\n",
      "  Step 3300/4407 - Loss: 1.9506 | PPL: 7.03 | LR: 3.941261e-05\n",
      "  Step 3300/4407 - Loss: 1.9506 | PPL: 7.03 | LR: 3.941261e-05\n",
      "  Step 3400/4407 - Loss: 1.9504 | PPL: 7.03 | LR: 3.933238e-05\n",
      "  Step 3400/4407 - Loss: 1.9504 | PPL: 7.03 | LR: 3.933238e-05\n",
      "  Step 3500/4407 - Loss: 1.9507 | PPL: 7.03 | LR: 3.925221e-05\n",
      "  Step 3500/4407 - Loss: 1.9507 | PPL: 7.03 | LR: 3.925221e-05\n",
      "  Step 3600/4407 - Loss: 1.9517 | PPL: 7.04 | LR: 3.917211e-05\n",
      "  Step 3600/4407 - Loss: 1.9517 | PPL: 7.04 | LR: 3.917211e-05\n",
      "  Step 3700/4407 - Loss: 1.9522 | PPL: 7.04 | LR: 3.909209e-05\n",
      "  Step 3700/4407 - Loss: 1.9522 | PPL: 7.04 | LR: 3.909209e-05\n",
      "  Step 3800/4407 - Loss: 1.9529 | PPL: 7.05 | LR: 3.901213e-05\n",
      "  Step 3800/4407 - Loss: 1.9529 | PPL: 7.05 | LR: 3.901213e-05\n",
      "  Step 3900/4407 - Loss: 1.9534 | PPL: 7.05 | LR: 3.893224e-05\n",
      "  Step 3900/4407 - Loss: 1.9534 | PPL: 7.05 | LR: 3.893224e-05\n",
      "  Step 4000/4407 - Loss: 1.9526 | PPL: 7.05 | LR: 3.885242e-05\n",
      "  Step 4000/4407 - Loss: 1.9526 | PPL: 7.05 | LR: 3.885242e-05\n",
      "  Step 4100/4407 - Loss: 1.9526 | PPL: 7.05 | LR: 3.877267e-05\n",
      "  Step 4100/4407 - Loss: 1.9526 | PPL: 7.05 | LR: 3.877267e-05\n",
      "  Step 4200/4407 - Loss: 1.9523 | PPL: 7.04 | LR: 3.869299e-05\n",
      "  Step 4200/4407 - Loss: 1.9523 | PPL: 7.04 | LR: 3.869299e-05\n",
      "  Step 4300/4407 - Loss: 1.9526 | PPL: 7.05 | LR: 3.861338e-05\n",
      "  Step 4300/4407 - Loss: 1.9526 | PPL: 7.05 | LR: 3.861338e-05\n",
      "  Step 4400/4407 - Loss: 1.9529 | PPL: 7.05 | LR: 3.853384e-05\n",
      "  Step 4400/4407 - Loss: 1.9529 | PPL: 7.05 | LR: 3.853384e-05\n",
      "Epoch 79/100 | Train Loss: 1.9527 | Train PPL: 7.05 | Val Loss: 2.5933 | Val PPL: 13.37\n",
      "Epoch 79/100 | Train Loss: 1.9527 | Train PPL: 7.05 | Val Loss: 2.5933 | Val PPL: 13.37\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.05\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.05\n",
      "  Step 100/4407 - Loss: 1.9574 | PPL: 7.08 | LR: 3.844881e-05\n",
      "  Step 100/4407 - Loss: 1.9574 | PPL: 7.08 | LR: 3.844881e-05\n",
      "  Step 200/4407 - Loss: 1.9510 | PPL: 7.04 | LR: 3.836942e-05\n",
      "  Step 200/4407 - Loss: 1.9510 | PPL: 7.04 | LR: 3.836942e-05\n",
      "  Step 300/4407 - Loss: 1.9461 | PPL: 7.00 | LR: 3.829009e-05\n",
      "  Step 300/4407 - Loss: 1.9461 | PPL: 7.00 | LR: 3.829009e-05\n",
      "  Step 400/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 3.821084e-05\n",
      "  Step 400/4407 - Loss: 1.9606 | PPL: 7.10 | LR: 3.821084e-05\n",
      "  Step 500/4407 - Loss: 1.9579 | PPL: 7.08 | LR: 3.813165e-05\n",
      "  Step 500/4407 - Loss: 1.9579 | PPL: 7.08 | LR: 3.813165e-05\n",
      "  Step 600/4407 - Loss: 1.9555 | PPL: 7.07 | LR: 3.805254e-05\n",
      "  Step 600/4407 - Loss: 1.9555 | PPL: 7.07 | LR: 3.805254e-05\n",
      "  Step 700/4407 - Loss: 1.9460 | PPL: 7.00 | LR: 3.797349e-05\n",
      "  Step 700/4407 - Loss: 1.9460 | PPL: 7.00 | LR: 3.797349e-05\n",
      "  Step 800/4407 - Loss: 1.9473 | PPL: 7.01 | LR: 3.789452e-05\n",
      "  Step 800/4407 - Loss: 1.9473 | PPL: 7.01 | LR: 3.789452e-05\n",
      "  Step 900/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 3.781562e-05\n",
      "  Step 900/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 3.781562e-05\n",
      "  Step 1000/4407 - Loss: 1.9479 | PPL: 7.01 | LR: 3.773678e-05\n",
      "  Step 1000/4407 - Loss: 1.9479 | PPL: 7.01 | LR: 3.773678e-05\n",
      "  Step 1100/4407 - Loss: 1.9485 | PPL: 7.02 | LR: 3.765802e-05\n",
      "  Step 1100/4407 - Loss: 1.9485 | PPL: 7.02 | LR: 3.765802e-05\n",
      "  Step 1200/4407 - Loss: 1.9485 | PPL: 7.02 | LR: 3.757933e-05\n",
      "  Step 1200/4407 - Loss: 1.9485 | PPL: 7.02 | LR: 3.757933e-05\n",
      "  Step 1300/4407 - Loss: 1.9505 | PPL: 7.03 | LR: 3.750071e-05\n",
      "  Step 1300/4407 - Loss: 1.9505 | PPL: 7.03 | LR: 3.750071e-05\n",
      "  Step 1400/4407 - Loss: 1.9483 | PPL: 7.02 | LR: 3.742215e-05\n",
      "  Step 1400/4407 - Loss: 1.9483 | PPL: 7.02 | LR: 3.742215e-05\n",
      "  Step 1500/4407 - Loss: 1.9514 | PPL: 7.04 | LR: 3.734367e-05\n",
      "  Step 1500/4407 - Loss: 1.9514 | PPL: 7.04 | LR: 3.734367e-05\n",
      "  Step 1600/4407 - Loss: 1.9507 | PPL: 7.03 | LR: 3.726526e-05\n",
      "  Step 1600/4407 - Loss: 1.9507 | PPL: 7.03 | LR: 3.726526e-05\n",
      "  Step 1700/4407 - Loss: 1.9524 | PPL: 7.05 | LR: 3.718692e-05\n",
      "  Step 1700/4407 - Loss: 1.9524 | PPL: 7.05 | LR: 3.718692e-05\n",
      "  Step 1800/4407 - Loss: 1.9518 | PPL: 7.04 | LR: 3.710866e-05\n",
      "  Step 1800/4407 - Loss: 1.9518 | PPL: 7.04 | LR: 3.710866e-05\n",
      "  Step 1900/4407 - Loss: 1.9539 | PPL: 7.06 | LR: 3.703046e-05\n",
      "  Step 1900/4407 - Loss: 1.9539 | PPL: 7.06 | LR: 3.703046e-05\n",
      "  Step 2000/4407 - Loss: 1.9537 | PPL: 7.05 | LR: 3.695233e-05\n",
      "  Step 2000/4407 - Loss: 1.9537 | PPL: 7.05 | LR: 3.695233e-05\n",
      "  Step 2100/4407 - Loss: 1.9529 | PPL: 7.05 | LR: 3.687428e-05\n",
      "  Step 2100/4407 - Loss: 1.9529 | PPL: 7.05 | LR: 3.687428e-05\n",
      "  Step 2200/4407 - Loss: 1.9514 | PPL: 7.04 | LR: 3.679629e-05\n",
      "  Step 2200/4407 - Loss: 1.9514 | PPL: 7.04 | LR: 3.679629e-05\n",
      "  Step 2300/4407 - Loss: 1.9518 | PPL: 7.04 | LR: 3.671838e-05\n",
      "  Step 2300/4407 - Loss: 1.9518 | PPL: 7.04 | LR: 3.671838e-05\n",
      "  Step 2400/4407 - Loss: 1.9519 | PPL: 7.04 | LR: 3.664053e-05\n",
      "  Step 2400/4407 - Loss: 1.9519 | PPL: 7.04 | LR: 3.664053e-05\n",
      "  Step 2500/4407 - Loss: 1.9522 | PPL: 7.04 | LR: 3.656276e-05\n",
      "  Step 2500/4407 - Loss: 1.9522 | PPL: 7.04 | LR: 3.656276e-05\n",
      "  Step 2600/4407 - Loss: 1.9504 | PPL: 7.03 | LR: 3.648506e-05\n",
      "  Step 2600/4407 - Loss: 1.9504 | PPL: 7.03 | LR: 3.648506e-05\n",
      "  Step 2700/4407 - Loss: 1.9508 | PPL: 7.03 | LR: 3.640743e-05\n",
      "  Step 2700/4407 - Loss: 1.9508 | PPL: 7.03 | LR: 3.640743e-05\n",
      "  Step 2800/4407 - Loss: 1.9497 | PPL: 7.03 | LR: 3.632988e-05\n",
      "  Step 2800/4407 - Loss: 1.9497 | PPL: 7.03 | LR: 3.632988e-05\n",
      "  Step 2900/4407 - Loss: 1.9493 | PPL: 7.02 | LR: 3.625239e-05\n",
      "  Step 2900/4407 - Loss: 1.9493 | PPL: 7.02 | LR: 3.625239e-05\n",
      "  Step 3000/4407 - Loss: 1.9487 | PPL: 7.02 | LR: 3.617497e-05\n",
      "  Step 3000/4407 - Loss: 1.9487 | PPL: 7.02 | LR: 3.617497e-05\n",
      "  Step 3100/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 3.609763e-05\n",
      "  Step 3100/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 3.609763e-05\n",
      "  Step 3200/4407 - Loss: 1.9503 | PPL: 7.03 | LR: 3.602036e-05\n",
      "  Step 3200/4407 - Loss: 1.9503 | PPL: 7.03 | LR: 3.602036e-05\n",
      "  Step 3300/4407 - Loss: 1.9502 | PPL: 7.03 | LR: 3.594316e-05\n",
      "  Step 3300/4407 - Loss: 1.9502 | PPL: 7.03 | LR: 3.594316e-05\n",
      "  Step 3400/4407 - Loss: 1.9498 | PPL: 7.03 | LR: 3.586603e-05\n",
      "  Step 3400/4407 - Loss: 1.9498 | PPL: 7.03 | LR: 3.586603e-05\n",
      "  Step 3500/4407 - Loss: 1.9513 | PPL: 7.04 | LR: 3.578897e-05\n",
      "  Step 3500/4407 - Loss: 1.9513 | PPL: 7.04 | LR: 3.578897e-05\n",
      "  Step 3600/4407 - Loss: 1.9507 | PPL: 7.03 | LR: 3.571198e-05\n",
      "  Step 3600/4407 - Loss: 1.9507 | PPL: 7.03 | LR: 3.571198e-05\n",
      "  Step 3700/4407 - Loss: 1.9511 | PPL: 7.04 | LR: 3.563507e-05\n",
      "  Step 3700/4407 - Loss: 1.9511 | PPL: 7.04 | LR: 3.563507e-05\n",
      "  Step 3800/4407 - Loss: 1.9498 | PPL: 7.03 | LR: 3.555823e-05\n",
      "  Step 3800/4407 - Loss: 1.9498 | PPL: 7.03 | LR: 3.555823e-05\n",
      "  Step 3900/4407 - Loss: 1.9497 | PPL: 7.03 | LR: 3.548146e-05\n",
      "  Step 3900/4407 - Loss: 1.9497 | PPL: 7.03 | LR: 3.548146e-05\n",
      "  Step 4000/4407 - Loss: 1.9486 | PPL: 7.02 | LR: 3.540476e-05\n",
      "  Step 4000/4407 - Loss: 1.9486 | PPL: 7.02 | LR: 3.540476e-05\n",
      "  Step 4100/4407 - Loss: 1.9486 | PPL: 7.02 | LR: 3.532813e-05\n",
      "  Step 4100/4407 - Loss: 1.9486 | PPL: 7.02 | LR: 3.532813e-05\n",
      "  Step 4200/4407 - Loss: 1.9491 | PPL: 7.02 | LR: 3.525157e-05\n",
      "  Step 4200/4407 - Loss: 1.9491 | PPL: 7.02 | LR: 3.525157e-05\n",
      "  Step 4300/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 3.517509e-05\n",
      "  Step 4300/4407 - Loss: 1.9490 | PPL: 7.02 | LR: 3.517509e-05\n",
      "  Step 4400/4407 - Loss: 1.9499 | PPL: 7.03 | LR: 3.509868e-05\n",
      "  Step 4400/4407 - Loss: 1.9499 | PPL: 7.03 | LR: 3.509868e-05\n",
      "Epoch 80/100 | Train Loss: 1.9501 | Train PPL: 7.03 | Val Loss: 2.5905 | Val PPL: 13.34\n",
      "Epoch 80/100 | Train Loss: 1.9501 | Train PPL: 7.03 | Val Loss: 2.5905 | Val PPL: 13.34\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.03\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.03\n",
      "  Step 100/4407 - Loss: 1.9413 | PPL: 6.97 | LR: 3.501700e-05\n",
      "  Step 100/4407 - Loss: 1.9413 | PPL: 6.97 | LR: 3.501700e-05\n",
      "  Step 200/4407 - Loss: 1.9467 | PPL: 7.01 | LR: 3.494074e-05\n",
      "  Step 200/4407 - Loss: 1.9467 | PPL: 7.01 | LR: 3.494074e-05\n",
      "  Step 300/4407 - Loss: 1.9447 | PPL: 6.99 | LR: 3.486455e-05\n",
      "  Step 300/4407 - Loss: 1.9447 | PPL: 6.99 | LR: 3.486455e-05\n",
      "  Step 400/4407 - Loss: 1.9417 | PPL: 6.97 | LR: 3.478843e-05\n",
      "  Step 400/4407 - Loss: 1.9417 | PPL: 6.97 | LR: 3.478843e-05\n",
      "  Step 500/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 3.471239e-05\n",
      "  Step 500/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 3.471239e-05\n",
      "  Step 600/4407 - Loss: 1.9349 | PPL: 6.92 | LR: 3.463641e-05\n",
      "  Step 600/4407 - Loss: 1.9349 | PPL: 6.92 | LR: 3.463641e-05\n",
      "  Step 700/4407 - Loss: 1.9340 | PPL: 6.92 | LR: 3.456051e-05\n",
      "  Step 700/4407 - Loss: 1.9340 | PPL: 6.92 | LR: 3.456051e-05\n",
      "  Step 800/4407 - Loss: 1.9361 | PPL: 6.93 | LR: 3.448468e-05\n",
      "  Step 800/4407 - Loss: 1.9361 | PPL: 6.93 | LR: 3.448468e-05\n",
      "  Step 900/4407 - Loss: 1.9384 | PPL: 6.95 | LR: 3.440893e-05\n",
      "  Step 900/4407 - Loss: 1.9384 | PPL: 6.95 | LR: 3.440893e-05\n",
      "  Step 1000/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 3.433325e-05\n",
      "  Step 1000/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 3.433325e-05\n",
      "  Step 1100/4407 - Loss: 1.9392 | PPL: 6.95 | LR: 3.425764e-05\n",
      "  Step 1100/4407 - Loss: 1.9392 | PPL: 6.95 | LR: 3.425764e-05\n",
      "  Step 1200/4407 - Loss: 1.9402 | PPL: 6.96 | LR: 3.418210e-05\n",
      "  Step 1200/4407 - Loss: 1.9402 | PPL: 6.96 | LR: 3.418210e-05\n",
      "  Step 1300/4407 - Loss: 1.9389 | PPL: 6.95 | LR: 3.410663e-05\n",
      "  Step 1300/4407 - Loss: 1.9389 | PPL: 6.95 | LR: 3.410663e-05\n",
      "  Step 1400/4407 - Loss: 1.9406 | PPL: 6.96 | LR: 3.403124e-05\n",
      "  Step 1400/4407 - Loss: 1.9406 | PPL: 6.96 | LR: 3.403124e-05\n",
      "  Step 1500/4407 - Loss: 1.9398 | PPL: 6.96 | LR: 3.395592e-05\n",
      "  Step 1500/4407 - Loss: 1.9398 | PPL: 6.96 | LR: 3.395592e-05\n",
      "  Step 1600/4407 - Loss: 1.9392 | PPL: 6.95 | LR: 3.388067e-05\n",
      "  Step 1600/4407 - Loss: 1.9392 | PPL: 6.95 | LR: 3.388067e-05\n",
      "  Step 1700/4407 - Loss: 1.9411 | PPL: 6.97 | LR: 3.380550e-05\n",
      "  Step 1700/4407 - Loss: 1.9411 | PPL: 6.97 | LR: 3.380550e-05\n",
      "  Step 1800/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 3.373040e-05\n",
      "  Step 1800/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 3.373040e-05\n",
      "  Step 1900/4407 - Loss: 1.9433 | PPL: 6.98 | LR: 3.365537e-05\n",
      "  Step 1900/4407 - Loss: 1.9433 | PPL: 6.98 | LR: 3.365537e-05\n",
      "  Step 2000/4407 - Loss: 1.9426 | PPL: 6.98 | LR: 3.358041e-05\n",
      "  Step 2000/4407 - Loss: 1.9426 | PPL: 6.98 | LR: 3.358041e-05\n",
      "  Step 2100/4407 - Loss: 1.9430 | PPL: 6.98 | LR: 3.350553e-05\n",
      "  Step 2100/4407 - Loss: 1.9430 | PPL: 6.98 | LR: 3.350553e-05\n",
      "  Step 2200/4407 - Loss: 1.9433 | PPL: 6.98 | LR: 3.343072e-05\n",
      "  Step 2200/4407 - Loss: 1.9433 | PPL: 6.98 | LR: 3.343072e-05\n",
      "  Step 2300/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 3.335599e-05\n",
      "  Step 2300/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 3.335599e-05\n",
      "  Step 2400/4407 - Loss: 1.9422 | PPL: 6.97 | LR: 3.328132e-05\n",
      "  Step 2400/4407 - Loss: 1.9422 | PPL: 6.97 | LR: 3.328132e-05\n",
      "  Step 2500/4407 - Loss: 1.9419 | PPL: 6.97 | LR: 3.320673e-05\n",
      "  Step 2500/4407 - Loss: 1.9419 | PPL: 6.97 | LR: 3.320673e-05\n",
      "  Step 2600/4407 - Loss: 1.9429 | PPL: 6.98 | LR: 3.313222e-05\n",
      "  Step 2600/4407 - Loss: 1.9429 | PPL: 6.98 | LR: 3.313222e-05\n",
      "  Step 2700/4407 - Loss: 1.9423 | PPL: 6.98 | LR: 3.305778e-05\n",
      "  Step 2700/4407 - Loss: 1.9423 | PPL: 6.98 | LR: 3.305778e-05\n",
      "  Step 2800/4407 - Loss: 1.9433 | PPL: 6.98 | LR: 3.298341e-05\n",
      "  Step 2800/4407 - Loss: 1.9433 | PPL: 6.98 | LR: 3.298341e-05\n",
      "  Step 2900/4407 - Loss: 1.9435 | PPL: 6.98 | LR: 3.290911e-05\n",
      "  Step 2900/4407 - Loss: 1.9435 | PPL: 6.98 | LR: 3.290911e-05\n",
      "  Step 3000/4407 - Loss: 1.9417 | PPL: 6.97 | LR: 3.283489e-05\n",
      "  Step 3000/4407 - Loss: 1.9417 | PPL: 6.97 | LR: 3.283489e-05\n",
      "  Step 3100/4407 - Loss: 1.9425 | PPL: 6.98 | LR: 3.276074e-05\n",
      "  Step 3100/4407 - Loss: 1.9425 | PPL: 6.98 | LR: 3.276074e-05\n",
      "  Step 3200/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 3.268666e-05\n",
      "  Step 3200/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 3.268666e-05\n",
      "  Step 3300/4407 - Loss: 1.9438 | PPL: 6.98 | LR: 3.261266e-05\n",
      "  Step 3300/4407 - Loss: 1.9438 | PPL: 6.98 | LR: 3.261266e-05\n",
      "  Step 3400/4407 - Loss: 1.9440 | PPL: 6.99 | LR: 3.253873e-05\n",
      "  Step 3400/4407 - Loss: 1.9440 | PPL: 6.99 | LR: 3.253873e-05\n",
      "  Step 3500/4407 - Loss: 1.9444 | PPL: 6.99 | LR: 3.246488e-05\n",
      "  Step 3500/4407 - Loss: 1.9444 | PPL: 6.99 | LR: 3.246488e-05\n",
      "  Step 3600/4407 - Loss: 1.9450 | PPL: 6.99 | LR: 3.239110e-05\n",
      "  Step 3600/4407 - Loss: 1.9450 | PPL: 6.99 | LR: 3.239110e-05\n",
      "  Step 3700/4407 - Loss: 1.9459 | PPL: 7.00 | LR: 3.231739e-05\n",
      "  Step 3700/4407 - Loss: 1.9459 | PPL: 7.00 | LR: 3.231739e-05\n",
      "  Step 3800/4407 - Loss: 1.9464 | PPL: 7.00 | LR: 3.224375e-05\n",
      "  Step 3800/4407 - Loss: 1.9464 | PPL: 7.00 | LR: 3.224375e-05\n",
      "  Step 3900/4407 - Loss: 1.9470 | PPL: 7.01 | LR: 3.217020e-05\n",
      "  Step 3900/4407 - Loss: 1.9470 | PPL: 7.01 | LR: 3.217020e-05\n",
      "  Step 4000/4407 - Loss: 1.9478 | PPL: 7.01 | LR: 3.209671e-05\n",
      "  Step 4000/4407 - Loss: 1.9478 | PPL: 7.01 | LR: 3.209671e-05\n",
      "  Step 4100/4407 - Loss: 1.9478 | PPL: 7.01 | LR: 3.202330e-05\n",
      "  Step 4100/4407 - Loss: 1.9478 | PPL: 7.01 | LR: 3.202330e-05\n",
      "  Step 4200/4407 - Loss: 1.9475 | PPL: 7.01 | LR: 3.194996e-05\n",
      "  Step 4200/4407 - Loss: 1.9475 | PPL: 7.01 | LR: 3.194996e-05\n",
      "  Step 4300/4407 - Loss: 1.9481 | PPL: 7.02 | LR: 3.187670e-05\n",
      "  Step 4300/4407 - Loss: 1.9481 | PPL: 7.02 | LR: 3.187670e-05\n",
      "  Step 4400/4407 - Loss: 1.9480 | PPL: 7.01 | LR: 3.180351e-05\n",
      "  Step 4400/4407 - Loss: 1.9480 | PPL: 7.01 | LR: 3.180351e-05\n",
      "Epoch 81/100 | Train Loss: 1.9481 | Train PPL: 7.02 | Val Loss: 2.5954 | Val PPL: 13.40\n",
      "Epoch 81/100 | Train Loss: 1.9481 | Train PPL: 7.02 | Val Loss: 2.5954 | Val PPL: 13.40\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.02\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.02\n",
      "  Step 100/4407 - Loss: 1.9425 | PPL: 6.98 | LR: 3.172528e-05\n",
      "  Step 100/4407 - Loss: 1.9425 | PPL: 6.98 | LR: 3.172528e-05\n",
      "  Step 200/4407 - Loss: 1.9368 | PPL: 6.94 | LR: 3.165224e-05\n",
      "  Step 200/4407 - Loss: 1.9368 | PPL: 6.94 | LR: 3.165224e-05\n",
      "  Step 300/4407 - Loss: 1.9429 | PPL: 6.98 | LR: 3.157928e-05\n",
      "  Step 300/4407 - Loss: 1.9429 | PPL: 6.98 | LR: 3.157928e-05\n",
      "  Step 400/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 3.150639e-05\n",
      "  Step 400/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 3.150639e-05\n",
      "  Step 500/4407 - Loss: 1.9359 | PPL: 6.93 | LR: 3.143358e-05\n",
      "  Step 500/4407 - Loss: 1.9359 | PPL: 6.93 | LR: 3.143358e-05\n",
      "  Step 600/4407 - Loss: 1.9334 | PPL: 6.91 | LR: 3.136084e-05\n",
      "  Step 600/4407 - Loss: 1.9334 | PPL: 6.91 | LR: 3.136084e-05\n",
      "  Step 700/4407 - Loss: 1.9344 | PPL: 6.92 | LR: 3.128818e-05\n",
      "  Step 700/4407 - Loss: 1.9344 | PPL: 6.92 | LR: 3.128818e-05\n",
      "  Step 800/4407 - Loss: 1.9380 | PPL: 6.94 | LR: 3.121559e-05\n",
      "  Step 800/4407 - Loss: 1.9380 | PPL: 6.94 | LR: 3.121559e-05\n",
      "  Step 900/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 3.114307e-05\n",
      "  Step 900/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 3.114307e-05\n",
      "  Step 1000/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 3.107063e-05\n",
      "  Step 1000/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 3.107063e-05\n",
      "  Step 1100/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 3.099826e-05\n",
      "  Step 1100/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 3.099826e-05\n",
      "  Step 1200/4407 - Loss: 1.9375 | PPL: 6.94 | LR: 3.092597e-05\n",
      "  Step 1200/4407 - Loss: 1.9375 | PPL: 6.94 | LR: 3.092597e-05\n",
      "  Step 1300/4407 - Loss: 1.9384 | PPL: 6.95 | LR: 3.085376e-05\n",
      "  Step 1300/4407 - Loss: 1.9384 | PPL: 6.95 | LR: 3.085376e-05\n",
      "  Step 1400/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 3.078161e-05\n",
      "  Step 1400/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 3.078161e-05\n",
      "  Step 1500/4407 - Loss: 1.9340 | PPL: 6.92 | LR: 3.070955e-05\n",
      "  Step 1500/4407 - Loss: 1.9340 | PPL: 6.92 | LR: 3.070955e-05\n",
      "  Step 1600/4407 - Loss: 1.9308 | PPL: 6.90 | LR: 3.063756e-05\n",
      "  Step 1600/4407 - Loss: 1.9308 | PPL: 6.90 | LR: 3.063756e-05\n",
      "  Step 1700/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 3.056564e-05\n",
      "  Step 1700/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 3.056564e-05\n",
      "  Step 1800/4407 - Loss: 1.9327 | PPL: 6.91 | LR: 3.049380e-05\n",
      "  Step 1800/4407 - Loss: 1.9327 | PPL: 6.91 | LR: 3.049380e-05\n",
      "  Step 1900/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 3.042203e-05\n",
      "  Step 1900/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 3.042203e-05\n",
      "  Step 2000/4407 - Loss: 1.9319 | PPL: 6.90 | LR: 3.035034e-05\n",
      "  Step 2000/4407 - Loss: 1.9319 | PPL: 6.90 | LR: 3.035034e-05\n",
      "  Step 2100/4407 - Loss: 1.9336 | PPL: 6.91 | LR: 3.027872e-05\n",
      "  Step 2100/4407 - Loss: 1.9336 | PPL: 6.91 | LR: 3.027872e-05\n",
      "  Step 2200/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 3.020718e-05\n",
      "  Step 2200/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 3.020718e-05\n",
      "  Step 2300/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 3.013571e-05\n",
      "  Step 2300/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 3.013571e-05\n",
      "  Step 2400/4407 - Loss: 1.9350 | PPL: 6.92 | LR: 3.006432e-05\n",
      "  Step 2400/4407 - Loss: 1.9350 | PPL: 6.92 | LR: 3.006432e-05\n",
      "  Step 2500/4407 - Loss: 1.9373 | PPL: 6.94 | LR: 2.999300e-05\n",
      "  Step 2500/4407 - Loss: 1.9373 | PPL: 6.94 | LR: 2.999300e-05\n",
      "  Step 2600/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 2.992176e-05\n",
      "  Step 2600/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 2.992176e-05\n",
      "  Step 2700/4407 - Loss: 1.9380 | PPL: 6.95 | LR: 2.985059e-05\n",
      "  Step 2700/4407 - Loss: 1.9380 | PPL: 6.95 | LR: 2.985059e-05\n",
      "  Step 2800/4407 - Loss: 1.9383 | PPL: 6.95 | LR: 2.977950e-05\n",
      "  Step 2800/4407 - Loss: 1.9383 | PPL: 6.95 | LR: 2.977950e-05\n",
      "  Step 2900/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 2.970849e-05\n",
      "  Step 2900/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 2.970849e-05\n",
      "  Step 3000/4407 - Loss: 1.9393 | PPL: 6.95 | LR: 2.963755e-05\n",
      "  Step 3000/4407 - Loss: 1.9393 | PPL: 6.95 | LR: 2.963755e-05\n",
      "  Step 3100/4407 - Loss: 1.9404 | PPL: 6.96 | LR: 2.956668e-05\n",
      "  Step 3100/4407 - Loss: 1.9404 | PPL: 6.96 | LR: 2.956668e-05\n",
      "  Step 3200/4407 - Loss: 1.9406 | PPL: 6.96 | LR: 2.949589e-05\n",
      "  Step 3200/4407 - Loss: 1.9406 | PPL: 6.96 | LR: 2.949589e-05\n",
      "  Step 3300/4407 - Loss: 1.9419 | PPL: 6.97 | LR: 2.942518e-05\n",
      "  Step 3300/4407 - Loss: 1.9419 | PPL: 6.97 | LR: 2.942518e-05\n",
      "  Step 3400/4407 - Loss: 1.9413 | PPL: 6.97 | LR: 2.935454e-05\n",
      "  Step 3400/4407 - Loss: 1.9413 | PPL: 6.97 | LR: 2.935454e-05\n",
      "  Step 3500/4407 - Loss: 1.9409 | PPL: 6.96 | LR: 2.928398e-05\n",
      "  Step 3500/4407 - Loss: 1.9409 | PPL: 6.96 | LR: 2.928398e-05\n",
      "  Step 3600/4407 - Loss: 1.9417 | PPL: 6.97 | LR: 2.921350e-05\n",
      "  Step 3600/4407 - Loss: 1.9417 | PPL: 6.97 | LR: 2.921350e-05\n",
      "  Step 3700/4407 - Loss: 1.9424 | PPL: 6.98 | LR: 2.914309e-05\n",
      "  Step 3700/4407 - Loss: 1.9424 | PPL: 6.98 | LR: 2.914309e-05\n",
      "  Step 3800/4407 - Loss: 1.9422 | PPL: 6.97 | LR: 2.907275e-05\n",
      "  Step 3800/4407 - Loss: 1.9422 | PPL: 6.97 | LR: 2.907275e-05\n",
      "  Step 3900/4407 - Loss: 1.9439 | PPL: 6.99 | LR: 2.900249e-05\n",
      "  Step 3900/4407 - Loss: 1.9439 | PPL: 6.99 | LR: 2.900249e-05\n",
      "  Step 4000/4407 - Loss: 1.9442 | PPL: 6.99 | LR: 2.893231e-05\n",
      "  Step 4000/4407 - Loss: 1.9442 | PPL: 6.99 | LR: 2.893231e-05\n",
      "  Step 4100/4407 - Loss: 1.9435 | PPL: 6.98 | LR: 2.886220e-05\n",
      "  Step 4100/4407 - Loss: 1.9435 | PPL: 6.98 | LR: 2.886220e-05\n",
      "  Step 4200/4407 - Loss: 1.9450 | PPL: 6.99 | LR: 2.879217e-05\n",
      "  Step 4200/4407 - Loss: 1.9450 | PPL: 6.99 | LR: 2.879217e-05\n",
      "  Step 4300/4407 - Loss: 1.9450 | PPL: 6.99 | LR: 2.872222e-05\n",
      "  Step 4300/4407 - Loss: 1.9450 | PPL: 6.99 | LR: 2.872222e-05\n",
      "  Step 4400/4407 - Loss: 1.9456 | PPL: 7.00 | LR: 2.865234e-05\n",
      "  Step 4400/4407 - Loss: 1.9456 | PPL: 7.00 | LR: 2.865234e-05\n",
      "Epoch 82/100 | Train Loss: 1.9459 | Train PPL: 7.00 | Val Loss: 2.5927 | Val PPL: 13.37\n",
      "Epoch 82/100 | Train Loss: 1.9459 | Train PPL: 7.00 | Val Loss: 2.5927 | Val PPL: 13.37\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.00\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 7.00\n",
      "  Step 100/4407 - Loss: 1.9088 | PPL: 6.75 | LR: 2.857765e-05\n",
      "  Step 100/4407 - Loss: 1.9088 | PPL: 6.75 | LR: 2.857765e-05\n",
      "  Step 200/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 2.850793e-05\n",
      "  Step 200/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 2.850793e-05\n",
      "  Step 300/4407 - Loss: 1.9200 | PPL: 6.82 | LR: 2.843829e-05\n",
      "  Step 300/4407 - Loss: 1.9200 | PPL: 6.82 | LR: 2.843829e-05\n",
      "  Step 400/4407 - Loss: 1.9117 | PPL: 6.76 | LR: 2.836872e-05\n",
      "  Step 400/4407 - Loss: 1.9117 | PPL: 6.76 | LR: 2.836872e-05\n",
      "  Step 500/4407 - Loss: 1.9137 | PPL: 6.78 | LR: 2.829923e-05\n",
      "  Step 500/4407 - Loss: 1.9137 | PPL: 6.78 | LR: 2.829923e-05\n",
      "  Step 600/4407 - Loss: 1.9203 | PPL: 6.82 | LR: 2.822981e-05\n",
      "  Step 600/4407 - Loss: 1.9203 | PPL: 6.82 | LR: 2.822981e-05\n",
      "  Step 700/4407 - Loss: 1.9261 | PPL: 6.86 | LR: 2.816047e-05\n",
      "  Step 700/4407 - Loss: 1.9261 | PPL: 6.86 | LR: 2.816047e-05\n",
      "  Step 800/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 2.809121e-05\n",
      "  Step 800/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 2.809121e-05\n",
      "  Step 900/4407 - Loss: 1.9343 | PPL: 6.92 | LR: 2.802202e-05\n",
      "  Step 900/4407 - Loss: 1.9343 | PPL: 6.92 | LR: 2.802202e-05\n",
      "  Step 1000/4407 - Loss: 1.9386 | PPL: 6.95 | LR: 2.795291e-05\n",
      "  Step 1000/4407 - Loss: 1.9386 | PPL: 6.95 | LR: 2.795291e-05\n",
      "  Step 1100/4407 - Loss: 1.9361 | PPL: 6.93 | LR: 2.788388e-05\n",
      "  Step 1100/4407 - Loss: 1.9361 | PPL: 6.93 | LR: 2.788388e-05\n",
      "  Step 1200/4407 - Loss: 1.9357 | PPL: 6.93 | LR: 2.781492e-05\n",
      "  Step 1200/4407 - Loss: 1.9357 | PPL: 6.93 | LR: 2.781492e-05\n",
      "  Step 1300/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 2.774604e-05\n",
      "  Step 1300/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 2.774604e-05\n",
      "  Step 1400/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 2.767724e-05\n",
      "  Step 1400/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 2.767724e-05\n",
      "  Step 1500/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 2.760851e-05\n",
      "  Step 1500/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 2.760851e-05\n",
      "  Step 1600/4407 - Loss: 1.9357 | PPL: 6.93 | LR: 2.753986e-05\n",
      "  Step 1600/4407 - Loss: 1.9357 | PPL: 6.93 | LR: 2.753986e-05\n",
      "  Step 1700/4407 - Loss: 1.9351 | PPL: 6.92 | LR: 2.747129e-05\n",
      "  Step 1700/4407 - Loss: 1.9351 | PPL: 6.92 | LR: 2.747129e-05\n",
      "  Step 1800/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 2.740279e-05\n",
      "  Step 1800/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 2.740279e-05\n",
      "  Step 1900/4407 - Loss: 1.9366 | PPL: 6.94 | LR: 2.733437e-05\n",
      "  Step 1900/4407 - Loss: 1.9366 | PPL: 6.94 | LR: 2.733437e-05\n",
      "  Step 2000/4407 - Loss: 1.9371 | PPL: 6.94 | LR: 2.726603e-05\n",
      "  Step 2000/4407 - Loss: 1.9371 | PPL: 6.94 | LR: 2.726603e-05\n",
      "  Step 2100/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 2.719777e-05\n",
      "  Step 2100/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 2.719777e-05\n",
      "  Step 2200/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 2.712958e-05\n",
      "  Step 2200/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 2.712958e-05\n",
      "  Step 2300/4407 - Loss: 1.9359 | PPL: 6.93 | LR: 2.706147e-05\n",
      "  Step 2300/4407 - Loss: 1.9359 | PPL: 6.93 | LR: 2.706147e-05\n",
      "  Step 2400/4407 - Loss: 1.9345 | PPL: 6.92 | LR: 2.699343e-05\n",
      "  Step 2400/4407 - Loss: 1.9345 | PPL: 6.92 | LR: 2.699343e-05\n",
      "  Step 2500/4407 - Loss: 1.9361 | PPL: 6.93 | LR: 2.692548e-05\n",
      "  Step 2500/4407 - Loss: 1.9361 | PPL: 6.93 | LR: 2.692548e-05\n",
      "  Step 2600/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 2.685760e-05\n",
      "  Step 2600/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 2.685760e-05\n",
      "  Step 2700/4407 - Loss: 1.9391 | PPL: 6.95 | LR: 2.678979e-05\n",
      "  Step 2700/4407 - Loss: 1.9391 | PPL: 6.95 | LR: 2.678979e-05\n",
      "  Step 2800/4407 - Loss: 1.9405 | PPL: 6.96 | LR: 2.672207e-05\n",
      "  Step 2800/4407 - Loss: 1.9405 | PPL: 6.96 | LR: 2.672207e-05\n",
      "  Step 2900/4407 - Loss: 1.9418 | PPL: 6.97 | LR: 2.665442e-05\n",
      "  Step 2900/4407 - Loss: 1.9418 | PPL: 6.97 | LR: 2.665442e-05\n",
      "  Step 3000/4407 - Loss: 1.9406 | PPL: 6.96 | LR: 2.658685e-05\n",
      "  Step 3000/4407 - Loss: 1.9406 | PPL: 6.96 | LR: 2.658685e-05\n",
      "  Step 3100/4407 - Loss: 1.9418 | PPL: 6.97 | LR: 2.651936e-05\n",
      "  Step 3100/4407 - Loss: 1.9418 | PPL: 6.97 | LR: 2.651936e-05\n",
      "  Step 3200/4407 - Loss: 1.9426 | PPL: 6.98 | LR: 2.645194e-05\n",
      "  Step 3200/4407 - Loss: 1.9426 | PPL: 6.98 | LR: 2.645194e-05\n",
      "  Step 3300/4407 - Loss: 1.9424 | PPL: 6.98 | LR: 2.638460e-05\n",
      "  Step 3300/4407 - Loss: 1.9424 | PPL: 6.98 | LR: 2.638460e-05\n",
      "  Step 3400/4407 - Loss: 1.9428 | PPL: 6.98 | LR: 2.631734e-05\n",
      "  Step 3400/4407 - Loss: 1.9428 | PPL: 6.98 | LR: 2.631734e-05\n",
      "  Step 3500/4407 - Loss: 1.9428 | PPL: 6.98 | LR: 2.625016e-05\n",
      "  Step 3500/4407 - Loss: 1.9428 | PPL: 6.98 | LR: 2.625016e-05\n",
      "  Step 3600/4407 - Loss: 1.9427 | PPL: 6.98 | LR: 2.618306e-05\n",
      "  Step 3600/4407 - Loss: 1.9427 | PPL: 6.98 | LR: 2.618306e-05\n",
      "  Step 3700/4407 - Loss: 1.9426 | PPL: 6.98 | LR: 2.611603e-05\n",
      "  Step 3700/4407 - Loss: 1.9426 | PPL: 6.98 | LR: 2.611603e-05\n",
      "  Step 3800/4407 - Loss: 1.9428 | PPL: 6.98 | LR: 2.604908e-05\n",
      "  Step 3800/4407 - Loss: 1.9428 | PPL: 6.98 | LR: 2.604908e-05\n",
      "  Step 3900/4407 - Loss: 1.9431 | PPL: 6.98 | LR: 2.598221e-05\n",
      "  Step 3900/4407 - Loss: 1.9431 | PPL: 6.98 | LR: 2.598221e-05\n",
      "  Step 4000/4407 - Loss: 1.9431 | PPL: 6.98 | LR: 2.591541e-05\n",
      "  Step 4000/4407 - Loss: 1.9431 | PPL: 6.98 | LR: 2.591541e-05\n",
      "  Step 4100/4407 - Loss: 1.9430 | PPL: 6.98 | LR: 2.584870e-05\n",
      "  Step 4100/4407 - Loss: 1.9430 | PPL: 6.98 | LR: 2.584870e-05\n",
      "  Step 4200/4407 - Loss: 1.9437 | PPL: 6.98 | LR: 2.578206e-05\n",
      "  Step 4200/4407 - Loss: 1.9437 | PPL: 6.98 | LR: 2.578206e-05\n",
      "  Step 4300/4407 - Loss: 1.9442 | PPL: 6.99 | LR: 2.571550e-05\n",
      "  Step 4300/4407 - Loss: 1.9442 | PPL: 6.99 | LR: 2.571550e-05\n",
      "  Step 4400/4407 - Loss: 1.9441 | PPL: 6.99 | LR: 2.564901e-05\n",
      "  Step 4400/4407 - Loss: 1.9441 | PPL: 6.99 | LR: 2.564901e-05\n",
      "Epoch 83/100 | Train Loss: 1.9441 | Train PPL: 6.99 | Val Loss: 2.5904 | Val PPL: 13.34\n",
      "Epoch 83/100 | Train Loss: 1.9441 | Train PPL: 6.99 | Val Loss: 2.5904 | Val PPL: 13.34\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.99\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.99\n",
      "  Step 100/4407 - Loss: 1.9337 | PPL: 6.92 | LR: 2.557797e-05\n",
      "  Step 100/4407 - Loss: 1.9337 | PPL: 6.92 | LR: 2.557797e-05\n",
      "  Step 200/4407 - Loss: 1.9406 | PPL: 6.96 | LR: 2.551164e-05\n",
      "  Step 200/4407 - Loss: 1.9406 | PPL: 6.96 | LR: 2.551164e-05\n",
      "  Step 300/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 2.544540e-05\n",
      "  Step 300/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 2.544540e-05\n",
      "  Step 400/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 2.537924e-05\n",
      "  Step 400/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 2.537924e-05\n",
      "  Step 500/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 2.531315e-05\n",
      "  Step 500/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 2.531315e-05\n",
      "  Step 600/4407 - Loss: 1.9387 | PPL: 6.95 | LR: 2.524714e-05\n",
      "  Step 600/4407 - Loss: 1.9387 | PPL: 6.95 | LR: 2.524714e-05\n",
      "  Step 700/4407 - Loss: 1.9359 | PPL: 6.93 | LR: 2.518121e-05\n",
      "  Step 700/4407 - Loss: 1.9359 | PPL: 6.93 | LR: 2.518121e-05\n",
      "  Step 800/4407 - Loss: 1.9349 | PPL: 6.92 | LR: 2.511536e-05\n",
      "  Step 800/4407 - Loss: 1.9349 | PPL: 6.92 | LR: 2.511536e-05\n",
      "  Step 900/4407 - Loss: 1.9305 | PPL: 6.89 | LR: 2.504959e-05\n",
      "  Step 900/4407 - Loss: 1.9305 | PPL: 6.89 | LR: 2.504959e-05\n",
      "  Step 1000/4407 - Loss: 1.9349 | PPL: 6.92 | LR: 2.498389e-05\n",
      "  Step 1000/4407 - Loss: 1.9349 | PPL: 6.92 | LR: 2.498389e-05\n",
      "  Step 1100/4407 - Loss: 1.9355 | PPL: 6.93 | LR: 2.491827e-05\n",
      "  Step 1100/4407 - Loss: 1.9355 | PPL: 6.93 | LR: 2.491827e-05\n",
      "  Step 1200/4407 - Loss: 1.9334 | PPL: 6.91 | LR: 2.485274e-05\n",
      "  Step 1200/4407 - Loss: 1.9334 | PPL: 6.91 | LR: 2.485274e-05\n",
      "  Step 1300/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 2.478728e-05\n",
      "  Step 1300/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 2.478728e-05\n",
      "  Step 1400/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 2.472190e-05\n",
      "  Step 1400/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 2.472190e-05\n",
      "  Step 1500/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 2.465659e-05\n",
      "  Step 1500/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 2.465659e-05\n",
      "  Step 1600/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 2.459137e-05\n",
      "  Step 1600/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 2.459137e-05\n",
      "  Step 1700/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 2.452622e-05\n",
      "  Step 1700/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 2.452622e-05\n",
      "  Step 1800/4407 - Loss: 1.9334 | PPL: 6.91 | LR: 2.446116e-05\n",
      "  Step 1800/4407 - Loss: 1.9334 | PPL: 6.91 | LR: 2.446116e-05\n",
      "  Step 1900/4407 - Loss: 1.9327 | PPL: 6.91 | LR: 2.439617e-05\n",
      "  Step 1900/4407 - Loss: 1.9327 | PPL: 6.91 | LR: 2.439617e-05\n",
      "  Step 2000/4407 - Loss: 1.9337 | PPL: 6.91 | LR: 2.433126e-05\n",
      "  Step 2000/4407 - Loss: 1.9337 | PPL: 6.91 | LR: 2.433126e-05\n",
      "  Step 2100/4407 - Loss: 1.9351 | PPL: 6.93 | LR: 2.426643e-05\n",
      "  Step 2100/4407 - Loss: 1.9351 | PPL: 6.93 | LR: 2.426643e-05\n",
      "  Step 2200/4407 - Loss: 1.9375 | PPL: 6.94 | LR: 2.420168e-05\n",
      "  Step 2200/4407 - Loss: 1.9375 | PPL: 6.94 | LR: 2.420168e-05\n",
      "  Step 2300/4407 - Loss: 1.9382 | PPL: 6.95 | LR: 2.413701e-05\n",
      "  Step 2300/4407 - Loss: 1.9382 | PPL: 6.95 | LR: 2.413701e-05\n",
      "  Step 2400/4407 - Loss: 1.9381 | PPL: 6.95 | LR: 2.407241e-05\n",
      "  Step 2400/4407 - Loss: 1.9381 | PPL: 6.95 | LR: 2.407241e-05\n",
      "  Step 2500/4407 - Loss: 1.9381 | PPL: 6.95 | LR: 2.400790e-05\n",
      "  Step 2500/4407 - Loss: 1.9381 | PPL: 6.95 | LR: 2.400790e-05\n",
      "  Step 2600/4407 - Loss: 1.9385 | PPL: 6.95 | LR: 2.394346e-05\n",
      "  Step 2600/4407 - Loss: 1.9385 | PPL: 6.95 | LR: 2.394346e-05\n",
      "  Step 2700/4407 - Loss: 1.9403 | PPL: 6.96 | LR: 2.387911e-05\n",
      "  Step 2700/4407 - Loss: 1.9403 | PPL: 6.96 | LR: 2.387911e-05\n",
      "  Step 2800/4407 - Loss: 1.9409 | PPL: 6.97 | LR: 2.381483e-05\n",
      "  Step 2800/4407 - Loss: 1.9409 | PPL: 6.97 | LR: 2.381483e-05\n",
      "  Step 2900/4407 - Loss: 1.9402 | PPL: 6.96 | LR: 2.375063e-05\n",
      "  Step 2900/4407 - Loss: 1.9402 | PPL: 6.96 | LR: 2.375063e-05\n",
      "  Step 3000/4407 - Loss: 1.9418 | PPL: 6.97 | LR: 2.368651e-05\n",
      "  Step 3000/4407 - Loss: 1.9418 | PPL: 6.97 | LR: 2.368651e-05\n",
      "  Step 3100/4407 - Loss: 1.9398 | PPL: 6.96 | LR: 2.362248e-05\n",
      "  Step 3100/4407 - Loss: 1.9398 | PPL: 6.96 | LR: 2.362248e-05\n",
      "  Step 3200/4407 - Loss: 1.9394 | PPL: 6.95 | LR: 2.355852e-05\n",
      "  Step 3200/4407 - Loss: 1.9394 | PPL: 6.95 | LR: 2.355852e-05\n",
      "  Step 3300/4407 - Loss: 1.9397 | PPL: 6.96 | LR: 2.349463e-05\n",
      "  Step 3300/4407 - Loss: 1.9397 | PPL: 6.96 | LR: 2.349463e-05\n",
      "  Step 3400/4407 - Loss: 1.9392 | PPL: 6.95 | LR: 2.343083e-05\n",
      "  Step 3400/4407 - Loss: 1.9392 | PPL: 6.95 | LR: 2.343083e-05\n",
      "  Step 3500/4407 - Loss: 1.9391 | PPL: 6.95 | LR: 2.336711e-05\n",
      "  Step 3500/4407 - Loss: 1.9391 | PPL: 6.95 | LR: 2.336711e-05\n",
      "  Step 3600/4407 - Loss: 1.9394 | PPL: 6.95 | LR: 2.330347e-05\n",
      "  Step 3600/4407 - Loss: 1.9394 | PPL: 6.95 | LR: 2.330347e-05\n",
      "  Step 3700/4407 - Loss: 1.9396 | PPL: 6.96 | LR: 2.323991e-05\n",
      "  Step 3700/4407 - Loss: 1.9396 | PPL: 6.96 | LR: 2.323991e-05\n",
      "  Step 3800/4407 - Loss: 1.9403 | PPL: 6.96 | LR: 2.317642e-05\n",
      "  Step 3800/4407 - Loss: 1.9403 | PPL: 6.96 | LR: 2.317642e-05\n",
      "  Step 3900/4407 - Loss: 1.9412 | PPL: 6.97 | LR: 2.311302e-05\n",
      "  Step 3900/4407 - Loss: 1.9412 | PPL: 6.97 | LR: 2.311302e-05\n",
      "  Step 4000/4407 - Loss: 1.9404 | PPL: 6.96 | LR: 2.304969e-05\n",
      "  Step 4000/4407 - Loss: 1.9404 | PPL: 6.96 | LR: 2.304969e-05\n",
      "  Step 4100/4407 - Loss: 1.9409 | PPL: 6.96 | LR: 2.298645e-05\n",
      "  Step 4100/4407 - Loss: 1.9409 | PPL: 6.96 | LR: 2.298645e-05\n",
      "  Step 4200/4407 - Loss: 1.9424 | PPL: 6.98 | LR: 2.292328e-05\n",
      "  Step 4200/4407 - Loss: 1.9424 | PPL: 6.98 | LR: 2.292328e-05\n",
      "  Step 4300/4407 - Loss: 1.9416 | PPL: 6.97 | LR: 2.286020e-05\n",
      "  Step 4300/4407 - Loss: 1.9416 | PPL: 6.97 | LR: 2.286020e-05\n",
      "  Step 4400/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 2.279719e-05\n",
      "  Step 4400/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 2.279719e-05\n",
      "Epoch 84/100 | Train Loss: 1.9423 | Train PPL: 6.97 | Val Loss: 2.5921 | Val PPL: 13.36\n",
      "Epoch 84/100 | Train Loss: 1.9423 | Train PPL: 6.97 | Val Loss: 2.5921 | Val PPL: 13.36\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.97\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.97\n",
      "  Step 100/4407 - Loss: 1.9742 | PPL: 7.20 | LR: 2.272987e-05\n",
      "  Step 100/4407 - Loss: 1.9742 | PPL: 7.20 | LR: 2.272987e-05\n",
      "  Step 200/4407 - Loss: 1.9436 | PPL: 6.98 | LR: 2.266703e-05\n",
      "  Step 200/4407 - Loss: 1.9436 | PPL: 6.98 | LR: 2.266703e-05\n",
      "  Step 300/4407 - Loss: 1.9342 | PPL: 6.92 | LR: 2.260426e-05\n",
      "  Step 300/4407 - Loss: 1.9342 | PPL: 6.92 | LR: 2.260426e-05\n",
      "  Step 400/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 2.254158e-05\n",
      "  Step 400/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 2.254158e-05\n",
      "  Step 500/4407 - Loss: 1.9334 | PPL: 6.91 | LR: 2.247898e-05\n",
      "  Step 500/4407 - Loss: 1.9334 | PPL: 6.91 | LR: 2.247898e-05\n",
      "  Step 600/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 2.241646e-05\n",
      "  Step 600/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 2.241646e-05\n",
      "  Step 700/4407 - Loss: 1.9339 | PPL: 6.92 | LR: 2.235402e-05\n",
      "  Step 700/4407 - Loss: 1.9339 | PPL: 6.92 | LR: 2.235402e-05\n",
      "  Step 800/4407 - Loss: 1.9319 | PPL: 6.90 | LR: 2.229166e-05\n",
      "  Step 800/4407 - Loss: 1.9319 | PPL: 6.90 | LR: 2.229166e-05\n",
      "  Step 900/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 2.222938e-05\n",
      "  Step 900/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 2.222938e-05\n",
      "  Step 1000/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 2.216718e-05\n",
      "  Step 1000/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 2.216718e-05\n",
      "  Step 1100/4407 - Loss: 1.9275 | PPL: 6.87 | LR: 2.210506e-05\n",
      "  Step 1100/4407 - Loss: 1.9275 | PPL: 6.87 | LR: 2.210506e-05\n",
      "  Step 1200/4407 - Loss: 1.9319 | PPL: 6.90 | LR: 2.204302e-05\n",
      "  Step 1200/4407 - Loss: 1.9319 | PPL: 6.90 | LR: 2.204302e-05\n",
      "  Step 1300/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 2.198106e-05\n",
      "  Step 1300/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 2.198106e-05\n",
      "  Step 1400/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 2.191918e-05\n",
      "  Step 1400/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 2.191918e-05\n",
      "  Step 1500/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 2.185738e-05\n",
      "  Step 1500/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 2.185738e-05\n",
      "  Step 1600/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 2.179567e-05\n",
      "  Step 1600/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 2.179567e-05\n",
      "  Step 1700/4407 - Loss: 1.9266 | PPL: 6.87 | LR: 2.173403e-05\n",
      "  Step 1700/4407 - Loss: 1.9266 | PPL: 6.87 | LR: 2.173403e-05\n",
      "  Step 1800/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 2.167247e-05\n",
      "  Step 1800/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 2.167247e-05\n",
      "  Step 1900/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 2.161099e-05\n",
      "  Step 1900/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 2.161099e-05\n",
      "  Step 2000/4407 - Loss: 1.9304 | PPL: 6.89 | LR: 2.154960e-05\n",
      "  Step 2000/4407 - Loss: 1.9304 | PPL: 6.89 | LR: 2.154960e-05\n",
      "  Step 2100/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 2.148828e-05\n",
      "  Step 2100/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 2.148828e-05\n",
      "  Step 2200/4407 - Loss: 1.9313 | PPL: 6.90 | LR: 2.142705e-05\n",
      "  Step 2200/4407 - Loss: 1.9313 | PPL: 6.90 | LR: 2.142705e-05\n",
      "  Step 2300/4407 - Loss: 1.9304 | PPL: 6.89 | LR: 2.136589e-05\n",
      "  Step 2300/4407 - Loss: 1.9304 | PPL: 6.89 | LR: 2.136589e-05\n",
      "  Step 2400/4407 - Loss: 1.9297 | PPL: 6.89 | LR: 2.130482e-05\n",
      "  Step 2400/4407 - Loss: 1.9297 | PPL: 6.89 | LR: 2.130482e-05\n",
      "  Step 2500/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 2.124383e-05\n",
      "  Step 2500/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 2.124383e-05\n",
      "  Step 2600/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 2.118291e-05\n",
      "  Step 2600/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 2.118291e-05\n",
      "  Step 2700/4407 - Loss: 1.9331 | PPL: 6.91 | LR: 2.112208e-05\n",
      "  Step 2700/4407 - Loss: 1.9331 | PPL: 6.91 | LR: 2.112208e-05\n",
      "  Step 2800/4407 - Loss: 1.9342 | PPL: 6.92 | LR: 2.106133e-05\n",
      "  Step 2800/4407 - Loss: 1.9342 | PPL: 6.92 | LR: 2.106133e-05\n",
      "  Step 2900/4407 - Loss: 1.9342 | PPL: 6.92 | LR: 2.100066e-05\n",
      "  Step 2900/4407 - Loss: 1.9342 | PPL: 6.92 | LR: 2.100066e-05\n",
      "  Step 3000/4407 - Loss: 1.9355 | PPL: 6.93 | LR: 2.094007e-05\n",
      "  Step 3000/4407 - Loss: 1.9355 | PPL: 6.93 | LR: 2.094007e-05\n",
      "  Step 3100/4407 - Loss: 1.9350 | PPL: 6.92 | LR: 2.087956e-05\n",
      "  Step 3100/4407 - Loss: 1.9350 | PPL: 6.92 | LR: 2.087956e-05\n",
      "  Step 3200/4407 - Loss: 1.9352 | PPL: 6.93 | LR: 2.081914e-05\n",
      "  Step 3200/4407 - Loss: 1.9352 | PPL: 6.93 | LR: 2.081914e-05\n",
      "  Step 3300/4407 - Loss: 1.9358 | PPL: 6.93 | LR: 2.075879e-05\n",
      "  Step 3300/4407 - Loss: 1.9358 | PPL: 6.93 | LR: 2.075879e-05\n",
      "  Step 3400/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 2.069853e-05\n",
      "  Step 3400/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 2.069853e-05\n",
      "  Step 3500/4407 - Loss: 1.9368 | PPL: 6.94 | LR: 2.063834e-05\n",
      "  Step 3500/4407 - Loss: 1.9368 | PPL: 6.94 | LR: 2.063834e-05\n",
      "  Step 3600/4407 - Loss: 1.9359 | PPL: 6.93 | LR: 2.057824e-05\n",
      "  Step 3600/4407 - Loss: 1.9359 | PPL: 6.93 | LR: 2.057824e-05\n",
      "  Step 3700/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 2.051822e-05\n",
      "  Step 3700/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 2.051822e-05\n",
      "  Step 3800/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 2.045828e-05\n",
      "  Step 3800/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 2.045828e-05\n",
      "  Step 3900/4407 - Loss: 1.9391 | PPL: 6.95 | LR: 2.039842e-05\n",
      "  Step 3900/4407 - Loss: 1.9391 | PPL: 6.95 | LR: 2.039842e-05\n",
      "  Step 4000/4407 - Loss: 1.9394 | PPL: 6.95 | LR: 2.033864e-05\n",
      "  Step 4000/4407 - Loss: 1.9394 | PPL: 6.95 | LR: 2.033864e-05\n",
      "  Step 4100/4407 - Loss: 1.9399 | PPL: 6.96 | LR: 2.027895e-05\n",
      "  Step 4100/4407 - Loss: 1.9399 | PPL: 6.96 | LR: 2.027895e-05\n",
      "  Step 4200/4407 - Loss: 1.9399 | PPL: 6.96 | LR: 2.021933e-05\n",
      "  Step 4200/4407 - Loss: 1.9399 | PPL: 6.96 | LR: 2.021933e-05\n",
      "  Step 4300/4407 - Loss: 1.9405 | PPL: 6.96 | LR: 2.015980e-05\n",
      "  Step 4300/4407 - Loss: 1.9405 | PPL: 6.96 | LR: 2.015980e-05\n",
      "  Step 4400/4407 - Loss: 1.9403 | PPL: 6.96 | LR: 2.010035e-05\n",
      "  Step 4400/4407 - Loss: 1.9403 | PPL: 6.96 | LR: 2.010035e-05\n",
      "Epoch 85/100 | Train Loss: 1.9405 | Train PPL: 6.96 | Val Loss: 2.5922 | Val PPL: 13.36\n",
      "Epoch 85/100 | Train Loss: 1.9405 | Train PPL: 6.96 | Val Loss: 2.5922 | Val PPL: 13.36\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.96\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.96\n",
      "  Step 100/4407 - Loss: 1.9305 | PPL: 6.89 | LR: 2.003682e-05\n",
      "  Step 100/4407 - Loss: 1.9305 | PPL: 6.89 | LR: 2.003682e-05\n",
      "  Step 200/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 1.997754e-05\n",
      "  Step 200/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 1.997754e-05\n",
      "  Step 300/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 1.991834e-05\n",
      "  Step 300/4407 - Loss: 1.9421 | PPL: 6.97 | LR: 1.991834e-05\n",
      "  Step 400/4407 - Loss: 1.9395 | PPL: 6.95 | LR: 1.985922e-05\n",
      "  Step 400/4407 - Loss: 1.9395 | PPL: 6.95 | LR: 1.985922e-05\n",
      "  Step 500/4407 - Loss: 1.9440 | PPL: 6.99 | LR: 1.980018e-05\n",
      "  Step 500/4407 - Loss: 1.9440 | PPL: 6.99 | LR: 1.980018e-05\n",
      "  Step 600/4407 - Loss: 1.9453 | PPL: 7.00 | LR: 1.974123e-05\n",
      "  Step 600/4407 - Loss: 1.9453 | PPL: 7.00 | LR: 1.974123e-05\n",
      "  Step 700/4407 - Loss: 1.9408 | PPL: 6.96 | LR: 1.968235e-05\n",
      "  Step 700/4407 - Loss: 1.9408 | PPL: 6.96 | LR: 1.968235e-05\n",
      "  Step 800/4407 - Loss: 1.9415 | PPL: 6.97 | LR: 1.962356e-05\n",
      "  Step 800/4407 - Loss: 1.9415 | PPL: 6.97 | LR: 1.962356e-05\n",
      "  Step 900/4407 - Loss: 1.9410 | PPL: 6.97 | LR: 1.956485e-05\n",
      "  Step 900/4407 - Loss: 1.9410 | PPL: 6.97 | LR: 1.956485e-05\n",
      "  Step 1000/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 1.950622e-05\n",
      "  Step 1000/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 1.950622e-05\n",
      "  Step 1100/4407 - Loss: 1.9355 | PPL: 6.93 | LR: 1.944767e-05\n",
      "  Step 1100/4407 - Loss: 1.9355 | PPL: 6.93 | LR: 1.944767e-05\n",
      "  Step 1200/4407 - Loss: 1.9365 | PPL: 6.93 | LR: 1.938920e-05\n",
      "  Step 1200/4407 - Loss: 1.9365 | PPL: 6.93 | LR: 1.938920e-05\n",
      "  Step 1300/4407 - Loss: 1.9346 | PPL: 6.92 | LR: 1.933082e-05\n",
      "  Step 1300/4407 - Loss: 1.9346 | PPL: 6.92 | LR: 1.933082e-05\n",
      "  Step 1400/4407 - Loss: 1.9343 | PPL: 6.92 | LR: 1.927252e-05\n",
      "  Step 1400/4407 - Loss: 1.9343 | PPL: 6.92 | LR: 1.927252e-05\n",
      "  Step 1500/4407 - Loss: 1.9340 | PPL: 6.92 | LR: 1.921430e-05\n",
      "  Step 1500/4407 - Loss: 1.9340 | PPL: 6.92 | LR: 1.921430e-05\n",
      "  Step 1600/4407 - Loss: 1.9340 | PPL: 6.92 | LR: 1.915616e-05\n",
      "  Step 1600/4407 - Loss: 1.9340 | PPL: 6.92 | LR: 1.915616e-05\n",
      "  Step 1700/4407 - Loss: 1.9351 | PPL: 6.92 | LR: 1.909811e-05\n",
      "  Step 1700/4407 - Loss: 1.9351 | PPL: 6.92 | LR: 1.909811e-05\n",
      "  Step 1800/4407 - Loss: 1.9352 | PPL: 6.93 | LR: 1.904013e-05\n",
      "  Step 1800/4407 - Loss: 1.9352 | PPL: 6.93 | LR: 1.904013e-05\n",
      "  Step 1900/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 1.898224e-05\n",
      "  Step 1900/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 1.898224e-05\n",
      "  Step 2000/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 1.892443e-05\n",
      "  Step 2000/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 1.892443e-05\n",
      "  Step 2100/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 1.886670e-05\n",
      "  Step 2100/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 1.886670e-05\n",
      "  Step 2200/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 1.880906e-05\n",
      "  Step 2200/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 1.880906e-05\n",
      "  Step 2300/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 1.875150e-05\n",
      "  Step 2300/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 1.875150e-05\n",
      "  Step 2400/4407 - Loss: 1.9338 | PPL: 6.92 | LR: 1.869402e-05\n",
      "  Step 2400/4407 - Loss: 1.9338 | PPL: 6.92 | LR: 1.869402e-05\n",
      "  Step 2500/4407 - Loss: 1.9346 | PPL: 6.92 | LR: 1.863662e-05\n",
      "  Step 2500/4407 - Loss: 1.9346 | PPL: 6.92 | LR: 1.863662e-05\n",
      "  Step 2600/4407 - Loss: 1.9347 | PPL: 6.92 | LR: 1.857931e-05\n",
      "  Step 2600/4407 - Loss: 1.9347 | PPL: 6.92 | LR: 1.857931e-05\n",
      "  Step 2700/4407 - Loss: 1.9368 | PPL: 6.94 | LR: 1.852207e-05\n",
      "  Step 2700/4407 - Loss: 1.9368 | PPL: 6.94 | LR: 1.852207e-05\n",
      "  Step 2800/4407 - Loss: 1.9355 | PPL: 6.93 | LR: 1.846492e-05\n",
      "  Step 2800/4407 - Loss: 1.9355 | PPL: 6.93 | LR: 1.846492e-05\n",
      "  Step 2900/4407 - Loss: 1.9348 | PPL: 6.92 | LR: 1.840785e-05\n",
      "  Step 2900/4407 - Loss: 1.9348 | PPL: 6.92 | LR: 1.840785e-05\n",
      "  Step 3000/4407 - Loss: 1.9350 | PPL: 6.92 | LR: 1.835087e-05\n",
      "  Step 3000/4407 - Loss: 1.9350 | PPL: 6.92 | LR: 1.835087e-05\n",
      "  Step 3100/4407 - Loss: 1.9348 | PPL: 6.92 | LR: 1.829397e-05\n",
      "  Step 3100/4407 - Loss: 1.9348 | PPL: 6.92 | LR: 1.829397e-05\n",
      "  Step 3200/4407 - Loss: 1.9359 | PPL: 6.93 | LR: 1.823715e-05\n",
      "  Step 3200/4407 - Loss: 1.9359 | PPL: 6.93 | LR: 1.823715e-05\n",
      "  Step 3300/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 1.818041e-05\n",
      "  Step 3300/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 1.818041e-05\n",
      "  Step 3400/4407 - Loss: 1.9360 | PPL: 6.93 | LR: 1.812375e-05\n",
      "  Step 3400/4407 - Loss: 1.9360 | PPL: 6.93 | LR: 1.812375e-05\n",
      "  Step 3500/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 1.806718e-05\n",
      "  Step 3500/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 1.806718e-05\n",
      "  Step 3600/4407 - Loss: 1.9365 | PPL: 6.93 | LR: 1.801069e-05\n",
      "  Step 3600/4407 - Loss: 1.9365 | PPL: 6.93 | LR: 1.801069e-05\n",
      "  Step 3700/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 1.795429e-05\n",
      "  Step 3700/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 1.795429e-05\n",
      "  Step 3800/4407 - Loss: 1.9376 | PPL: 6.94 | LR: 1.789796e-05\n",
      "  Step 3800/4407 - Loss: 1.9376 | PPL: 6.94 | LR: 1.789796e-05\n",
      "  Step 3900/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.784172e-05\n",
      "  Step 3900/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.784172e-05\n",
      "  Step 4000/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 1.778557e-05\n",
      "  Step 4000/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 1.778557e-05\n",
      "  Step 4100/4407 - Loss: 1.9373 | PPL: 6.94 | LR: 1.772949e-05\n",
      "  Step 4100/4407 - Loss: 1.9373 | PPL: 6.94 | LR: 1.772949e-05\n",
      "  Step 4200/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 1.767350e-05\n",
      "  Step 4200/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 1.767350e-05\n",
      "  Step 4300/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.761759e-05\n",
      "  Step 4300/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.761759e-05\n",
      "  Step 4400/4407 - Loss: 1.9386 | PPL: 6.95 | LR: 1.756177e-05\n",
      "  Step 4400/4407 - Loss: 1.9386 | PPL: 6.95 | LR: 1.756177e-05\n",
      "Epoch 86/100 | Train Loss: 1.9387 | Train PPL: 6.95 | Val Loss: 2.5924 | Val PPL: 13.36\n",
      "Epoch 86/100 | Train Loss: 1.9387 | Train PPL: 6.95 | Val Loss: 2.5924 | Val PPL: 13.36\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.95\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.95\n",
      "  Step 100/4407 - Loss: 1.9583 | PPL: 7.09 | LR: 1.750212e-05\n",
      "  Step 100/4407 - Loss: 1.9583 | PPL: 7.09 | LR: 1.750212e-05\n",
      "  Step 200/4407 - Loss: 1.9542 | PPL: 7.06 | LR: 1.744647e-05\n",
      "  Step 200/4407 - Loss: 1.9542 | PPL: 7.06 | LR: 1.744647e-05\n",
      "  Step 300/4407 - Loss: 1.9409 | PPL: 6.96 | LR: 1.739090e-05\n",
      "  Step 300/4407 - Loss: 1.9409 | PPL: 6.96 | LR: 1.739090e-05\n",
      "  Step 400/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 1.733541e-05\n",
      "  Step 400/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 1.733541e-05\n",
      "  Step 500/4407 - Loss: 1.9330 | PPL: 6.91 | LR: 1.728001e-05\n",
      "  Step 500/4407 - Loss: 1.9330 | PPL: 6.91 | LR: 1.728001e-05\n",
      "  Step 600/4407 - Loss: 1.9341 | PPL: 6.92 | LR: 1.722469e-05\n",
      "  Step 600/4407 - Loss: 1.9341 | PPL: 6.92 | LR: 1.722469e-05\n",
      "  Step 700/4407 - Loss: 1.9396 | PPL: 6.96 | LR: 1.716945e-05\n",
      "  Step 700/4407 - Loss: 1.9396 | PPL: 6.96 | LR: 1.716945e-05\n",
      "  Step 800/4407 - Loss: 1.9413 | PPL: 6.97 | LR: 1.711430e-05\n",
      "  Step 800/4407 - Loss: 1.9413 | PPL: 6.97 | LR: 1.711430e-05\n",
      "  Step 900/4407 - Loss: 1.9384 | PPL: 6.95 | LR: 1.705923e-05\n",
      "  Step 900/4407 - Loss: 1.9384 | PPL: 6.95 | LR: 1.705923e-05\n",
      "  Step 1000/4407 - Loss: 1.9364 | PPL: 6.93 | LR: 1.700424e-05\n",
      "  Step 1000/4407 - Loss: 1.9364 | PPL: 6.93 | LR: 1.700424e-05\n",
      "  Step 1100/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 1.694934e-05\n",
      "  Step 1100/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 1.694934e-05\n",
      "  Step 1200/4407 - Loss: 1.9392 | PPL: 6.95 | LR: 1.689451e-05\n",
      "  Step 1200/4407 - Loss: 1.9392 | PPL: 6.95 | LR: 1.689451e-05\n",
      "  Step 1300/4407 - Loss: 1.9398 | PPL: 6.96 | LR: 1.683978e-05\n",
      "  Step 1300/4407 - Loss: 1.9398 | PPL: 6.96 | LR: 1.683978e-05\n",
      "  Step 1400/4407 - Loss: 1.9385 | PPL: 6.95 | LR: 1.678512e-05\n",
      "  Step 1400/4407 - Loss: 1.9385 | PPL: 6.95 | LR: 1.678512e-05\n",
      "  Step 1500/4407 - Loss: 1.9372 | PPL: 6.94 | LR: 1.673055e-05\n",
      "  Step 1500/4407 - Loss: 1.9372 | PPL: 6.94 | LR: 1.673055e-05\n",
      "  Step 1600/4407 - Loss: 1.9408 | PPL: 6.96 | LR: 1.667607e-05\n",
      "  Step 1600/4407 - Loss: 1.9408 | PPL: 6.96 | LR: 1.667607e-05\n",
      "  Step 1700/4407 - Loss: 1.9396 | PPL: 6.96 | LR: 1.662167e-05\n",
      "  Step 1700/4407 - Loss: 1.9396 | PPL: 6.96 | LR: 1.662167e-05\n",
      "  Step 1800/4407 - Loss: 1.9386 | PPL: 6.95 | LR: 1.656735e-05\n",
      "  Step 1800/4407 - Loss: 1.9386 | PPL: 6.95 | LR: 1.656735e-05\n",
      "  Step 1900/4407 - Loss: 1.9393 | PPL: 6.95 | LR: 1.651311e-05\n",
      "  Step 1900/4407 - Loss: 1.9393 | PPL: 6.95 | LR: 1.651311e-05\n",
      "  Step 2000/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 1.645896e-05\n",
      "  Step 2000/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 1.645896e-05\n",
      "  Step 2100/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 1.640489e-05\n",
      "  Step 2100/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 1.640489e-05\n",
      "  Step 2200/4407 - Loss: 1.9394 | PPL: 6.95 | LR: 1.635091e-05\n",
      "  Step 2200/4407 - Loss: 1.9394 | PPL: 6.95 | LR: 1.635091e-05\n",
      "  Step 2300/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 1.629701e-05\n",
      "  Step 2300/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 1.629701e-05\n",
      "  Step 2400/4407 - Loss: 1.9386 | PPL: 6.95 | LR: 1.624319e-05\n",
      "  Step 2400/4407 - Loss: 1.9386 | PPL: 6.95 | LR: 1.624319e-05\n",
      "  Step 2500/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 1.618946e-05\n",
      "  Step 2500/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 1.618946e-05\n",
      "  Step 2600/4407 - Loss: 1.9375 | PPL: 6.94 | LR: 1.613581e-05\n",
      "  Step 2600/4407 - Loss: 1.9375 | PPL: 6.94 | LR: 1.613581e-05\n",
      "  Step 2700/4407 - Loss: 1.9376 | PPL: 6.94 | LR: 1.608225e-05\n",
      "  Step 2700/4407 - Loss: 1.9376 | PPL: 6.94 | LR: 1.608225e-05\n",
      "  Step 2800/4407 - Loss: 1.9381 | PPL: 6.95 | LR: 1.602877e-05\n",
      "  Step 2800/4407 - Loss: 1.9381 | PPL: 6.95 | LR: 1.602877e-05\n",
      "  Step 2900/4407 - Loss: 1.9382 | PPL: 6.95 | LR: 1.597537e-05\n",
      "  Step 2900/4407 - Loss: 1.9382 | PPL: 6.95 | LR: 1.597537e-05\n",
      "  Step 3000/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 1.592206e-05\n",
      "  Step 3000/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 1.592206e-05\n",
      "  Step 3100/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 1.586883e-05\n",
      "  Step 3100/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 1.586883e-05\n",
      "  Step 3200/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 1.581569e-05\n",
      "  Step 3200/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 1.581569e-05\n",
      "  Step 3300/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 1.576263e-05\n",
      "  Step 3300/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 1.576263e-05\n",
      "  Step 3400/4407 - Loss: 1.9395 | PPL: 6.96 | LR: 1.570965e-05\n",
      "  Step 3400/4407 - Loss: 1.9395 | PPL: 6.96 | LR: 1.570965e-05\n",
      "  Step 3500/4407 - Loss: 1.9396 | PPL: 6.96 | LR: 1.565676e-05\n",
      "  Step 3500/4407 - Loss: 1.9396 | PPL: 6.96 | LR: 1.565676e-05\n",
      "  Step 3600/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 1.560396e-05\n",
      "  Step 3600/4407 - Loss: 1.9388 | PPL: 6.95 | LR: 1.560396e-05\n",
      "  Step 3700/4407 - Loss: 1.9383 | PPL: 6.95 | LR: 1.555123e-05\n",
      "  Step 3700/4407 - Loss: 1.9383 | PPL: 6.95 | LR: 1.555123e-05\n",
      "  Step 3800/4407 - Loss: 1.9382 | PPL: 6.95 | LR: 1.549859e-05\n",
      "  Step 3800/4407 - Loss: 1.9382 | PPL: 6.95 | LR: 1.549859e-05\n",
      "  Step 3900/4407 - Loss: 1.9384 | PPL: 6.95 | LR: 1.544604e-05\n",
      "  Step 3900/4407 - Loss: 1.9384 | PPL: 6.95 | LR: 1.544604e-05\n",
      "  Step 4000/4407 - Loss: 1.9384 | PPL: 6.95 | LR: 1.539357e-05\n",
      "  Step 4000/4407 - Loss: 1.9384 | PPL: 6.95 | LR: 1.539357e-05\n",
      "  Step 4100/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.534119e-05\n",
      "  Step 4100/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.534119e-05\n",
      "  Step 4200/4407 - Loss: 1.9376 | PPL: 6.94 | LR: 1.528889e-05\n",
      "  Step 4200/4407 - Loss: 1.9376 | PPL: 6.94 | LR: 1.528889e-05\n",
      "  Step 4300/4407 - Loss: 1.9371 | PPL: 6.94 | LR: 1.523667e-05\n",
      "  Step 4300/4407 - Loss: 1.9371 | PPL: 6.94 | LR: 1.523667e-05\n",
      "  Step 4400/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 1.518454e-05\n",
      "  Step 4400/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 1.518454e-05\n",
      "Epoch 87/100 | Train Loss: 1.9374 | Train PPL: 6.94 | Val Loss: 2.5932 | Val PPL: 13.37\n",
      "Epoch 87/100 | Train Loss: 1.9374 | Train PPL: 6.94 | Val Loss: 2.5932 | Val PPL: 13.37\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.94\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.94\n",
      "  Step 100/4407 - Loss: 1.9401 | PPL: 6.96 | LR: 1.512885e-05\n",
      "  Step 100/4407 - Loss: 1.9401 | PPL: 6.96 | LR: 1.512885e-05\n",
      "  Step 200/4407 - Loss: 1.9306 | PPL: 6.89 | LR: 1.507690e-05\n",
      "  Step 200/4407 - Loss: 1.9306 | PPL: 6.89 | LR: 1.507690e-05\n",
      "  Step 300/4407 - Loss: 1.9376 | PPL: 6.94 | LR: 1.502502e-05\n",
      "  Step 300/4407 - Loss: 1.9376 | PPL: 6.94 | LR: 1.502502e-05\n",
      "  Step 400/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.497324e-05\n",
      "  Step 400/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.497324e-05\n",
      "  Step 500/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 1.492154e-05\n",
      "  Step 500/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 1.492154e-05\n",
      "  Step 600/4407 - Loss: 1.9289 | PPL: 6.88 | LR: 1.486992e-05\n",
      "  Step 600/4407 - Loss: 1.9289 | PPL: 6.88 | LR: 1.486992e-05\n",
      "  Step 700/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 1.481838e-05\n",
      "  Step 700/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 1.481838e-05\n",
      "  Step 800/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 1.476694e-05\n",
      "  Step 800/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 1.476694e-05\n",
      "  Step 900/4407 - Loss: 1.9306 | PPL: 6.89 | LR: 1.471557e-05\n",
      "  Step 900/4407 - Loss: 1.9306 | PPL: 6.89 | LR: 1.471557e-05\n",
      "  Step 1000/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 1.466430e-05\n",
      "  Step 1000/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 1.466430e-05\n",
      "  Step 1100/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 1.461310e-05\n",
      "  Step 1100/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 1.461310e-05\n",
      "  Step 1200/4407 - Loss: 1.9281 | PPL: 6.88 | LR: 1.456199e-05\n",
      "  Step 1200/4407 - Loss: 1.9281 | PPL: 6.88 | LR: 1.456199e-05\n",
      "  Step 1300/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 1.451097e-05\n",
      "  Step 1300/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 1.451097e-05\n",
      "  Step 1400/4407 - Loss: 1.9305 | PPL: 6.89 | LR: 1.446003e-05\n",
      "  Step 1400/4407 - Loss: 1.9305 | PPL: 6.89 | LR: 1.446003e-05\n",
      "  Step 1500/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 1.440918e-05\n",
      "  Step 1500/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 1.440918e-05\n",
      "  Step 1600/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 1.435841e-05\n",
      "  Step 1600/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 1.435841e-05\n",
      "  Step 1700/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 1.430773e-05\n",
      "  Step 1700/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 1.430773e-05\n",
      "  Step 1800/4407 - Loss: 1.9319 | PPL: 6.90 | LR: 1.425713e-05\n",
      "  Step 1800/4407 - Loss: 1.9319 | PPL: 6.90 | LR: 1.425713e-05\n",
      "  Step 1900/4407 - Loss: 1.9334 | PPL: 6.91 | LR: 1.420662e-05\n",
      "  Step 1900/4407 - Loss: 1.9334 | PPL: 6.91 | LR: 1.420662e-05\n",
      "  Step 2000/4407 - Loss: 1.9340 | PPL: 6.92 | LR: 1.415619e-05\n",
      "  Step 2000/4407 - Loss: 1.9340 | PPL: 6.92 | LR: 1.415619e-05\n",
      "  Step 2100/4407 - Loss: 1.9350 | PPL: 6.92 | LR: 1.410585e-05\n",
      "  Step 2100/4407 - Loss: 1.9350 | PPL: 6.92 | LR: 1.410585e-05\n",
      "  Step 2200/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 1.405559e-05\n",
      "  Step 2200/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 1.405559e-05\n",
      "  Step 2300/4407 - Loss: 1.9364 | PPL: 6.93 | LR: 1.400542e-05\n",
      "  Step 2300/4407 - Loss: 1.9364 | PPL: 6.93 | LR: 1.400542e-05\n",
      "  Step 2400/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 1.395533e-05\n",
      "  Step 2400/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 1.395533e-05\n",
      "  Step 2500/4407 - Loss: 1.9366 | PPL: 6.93 | LR: 1.390533e-05\n",
      "  Step 2500/4407 - Loss: 1.9366 | PPL: 6.93 | LR: 1.390533e-05\n",
      "  Step 2600/4407 - Loss: 1.9368 | PPL: 6.94 | LR: 1.385541e-05\n",
      "  Step 2600/4407 - Loss: 1.9368 | PPL: 6.94 | LR: 1.385541e-05\n",
      "  Step 2700/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 1.380558e-05\n",
      "  Step 2700/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 1.380558e-05\n",
      "  Step 2800/4407 - Loss: 1.9377 | PPL: 6.94 | LR: 1.375584e-05\n",
      "  Step 2800/4407 - Loss: 1.9377 | PPL: 6.94 | LR: 1.375584e-05\n",
      "  Step 2900/4407 - Loss: 1.9383 | PPL: 6.95 | LR: 1.370618e-05\n",
      "  Step 2900/4407 - Loss: 1.9383 | PPL: 6.95 | LR: 1.370618e-05\n",
      "  Step 3000/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 1.365660e-05\n",
      "  Step 3000/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 1.365660e-05\n",
      "  Step 3100/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 1.360712e-05\n",
      "  Step 3100/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 1.360712e-05\n",
      "  Step 3200/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 1.355771e-05\n",
      "  Step 3200/4407 - Loss: 1.9374 | PPL: 6.94 | LR: 1.355771e-05\n",
      "  Step 3300/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 1.350840e-05\n",
      "  Step 3300/4407 - Loss: 1.9369 | PPL: 6.94 | LR: 1.350840e-05\n",
      "  Step 3400/4407 - Loss: 1.9368 | PPL: 6.94 | LR: 1.345916e-05\n",
      "  Step 3400/4407 - Loss: 1.9368 | PPL: 6.94 | LR: 1.345916e-05\n",
      "  Step 3500/4407 - Loss: 1.9365 | PPL: 6.93 | LR: 1.341002e-05\n",
      "  Step 3500/4407 - Loss: 1.9365 | PPL: 6.93 | LR: 1.341002e-05\n",
      "  Step 3600/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 1.336096e-05\n",
      "  Step 3600/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 1.336096e-05\n",
      "  Step 3700/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 1.331198e-05\n",
      "  Step 3700/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 1.331198e-05\n",
      "  Step 3800/4407 - Loss: 1.9355 | PPL: 6.93 | LR: 1.326309e-05\n",
      "  Step 3800/4407 - Loss: 1.9355 | PPL: 6.93 | LR: 1.326309e-05\n",
      "  Step 3900/4407 - Loss: 1.9350 | PPL: 6.92 | LR: 1.321429e-05\n",
      "  Step 3900/4407 - Loss: 1.9350 | PPL: 6.92 | LR: 1.321429e-05\n",
      "  Step 4000/4407 - Loss: 1.9356 | PPL: 6.93 | LR: 1.316557e-05\n",
      "  Step 4000/4407 - Loss: 1.9356 | PPL: 6.93 | LR: 1.316557e-05\n",
      "  Step 4100/4407 - Loss: 1.9358 | PPL: 6.93 | LR: 1.311694e-05\n",
      "  Step 4100/4407 - Loss: 1.9358 | PPL: 6.93 | LR: 1.311694e-05\n",
      "  Step 4200/4407 - Loss: 1.9357 | PPL: 6.93 | LR: 1.306840e-05\n",
      "  Step 4200/4407 - Loss: 1.9357 | PPL: 6.93 | LR: 1.306840e-05\n",
      "  Step 4300/4407 - Loss: 1.9364 | PPL: 6.93 | LR: 1.301994e-05\n",
      "  Step 4300/4407 - Loss: 1.9364 | PPL: 6.93 | LR: 1.301994e-05\n",
      "  Step 4400/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 1.297156e-05\n",
      "  Step 4400/4407 - Loss: 1.9363 | PPL: 6.93 | LR: 1.297156e-05\n",
      "Epoch 88/100 | Train Loss: 1.9360 | Train PPL: 6.93 | Val Loss: 2.5942 | Val PPL: 13.39\n",
      "Epoch 88/100 | Train Loss: 1.9360 | Train PPL: 6.93 | Val Loss: 2.5942 | Val PPL: 13.39\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.93\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.93\n",
      "  Step 100/4407 - Loss: 1.9486 | PPL: 7.02 | LR: 1.291990e-05\n",
      "  Step 100/4407 - Loss: 1.9486 | PPL: 7.02 | LR: 1.291990e-05\n",
      "  Step 200/4407 - Loss: 1.9519 | PPL: 7.04 | LR: 1.287170e-05\n",
      "  Step 200/4407 - Loss: 1.9519 | PPL: 7.04 | LR: 1.287170e-05\n",
      "  Step 300/4407 - Loss: 1.9389 | PPL: 6.95 | LR: 1.282359e-05\n",
      "  Step 300/4407 - Loss: 1.9389 | PPL: 6.95 | LR: 1.282359e-05\n",
      "  Step 400/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 1.277557e-05\n",
      "  Step 400/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 1.277557e-05\n",
      "  Step 500/4407 - Loss: 1.9213 | PPL: 6.83 | LR: 1.272763e-05\n",
      "  Step 500/4407 - Loss: 1.9213 | PPL: 6.83 | LR: 1.272763e-05\n",
      "  Step 600/4407 - Loss: 1.9144 | PPL: 6.78 | LR: 1.267978e-05\n",
      "  Step 600/4407 - Loss: 1.9144 | PPL: 6.78 | LR: 1.267978e-05\n",
      "  Step 700/4407 - Loss: 1.9170 | PPL: 6.80 | LR: 1.263202e-05\n",
      "  Step 700/4407 - Loss: 1.9170 | PPL: 6.80 | LR: 1.263202e-05\n",
      "  Step 800/4407 - Loss: 1.9147 | PPL: 6.78 | LR: 1.258434e-05\n",
      "  Step 800/4407 - Loss: 1.9147 | PPL: 6.78 | LR: 1.258434e-05\n",
      "  Step 900/4407 - Loss: 1.9210 | PPL: 6.83 | LR: 1.253674e-05\n",
      "  Step 900/4407 - Loss: 1.9210 | PPL: 6.83 | LR: 1.253674e-05\n",
      "  Step 1000/4407 - Loss: 1.9245 | PPL: 6.85 | LR: 1.248924e-05\n",
      "  Step 1000/4407 - Loss: 1.9245 | PPL: 6.85 | LR: 1.248924e-05\n",
      "  Step 1100/4407 - Loss: 1.9250 | PPL: 6.85 | LR: 1.244182e-05\n",
      "  Step 1100/4407 - Loss: 1.9250 | PPL: 6.85 | LR: 1.244182e-05\n",
      "  Step 1200/4407 - Loss: 1.9249 | PPL: 6.85 | LR: 1.239448e-05\n",
      "  Step 1200/4407 - Loss: 1.9249 | PPL: 6.85 | LR: 1.239448e-05\n",
      "  Step 1300/4407 - Loss: 1.9298 | PPL: 6.89 | LR: 1.234724e-05\n",
      "  Step 1300/4407 - Loss: 1.9298 | PPL: 6.89 | LR: 1.234724e-05\n",
      "  Step 1400/4407 - Loss: 1.9279 | PPL: 6.87 | LR: 1.230007e-05\n",
      "  Step 1400/4407 - Loss: 1.9279 | PPL: 6.87 | LR: 1.230007e-05\n",
      "  Step 1500/4407 - Loss: 1.9297 | PPL: 6.89 | LR: 1.225300e-05\n",
      "  Step 1500/4407 - Loss: 1.9297 | PPL: 6.89 | LR: 1.225300e-05\n",
      "  Step 1600/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 1.220601e-05\n",
      "  Step 1600/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 1.220601e-05\n",
      "  Step 1700/4407 - Loss: 1.9264 | PPL: 6.86 | LR: 1.215911e-05\n",
      "  Step 1700/4407 - Loss: 1.9264 | PPL: 6.86 | LR: 1.215911e-05\n",
      "  Step 1800/4407 - Loss: 1.9263 | PPL: 6.86 | LR: 1.211229e-05\n",
      "  Step 1800/4407 - Loss: 1.9263 | PPL: 6.86 | LR: 1.211229e-05\n",
      "  Step 1900/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 1.206556e-05\n",
      "  Step 1900/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 1.206556e-05\n",
      "  Step 2000/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 1.201892e-05\n",
      "  Step 2000/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 1.201892e-05\n",
      "  Step 2100/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 1.197237e-05\n",
      "  Step 2100/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 1.197237e-05\n",
      "  Step 2200/4407 - Loss: 1.9283 | PPL: 6.88 | LR: 1.192590e-05\n",
      "  Step 2200/4407 - Loss: 1.9283 | PPL: 6.88 | LR: 1.192590e-05\n",
      "  Step 2300/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 1.187951e-05\n",
      "  Step 2300/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 1.187951e-05\n",
      "  Step 2400/4407 - Loss: 1.9283 | PPL: 6.88 | LR: 1.183322e-05\n",
      "  Step 2400/4407 - Loss: 1.9283 | PPL: 6.88 | LR: 1.183322e-05\n",
      "  Step 2500/4407 - Loss: 1.9292 | PPL: 6.88 | LR: 1.178701e-05\n",
      "  Step 2500/4407 - Loss: 1.9292 | PPL: 6.88 | LR: 1.178701e-05\n",
      "  Step 2600/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 1.174089e-05\n",
      "  Step 2600/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 1.174089e-05\n",
      "  Step 2700/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 1.169485e-05\n",
      "  Step 2700/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 1.169485e-05\n",
      "  Step 2800/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 1.164890e-05\n",
      "  Step 2800/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 1.164890e-05\n",
      "  Step 2900/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 1.160304e-05\n",
      "  Step 2900/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 1.160304e-05\n",
      "  Step 3000/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 1.155726e-05\n",
      "  Step 3000/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 1.155726e-05\n",
      "  Step 3100/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 1.151157e-05\n",
      "  Step 3100/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 1.151157e-05\n",
      "  Step 3200/4407 - Loss: 1.9301 | PPL: 6.89 | LR: 1.146597e-05\n",
      "  Step 3200/4407 - Loss: 1.9301 | PPL: 6.89 | LR: 1.146597e-05\n",
      "  Step 3300/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 1.142046e-05\n",
      "  Step 3300/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 1.142046e-05\n",
      "  Step 3400/4407 - Loss: 1.9304 | PPL: 6.89 | LR: 1.137503e-05\n",
      "  Step 3400/4407 - Loss: 1.9304 | PPL: 6.89 | LR: 1.137503e-05\n",
      "  Step 3500/4407 - Loss: 1.9307 | PPL: 6.89 | LR: 1.132969e-05\n",
      "  Step 3500/4407 - Loss: 1.9307 | PPL: 6.89 | LR: 1.132969e-05\n",
      "  Step 3600/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 1.128443e-05\n",
      "  Step 3600/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 1.128443e-05\n",
      "  Step 3700/4407 - Loss: 1.9320 | PPL: 6.90 | LR: 1.123927e-05\n",
      "  Step 3700/4407 - Loss: 1.9320 | PPL: 6.90 | LR: 1.123927e-05\n",
      "  Step 3800/4407 - Loss: 1.9320 | PPL: 6.90 | LR: 1.119419e-05\n",
      "  Step 3800/4407 - Loss: 1.9320 | PPL: 6.90 | LR: 1.119419e-05\n",
      "  Step 3900/4407 - Loss: 1.9317 | PPL: 6.90 | LR: 1.114919e-05\n",
      "  Step 3900/4407 - Loss: 1.9317 | PPL: 6.90 | LR: 1.114919e-05\n",
      "  Step 4000/4407 - Loss: 1.9329 | PPL: 6.91 | LR: 1.110429e-05\n",
      "  Step 4000/4407 - Loss: 1.9329 | PPL: 6.91 | LR: 1.110429e-05\n",
      "  Step 4100/4407 - Loss: 1.9331 | PPL: 6.91 | LR: 1.105947e-05\n",
      "  Step 4100/4407 - Loss: 1.9331 | PPL: 6.91 | LR: 1.105947e-05\n",
      "  Step 4200/4407 - Loss: 1.9335 | PPL: 6.91 | LR: 1.101474e-05\n",
      "  Step 4200/4407 - Loss: 1.9335 | PPL: 6.91 | LR: 1.101474e-05\n",
      "  Step 4300/4407 - Loss: 1.9338 | PPL: 6.92 | LR: 1.097009e-05\n",
      "  Step 4300/4407 - Loss: 1.9338 | PPL: 6.92 | LR: 1.097009e-05\n",
      "  Step 4400/4407 - Loss: 1.9344 | PPL: 6.92 | LR: 1.092554e-05\n",
      "  Step 4400/4407 - Loss: 1.9344 | PPL: 6.92 | LR: 1.092554e-05\n",
      "Epoch 89/100 | Train Loss: 1.9344 | Train PPL: 6.92 | Val Loss: 2.5925 | Val PPL: 13.36\n",
      "Epoch 89/100 | Train Loss: 1.9344 | Train PPL: 6.92 | Val Loss: 2.5925 | Val PPL: 13.36\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.92\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.92\n",
      "  Step 100/4407 - Loss: 1.9182 | PPL: 6.81 | LR: 1.087796e-05\n",
      "  Step 100/4407 - Loss: 1.9182 | PPL: 6.81 | LR: 1.087796e-05\n",
      "  Step 200/4407 - Loss: 1.9220 | PPL: 6.83 | LR: 1.083358e-05\n",
      "  Step 200/4407 - Loss: 1.9220 | PPL: 6.83 | LR: 1.083358e-05\n",
      "  Step 300/4407 - Loss: 1.9166 | PPL: 6.80 | LR: 1.078929e-05\n",
      "  Step 300/4407 - Loss: 1.9166 | PPL: 6.80 | LR: 1.078929e-05\n",
      "  Step 400/4407 - Loss: 1.9263 | PPL: 6.86 | LR: 1.074509e-05\n",
      "  Step 400/4407 - Loss: 1.9263 | PPL: 6.86 | LR: 1.074509e-05\n",
      "  Step 500/4407 - Loss: 1.9268 | PPL: 6.87 | LR: 1.070098e-05\n",
      "  Step 500/4407 - Loss: 1.9268 | PPL: 6.87 | LR: 1.070098e-05\n",
      "  Step 600/4407 - Loss: 1.9233 | PPL: 6.84 | LR: 1.065695e-05\n",
      "  Step 600/4407 - Loss: 1.9233 | PPL: 6.84 | LR: 1.065695e-05\n",
      "  Step 700/4407 - Loss: 1.9326 | PPL: 6.91 | LR: 1.061301e-05\n",
      "  Step 700/4407 - Loss: 1.9326 | PPL: 6.91 | LR: 1.061301e-05\n",
      "  Step 800/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 1.056916e-05\n",
      "  Step 800/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 1.056916e-05\n",
      "  Step 900/4407 - Loss: 1.9339 | PPL: 6.92 | LR: 1.052539e-05\n",
      "  Step 900/4407 - Loss: 1.9339 | PPL: 6.92 | LR: 1.052539e-05\n",
      "  Step 1000/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 1.048172e-05\n",
      "  Step 1000/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 1.048172e-05\n",
      "  Step 1100/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.043813e-05\n",
      "  Step 1100/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.043813e-05\n",
      "  Step 1200/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.039462e-05\n",
      "  Step 1200/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.039462e-05\n",
      "  Step 1300/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 1.035121e-05\n",
      "  Step 1300/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 1.035121e-05\n",
      "  Step 1400/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 1.030788e-05\n",
      "  Step 1400/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 1.030788e-05\n",
      "  Step 1500/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.026464e-05\n",
      "  Step 1500/4407 - Loss: 1.9378 | PPL: 6.94 | LR: 1.026464e-05\n",
      "  Step 1600/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 1.022149e-05\n",
      "  Step 1600/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 1.022149e-05\n",
      "  Step 1700/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 1.017843e-05\n",
      "  Step 1700/4407 - Loss: 1.9379 | PPL: 6.94 | LR: 1.017843e-05\n",
      "  Step 1800/4407 - Loss: 1.9365 | PPL: 6.93 | LR: 1.013545e-05\n",
      "  Step 1800/4407 - Loss: 1.9365 | PPL: 6.93 | LR: 1.013545e-05\n",
      "  Step 1900/4407 - Loss: 1.9356 | PPL: 6.93 | LR: 1.009256e-05\n",
      "  Step 1900/4407 - Loss: 1.9356 | PPL: 6.93 | LR: 1.009256e-05\n",
      "  Step 2000/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 1.004976e-05\n",
      "  Step 2000/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 1.004976e-05\n",
      "  Step 2100/4407 - Loss: 1.9352 | PPL: 6.93 | LR: 1.000705e-05\n",
      "  Step 2100/4407 - Loss: 1.9352 | PPL: 6.93 | LR: 1.000705e-05\n",
      "  Step 2200/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 9.964426e-06\n",
      "  Step 2200/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 9.964426e-06\n",
      "  Step 2300/4407 - Loss: 1.9345 | PPL: 6.92 | LR: 9.921889e-06\n",
      "  Step 2300/4407 - Loss: 1.9345 | PPL: 6.92 | LR: 9.921889e-06\n",
      "  Step 2400/4407 - Loss: 1.9340 | PPL: 6.92 | LR: 9.879440e-06\n",
      "  Step 2400/4407 - Loss: 1.9340 | PPL: 6.92 | LR: 9.879440e-06\n",
      "  Step 2500/4407 - Loss: 1.9349 | PPL: 6.92 | LR: 9.837079e-06\n",
      "  Step 2500/4407 - Loss: 1.9349 | PPL: 6.92 | LR: 9.837079e-06\n",
      "  Step 2600/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 9.794805e-06\n",
      "  Step 2600/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 9.794805e-06\n",
      "  Step 2700/4407 - Loss: 1.9346 | PPL: 6.92 | LR: 9.752620e-06\n",
      "  Step 2700/4407 - Loss: 1.9346 | PPL: 6.92 | LR: 9.752620e-06\n",
      "  Step 2800/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 9.710523e-06\n",
      "  Step 2800/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 9.710523e-06\n",
      "  Step 2900/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 9.668513e-06\n",
      "  Step 2900/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 9.668513e-06\n",
      "  Step 3000/4407 - Loss: 1.9330 | PPL: 6.91 | LR: 9.626592e-06\n",
      "  Step 3000/4407 - Loss: 1.9330 | PPL: 6.91 | LR: 9.626592e-06\n",
      "  Step 3100/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 9.584758e-06\n",
      "  Step 3100/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 9.584758e-06\n",
      "  Step 3200/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 9.543013e-06\n",
      "  Step 3200/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 9.543013e-06\n",
      "  Step 3300/4407 - Loss: 1.9325 | PPL: 6.91 | LR: 9.501356e-06\n",
      "  Step 3300/4407 - Loss: 1.9325 | PPL: 6.91 | LR: 9.501356e-06\n",
      "  Step 3400/4407 - Loss: 1.9338 | PPL: 6.92 | LR: 9.459787e-06\n",
      "  Step 3400/4407 - Loss: 1.9338 | PPL: 6.92 | LR: 9.459787e-06\n",
      "  Step 3500/4407 - Loss: 1.9344 | PPL: 6.92 | LR: 9.418306e-06\n",
      "  Step 3500/4407 - Loss: 1.9344 | PPL: 6.92 | LR: 9.418306e-06\n",
      "  Step 3600/4407 - Loss: 1.9343 | PPL: 6.92 | LR: 9.376914e-06\n",
      "  Step 3600/4407 - Loss: 1.9343 | PPL: 6.92 | LR: 9.376914e-06\n",
      "  Step 3700/4407 - Loss: 1.9352 | PPL: 6.93 | LR: 9.335610e-06\n",
      "  Step 3700/4407 - Loss: 1.9352 | PPL: 6.93 | LR: 9.335610e-06\n",
      "  Step 3800/4407 - Loss: 1.9348 | PPL: 6.92 | LR: 9.294393e-06\n",
      "  Step 3800/4407 - Loss: 1.9348 | PPL: 6.92 | LR: 9.294393e-06\n",
      "  Step 3900/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 9.253266e-06\n",
      "  Step 3900/4407 - Loss: 1.9353 | PPL: 6.93 | LR: 9.253266e-06\n",
      "  Step 4000/4407 - Loss: 1.9335 | PPL: 6.91 | LR: 9.212226e-06\n",
      "  Step 4000/4407 - Loss: 1.9335 | PPL: 6.91 | LR: 9.212226e-06\n",
      "  Step 4100/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 9.171275e-06\n",
      "  Step 4100/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 9.171275e-06\n",
      "  Step 4200/4407 - Loss: 1.9330 | PPL: 6.91 | LR: 9.130412e-06\n",
      "  Step 4200/4407 - Loss: 1.9330 | PPL: 6.91 | LR: 9.130412e-06\n",
      "  Step 4300/4407 - Loss: 1.9331 | PPL: 6.91 | LR: 9.089638e-06\n",
      "  Step 4300/4407 - Loss: 1.9331 | PPL: 6.91 | LR: 9.089638e-06\n",
      "  Step 4400/4407 - Loss: 1.9332 | PPL: 6.91 | LR: 9.048952e-06\n",
      "  Step 4400/4407 - Loss: 1.9332 | PPL: 6.91 | LR: 9.048952e-06\n",
      "Epoch 90/100 | Train Loss: 1.9333 | Train PPL: 6.91 | Val Loss: 2.5942 | Val PPL: 13.39\n",
      "Epoch 90/100 | Train Loss: 1.9333 | Train PPL: 6.91 | Val Loss: 2.5942 | Val PPL: 13.39\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.91\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.91\n",
      "  Step 100/4407 - Loss: 1.9249 | PPL: 6.85 | LR: 9.005515e-06\n",
      "  Step 100/4407 - Loss: 1.9249 | PPL: 6.85 | LR: 9.005515e-06\n",
      "  Step 200/4407 - Loss: 1.9059 | PPL: 6.73 | LR: 8.965012e-06\n",
      "  Step 200/4407 - Loss: 1.9059 | PPL: 6.73 | LR: 8.965012e-06\n",
      "  Step 300/4407 - Loss: 1.9071 | PPL: 6.73 | LR: 8.924598e-06\n",
      "  Step 300/4407 - Loss: 1.9071 | PPL: 6.73 | LR: 8.924598e-06\n",
      "  Step 400/4407 - Loss: 1.9135 | PPL: 6.78 | LR: 8.884272e-06\n",
      "  Step 400/4407 - Loss: 1.9135 | PPL: 6.78 | LR: 8.884272e-06\n",
      "  Step 500/4407 - Loss: 1.9203 | PPL: 6.82 | LR: 8.844035e-06\n",
      "  Step 500/4407 - Loss: 1.9203 | PPL: 6.82 | LR: 8.844035e-06\n",
      "  Step 600/4407 - Loss: 1.9263 | PPL: 6.86 | LR: 8.803886e-06\n",
      "  Step 600/4407 - Loss: 1.9263 | PPL: 6.86 | LR: 8.803886e-06\n",
      "  Step 700/4407 - Loss: 1.9228 | PPL: 6.84 | LR: 8.763825e-06\n",
      "  Step 700/4407 - Loss: 1.9228 | PPL: 6.84 | LR: 8.763825e-06\n",
      "  Step 800/4407 - Loss: 1.9246 | PPL: 6.85 | LR: 8.723854e-06\n",
      "  Step 800/4407 - Loss: 1.9246 | PPL: 6.85 | LR: 8.723854e-06\n",
      "  Step 900/4407 - Loss: 1.9238 | PPL: 6.85 | LR: 8.683971e-06\n",
      "  Step 900/4407 - Loss: 1.9238 | PPL: 6.85 | LR: 8.683971e-06\n",
      "  Step 1000/4407 - Loss: 1.9242 | PPL: 6.85 | LR: 8.644176e-06\n",
      "  Step 1000/4407 - Loss: 1.9242 | PPL: 6.85 | LR: 8.644176e-06\n",
      "  Step 1100/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 8.604470e-06\n",
      "  Step 1100/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 8.604470e-06\n",
      "  Step 1200/4407 - Loss: 1.9306 | PPL: 6.89 | LR: 8.564853e-06\n",
      "  Step 1200/4407 - Loss: 1.9306 | PPL: 6.89 | LR: 8.564853e-06\n",
      "  Step 1300/4407 - Loss: 1.9285 | PPL: 6.88 | LR: 8.525325e-06\n",
      "  Step 1300/4407 - Loss: 1.9285 | PPL: 6.88 | LR: 8.525325e-06\n",
      "  Step 1400/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 8.485886e-06\n",
      "  Step 1400/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 8.485886e-06\n",
      "  Step 1500/4407 - Loss: 1.9341 | PPL: 6.92 | LR: 8.446535e-06\n",
      "  Step 1500/4407 - Loss: 1.9341 | PPL: 6.92 | LR: 8.446535e-06\n",
      "  Step 1600/4407 - Loss: 1.9317 | PPL: 6.90 | LR: 8.407273e-06\n",
      "  Step 1600/4407 - Loss: 1.9317 | PPL: 6.90 | LR: 8.407273e-06\n",
      "  Step 1700/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 8.368100e-06\n",
      "  Step 1700/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 8.368100e-06\n",
      "  Step 1800/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 8.329016e-06\n",
      "  Step 1800/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 8.329016e-06\n",
      "  Step 1900/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 8.290020e-06\n",
      "  Step 1900/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 8.290020e-06\n",
      "  Step 2000/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 8.251114e-06\n",
      "  Step 2000/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 8.251114e-06\n",
      "  Step 2100/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 8.212296e-06\n",
      "  Step 2100/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 8.212296e-06\n",
      "  Step 2200/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 8.173568e-06\n",
      "  Step 2200/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 8.173568e-06\n",
      "  Step 2300/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 8.134928e-06\n",
      "  Step 2300/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 8.134928e-06\n",
      "  Step 2400/4407 - Loss: 1.9289 | PPL: 6.88 | LR: 8.096377e-06\n",
      "  Step 2400/4407 - Loss: 1.9289 | PPL: 6.88 | LR: 8.096377e-06\n",
      "  Step 2500/4407 - Loss: 1.9293 | PPL: 6.88 | LR: 8.057916e-06\n",
      "  Step 2500/4407 - Loss: 1.9293 | PPL: 6.88 | LR: 8.057916e-06\n",
      "  Step 2600/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 8.019543e-06\n",
      "  Step 2600/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 8.019543e-06\n",
      "  Step 2700/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 7.981260e-06\n",
      "  Step 2700/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 7.981260e-06\n",
      "  Step 2800/4407 - Loss: 1.9313 | PPL: 6.90 | LR: 7.943065e-06\n",
      "  Step 2800/4407 - Loss: 1.9313 | PPL: 6.90 | LR: 7.943065e-06\n",
      "  Step 2900/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 7.904960e-06\n",
      "  Step 2900/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 7.904960e-06\n",
      "  Step 3000/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 7.866944e-06\n",
      "  Step 3000/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 7.866944e-06\n",
      "  Step 3100/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 7.829017e-06\n",
      "  Step 3100/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 7.829017e-06\n",
      "  Step 3200/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 7.791180e-06\n",
      "  Step 3200/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 7.791180e-06\n",
      "  Step 3300/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 7.753431e-06\n",
      "  Step 3300/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 7.753431e-06\n",
      "  Step 3400/4407 - Loss: 1.9324 | PPL: 6.91 | LR: 7.715772e-06\n",
      "  Step 3400/4407 - Loss: 1.9324 | PPL: 6.91 | LR: 7.715772e-06\n",
      "  Step 3500/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 7.678202e-06\n",
      "  Step 3500/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 7.678202e-06\n",
      "  Step 3600/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 7.640721e-06\n",
      "  Step 3600/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 7.640721e-06\n",
      "  Step 3700/4407 - Loss: 1.9307 | PPL: 6.89 | LR: 7.603330e-06\n",
      "  Step 3700/4407 - Loss: 1.9307 | PPL: 6.89 | LR: 7.603330e-06\n",
      "  Step 3800/4407 - Loss: 1.9325 | PPL: 6.91 | LR: 7.566028e-06\n",
      "  Step 3800/4407 - Loss: 1.9325 | PPL: 6.91 | LR: 7.566028e-06\n",
      "  Step 3900/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 7.528815e-06\n",
      "  Step 3900/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 7.528815e-06\n",
      "  Step 4000/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 7.491692e-06\n",
      "  Step 4000/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 7.491692e-06\n",
      "  Step 4100/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 7.454658e-06\n",
      "  Step 4100/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 7.454658e-06\n",
      "  Step 4200/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 7.417713e-06\n",
      "  Step 4200/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 7.417713e-06\n",
      "  Step 4300/4407 - Loss: 1.9319 | PPL: 6.90 | LR: 7.380858e-06\n",
      "  Step 4300/4407 - Loss: 1.9319 | PPL: 6.90 | LR: 7.380858e-06\n",
      "  Step 4400/4407 - Loss: 1.9319 | PPL: 6.90 | LR: 7.344093e-06\n",
      "  Step 4400/4407 - Loss: 1.9319 | PPL: 6.90 | LR: 7.344093e-06\n",
      "Epoch 91/100 | Train Loss: 1.9320 | Train PPL: 6.90 | Val Loss: 2.5915 | Val PPL: 13.35\n",
      "Epoch 91/100 | Train Loss: 1.9320 | Train PPL: 6.90 | Val Loss: 2.5915 | Val PPL: 13.35\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.90\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.90\n",
      "  Step 100/4407 - Loss: 1.9116 | PPL: 6.76 | LR: 7.304853e-06\n",
      "  Step 100/4407 - Loss: 1.9116 | PPL: 6.76 | LR: 7.304853e-06\n",
      "  Step 200/4407 - Loss: 1.8946 | PPL: 6.65 | LR: 7.268273e-06\n",
      "  Step 200/4407 - Loss: 1.8946 | PPL: 6.65 | LR: 7.268273e-06\n",
      "  Step 300/4407 - Loss: 1.9245 | PPL: 6.85 | LR: 7.231782e-06\n",
      "  Step 300/4407 - Loss: 1.9245 | PPL: 6.85 | LR: 7.231782e-06\n",
      "  Step 400/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 7.195381e-06\n",
      "  Step 400/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 7.195381e-06\n",
      "  Step 500/4407 - Loss: 1.9383 | PPL: 6.95 | LR: 7.159069e-06\n",
      "  Step 500/4407 - Loss: 1.9383 | PPL: 6.95 | LR: 7.159069e-06\n",
      "  Step 600/4407 - Loss: 1.9409 | PPL: 6.96 | LR: 7.122847e-06\n",
      "  Step 600/4407 - Loss: 1.9409 | PPL: 6.96 | LR: 7.122847e-06\n",
      "  Step 700/4407 - Loss: 1.9317 | PPL: 6.90 | LR: 7.086715e-06\n",
      "  Step 700/4407 - Loss: 1.9317 | PPL: 6.90 | LR: 7.086715e-06\n",
      "  Step 800/4407 - Loss: 1.9338 | PPL: 6.92 | LR: 7.050672e-06\n",
      "  Step 800/4407 - Loss: 1.9338 | PPL: 6.92 | LR: 7.050672e-06\n",
      "  Step 900/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 7.014719e-06\n",
      "  Step 900/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 7.014719e-06\n",
      "  Step 1000/4407 - Loss: 1.9261 | PPL: 6.86 | LR: 6.978856e-06\n",
      "  Step 1000/4407 - Loss: 1.9261 | PPL: 6.86 | LR: 6.978856e-06\n",
      "  Step 1100/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 6.943083e-06\n",
      "  Step 1100/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 6.943083e-06\n",
      "  Step 1200/4407 - Loss: 1.9268 | PPL: 6.87 | LR: 6.907399e-06\n",
      "  Step 1200/4407 - Loss: 1.9268 | PPL: 6.87 | LR: 6.907399e-06\n",
      "  Step 1300/4407 - Loss: 1.9289 | PPL: 6.88 | LR: 6.871805e-06\n",
      "  Step 1300/4407 - Loss: 1.9289 | PPL: 6.88 | LR: 6.871805e-06\n",
      "  Step 1400/4407 - Loss: 1.9269 | PPL: 6.87 | LR: 6.836301e-06\n",
      "  Step 1400/4407 - Loss: 1.9269 | PPL: 6.87 | LR: 6.836301e-06\n",
      "  Step 1500/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 6.800886e-06\n",
      "  Step 1500/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 6.800886e-06\n",
      "  Step 1600/4407 - Loss: 1.9289 | PPL: 6.88 | LR: 6.765562e-06\n",
      "  Step 1600/4407 - Loss: 1.9289 | PPL: 6.88 | LR: 6.765562e-06\n",
      "  Step 1700/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 6.730327e-06\n",
      "  Step 1700/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 6.730327e-06\n",
      "  Step 1800/4407 - Loss: 1.9297 | PPL: 6.89 | LR: 6.695182e-06\n",
      "  Step 1800/4407 - Loss: 1.9297 | PPL: 6.89 | LR: 6.695182e-06\n",
      "  Step 1900/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 6.660127e-06\n",
      "  Step 1900/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 6.660127e-06\n",
      "  Step 2000/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 6.625162e-06\n",
      "  Step 2000/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 6.625162e-06\n",
      "  Step 2100/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 6.590287e-06\n",
      "  Step 2100/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 6.590287e-06\n",
      "  Step 2200/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 6.555502e-06\n",
      "  Step 2200/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 6.555502e-06\n",
      "  Step 2300/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 6.520807e-06\n",
      "  Step 2300/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 6.520807e-06\n",
      "  Step 2400/4407 - Loss: 1.9278 | PPL: 6.87 | LR: 6.486202e-06\n",
      "  Step 2400/4407 - Loss: 1.9278 | PPL: 6.87 | LR: 6.486202e-06\n",
      "  Step 2500/4407 - Loss: 1.9274 | PPL: 6.87 | LR: 6.451687e-06\n",
      "  Step 2500/4407 - Loss: 1.9274 | PPL: 6.87 | LR: 6.451687e-06\n",
      "  Step 2600/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 6.417262e-06\n",
      "  Step 2600/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 6.417262e-06\n",
      "  Step 2700/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 6.382928e-06\n",
      "  Step 2700/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 6.382928e-06\n",
      "  Step 2800/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 6.348683e-06\n",
      "  Step 2800/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 6.348683e-06\n",
      "  Step 2900/4407 - Loss: 1.9283 | PPL: 6.88 | LR: 6.314528e-06\n",
      "  Step 2900/4407 - Loss: 1.9283 | PPL: 6.88 | LR: 6.314528e-06\n",
      "  Step 3000/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 6.280464e-06\n",
      "  Step 3000/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 6.280464e-06\n",
      "  Step 3100/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 6.246489e-06\n",
      "  Step 3100/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 6.246489e-06\n",
      "  Step 3200/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 6.212605e-06\n",
      "  Step 3200/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 6.212605e-06\n",
      "  Step 3300/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 6.178811e-06\n",
      "  Step 3300/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 6.178811e-06\n",
      "  Step 3400/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 6.145108e-06\n",
      "  Step 3400/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 6.145108e-06\n",
      "  Step 3500/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 6.111494e-06\n",
      "  Step 3500/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 6.111494e-06\n",
      "  Step 3600/4407 - Loss: 1.9306 | PPL: 6.89 | LR: 6.077971e-06\n",
      "  Step 3600/4407 - Loss: 1.9306 | PPL: 6.89 | LR: 6.077971e-06\n",
      "  Step 3700/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 6.044538e-06\n",
      "  Step 3700/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 6.044538e-06\n",
      "  Step 3800/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 6.011196e-06\n",
      "  Step 3800/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 6.011196e-06\n",
      "  Step 3900/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 5.977944e-06\n",
      "  Step 3900/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 5.977944e-06\n",
      "  Step 4000/4407 - Loss: 1.9305 | PPL: 6.89 | LR: 5.944782e-06\n",
      "  Step 4000/4407 - Loss: 1.9305 | PPL: 6.89 | LR: 5.944782e-06\n",
      "  Step 4100/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 5.911710e-06\n",
      "  Step 4100/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 5.911710e-06\n",
      "  Step 4200/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 5.878729e-06\n",
      "  Step 4200/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 5.878729e-06\n",
      "  Step 4300/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 5.845838e-06\n",
      "  Step 4300/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 5.845838e-06\n",
      "  Step 4400/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 5.813038e-06\n",
      "  Step 4400/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 5.813038e-06\n",
      "Epoch 92/100 | Train Loss: 1.9312 | Train PPL: 6.90 | Val Loss: 2.5923 | Val PPL: 13.36\n",
      "Epoch 92/100 | Train Loss: 1.9312 | Train PPL: 6.90 | Val Loss: 2.5923 | Val PPL: 13.36\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.90\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.90\n",
      "  Step 100/4407 - Loss: 1.9510 | PPL: 7.04 | LR: 5.778042e-06\n",
      "  Step 100/4407 - Loss: 1.9510 | PPL: 7.04 | LR: 5.778042e-06\n",
      "  Step 200/4407 - Loss: 1.9450 | PPL: 6.99 | LR: 5.745429e-06\n",
      "  Step 200/4407 - Loss: 1.9450 | PPL: 6.99 | LR: 5.745429e-06\n",
      "  Step 300/4407 - Loss: 1.9537 | PPL: 7.05 | LR: 5.712907e-06\n",
      "  Step 300/4407 - Loss: 1.9537 | PPL: 7.05 | LR: 5.712907e-06\n",
      "  Step 400/4407 - Loss: 1.9444 | PPL: 6.99 | LR: 5.680475e-06\n",
      "  Step 400/4407 - Loss: 1.9444 | PPL: 6.99 | LR: 5.680475e-06\n",
      "  Step 500/4407 - Loss: 1.9417 | PPL: 6.97 | LR: 5.648133e-06\n",
      "  Step 500/4407 - Loss: 1.9417 | PPL: 6.97 | LR: 5.648133e-06\n",
      "  Step 600/4407 - Loss: 1.9375 | PPL: 6.94 | LR: 5.615883e-06\n",
      "  Step 600/4407 - Loss: 1.9375 | PPL: 6.94 | LR: 5.615883e-06\n",
      "  Step 700/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 5.583722e-06\n",
      "  Step 700/4407 - Loss: 1.9367 | PPL: 6.94 | LR: 5.583722e-06\n",
      "  Step 800/4407 - Loss: 1.9351 | PPL: 6.92 | LR: 5.551653e-06\n",
      "  Step 800/4407 - Loss: 1.9351 | PPL: 6.92 | LR: 5.551653e-06\n",
      "  Step 900/4407 - Loss: 1.9338 | PPL: 6.92 | LR: 5.519674e-06\n",
      "  Step 900/4407 - Loss: 1.9338 | PPL: 6.92 | LR: 5.519674e-06\n",
      "  Step 1000/4407 - Loss: 1.9344 | PPL: 6.92 | LR: 5.487785e-06\n",
      "  Step 1000/4407 - Loss: 1.9344 | PPL: 6.92 | LR: 5.487785e-06\n",
      "  Step 1100/4407 - Loss: 1.9325 | PPL: 6.91 | LR: 5.455988e-06\n",
      "  Step 1100/4407 - Loss: 1.9325 | PPL: 6.91 | LR: 5.455988e-06\n",
      "  Step 1200/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 5.424281e-06\n",
      "  Step 1200/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 5.424281e-06\n",
      "  Step 1300/4407 - Loss: 1.9326 | PPL: 6.91 | LR: 5.392664e-06\n",
      "  Step 1300/4407 - Loss: 1.9326 | PPL: 6.91 | LR: 5.392664e-06\n",
      "  Step 1400/4407 - Loss: 1.9327 | PPL: 6.91 | LR: 5.361139e-06\n",
      "  Step 1400/4407 - Loss: 1.9327 | PPL: 6.91 | LR: 5.361139e-06\n",
      "  Step 1500/4407 - Loss: 1.9337 | PPL: 6.91 | LR: 5.329704e-06\n",
      "  Step 1500/4407 - Loss: 1.9337 | PPL: 6.91 | LR: 5.329704e-06\n",
      "  Step 1600/4407 - Loss: 1.9337 | PPL: 6.92 | LR: 5.298360e-06\n",
      "  Step 1600/4407 - Loss: 1.9337 | PPL: 6.92 | LR: 5.298360e-06\n",
      "  Step 1700/4407 - Loss: 1.9324 | PPL: 6.91 | LR: 5.267106e-06\n",
      "  Step 1700/4407 - Loss: 1.9324 | PPL: 6.91 | LR: 5.267106e-06\n",
      "  Step 1800/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 5.235944e-06\n",
      "  Step 1800/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 5.235944e-06\n",
      "  Step 1900/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 5.204872e-06\n",
      "  Step 1900/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 5.204872e-06\n",
      "  Step 2000/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 5.173891e-06\n",
      "  Step 2000/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 5.173891e-06\n",
      "  Step 2100/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 5.143001e-06\n",
      "  Step 2100/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 5.143001e-06\n",
      "  Step 2200/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 5.112202e-06\n",
      "  Step 2200/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 5.112202e-06\n",
      "  Step 2300/4407 - Loss: 1.9307 | PPL: 6.89 | LR: 5.081494e-06\n",
      "  Step 2300/4407 - Loss: 1.9307 | PPL: 6.89 | LR: 5.081494e-06\n",
      "  Step 2400/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 5.050877e-06\n",
      "  Step 2400/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 5.050877e-06\n",
      "  Step 2500/4407 - Loss: 1.9285 | PPL: 6.88 | LR: 5.020351e-06\n",
      "  Step 2500/4407 - Loss: 1.9285 | PPL: 6.88 | LR: 5.020351e-06\n",
      "  Step 2600/4407 - Loss: 1.9293 | PPL: 6.88 | LR: 4.989915e-06\n",
      "  Step 2600/4407 - Loss: 1.9293 | PPL: 6.88 | LR: 4.989915e-06\n",
      "  Step 2700/4407 - Loss: 1.9292 | PPL: 6.88 | LR: 4.959571e-06\n",
      "  Step 2700/4407 - Loss: 1.9292 | PPL: 6.88 | LR: 4.959571e-06\n",
      "  Step 2800/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 4.929317e-06\n",
      "  Step 2800/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 4.929317e-06\n",
      "  Step 2900/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 4.899155e-06\n",
      "  Step 2900/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 4.899155e-06\n",
      "  Step 3000/4407 - Loss: 1.9292 | PPL: 6.88 | LR: 4.869084e-06\n",
      "  Step 3000/4407 - Loss: 1.9292 | PPL: 6.88 | LR: 4.869084e-06\n",
      "  Step 3100/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 4.839103e-06\n",
      "  Step 3100/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 4.839103e-06\n",
      "  Step 3200/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 4.809214e-06\n",
      "  Step 3200/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 4.809214e-06\n",
      "  Step 3300/4407 - Loss: 1.9292 | PPL: 6.88 | LR: 4.779416e-06\n",
      "  Step 3300/4407 - Loss: 1.9292 | PPL: 6.88 | LR: 4.779416e-06\n",
      "  Step 3400/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 4.749709e-06\n",
      "  Step 3400/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 4.749709e-06\n",
      "  Step 3500/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 4.720093e-06\n",
      "  Step 3500/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 4.720093e-06\n",
      "  Step 3600/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 4.690568e-06\n",
      "  Step 3600/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 4.690568e-06\n",
      "  Step 3700/4407 - Loss: 1.9275 | PPL: 6.87 | LR: 4.661134e-06\n",
      "  Step 3700/4407 - Loss: 1.9275 | PPL: 6.87 | LR: 4.661134e-06\n",
      "  Step 3800/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 4.631792e-06\n",
      "  Step 3800/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 4.631792e-06\n",
      "  Step 3900/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 4.602541e-06\n",
      "  Step 3900/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 4.602541e-06\n",
      "  Step 4000/4407 - Loss: 1.9281 | PPL: 6.88 | LR: 4.573381e-06\n",
      "  Step 4000/4407 - Loss: 1.9281 | PPL: 6.88 | LR: 4.573381e-06\n",
      "  Step 4100/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 4.544312e-06\n",
      "  Step 4100/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 4.544312e-06\n",
      "  Step 4200/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 4.515334e-06\n",
      "  Step 4200/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 4.515334e-06\n",
      "  Step 4300/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 4.486448e-06\n",
      "  Step 4300/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 4.486448e-06\n",
      "  Step 4400/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 4.457653e-06\n",
      "  Step 4400/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 4.457653e-06\n",
      "Epoch 93/100 | Train Loss: 1.9303 | Train PPL: 6.89 | Val Loss: 2.5934 | Val PPL: 13.38\n",
      "Epoch 93/100 | Train Loss: 1.9303 | Train PPL: 6.89 | Val Loss: 2.5934 | Val PPL: 13.38\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.89\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.89\n",
      "  Step 100/4407 - Loss: 1.9386 | PPL: 6.95 | LR: 4.426944e-06\n",
      "  Step 100/4407 - Loss: 1.9386 | PPL: 6.95 | LR: 4.426944e-06\n",
      "  Step 200/4407 - Loss: 1.9326 | PPL: 6.91 | LR: 4.398338e-06\n",
      "  Step 200/4407 - Loss: 1.9326 | PPL: 6.91 | LR: 4.398338e-06\n",
      "  Step 300/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 4.369823e-06\n",
      "  Step 300/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 4.369823e-06\n",
      "  Step 400/4407 - Loss: 1.9371 | PPL: 6.94 | LR: 4.341400e-06\n",
      "  Step 400/4407 - Loss: 1.9371 | PPL: 6.94 | LR: 4.341400e-06\n",
      "  Step 500/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 4.313068e-06\n",
      "  Step 500/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 4.313068e-06\n",
      "  Step 600/4407 - Loss: 1.9352 | PPL: 6.93 | LR: 4.284828e-06\n",
      "  Step 600/4407 - Loss: 1.9352 | PPL: 6.93 | LR: 4.284828e-06\n",
      "  Step 700/4407 - Loss: 1.9366 | PPL: 6.94 | LR: 4.256679e-06\n",
      "  Step 700/4407 - Loss: 1.9366 | PPL: 6.94 | LR: 4.256679e-06\n",
      "  Step 800/4407 - Loss: 1.9360 | PPL: 6.93 | LR: 4.228621e-06\n",
      "  Step 800/4407 - Loss: 1.9360 | PPL: 6.93 | LR: 4.228621e-06\n",
      "  Step 900/4407 - Loss: 1.9361 | PPL: 6.93 | LR: 4.200655e-06\n",
      "  Step 900/4407 - Loss: 1.9361 | PPL: 6.93 | LR: 4.200655e-06\n",
      "  Step 1000/4407 - Loss: 1.9332 | PPL: 6.91 | LR: 4.172780e-06\n",
      "  Step 1000/4407 - Loss: 1.9332 | PPL: 6.91 | LR: 4.172780e-06\n",
      "  Step 1100/4407 - Loss: 1.9306 | PPL: 6.89 | LR: 4.144997e-06\n",
      "  Step 1100/4407 - Loss: 1.9306 | PPL: 6.89 | LR: 4.144997e-06\n",
      "  Step 1200/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 4.117306e-06\n",
      "  Step 1200/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 4.117306e-06\n",
      "  Step 1300/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 4.089706e-06\n",
      "  Step 1300/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 4.089706e-06\n",
      "  Step 1400/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 4.062197e-06\n",
      "  Step 1400/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 4.062197e-06\n",
      "  Step 1500/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 4.034780e-06\n",
      "  Step 1500/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 4.034780e-06\n",
      "  Step 1600/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 4.007454e-06\n",
      "  Step 1600/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 4.007454e-06\n",
      "  Step 1700/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 3.980221e-06\n",
      "  Step 1700/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 3.980221e-06\n",
      "  Step 1800/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 3.953078e-06\n",
      "  Step 1800/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 3.953078e-06\n",
      "  Step 1900/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 3.926028e-06\n",
      "  Step 1900/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 3.926028e-06\n",
      "  Step 2000/4407 - Loss: 1.9285 | PPL: 6.88 | LR: 3.899069e-06\n",
      "  Step 2000/4407 - Loss: 1.9285 | PPL: 6.88 | LR: 3.899069e-06\n",
      "  Step 2100/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 3.872201e-06\n",
      "  Step 2100/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 3.872201e-06\n",
      "  Step 2200/4407 - Loss: 1.9298 | PPL: 6.89 | LR: 3.845426e-06\n",
      "  Step 2200/4407 - Loss: 1.9298 | PPL: 6.89 | LR: 3.845426e-06\n",
      "  Step 2300/4407 - Loss: 1.9324 | PPL: 6.91 | LR: 3.818742e-06\n",
      "  Step 2300/4407 - Loss: 1.9324 | PPL: 6.91 | LR: 3.818742e-06\n",
      "  Step 2400/4407 - Loss: 1.9324 | PPL: 6.91 | LR: 3.792150e-06\n",
      "  Step 2400/4407 - Loss: 1.9324 | PPL: 6.91 | LR: 3.792150e-06\n",
      "  Step 2500/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 3.765649e-06\n",
      "  Step 2500/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 3.765649e-06\n",
      "  Step 2600/4407 - Loss: 1.9333 | PPL: 6.91 | LR: 3.739240e-06\n",
      "  Step 2600/4407 - Loss: 1.9333 | PPL: 6.91 | LR: 3.739240e-06\n",
      "  Step 2700/4407 - Loss: 1.9332 | PPL: 6.91 | LR: 3.712923e-06\n",
      "  Step 2700/4407 - Loss: 1.9332 | PPL: 6.91 | LR: 3.712923e-06\n",
      "  Step 2800/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 3.686698e-06\n",
      "  Step 2800/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 3.686698e-06\n",
      "  Step 2900/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 3.660565e-06\n",
      "  Step 2900/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 3.660565e-06\n",
      "  Step 3000/4407 - Loss: 1.9317 | PPL: 6.90 | LR: 3.634523e-06\n",
      "  Step 3000/4407 - Loss: 1.9317 | PPL: 6.90 | LR: 3.634523e-06\n",
      "  Step 3100/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 3.608573e-06\n",
      "  Step 3100/4407 - Loss: 1.9318 | PPL: 6.90 | LR: 3.608573e-06\n",
      "  Step 3200/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 3.582715e-06\n",
      "  Step 3200/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 3.582715e-06\n",
      "  Step 3300/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 3.556949e-06\n",
      "  Step 3300/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 3.556949e-06\n",
      "  Step 3400/4407 - Loss: 1.9307 | PPL: 6.89 | LR: 3.531275e-06\n",
      "  Step 3400/4407 - Loss: 1.9307 | PPL: 6.89 | LR: 3.531275e-06\n",
      "  Step 3500/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 3.505693e-06\n",
      "  Step 3500/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 3.505693e-06\n",
      "  Step 3600/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 3.480202e-06\n",
      "  Step 3600/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 3.480202e-06\n",
      "  Step 3700/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 3.454804e-06\n",
      "  Step 3700/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 3.454804e-06\n",
      "  Step 3800/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 3.429497e-06\n",
      "  Step 3800/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 3.429497e-06\n",
      "  Step 3900/4407 - Loss: 1.9301 | PPL: 6.89 | LR: 3.404282e-06\n",
      "  Step 3900/4407 - Loss: 1.9301 | PPL: 6.89 | LR: 3.404282e-06\n",
      "  Step 4000/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 3.379160e-06\n",
      "  Step 4000/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 3.379160e-06\n",
      "  Step 4100/4407 - Loss: 1.9293 | PPL: 6.88 | LR: 3.354129e-06\n",
      "  Step 4100/4407 - Loss: 1.9293 | PPL: 6.88 | LR: 3.354129e-06\n",
      "  Step 4200/4407 - Loss: 1.9298 | PPL: 6.89 | LR: 3.329190e-06\n",
      "  Step 4200/4407 - Loss: 1.9298 | PPL: 6.89 | LR: 3.329190e-06\n",
      "  Step 4300/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 3.304344e-06\n",
      "  Step 4300/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 3.304344e-06\n",
      "  Step 4400/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 3.279589e-06\n",
      "  Step 4400/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 3.279589e-06\n",
      "Epoch 94/100 | Train Loss: 1.9298 | Train PPL: 6.89 | Val Loss: 2.5937 | Val PPL: 13.38\n",
      "Epoch 94/100 | Train Loss: 1.9298 | Train PPL: 6.89 | Val Loss: 2.5937 | Val PPL: 13.38\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.89\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.89\n",
      "  Step 100/4407 - Loss: 1.9440 | PPL: 6.99 | LR: 3.253204e-06\n",
      "  Step 100/4407 - Loss: 1.9440 | PPL: 6.99 | LR: 3.253204e-06\n",
      "  Step 200/4407 - Loss: 1.9584 | PPL: 7.09 | LR: 3.228640e-06\n",
      "  Step 200/4407 - Loss: 1.9584 | PPL: 7.09 | LR: 3.228640e-06\n",
      "  Step 300/4407 - Loss: 1.9553 | PPL: 7.07 | LR: 3.204168e-06\n",
      "  Step 300/4407 - Loss: 1.9553 | PPL: 7.07 | LR: 3.204168e-06\n",
      "  Step 400/4407 - Loss: 1.9521 | PPL: 7.04 | LR: 3.179788e-06\n",
      "  Step 400/4407 - Loss: 1.9521 | PPL: 7.04 | LR: 3.179788e-06\n",
      "  Step 500/4407 - Loss: 1.9436 | PPL: 6.98 | LR: 3.155500e-06\n",
      "  Step 500/4407 - Loss: 1.9436 | PPL: 6.98 | LR: 3.155500e-06\n",
      "  Step 600/4407 - Loss: 1.9402 | PPL: 6.96 | LR: 3.131305e-06\n",
      "  Step 600/4407 - Loss: 1.9402 | PPL: 6.96 | LR: 3.131305e-06\n",
      "  Step 700/4407 - Loss: 1.9333 | PPL: 6.91 | LR: 3.107201e-06\n",
      "  Step 700/4407 - Loss: 1.9333 | PPL: 6.91 | LR: 3.107201e-06\n",
      "  Step 800/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 3.083190e-06\n",
      "  Step 800/4407 - Loss: 1.9323 | PPL: 6.91 | LR: 3.083190e-06\n",
      "  Step 900/4407 - Loss: 1.9320 | PPL: 6.90 | LR: 3.059271e-06\n",
      "  Step 900/4407 - Loss: 1.9320 | PPL: 6.90 | LR: 3.059271e-06\n",
      "  Step 1000/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 3.035444e-06\n",
      "  Step 1000/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 3.035444e-06\n",
      "  Step 1100/4407 - Loss: 1.9307 | PPL: 6.89 | LR: 3.011709e-06\n",
      "  Step 1100/4407 - Loss: 1.9307 | PPL: 6.89 | LR: 3.011709e-06\n",
      "  Step 1200/4407 - Loss: 1.9333 | PPL: 6.91 | LR: 2.988066e-06\n",
      "  Step 1200/4407 - Loss: 1.9333 | PPL: 6.91 | LR: 2.988066e-06\n",
      "  Step 1300/4407 - Loss: 1.9350 | PPL: 6.92 | LR: 2.964516e-06\n",
      "  Step 1300/4407 - Loss: 1.9350 | PPL: 6.92 | LR: 2.964516e-06\n",
      "  Step 1400/4407 - Loss: 1.9346 | PPL: 6.92 | LR: 2.941058e-06\n",
      "  Step 1400/4407 - Loss: 1.9346 | PPL: 6.92 | LR: 2.941058e-06\n",
      "  Step 1500/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 2.917692e-06\n",
      "  Step 1500/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 2.917692e-06\n",
      "  Step 1600/4407 - Loss: 1.9313 | PPL: 6.90 | LR: 2.894419e-06\n",
      "  Step 1600/4407 - Loss: 1.9313 | PPL: 6.90 | LR: 2.894419e-06\n",
      "  Step 1700/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 2.871237e-06\n",
      "  Step 1700/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 2.871237e-06\n",
      "  Step 1800/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 2.848148e-06\n",
      "  Step 1800/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 2.848148e-06\n",
      "  Step 1900/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 2.825152e-06\n",
      "  Step 1900/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 2.825152e-06\n",
      "  Step 2000/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 2.802247e-06\n",
      "  Step 2000/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 2.802247e-06\n",
      "  Step 2100/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 2.779436e-06\n",
      "  Step 2100/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 2.779436e-06\n",
      "  Step 2200/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 2.756716e-06\n",
      "  Step 2200/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 2.756716e-06\n",
      "  Step 2300/4407 - Loss: 1.9243 | PPL: 6.85 | LR: 2.734089e-06\n",
      "  Step 2300/4407 - Loss: 1.9243 | PPL: 6.85 | LR: 2.734089e-06\n",
      "  Step 2400/4407 - Loss: 1.9243 | PPL: 6.85 | LR: 2.711554e-06\n",
      "  Step 2400/4407 - Loss: 1.9243 | PPL: 6.85 | LR: 2.711554e-06\n",
      "  Step 2500/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 2.689112e-06\n",
      "  Step 2500/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 2.689112e-06\n",
      "  Step 2600/4407 - Loss: 1.9281 | PPL: 6.88 | LR: 2.666762e-06\n",
      "  Step 2600/4407 - Loss: 1.9281 | PPL: 6.88 | LR: 2.666762e-06\n",
      "  Step 2700/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 2.644504e-06\n",
      "  Step 2700/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 2.644504e-06\n",
      "  Step 2800/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 2.622339e-06\n",
      "  Step 2800/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 2.622339e-06\n",
      "  Step 2900/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 2.600266e-06\n",
      "  Step 2900/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 2.600266e-06\n",
      "  Step 3000/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 2.578286e-06\n",
      "  Step 3000/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 2.578286e-06\n",
      "  Step 3100/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 2.556399e-06\n",
      "  Step 3100/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 2.556399e-06\n",
      "  Step 3200/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 2.534603e-06\n",
      "  Step 3200/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 2.534603e-06\n",
      "  Step 3300/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 2.512901e-06\n",
      "  Step 3300/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 2.512901e-06\n",
      "  Step 3400/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 2.491291e-06\n",
      "  Step 3400/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 2.491291e-06\n",
      "  Step 3500/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 2.469773e-06\n",
      "  Step 3500/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 2.469773e-06\n",
      "  Step 3600/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 2.448348e-06\n",
      "  Step 3600/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 2.448348e-06\n",
      "  Step 3700/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 2.427016e-06\n",
      "  Step 3700/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 2.427016e-06\n",
      "  Step 3800/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 2.405776e-06\n",
      "  Step 3800/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 2.405776e-06\n",
      "  Step 3900/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 2.384628e-06\n",
      "  Step 3900/4407 - Loss: 1.9302 | PPL: 6.89 | LR: 2.384628e-06\n",
      "  Step 4000/4407 - Loss: 1.9306 | PPL: 6.89 | LR: 2.363574e-06\n",
      "  Step 4000/4407 - Loss: 1.9306 | PPL: 6.89 | LR: 2.363574e-06\n",
      "  Step 4100/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 2.342612e-06\n",
      "  Step 4100/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 2.342612e-06\n",
      "  Step 4200/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 2.321742e-06\n",
      "  Step 4200/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 2.321742e-06\n",
      "  Step 4300/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 2.300966e-06\n",
      "  Step 4300/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 2.300966e-06\n",
      "  Step 4400/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 2.280281e-06\n",
      "  Step 4400/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 2.280281e-06\n",
      "Epoch 95/100 | Train Loss: 1.9291 | Train PPL: 6.88 | Val Loss: 2.5934 | Val PPL: 13.38\n",
      "Epoch 95/100 | Train Loss: 1.9291 | Train PPL: 6.88 | Val Loss: 2.5934 | Val PPL: 13.38\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.88\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.88\n",
      "  Step 100/4407 - Loss: 1.9213 | PPL: 6.83 | LR: 2.258252e-06\n",
      "  Step 100/4407 - Loss: 1.9213 | PPL: 6.83 | LR: 2.258252e-06\n",
      "  Step 200/4407 - Loss: 1.9081 | PPL: 6.74 | LR: 2.237760e-06\n",
      "  Step 200/4407 - Loss: 1.9081 | PPL: 6.74 | LR: 2.237760e-06\n",
      "  Step 300/4407 - Loss: 1.9224 | PPL: 6.84 | LR: 2.217360e-06\n",
      "  Step 300/4407 - Loss: 1.9224 | PPL: 6.84 | LR: 2.217360e-06\n",
      "  Step 400/4407 - Loss: 1.9172 | PPL: 6.80 | LR: 2.197054e-06\n",
      "  Step 400/4407 - Loss: 1.9172 | PPL: 6.80 | LR: 2.197054e-06\n",
      "  Step 500/4407 - Loss: 1.9231 | PPL: 6.84 | LR: 2.176839e-06\n",
      "  Step 500/4407 - Loss: 1.9231 | PPL: 6.84 | LR: 2.176839e-06\n",
      "  Step 600/4407 - Loss: 1.9189 | PPL: 6.81 | LR: 2.156718e-06\n",
      "  Step 600/4407 - Loss: 1.9189 | PPL: 6.81 | LR: 2.156718e-06\n",
      "  Step 700/4407 - Loss: 1.9225 | PPL: 6.84 | LR: 2.136690e-06\n",
      "  Step 700/4407 - Loss: 1.9225 | PPL: 6.84 | LR: 2.136690e-06\n",
      "  Step 800/4407 - Loss: 1.9197 | PPL: 6.82 | LR: 2.116754e-06\n",
      "  Step 800/4407 - Loss: 1.9197 | PPL: 6.82 | LR: 2.116754e-06\n",
      "  Step 900/4407 - Loss: 1.9200 | PPL: 6.82 | LR: 2.096911e-06\n",
      "  Step 900/4407 - Loss: 1.9200 | PPL: 6.82 | LR: 2.096911e-06\n",
      "  Step 1000/4407 - Loss: 1.9211 | PPL: 6.83 | LR: 2.077160e-06\n",
      "  Step 1000/4407 - Loss: 1.9211 | PPL: 6.83 | LR: 2.077160e-06\n",
      "  Step 1100/4407 - Loss: 1.9213 | PPL: 6.83 | LR: 2.057503e-06\n",
      "  Step 1100/4407 - Loss: 1.9213 | PPL: 6.83 | LR: 2.057503e-06\n",
      "  Step 1200/4407 - Loss: 1.9236 | PPL: 6.85 | LR: 2.037938e-06\n",
      "  Step 1200/4407 - Loss: 1.9236 | PPL: 6.85 | LR: 2.037938e-06\n",
      "  Step 1300/4407 - Loss: 1.9244 | PPL: 6.85 | LR: 2.018467e-06\n",
      "  Step 1300/4407 - Loss: 1.9244 | PPL: 6.85 | LR: 2.018467e-06\n",
      "  Step 1400/4407 - Loss: 1.9252 | PPL: 6.86 | LR: 1.999088e-06\n",
      "  Step 1400/4407 - Loss: 1.9252 | PPL: 6.86 | LR: 1.999088e-06\n",
      "  Step 1500/4407 - Loss: 1.9265 | PPL: 6.87 | LR: 1.979801e-06\n",
      "  Step 1500/4407 - Loss: 1.9265 | PPL: 6.87 | LR: 1.979801e-06\n",
      "  Step 1600/4407 - Loss: 1.9240 | PPL: 6.85 | LR: 1.960608e-06\n",
      "  Step 1600/4407 - Loss: 1.9240 | PPL: 6.85 | LR: 1.960608e-06\n",
      "  Step 1700/4407 - Loss: 1.9228 | PPL: 6.84 | LR: 1.941508e-06\n",
      "  Step 1700/4407 - Loss: 1.9228 | PPL: 6.84 | LR: 1.941508e-06\n",
      "  Step 1800/4407 - Loss: 1.9244 | PPL: 6.85 | LR: 1.922500e-06\n",
      "  Step 1800/4407 - Loss: 1.9244 | PPL: 6.85 | LR: 1.922500e-06\n",
      "  Step 1900/4407 - Loss: 1.9254 | PPL: 6.86 | LR: 1.903586e-06\n",
      "  Step 1900/4407 - Loss: 1.9254 | PPL: 6.86 | LR: 1.903586e-06\n",
      "  Step 2000/4407 - Loss: 1.9252 | PPL: 6.86 | LR: 1.884764e-06\n",
      "  Step 2000/4407 - Loss: 1.9252 | PPL: 6.86 | LR: 1.884764e-06\n",
      "  Step 2100/4407 - Loss: 1.9261 | PPL: 6.86 | LR: 1.866035e-06\n",
      "  Step 2100/4407 - Loss: 1.9261 | PPL: 6.86 | LR: 1.866035e-06\n",
      "  Step 2200/4407 - Loss: 1.9264 | PPL: 6.86 | LR: 1.847399e-06\n",
      "  Step 2200/4407 - Loss: 1.9264 | PPL: 6.86 | LR: 1.847399e-06\n",
      "  Step 2300/4407 - Loss: 1.9272 | PPL: 6.87 | LR: 1.828857e-06\n",
      "  Step 2300/4407 - Loss: 1.9272 | PPL: 6.87 | LR: 1.828857e-06\n",
      "  Step 2400/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 1.810407e-06\n",
      "  Step 2400/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 1.810407e-06\n",
      "  Step 2500/4407 - Loss: 1.9281 | PPL: 6.88 | LR: 1.792050e-06\n",
      "  Step 2500/4407 - Loss: 1.9281 | PPL: 6.88 | LR: 1.792050e-06\n",
      "  Step 2600/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 1.773786e-06\n",
      "  Step 2600/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 1.773786e-06\n",
      "  Step 2700/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 1.755615e-06\n",
      "  Step 2700/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 1.755615e-06\n",
      "  Step 2800/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 1.737537e-06\n",
      "  Step 2800/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 1.737537e-06\n",
      "  Step 2900/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 1.719552e-06\n",
      "  Step 2900/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 1.719552e-06\n",
      "  Step 3000/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 1.701660e-06\n",
      "  Step 3000/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 1.701660e-06\n",
      "  Step 3100/4407 - Loss: 1.9274 | PPL: 6.87 | LR: 1.683861e-06\n",
      "  Step 3100/4407 - Loss: 1.9274 | PPL: 6.87 | LR: 1.683861e-06\n",
      "  Step 3200/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 1.666155e-06\n",
      "  Step 3200/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 1.666155e-06\n",
      "  Step 3300/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 1.648543e-06\n",
      "  Step 3300/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 1.648543e-06\n",
      "  Step 3400/4407 - Loss: 1.9275 | PPL: 6.87 | LR: 1.631023e-06\n",
      "  Step 3400/4407 - Loss: 1.9275 | PPL: 6.87 | LR: 1.631023e-06\n",
      "  Step 3500/4407 - Loss: 1.9264 | PPL: 6.86 | LR: 1.613596e-06\n",
      "  Step 3500/4407 - Loss: 1.9264 | PPL: 6.86 | LR: 1.613596e-06\n",
      "  Step 3600/4407 - Loss: 1.9263 | PPL: 6.86 | LR: 1.596263e-06\n",
      "  Step 3600/4407 - Loss: 1.9263 | PPL: 6.86 | LR: 1.596263e-06\n",
      "  Step 3700/4407 - Loss: 1.9260 | PPL: 6.86 | LR: 1.579022e-06\n",
      "  Step 3700/4407 - Loss: 1.9260 | PPL: 6.86 | LR: 1.579022e-06\n",
      "  Step 3800/4407 - Loss: 1.9260 | PPL: 6.86 | LR: 1.561875e-06\n",
      "  Step 3800/4407 - Loss: 1.9260 | PPL: 6.86 | LR: 1.561875e-06\n",
      "  Step 3900/4407 - Loss: 1.9266 | PPL: 6.87 | LR: 1.544821e-06\n",
      "  Step 3900/4407 - Loss: 1.9266 | PPL: 6.87 | LR: 1.544821e-06\n",
      "  Step 4000/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 1.527860e-06\n",
      "  Step 4000/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 1.527860e-06\n",
      "  Step 4100/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 1.510992e-06\n",
      "  Step 4100/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 1.510992e-06\n",
      "  Step 4200/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 1.494217e-06\n",
      "  Step 4200/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 1.494217e-06\n",
      "  Step 4300/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 1.477536e-06\n",
      "  Step 4300/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 1.477536e-06\n",
      "  Step 4400/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 1.460947e-06\n",
      "  Step 4400/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 1.460947e-06\n",
      "Epoch 96/100 | Train Loss: 1.9282 | Train PPL: 6.88 | Val Loss: 2.5937 | Val PPL: 13.38\n",
      "Epoch 96/100 | Train Loss: 1.9282 | Train PPL: 6.88 | Val Loss: 2.5937 | Val PPL: 13.38\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.88\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.88\n",
      "  Step 100/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 1.443301e-06\n",
      "  Step 100/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 1.443301e-06\n",
      "  Step 200/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 1.426906e-06\n",
      "  Step 200/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 1.426906e-06\n",
      "  Step 300/4407 - Loss: 1.9170 | PPL: 6.80 | LR: 1.410603e-06\n",
      "  Step 300/4407 - Loss: 1.9170 | PPL: 6.80 | LR: 1.410603e-06\n",
      "  Step 400/4407 - Loss: 1.9228 | PPL: 6.84 | LR: 1.394394e-06\n",
      "  Step 400/4407 - Loss: 1.9228 | PPL: 6.84 | LR: 1.394394e-06\n",
      "  Step 500/4407 - Loss: 1.9230 | PPL: 6.84 | LR: 1.378279e-06\n",
      "  Step 500/4407 - Loss: 1.9230 | PPL: 6.84 | LR: 1.378279e-06\n",
      "  Step 600/4407 - Loss: 1.9266 | PPL: 6.87 | LR: 1.362256e-06\n",
      "  Step 600/4407 - Loss: 1.9266 | PPL: 6.87 | LR: 1.362256e-06\n",
      "  Step 700/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 1.346327e-06\n",
      "  Step 700/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 1.346327e-06\n",
      "  Step 800/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 1.330491e-06\n",
      "  Step 800/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 1.330491e-06\n",
      "  Step 900/4407 - Loss: 1.9325 | PPL: 6.91 | LR: 1.314748e-06\n",
      "  Step 900/4407 - Loss: 1.9325 | PPL: 6.91 | LR: 1.314748e-06\n",
      "  Step 1000/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 1.299098e-06\n",
      "  Step 1000/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 1.299098e-06\n",
      "  Step 1100/4407 - Loss: 1.9279 | PPL: 6.87 | LR: 1.283542e-06\n",
      "  Step 1100/4407 - Loss: 1.9279 | PPL: 6.87 | LR: 1.283542e-06\n",
      "  Step 1200/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 1.268079e-06\n",
      "  Step 1200/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 1.268079e-06\n",
      "  Step 1300/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 1.252710e-06\n",
      "  Step 1300/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 1.252710e-06\n",
      "  Step 1400/4407 - Loss: 1.9326 | PPL: 6.91 | LR: 1.237433e-06\n",
      "  Step 1400/4407 - Loss: 1.9326 | PPL: 6.91 | LR: 1.237433e-06\n",
      "  Step 1500/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 1.222251e-06\n",
      "  Step 1500/4407 - Loss: 1.9290 | PPL: 6.88 | LR: 1.222251e-06\n",
      "  Step 1600/4407 - Loss: 1.9269 | PPL: 6.87 | LR: 1.207161e-06\n",
      "  Step 1600/4407 - Loss: 1.9269 | PPL: 6.87 | LR: 1.207161e-06\n",
      "  Step 1700/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 1.192165e-06\n",
      "  Step 1700/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 1.192165e-06\n",
      "  Step 1800/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 1.177262e-06\n",
      "  Step 1800/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 1.177262e-06\n",
      "  Step 1900/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 1.162452e-06\n",
      "  Step 1900/4407 - Loss: 1.9309 | PPL: 6.90 | LR: 1.162452e-06\n",
      "  Step 2000/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 1.147736e-06\n",
      "  Step 2000/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 1.147736e-06\n",
      "  Step 2100/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 1.133113e-06\n",
      "  Step 2100/4407 - Loss: 1.9310 | PPL: 6.90 | LR: 1.133113e-06\n",
      "  Step 2200/4407 - Loss: 1.9313 | PPL: 6.90 | LR: 1.118584e-06\n",
      "  Step 2200/4407 - Loss: 1.9313 | PPL: 6.90 | LR: 1.118584e-06\n",
      "  Step 2300/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 1.104148e-06\n",
      "  Step 2300/4407 - Loss: 1.9311 | PPL: 6.90 | LR: 1.104148e-06\n",
      "  Step 2400/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 1.089806e-06\n",
      "  Step 2400/4407 - Loss: 1.9299 | PPL: 6.89 | LR: 1.089806e-06\n",
      "  Step 2500/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 1.075556e-06\n",
      "  Step 2500/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 1.075556e-06\n",
      "  Step 2600/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 1.061401e-06\n",
      "  Step 2600/4407 - Loss: 1.9308 | PPL: 6.89 | LR: 1.061401e-06\n",
      "  Step 2700/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 1.047339e-06\n",
      "  Step 2700/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 1.047339e-06\n",
      "  Step 2800/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 1.033370e-06\n",
      "  Step 2800/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 1.033370e-06\n",
      "  Step 2900/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 1.019494e-06\n",
      "  Step 2900/4407 - Loss: 1.9286 | PPL: 6.88 | LR: 1.019494e-06\n",
      "  Step 3000/4407 - Loss: 1.9283 | PPL: 6.88 | LR: 1.005713e-06\n",
      "  Step 3000/4407 - Loss: 1.9283 | PPL: 6.88 | LR: 1.005713e-06\n",
      "  Step 3100/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 9.920242e-07\n",
      "  Step 3100/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 9.920242e-07\n",
      "  Step 3200/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 9.784293e-07\n",
      "  Step 3200/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 9.784293e-07\n",
      "  Step 3300/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 9.649279e-07\n",
      "  Step 3300/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 9.649279e-07\n",
      "  Step 3400/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 9.515200e-07\n",
      "  Step 3400/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 9.515200e-07\n",
      "  Step 3500/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 9.382056e-07\n",
      "  Step 3500/4407 - Loss: 1.9287 | PPL: 6.88 | LR: 9.382056e-07\n",
      "  Step 3600/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 9.249847e-07\n",
      "  Step 3600/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 9.249847e-07\n",
      "  Step 3700/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 9.118573e-07\n",
      "  Step 3700/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 9.118573e-07\n",
      "  Step 3800/4407 - Loss: 1.9274 | PPL: 6.87 | LR: 8.988235e-07\n",
      "  Step 3800/4407 - Loss: 1.9274 | PPL: 6.87 | LR: 8.988235e-07\n",
      "  Step 3900/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 8.858832e-07\n",
      "  Step 3900/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 8.858832e-07\n",
      "  Step 4000/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 8.730365e-07\n",
      "  Step 4000/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 8.730365e-07\n",
      "  Step 4100/4407 - Loss: 1.9278 | PPL: 6.87 | LR: 8.602833e-07\n",
      "  Step 4100/4407 - Loss: 1.9278 | PPL: 6.87 | LR: 8.602833e-07\n",
      "  Step 4200/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 8.476237e-07\n",
      "  Step 4200/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 8.476237e-07\n",
      "  Step 4300/4407 - Loss: 1.9275 | PPL: 6.87 | LR: 8.350577e-07\n",
      "  Step 4300/4407 - Loss: 1.9275 | PPL: 6.87 | LR: 8.350577e-07\n",
      "  Step 4400/4407 - Loss: 1.9281 | PPL: 6.88 | LR: 8.225853e-07\n",
      "  Step 4400/4407 - Loss: 1.9281 | PPL: 6.88 | LR: 8.225853e-07\n",
      "Epoch 97/100 | Train Loss: 1.9279 | Train PPL: 6.88 | Val Loss: 2.5942 | Val PPL: 13.39\n",
      "Epoch 97/100 | Train Loss: 1.9279 | Train PPL: 6.88 | Val Loss: 2.5942 | Val PPL: 13.39\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.88\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.88\n",
      "  Step 100/4407 - Loss: 1.9044 | PPL: 6.72 | LR: 8.093434e-07\n",
      "  Step 100/4407 - Loss: 1.9044 | PPL: 6.72 | LR: 8.093434e-07\n",
      "  Step 200/4407 - Loss: 1.9255 | PPL: 6.86 | LR: 7.970647e-07\n",
      "  Step 200/4407 - Loss: 1.9255 | PPL: 6.86 | LR: 7.970647e-07\n",
      "  Step 300/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 7.848796e-07\n",
      "  Step 300/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 7.848796e-07\n",
      "  Step 400/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 7.727881e-07\n",
      "  Step 400/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 7.727881e-07\n",
      "  Step 500/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 7.607902e-07\n",
      "  Step 500/4407 - Loss: 1.9362 | PPL: 6.93 | LR: 7.607902e-07\n",
      "  Step 600/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 7.488860e-07\n",
      "  Step 600/4407 - Loss: 1.9370 | PPL: 6.94 | LR: 7.488860e-07\n",
      "  Step 700/4407 - Loss: 1.9354 | PPL: 6.93 | LR: 7.370754e-07\n",
      "  Step 700/4407 - Loss: 1.9354 | PPL: 6.93 | LR: 7.370754e-07\n",
      "  Step 800/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 7.253585e-07\n",
      "  Step 800/4407 - Loss: 1.9312 | PPL: 6.90 | LR: 7.253585e-07\n",
      "  Step 900/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 7.137352e-07\n",
      "  Step 900/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 7.137352e-07\n",
      "  Step 1000/4407 - Loss: 1.9327 | PPL: 6.91 | LR: 7.022055e-07\n",
      "  Step 1000/4407 - Loss: 1.9327 | PPL: 6.91 | LR: 7.022055e-07\n",
      "  Step 1100/4407 - Loss: 1.9315 | PPL: 6.90 | LR: 6.907695e-07\n",
      "  Step 1100/4407 - Loss: 1.9315 | PPL: 6.90 | LR: 6.907695e-07\n",
      "  Step 1200/4407 - Loss: 1.9298 | PPL: 6.89 | LR: 6.794272e-07\n",
      "  Step 1200/4407 - Loss: 1.9298 | PPL: 6.89 | LR: 6.794272e-07\n",
      "  Step 1300/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 6.681786e-07\n",
      "  Step 1300/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 6.681786e-07\n",
      "  Step 1400/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 6.570237e-07\n",
      "  Step 1400/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 6.570237e-07\n",
      "  Step 1500/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 6.459624e-07\n",
      "  Step 1500/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 6.459624e-07\n",
      "  Step 1600/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 6.349949e-07\n",
      "  Step 1600/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 6.349949e-07\n",
      "  Step 1700/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 6.241211e-07\n",
      "  Step 1700/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 6.241211e-07\n",
      "  Step 1800/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 6.133410e-07\n",
      "  Step 1800/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 6.133410e-07\n",
      "  Step 1900/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 6.026546e-07\n",
      "  Step 1900/4407 - Loss: 1.9294 | PPL: 6.89 | LR: 6.026546e-07\n",
      "  Step 2000/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 5.920619e-07\n",
      "  Step 2000/4407 - Loss: 1.9303 | PPL: 6.89 | LR: 5.920619e-07\n",
      "  Step 2100/4407 - Loss: 1.9293 | PPL: 6.88 | LR: 5.815630e-07\n",
      "  Step 2100/4407 - Loss: 1.9293 | PPL: 6.88 | LR: 5.815630e-07\n",
      "  Step 2200/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 5.711578e-07\n",
      "  Step 2200/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 5.711578e-07\n",
      "  Step 2300/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 5.608463e-07\n",
      "  Step 2300/4407 - Loss: 1.9295 | PPL: 6.89 | LR: 5.608463e-07\n",
      "  Step 2400/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 5.506286e-07\n",
      "  Step 2400/4407 - Loss: 1.9288 | PPL: 6.88 | LR: 5.506286e-07\n",
      "  Step 2500/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 5.405047e-07\n",
      "  Step 2500/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 5.405047e-07\n",
      "  Step 2600/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 5.304746e-07\n",
      "  Step 2600/4407 - Loss: 1.9282 | PPL: 6.88 | LR: 5.304746e-07\n",
      "  Step 2700/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 5.205382e-07\n",
      "  Step 2700/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 5.205382e-07\n",
      "  Step 2800/4407 - Loss: 1.9279 | PPL: 6.87 | LR: 5.106956e-07\n",
      "  Step 2800/4407 - Loss: 1.9279 | PPL: 6.87 | LR: 5.106956e-07\n",
      "  Step 2900/4407 - Loss: 1.9262 | PPL: 6.86 | LR: 5.009468e-07\n",
      "  Step 2900/4407 - Loss: 1.9262 | PPL: 6.86 | LR: 5.009468e-07\n",
      "  Step 3000/4407 - Loss: 1.9260 | PPL: 6.86 | LR: 4.912917e-07\n",
      "  Step 3000/4407 - Loss: 1.9260 | PPL: 6.86 | LR: 4.912917e-07\n",
      "  Step 3100/4407 - Loss: 1.9264 | PPL: 6.86 | LR: 4.817305e-07\n",
      "  Step 3100/4407 - Loss: 1.9264 | PPL: 6.86 | LR: 4.817305e-07\n",
      "  Step 3200/4407 - Loss: 1.9267 | PPL: 6.87 | LR: 4.722631e-07\n",
      "  Step 3200/4407 - Loss: 1.9267 | PPL: 6.87 | LR: 4.722631e-07\n",
      "  Step 3300/4407 - Loss: 1.9266 | PPL: 6.87 | LR: 4.628895e-07\n",
      "  Step 3300/4407 - Loss: 1.9266 | PPL: 6.87 | LR: 4.628895e-07\n",
      "  Step 3400/4407 - Loss: 1.9275 | PPL: 6.87 | LR: 4.536097e-07\n",
      "  Step 3400/4407 - Loss: 1.9275 | PPL: 6.87 | LR: 4.536097e-07\n",
      "  Step 3500/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 4.444237e-07\n",
      "  Step 3500/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 4.444237e-07\n",
      "  Step 3600/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 4.353315e-07\n",
      "  Step 3600/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 4.353315e-07\n",
      "  Step 3700/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 4.263332e-07\n",
      "  Step 3700/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 4.263332e-07\n",
      "  Step 3800/4407 - Loss: 1.9274 | PPL: 6.87 | LR: 4.174288e-07\n",
      "  Step 3800/4407 - Loss: 1.9274 | PPL: 6.87 | LR: 4.174288e-07\n",
      "  Step 3900/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 4.086181e-07\n",
      "  Step 3900/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 4.086181e-07\n",
      "  Step 4000/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 3.999013e-07\n",
      "  Step 4000/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 3.999013e-07\n",
      "  Step 4100/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 3.912784e-07\n",
      "  Step 4100/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 3.912784e-07\n",
      "  Step 4200/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 3.827494e-07\n",
      "  Step 4200/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 3.827494e-07\n",
      "  Step 4300/4407 - Loss: 1.9278 | PPL: 6.87 | LR: 3.743142e-07\n",
      "  Step 4300/4407 - Loss: 1.9278 | PPL: 6.87 | LR: 3.743142e-07\n",
      "  Step 4400/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 3.659728e-07\n",
      "  Step 4400/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 3.659728e-07\n",
      "Epoch 98/100 | Train Loss: 1.9278 | Train PPL: 6.87 | Val Loss: 2.5940 | Val PPL: 13.38\n",
      "Epoch 98/100 | Train Loss: 1.9278 | Train PPL: 6.87 | Val Loss: 2.5940 | Val PPL: 13.38\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.87\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.87\n",
      "  Step 100/4407 - Loss: 1.9180 | PPL: 6.81 | LR: 3.571516e-07\n",
      "  Step 100/4407 - Loss: 1.9180 | PPL: 6.81 | LR: 3.571516e-07\n",
      "  Step 200/4407 - Loss: 1.9005 | PPL: 6.69 | LR: 3.490046e-07\n",
      "  Step 200/4407 - Loss: 1.9005 | PPL: 6.69 | LR: 3.490046e-07\n",
      "  Step 300/4407 - Loss: 1.9116 | PPL: 6.76 | LR: 3.409515e-07\n",
      "  Step 300/4407 - Loss: 1.9116 | PPL: 6.76 | LR: 3.409515e-07\n",
      "  Step 400/4407 - Loss: 1.9125 | PPL: 6.77 | LR: 3.329923e-07\n",
      "  Step 400/4407 - Loss: 1.9125 | PPL: 6.77 | LR: 3.329923e-07\n",
      "  Step 500/4407 - Loss: 1.9130 | PPL: 6.77 | LR: 3.251270e-07\n",
      "  Step 500/4407 - Loss: 1.9130 | PPL: 6.77 | LR: 3.251270e-07\n",
      "  Step 600/4407 - Loss: 1.9091 | PPL: 6.75 | LR: 3.173555e-07\n",
      "  Step 600/4407 - Loss: 1.9091 | PPL: 6.75 | LR: 3.173555e-07\n",
      "  Step 700/4407 - Loss: 1.9161 | PPL: 6.79 | LR: 3.096780e-07\n",
      "  Step 700/4407 - Loss: 1.9161 | PPL: 6.79 | LR: 3.096780e-07\n",
      "  Step 800/4407 - Loss: 1.9160 | PPL: 6.79 | LR: 3.020944e-07\n",
      "  Step 800/4407 - Loss: 1.9160 | PPL: 6.79 | LR: 3.020944e-07\n",
      "  Step 900/4407 - Loss: 1.9217 | PPL: 6.83 | LR: 2.946048e-07\n",
      "  Step 900/4407 - Loss: 1.9217 | PPL: 6.83 | LR: 2.946048e-07\n",
      "  Step 1000/4407 - Loss: 1.9265 | PPL: 6.87 | LR: 2.872090e-07\n",
      "  Step 1000/4407 - Loss: 1.9265 | PPL: 6.87 | LR: 2.872090e-07\n",
      "  Step 1100/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 2.799072e-07\n",
      "  Step 1100/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 2.799072e-07\n",
      "  Step 1200/4407 - Loss: 1.9251 | PPL: 6.86 | LR: 2.726993e-07\n",
      "  Step 1200/4407 - Loss: 1.9251 | PPL: 6.86 | LR: 2.726993e-07\n",
      "  Step 1300/4407 - Loss: 1.9229 | PPL: 6.84 | LR: 2.655853e-07\n",
      "  Step 1300/4407 - Loss: 1.9229 | PPL: 6.84 | LR: 2.655853e-07\n",
      "  Step 1400/4407 - Loss: 1.9228 | PPL: 6.84 | LR: 2.585653e-07\n",
      "  Step 1400/4407 - Loss: 1.9228 | PPL: 6.84 | LR: 2.585653e-07\n",
      "  Step 1500/4407 - Loss: 1.9228 | PPL: 6.84 | LR: 2.516392e-07\n",
      "  Step 1500/4407 - Loss: 1.9228 | PPL: 6.84 | LR: 2.516392e-07\n",
      "  Step 1600/4407 - Loss: 1.9211 | PPL: 6.83 | LR: 2.448071e-07\n",
      "  Step 1600/4407 - Loss: 1.9211 | PPL: 6.83 | LR: 2.448071e-07\n",
      "  Step 1700/4407 - Loss: 1.9191 | PPL: 6.81 | LR: 2.380689e-07\n",
      "  Step 1700/4407 - Loss: 1.9191 | PPL: 6.81 | LR: 2.380689e-07\n",
      "  Step 1800/4407 - Loss: 1.9195 | PPL: 6.82 | LR: 2.314247e-07\n",
      "  Step 1800/4407 - Loss: 1.9195 | PPL: 6.82 | LR: 2.314247e-07\n",
      "  Step 1900/4407 - Loss: 1.9198 | PPL: 6.82 | LR: 2.248744e-07\n",
      "  Step 1900/4407 - Loss: 1.9198 | PPL: 6.82 | LR: 2.248744e-07\n",
      "  Step 2000/4407 - Loss: 1.9201 | PPL: 6.82 | LR: 2.184181e-07\n",
      "  Step 2000/4407 - Loss: 1.9201 | PPL: 6.82 | LR: 2.184181e-07\n",
      "  Step 2100/4407 - Loss: 1.9232 | PPL: 6.84 | LR: 2.120558e-07\n",
      "  Step 2100/4407 - Loss: 1.9232 | PPL: 6.84 | LR: 2.120558e-07\n",
      "  Step 2200/4407 - Loss: 1.9239 | PPL: 6.85 | LR: 2.057875e-07\n",
      "  Step 2200/4407 - Loss: 1.9239 | PPL: 6.85 | LR: 2.057875e-07\n",
      "  Step 2300/4407 - Loss: 1.9248 | PPL: 6.85 | LR: 1.996131e-07\n",
      "  Step 2300/4407 - Loss: 1.9248 | PPL: 6.85 | LR: 1.996131e-07\n",
      "  Step 2400/4407 - Loss: 1.9246 | PPL: 6.85 | LR: 1.935327e-07\n",
      "  Step 2400/4407 - Loss: 1.9246 | PPL: 6.85 | LR: 1.935327e-07\n",
      "  Step 2500/4407 - Loss: 1.9246 | PPL: 6.85 | LR: 1.875463e-07\n",
      "  Step 2500/4407 - Loss: 1.9246 | PPL: 6.85 | LR: 1.875463e-07\n",
      "  Step 2600/4407 - Loss: 1.9259 | PPL: 6.86 | LR: 1.816539e-07\n",
      "  Step 2600/4407 - Loss: 1.9259 | PPL: 6.86 | LR: 1.816539e-07\n",
      "  Step 2700/4407 - Loss: 1.9261 | PPL: 6.86 | LR: 1.758555e-07\n",
      "  Step 2700/4407 - Loss: 1.9261 | PPL: 6.86 | LR: 1.758555e-07\n",
      "  Step 2800/4407 - Loss: 1.9263 | PPL: 6.86 | LR: 1.701510e-07\n",
      "  Step 2800/4407 - Loss: 1.9263 | PPL: 6.86 | LR: 1.701510e-07\n",
      "  Step 2900/4407 - Loss: 1.9258 | PPL: 6.86 | LR: 1.645406e-07\n",
      "  Step 2900/4407 - Loss: 1.9258 | PPL: 6.86 | LR: 1.645406e-07\n",
      "  Step 3000/4407 - Loss: 1.9262 | PPL: 6.86 | LR: 1.590242e-07\n",
      "  Step 3000/4407 - Loss: 1.9262 | PPL: 6.86 | LR: 1.590242e-07\n",
      "  Step 3100/4407 - Loss: 1.9267 | PPL: 6.87 | LR: 1.536018e-07\n",
      "  Step 3100/4407 - Loss: 1.9267 | PPL: 6.87 | LR: 1.536018e-07\n",
      "  Step 3200/4407 - Loss: 1.9261 | PPL: 6.86 | LR: 1.482734e-07\n",
      "  Step 3200/4407 - Loss: 1.9261 | PPL: 6.86 | LR: 1.482734e-07\n",
      "  Step 3300/4407 - Loss: 1.9265 | PPL: 6.87 | LR: 1.430390e-07\n",
      "  Step 3300/4407 - Loss: 1.9265 | PPL: 6.87 | LR: 1.430390e-07\n",
      "  Step 3400/4407 - Loss: 1.9254 | PPL: 6.86 | LR: 1.378986e-07\n",
      "  Step 3400/4407 - Loss: 1.9254 | PPL: 6.86 | LR: 1.378986e-07\n",
      "  Step 3500/4407 - Loss: 1.9248 | PPL: 6.85 | LR: 1.328522e-07\n",
      "  Step 3500/4407 - Loss: 1.9248 | PPL: 6.85 | LR: 1.328522e-07\n",
      "  Step 3600/4407 - Loss: 1.9249 | PPL: 6.85 | LR: 1.278999e-07\n",
      "  Step 3600/4407 - Loss: 1.9249 | PPL: 6.85 | LR: 1.278999e-07\n",
      "  Step 3700/4407 - Loss: 1.9250 | PPL: 6.86 | LR: 1.230416e-07\n",
      "  Step 3700/4407 - Loss: 1.9250 | PPL: 6.86 | LR: 1.230416e-07\n",
      "  Step 3800/4407 - Loss: 1.9246 | PPL: 6.85 | LR: 1.182773e-07\n",
      "  Step 3800/4407 - Loss: 1.9246 | PPL: 6.85 | LR: 1.182773e-07\n",
      "  Step 3900/4407 - Loss: 1.9246 | PPL: 6.85 | LR: 1.136071e-07\n",
      "  Step 3900/4407 - Loss: 1.9246 | PPL: 6.85 | LR: 1.136071e-07\n",
      "  Step 4000/4407 - Loss: 1.9251 | PPL: 6.86 | LR: 1.090309e-07\n",
      "  Step 4000/4407 - Loss: 1.9251 | PPL: 6.86 | LR: 1.090309e-07\n",
      "  Step 4100/4407 - Loss: 1.9255 | PPL: 6.86 | LR: 1.045487e-07\n",
      "  Step 4100/4407 - Loss: 1.9255 | PPL: 6.86 | LR: 1.045487e-07\n",
      "  Step 4200/4407 - Loss: 1.9259 | PPL: 6.86 | LR: 1.001606e-07\n",
      "  Step 4200/4407 - Loss: 1.9259 | PPL: 6.86 | LR: 1.001606e-07\n",
      "  Step 4300/4407 - Loss: 1.9266 | PPL: 6.87 | LR: 9.586647e-08\n",
      "  Step 4300/4407 - Loss: 1.9266 | PPL: 6.87 | LR: 9.586647e-08\n",
      "  Step 4400/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 9.166643e-08\n",
      "  Step 4400/4407 - Loss: 1.9271 | PPL: 6.87 | LR: 9.166643e-08\n",
      "Epoch 99/100 | Train Loss: 1.9273 | Train PPL: 6.87 | Val Loss: 2.5942 | Val PPL: 13.39\n",
      "Epoch 99/100 | Train Loss: 1.9273 | Train PPL: 6.87 | Val Loss: 2.5942 | Val PPL: 13.39\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.87\n",
      "Saved new best model to models/tagalog_lm/best_model.pth with Train PPL: 6.87\n",
      "  Step 100/4407 - Loss: 1.9533 | PPL: 7.05 | LR: 8.727654e-08\n",
      "  Step 100/4407 - Loss: 1.9533 | PPL: 7.05 | LR: 8.727654e-08\n",
      "  Step 200/4407 - Loss: 1.9168 | PPL: 6.80 | LR: 8.327118e-08\n",
      "  Step 200/4407 - Loss: 1.9168 | PPL: 6.80 | LR: 8.327118e-08\n",
      "  Step 300/4407 - Loss: 1.9335 | PPL: 6.91 | LR: 7.935988e-08\n",
      "  Step 300/4407 - Loss: 1.9335 | PPL: 6.91 | LR: 7.935988e-08\n",
      "  Step 400/4407 - Loss: 1.9417 | PPL: 6.97 | LR: 7.554263e-08\n",
      "  Step 400/4407 - Loss: 1.9417 | PPL: 6.97 | LR: 7.554263e-08\n",
      "  Step 500/4407 - Loss: 1.9336 | PPL: 6.91 | LR: 7.181945e-08\n",
      "  Step 500/4407 - Loss: 1.9336 | PPL: 6.91 | LR: 7.181945e-08\n",
      "  Step 600/4407 - Loss: 1.9357 | PPL: 6.93 | LR: 6.819032e-08\n",
      "  Step 600/4407 - Loss: 1.9357 | PPL: 6.93 | LR: 6.819032e-08\n",
      "  Step 700/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 6.465526e-08\n",
      "  Step 700/4407 - Loss: 1.9316 | PPL: 6.90 | LR: 6.465526e-08\n",
      "  Step 800/4407 - Loss: 1.9339 | PPL: 6.92 | LR: 6.121426e-08\n",
      "  Step 800/4407 - Loss: 1.9339 | PPL: 6.92 | LR: 6.121426e-08\n",
      "  Step 900/4407 - Loss: 1.9324 | PPL: 6.91 | LR: 5.786734e-08\n",
      "  Step 900/4407 - Loss: 1.9324 | PPL: 6.91 | LR: 5.786734e-08\n",
      "  Step 1000/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 5.461448e-08\n",
      "  Step 1000/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 5.461448e-08\n",
      "  Step 1100/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 5.145569e-08\n",
      "  Step 1100/4407 - Loss: 1.9296 | PPL: 6.89 | LR: 5.145569e-08\n",
      "  Step 1200/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 4.839098e-08\n",
      "  Step 1200/4407 - Loss: 1.9291 | PPL: 6.88 | LR: 4.839098e-08\n",
      "  Step 1300/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 4.542035e-08\n",
      "  Step 1300/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 4.542035e-08\n",
      "  Step 1400/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 4.254379e-08\n",
      "  Step 1400/4407 - Loss: 1.9322 | PPL: 6.90 | LR: 4.254379e-08\n",
      "  Step 1500/4407 - Loss: 1.9324 | PPL: 6.91 | LR: 3.976132e-08\n",
      "  Step 1500/4407 - Loss: 1.9324 | PPL: 6.91 | LR: 3.976132e-08\n",
      "  Step 1600/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 3.707292e-08\n",
      "  Step 1600/4407 - Loss: 1.9328 | PPL: 6.91 | LR: 3.707292e-08\n",
      "  Step 1700/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 3.447861e-08\n",
      "  Step 1700/4407 - Loss: 1.9321 | PPL: 6.90 | LR: 3.447861e-08\n",
      "  Step 1800/4407 - Loss: 1.9326 | PPL: 6.91 | LR: 3.197838e-08\n",
      "  Step 1800/4407 - Loss: 1.9326 | PPL: 6.91 | LR: 3.197838e-08\n",
      "  Step 1900/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 2.957224e-08\n",
      "  Step 1900/4407 - Loss: 1.9314 | PPL: 6.90 | LR: 2.957224e-08\n",
      "  Step 2000/4407 - Loss: 1.9305 | PPL: 6.89 | LR: 2.726019e-08\n",
      "  Step 2000/4407 - Loss: 1.9305 | PPL: 6.89 | LR: 2.726019e-08\n",
      "  Step 2100/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 2.504223e-08\n",
      "  Step 2100/4407 - Loss: 1.9300 | PPL: 6.89 | LR: 2.504223e-08\n",
      "  Step 2200/4407 - Loss: 1.9283 | PPL: 6.88 | LR: 2.291836e-08\n",
      "  Step 2200/4407 - Loss: 1.9283 | PPL: 6.88 | LR: 2.291836e-08\n",
      "  Step 2300/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 2.088858e-08\n",
      "  Step 2300/4407 - Loss: 1.9277 | PPL: 6.87 | LR: 2.088858e-08\n",
      "  Step 2400/4407 - Loss: 1.9267 | PPL: 6.87 | LR: 1.895289e-08\n",
      "  Step 2400/4407 - Loss: 1.9267 | PPL: 6.87 | LR: 1.895289e-08\n",
      "  Step 2500/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 1.711130e-08\n",
      "  Step 2500/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 1.711130e-08\n",
      "  Step 2600/4407 - Loss: 1.9262 | PPL: 6.86 | LR: 1.536380e-08\n",
      "  Step 2600/4407 - Loss: 1.9262 | PPL: 6.86 | LR: 1.536380e-08\n",
      "  Step 2700/4407 - Loss: 1.9267 | PPL: 6.87 | LR: 1.371041e-08\n",
      "  Step 2700/4407 - Loss: 1.9267 | PPL: 6.87 | LR: 1.371041e-08\n",
      "  Step 2800/4407 - Loss: 1.9272 | PPL: 6.87 | LR: 1.215111e-08\n",
      "  Step 2800/4407 - Loss: 1.9272 | PPL: 6.87 | LR: 1.215111e-08\n",
      "  Step 2900/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 1.068590e-08\n",
      "  Step 2900/4407 - Loss: 1.9280 | PPL: 6.88 | LR: 1.068590e-08\n",
      "  Step 3000/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 9.314801e-09\n",
      "  Step 3000/4407 - Loss: 1.9276 | PPL: 6.87 | LR: 9.314801e-09\n",
      "  Step 3100/4407 - Loss: 1.9278 | PPL: 6.87 | LR: 8.037800e-09\n",
      "  Step 3100/4407 - Loss: 1.9278 | PPL: 6.87 | LR: 8.037800e-09\n",
      "  Step 3200/4407 - Loss: 1.9283 | PPL: 6.88 | LR: 6.854900e-09\n",
      "  Step 3200/4407 - Loss: 1.9283 | PPL: 6.88 | LR: 6.854900e-09\n",
      "  Step 3300/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 5.766102e-09\n",
      "  Step 3300/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 5.766102e-09\n",
      "  Step 3400/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 4.771408e-09\n",
      "  Step 3400/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 4.771408e-09\n",
      "  Step 3500/4407 - Loss: 1.9293 | PPL: 6.88 | LR: 3.870817e-09\n",
      "  Step 3500/4407 - Loss: 1.9293 | PPL: 6.88 | LR: 3.870817e-09\n",
      "  Step 3600/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 3.064330e-09\n",
      "  Step 3600/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 3.064330e-09\n",
      "  Step 3700/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 2.351947e-09\n",
      "  Step 3700/4407 - Loss: 1.9284 | PPL: 6.88 | LR: 2.351947e-09\n",
      "  Step 3800/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 1.733670e-09\n",
      "  Step 3800/4407 - Loss: 1.9279 | PPL: 6.88 | LR: 1.733670e-09\n",
      "  Step 3900/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 1.209498e-09\n",
      "  Step 3900/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 1.209498e-09\n",
      "  Step 4000/4407 - Loss: 1.9266 | PPL: 6.87 | LR: 7.794322e-10\n",
      "  Step 4000/4407 - Loss: 1.9266 | PPL: 6.87 | LR: 7.794322e-10\n",
      "  Step 4100/4407 - Loss: 1.9279 | PPL: 6.87 | LR: 4.434722e-10\n",
      "  Step 4100/4407 - Loss: 1.9279 | PPL: 6.87 | LR: 4.434722e-10\n",
      "  Step 4200/4407 - Loss: 1.9274 | PPL: 6.87 | LR: 2.016185e-10\n",
      "  Step 4200/4407 - Loss: 1.9274 | PPL: 6.87 | LR: 2.016185e-10\n",
      "  Step 4300/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 5.387130e-11\n",
      "  Step 4300/4407 - Loss: 1.9273 | PPL: 6.87 | LR: 5.387130e-11\n",
      "  Step 4400/4407 - Loss: 1.9274 | PPL: 6.87 | LR: 2.305611e-13\n",
      "  Step 4400/4407 - Loss: 1.9274 | PPL: 6.87 | LR: 2.305611e-13\n",
      "Epoch 100/100 | Train Loss: 1.9275 | Train PPL: 6.87 | Val Loss: 2.5941 | Val PPL: 13.38\n",
      "Epoch 100/100 | Train Loss: 1.9275 | Train PPL: 6.87 | Val Loss: 2.5941 | Val PPL: 13.38\n"
     ]
    }
   ],
   "source": [
    "history: List[dict] = []\n",
    "best_train_perplexity = float(\"inf\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_ppl = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device,\n",
    "        scaler,\n",
    "        GRADIENT_CLIPPING,\n",
    "        scheduler,\n",
    "        LOG_INTERVAL,\n",
    "    )\n",
    "    val_loss, val_ppl = evaluate(model, val_loader, criterion, device)\n",
    "    history.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_perplexity\": train_ppl,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_perplexity\": val_ppl,\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train PPL: {train_ppl:.2f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | Val PPL: {val_ppl:.2f}\"\n",
    "    )\n",
    "\n",
    "    if train_ppl < best_train_perplexity:\n",
    "        best_train_perplexity = train_ppl\n",
    "        if not os.path.exists(OUTPUT_DIR):\n",
    "            os.makedirs(OUTPUT_DIR)\n",
    "        torch.save(model, OUTPUT_DIR + \"/best_model.pth\")\n",
    "        print(f\"Saved new best model to {OUTPUT_DIR + '/best_model.pth'} with Train PPL: {train_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c888742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LM training curves\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_lm_training_curves(history):\n",
    "    \"\"\"Plot training loss and perplexity curves for the language model.\"\"\"\n",
    "    if not history:\n",
    "        print(\"No training history to plot.\")\n",
    "        return\n",
    "    \n",
    "    epochs = [h[\"epoch\"] for h in history]\n",
    "    train_loss = [h[\"train_loss\"] for h in history]\n",
    "    val_loss = [h[\"val_loss\"] for h in history]\n",
    "    train_ppl = [h[\"train_perplexity\"] for h in history]\n",
    "    val_ppl = [h[\"val_perplexity\"] for h in history]\n",
    "    lr = [h[\"learning_rate\"] for h in history]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot Training and Validation Loss\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(epochs, train_loss, label=\"Train Loss\", color='blue', marker='o', markersize=3)\n",
    "    ax1.plot(epochs, val_loss, label=\"Val Loss\", color='orange', marker='s', markersize=3)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Training and Validation Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot Training and Validation Perplexity\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(epochs, train_ppl, label=\"Train Perplexity\", color='blue', marker='o', markersize=3)\n",
    "    ax2.plot(epochs, val_ppl, label=\"Val Perplexity\", color='orange', marker='s', markersize=3)\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Perplexity\")\n",
    "    ax2.set_title(\"Training and Validation Perplexity\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot Perplexity (log scale for better visualization)\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.semilogy(epochs, train_ppl, label=\"Train Perplexity\", color='blue', marker='o', markersize=3)\n",
    "    ax3.semilogy(epochs, val_ppl, label=\"Val Perplexity\", color='orange', marker='s', markersize=3)\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(\"Perplexity (log scale)\")\n",
    "    ax3.set_title(\"Perplexity (Log Scale)\")\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot Learning Rate Schedule\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.plot(epochs, lr, label=\"Learning Rate\", color='green', marker='^', markersize=3)\n",
    "    ax4.set_xlabel(\"Epoch\")\n",
    "    ax4.set_ylabel(\"Learning Rate\")\n",
    "    ax4.set_title(\"Learning Rate Schedule\")\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Summary:\")\n",
    "    print(f\"  Final Train Loss: {train_loss[-1]:.4f} | Final Train PPL: {train_ppl[-1]:.2f}\")\n",
    "    print(f\"  Final Val Loss: {val_loss[-1]:.4f} | Final Val PPL: {val_ppl[-1]:.2f}\")\n",
    "    print(f\"  Best Train PPL: {min(train_ppl):.2f} (Epoch {train_ppl.index(min(train_ppl)) + 1})\")\n",
    "    print(f\"  Best Val PPL: {min(val_ppl):.2f} (Epoch {val_ppl.index(min(val_ppl)) + 1})\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "plot_lm_training_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01659554",
   "metadata": {},
   "source": [
    "Sanity check and try out the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49512f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17172/2424659314.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(f\"{OUTPUT_DIR}/best_model.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from models/tagalog_lm/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "# load best model\n",
    "if os.path.exists(f\"{OUTPUT_DIR}/best_model.pth\"):\n",
    "    model = torch.load(f\"{OUTPUT_DIR}/best_model.pth\", map_location=device)\n",
    "    model.eval()\n",
    "    print(f\"Loaded best model from {OUTPUT_DIR}/best_model.pth\")\n",
    "else:\n",
    "    print(f\"Model file not found at {OUTPUT_DIR}/best_model.pth. Please ensure training has completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12569f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "139f25e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec1a65746264a1187a2d2cbd66c1acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Interactive Model Tester</h3>'), Textarea(value='Ang', description='Prompt:', l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_text(prompt, max_new_tokens, temperature, top_k):\n",
    "    model.eval()\n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # If prompt is empty or just whitespace, handle gracefully\n",
    "    if input_ids.numel() == 0:\n",
    "        print(\"Please enter a valid prompt.\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        generated_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    except Exception as e:\n",
    "        return f\"Error during generation: {str(e)}\"\n",
    "\n",
    "# UI Components\n",
    "prompt_widget = widgets.Textarea(\n",
    "    value='Ang',\n",
    "    placeholder='Type your Tagalog prompt here...',\n",
    "    description='Prompt:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%', height='100px')\n",
    ")\n",
    "\n",
    "max_tokens_widget = widgets.IntSlider(\n",
    "    value=50,\n",
    "    min=10,\n",
    "    max=500,\n",
    "    step=10,\n",
    "    description='Max Tokens:',\n",
    "    style={'description_width': 'initial'},\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "temperature_widget = widgets.FloatSlider(\n",
    "    value=0.8,\n",
    "    min=0.1,\n",
    "    max=2.0,\n",
    "    step=0.1,\n",
    "    description='Temperature:',\n",
    "    style={'description_width': 'initial'},\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "top_k_widget = widgets.IntSlider(\n",
    "    value=40,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=5,\n",
    "    description='Top K (0=off):',\n",
    "    style={'description_width': 'initial'},\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description=\"Generate Text\",\n",
    "    button_style='primary', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    icon='magic'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output(layout={'border': '1px solid #ccc', 'padding': '10px'})\n",
    "\n",
    "def on_generate_click(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        print(\"Generating response...\")\n",
    "        \n",
    "        result = generate_text(\n",
    "            prompt_widget.value,\n",
    "            max_tokens_widget.value,\n",
    "            temperature_widget.value,\n",
    "            top_k_widget.value\n",
    "        )\n",
    "        \n",
    "        output_area.clear_output()\n",
    "        print(\"--- Generated Output ---\")\n",
    "        print(result)\n",
    "\n",
    "generate_button.on_click(on_generate_click)\n",
    "\n",
    "# Layout\n",
    "controls = widgets.VBox([\n",
    "    max_tokens_widget,\n",
    "    temperature_widget,\n",
    "    top_k_widget\n",
    "])\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Interactive Model Tester</h3>\"),\n",
    "    prompt_widget,\n",
    "    controls,\n",
    "    generate_button,\n",
    "    output_area\n",
    "])\n",
    "\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f1b65",
   "metadata": {},
   "source": [
    "Let's now train a classifier with the LM as the backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01193d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TagalogLMClassifierConfig:\n",
    "    num_labels: int = 2\n",
    "    hidden_dim: Optional[int] = None\n",
    "    classifier_dropout: float = 0.2\n",
    "    pooling: str = \"mean\"  # options: mean, last\n",
    "    fine_tune_base: bool = False\n",
    "\n",
    "class TagalogLMClassifier(nn.Module):\n",
    "    \"\"\"Sequence classifier that reuses a pretrained Tagalog LM backbone.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        language_model: MiniTransformerLanguageModel,\n",
    "        config: TagalogLMClassifierConfig,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.language_model = language_model\n",
    "        self.config = config\n",
    "        self.num_labels = config.num_labels\n",
    "        embed_dim = language_model.embed_dim\n",
    "        hidden_dim = config.hidden_dim or embed_dim\n",
    "\n",
    "        self.pre_classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.classifier_dropout),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, self.num_labels)\n",
    "\n",
    "        if not config.fine_tune_base:\n",
    "            self.freeze_backbone()\n",
    "        else:\n",
    "            self.language_model.train()\n",
    "\n",
    "    def freeze_backbone(self) -> None:\n",
    "        for param in self.language_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.language_model.eval()\n",
    "\n",
    "    def unfreeze_backbone(self) -> None:\n",
    "        for param in self.language_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.language_model.train()\n",
    "\n",
    "    def train(self, mode: bool = True) -> \"TagalogLMClassifier\":  # type: ignore[override]\n",
    "        super().train(mode)\n",
    "        if not self.config.fine_tune_base:\n",
    "            self.language_model.eval()\n",
    "        return self\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if not self.config.fine_tune_base:\n",
    "            with torch.no_grad():\n",
    "                hidden_states = self.language_model.encode(input_ids)\n",
    "        else:\n",
    "            hidden_states = self.language_model.encode(input_ids)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.to(hidden_states.dtype).unsqueeze(-1)\n",
    "            masked_hidden = hidden_states * mask\n",
    "            denom = mask.sum(dim=1).clamp_min(1.0)\n",
    "            pooled = masked_hidden.sum(dim=1) / denom\n",
    "        elif self.config.pooling == \"last\":\n",
    "            pooled = hidden_states[:, -1, :]\n",
    "        else:\n",
    "            pooled = hidden_states.mean(dim=1)\n",
    "\n",
    "        features = self.pre_classifier(pooled)\n",
    "        logits = self.output_layer(features)\n",
    "        return logits\n",
    "\n",
    "    def get_classifier_config(self) -> Dict[str, object]:\n",
    "        return {\n",
    "            \"num_labels\": self.num_labels,\n",
    "            \"hidden_dim\": self.config.hidden_dim,\n",
    "            \"classifier_dropout\": self.config.classifier_dropout,\n",
    "            \"pooling\": self.config.pooling,\n",
    "            \"fine_tune_base\": self.config.fine_tune_base,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_language_model_from_config(lm_config: Dict[str, object]) -> MiniTransformerLanguageModel:\n",
    "        return MiniTransformerLanguageModel(\n",
    "            vocab_size=int(lm_config[\"vocab_size\"]),\n",
    "            embed_dim=int(lm_config.get(\"embed_dim\", 256)),\n",
    "            num_heads=int(lm_config.get(\"num_heads\", 4)),\n",
    "            num_layers=int(lm_config.get(\"num_layers\", 4)),\n",
    "            max_position_embeddings=int(lm_config.get(\"max_position_embeddings\", 512)),\n",
    "            dropout=float(lm_config.get(\"dropout\", 0.1)),\n",
    "            ff_multiplier=int(lm_config.get(\"ff_multiplier\", 4)),\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_configs(\n",
    "        cls,\n",
    "        lm_config: Dict[str, object],\n",
    "        classifier_config: Dict[str, object],\n",
    "        device: Optional[torch.device] = None,\n",
    "    ) -> \"TagalogLMClassifier\":\n",
    "        lm = cls._build_language_model_from_config(lm_config)\n",
    "        config = TagalogLMClassifierConfig(\n",
    "            num_labels=int(classifier_config.get(\"num_labels\", 2)),\n",
    "            hidden_dim=classifier_config.get(\"hidden_dim\"),\n",
    "            classifier_dropout=float(classifier_config.get(\"classifier_dropout\", 0.2)),\n",
    "            pooling=classifier_config.get(\"pooling\", \"mean\"),\n",
    "            fine_tune_base=bool(classifier_config.get(\"fine_tune_base\", False)),\n",
    "        )\n",
    "        model = cls(lm, config)\n",
    "        if device is not None:\n",
    "            model.to(device)\n",
    "        return model\n",
    "\n",
    "    def load_backbone_weights(self, state_dict: Dict[str, torch.Tensor]) -> None:\n",
    "        missing, unexpected = self.language_model.load_state_dict(state_dict, strict=False)\n",
    "        if missing or unexpected:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to load language-model weights. Missing: {missing}, unexpected: {unexpected}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c0f66",
   "metadata": {},
   "source": [
    "Prepare hatespeech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c5d3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5978bcf",
   "metadata": {},
   "source": [
    "Classifier hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bcb814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION_BATCH_SIZE = 32\n",
    "FINE_TUNE_BASE = True\n",
    "CLASSIFIER_DROPOUT = 0.2\n",
    "CLASSIFIER_LEARNING_RATE = 1e-4\n",
    "CLASSIFIER_GRADIENT_CLIPPING = 1.0\n",
    "POOLING_STRATEGY = \"mean\"  # options: mean, last\n",
    "CLASSIFIER_EPOCHS = 250\n",
    "CLASSIFIER_WEIGHT_DECAY = 1e-2\n",
    "CLASSIFICATION_MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "751b8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = Path(\"data/combined/processed/train.csv\")\n",
    "val_csv = Path(\"data/combined/processed/validation.csv\")\n",
    "test_csv = Path(\"data/combined/processed/test.csv\")\n",
    "\n",
    "text_column = \"text\"\n",
    "label_column = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "402f68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts: Sequence[str], labels: Sequence[int]):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = [int(label) for label in labels]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[str, int]:\n",
    "        return self.texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf0f0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(path: Path, text_column: str, label_column: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if text_column not in df.columns or label_column not in df.columns:\n",
    "        raise ValueError(f\"File {path} must contain '{text_column}' and '{label_column}' columns.\")\n",
    "    return df[[text_column, label_column]].rename(columns={text_column: \"text\", label_column: \"label\"})\n",
    "\n",
    "train_df = load_dataframe(train_csv, text_column, label_column)\n",
    "val_df = load_dataframe(val_csv, text_column, label_column)\n",
    "test_df = load_dataframe(test_csv, text_column, label_column)\n",
    "\n",
    "train_dataset = TextClassificationDataset(train_df[\"text\"].tolist(), train_df[\"label\"].tolist())\n",
    "val_dataset = TextClassificationDataset(val_df[\"text\"].tolist(), val_df[\"label\"].tolist())\n",
    "test_dataset = TextClassificationDataset(test_df[\"text\"].tolist(), test_df[\"label\"].tolist())\n",
    "\n",
    "collate_fn = lambda batch: collate_batch(tokenizer, CLASSIFICATION_MAX_LENGTH, batch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CLASSIFICATION_BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CLASSIFICATION_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CLASSIFICATION_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62299774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(sorted(set(train_df[\"label\"].astype(int).tolist())))\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07effed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_config = TagalogLMClassifierConfig(\n",
    "    num_labels=2,\n",
    "    classifier_dropout=CLASSIFIER_DROPOUT,\n",
    "    pooling=POOLING_STRATEGY,\n",
    "    fine_tune_base=FINE_TUNE_BASE,\n",
    ")\n",
    "classifier_model = TagalogLMClassifier(model, classifier_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb4affcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating classifier model diagram...\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: model Pages: 1 -->\n",
       "<svg width=\"616pt\" height=\"1123pt\"\n",
       " viewBox=\"0.00 0.00 615.90 1123.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.88 0.88) rotate(0) translate(4 1276)\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-1276 698,-1276 698,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"8,-529 8,-998 502,-998 502,-529 8,-529\"/>\n",
       "<text text-anchor=\"middle\" x=\"65\" y=\"-984.4\" font-family=\"Times,serif\" font-size=\"12.00\">TransformerEncoder</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_3</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"264,-138 264,-373 428,-373 428,-138 264,-138\"/>\n",
       "<text text-anchor=\"middle\" x=\"297\" y=\"-359.4\" font-family=\"Times,serif\" font-size=\"12.00\">Sequential</text>\n",
       "</g>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"384,-1272 272,-1272 272,-1240 384,-1240 384,-1272\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"272,-1240 272,-1272 342,-1272 342,-1240 272,-1240\"/>\n",
       "<text text-anchor=\"start\" x=\"277\" y=\"-1259\" font-family=\"Linux libertine\" font-size=\"10.00\">input&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"288.5\" y=\"-1248\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"342,-1240 342,-1272 384,-1272 384,-1240 342,-1240\"/>\n",
       "<text text-anchor=\"start\" x=\"347\" y=\"-1253.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32)</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"419,-1204 237,-1204 237,-1162 419,-1162 419,-1204\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"237,-1162 237,-1204 302,-1204 302,-1162 237,-1162\"/>\n",
       "<text text-anchor=\"start\" x=\"242\" y=\"-1186\" font-family=\"Linux libertine\" font-size=\"10.00\">Embedding</text>\n",
       "<text text-anchor=\"start\" x=\"251\" y=\"-1175\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-1183 302,-1204 350,-1204 350,-1183 302,-1183\"/>\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-1191\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-1183 350,-1204 419,-1204 419,-1183 350,-1183\"/>\n",
       "<text text-anchor=\"start\" x=\"367\" y=\"-1191\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"302,-1162 302,-1183 350,-1183 350,-1162 302,-1162\"/>\n",
       "<text text-anchor=\"start\" x=\"307\" y=\"-1170\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"350,-1162 350,-1183 419,-1183 419,-1162 350,-1162\"/>\n",
       "<text text-anchor=\"start\" x=\"355\" y=\"-1170\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M328,-1239.94C328,-1232.45 328,-1223.12 328,-1214.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"331.5,-1214.16 328,-1204.16 324.5,-1214.16 331.5,-1214.16\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"410,-1126 246,-1126 246,-1084 410,-1084 410,-1126\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"246,-1084 246,-1126 293,-1126 293,-1084 246,-1084\"/>\n",
       "<text text-anchor=\"start\" x=\"260\" y=\"-1108\" font-family=\"Linux libertine\" font-size=\"10.00\">add</text>\n",
       "<text text-anchor=\"start\" x=\"251\" y=\"-1097\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"293,-1105 293,-1126 341,-1126 341,-1105 293,-1105\"/>\n",
       "<text text-anchor=\"start\" x=\"303\" y=\"-1113\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"341,-1105 341,-1126 410,-1126 410,-1105 341,-1105\"/>\n",
       "<text text-anchor=\"start\" x=\"346\" y=\"-1113\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"293,-1084 293,-1105 341,-1105 341,-1084 293,-1084\"/>\n",
       "<text text-anchor=\"start\" x=\"298\" y=\"-1092\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"341,-1084 341,-1105 410,-1105 410,-1084 341,-1084\"/>\n",
       "<text text-anchor=\"start\" x=\"346\" y=\"-1092\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M328,-1161.63C328,-1153.82 328,-1144.73 328,-1136.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"331.5,-1136.16 328,-1126.16 324.5,-1136.16 331.5,-1136.16\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"411.5,-1048 244.5,-1048 244.5,-1006 411.5,-1006 411.5,-1048\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"245,-1006 245,-1048 295,-1048 295,-1006 245,-1006\"/>\n",
       "<text text-anchor=\"start\" x=\"250\" y=\"-1030\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"251.5\" y=\"-1019\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"295,-1027 295,-1048 343,-1048 343,-1027 295,-1027\"/>\n",
       "<text text-anchor=\"start\" x=\"305\" y=\"-1035\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-1027 343,-1048 412,-1048 412,-1027 343,-1027\"/>\n",
       "<text text-anchor=\"start\" x=\"348\" y=\"-1035\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"295,-1006 295,-1027 343,-1027 343,-1006 295,-1006\"/>\n",
       "<text text-anchor=\"start\" x=\"300\" y=\"-1014\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-1006 343,-1027 412,-1027 412,-1006 343,-1006\"/>\n",
       "<text text-anchor=\"start\" x=\"348\" y=\"-1014\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M328,-1083.63C328,-1075.82 328,-1066.73 328,-1058.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"331.5,-1058.16 328,-1048.16 324.5,-1058.16 331.5,-1058.16\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"476,-969 180,-969 180,-927 476,-927 476,-969\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"180,-927 180,-969 315,-969 315,-927 180,-927\"/>\n",
       "<text text-anchor=\"start\" x=\"185\" y=\"-951\" font-family=\"Linux libertine\" font-size=\"10.00\">TransformerEncoderLayer</text>\n",
       "<text text-anchor=\"start\" x=\"229\" y=\"-940\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"315,-948 315,-969 363,-969 363,-948 315,-948\"/>\n",
       "<text text-anchor=\"start\" x=\"325\" y=\"-956\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"363,-948 363,-969 476,-969 476,-948 363,-948\"/>\n",
       "<text text-anchor=\"start\" x=\"368\" y=\"-956\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256), (32, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"315,-927 315,-948 363,-948 363,-927 315,-927\"/>\n",
       "<text text-anchor=\"start\" x=\"320\" y=\"-935\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"363,-927 363,-948 476,-948 476,-927 363,-927\"/>\n",
       "<text text-anchor=\"start\" x=\"390\" y=\"-935\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;8 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M328,-1005.77C328,-997.63 328,-988.08 328,-979.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"331.5,-979.1 328,-969.1 324.5,-979.1 331.5,-979.1\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"653,-1048 507,-1048 507,-1006 653,-1006 653,-1048\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"507,-1006 507,-1048 554,-1048 554,-1006 507,-1006\"/>\n",
       "<text text-anchor=\"start\" x=\"522\" y=\"-1030\" font-family=\"Linux libertine\" font-size=\"10.00\">triu</text>\n",
       "<text text-anchor=\"start\" x=\"512\" y=\"-1019\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"554,-1027 554,-1048 602,-1048 602,-1027 554,-1027\"/>\n",
       "<text text-anchor=\"start\" x=\"564\" y=\"-1035\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"602,-1027 602,-1048 653,-1048 653,-1027 602,-1027\"/>\n",
       "<text text-anchor=\"start\" x=\"607\" y=\"-1035\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"554,-1006 554,-1027 602,-1027 602,-1006 554,-1006\"/>\n",
       "<text text-anchor=\"start\" x=\"559\" y=\"-1014\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"602,-1006 602,-1027 653,-1027 653,-1006 602,-1006\"/>\n",
       "<text text-anchor=\"start\" x=\"607\" y=\"-1014\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 32) </text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"180,-891 16,-891 16,-849 180,-849 180,-891\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"16,-849 16,-891 63,-891 63,-849 16,-849\"/>\n",
       "<text text-anchor=\"start\" x=\"22.5\" y=\"-873\" font-family=\"Linux libertine\" font-size=\"10.00\">__eq__</text>\n",
       "<text text-anchor=\"start\" x=\"21\" y=\"-862\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"63,-870 63,-891 111,-891 111,-870 63,-870\"/>\n",
       "<text text-anchor=\"start\" x=\"73\" y=\"-878\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"111,-870 111,-891 180,-891 180,-870 111,-870\"/>\n",
       "<text text-anchor=\"start\" x=\"116\" y=\"-878\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (32, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"63,-849 63,-870 111,-870 111,-849 63,-849\"/>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-857\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"111,-849 111,-870 180,-870 180,-849 111,-849\"/>\n",
       "<text text-anchor=\"start\" x=\"125\" y=\"-857\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 32) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;6 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M527.73,-1005.93C524.23,-1003.54 520.94,-1000.91 518,-998 493.26,-973.53 513.15,-947.45 485,-927 433.7,-889.72 269.31,-901.01 190.37,-890.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"190.68,-887.45 180.27,-889.45 189.65,-894.37 190.68,-887.45\"/>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;8 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M514.45,-1005.97C480.46,-995.58 438.77,-982.85 403.45,-972.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"404.25,-968.64 393.66,-969.06 402.2,-975.33 404.25,-968.64\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"494,-891 198,-891 198,-849 494,-849 494,-891\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"198,-849 198,-891 333,-891 333,-849 198,-849\"/>\n",
       "<text text-anchor=\"start\" x=\"203\" y=\"-873\" font-family=\"Linux libertine\" font-size=\"10.00\">TransformerEncoderLayer</text>\n",
       "<text text-anchor=\"start\" x=\"247\" y=\"-862\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"333,-870 333,-891 381,-891 381,-870 333,-870\"/>\n",
       "<text text-anchor=\"start\" x=\"343\" y=\"-878\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-870 381,-891 494,-891 494,-870 381,-870\"/>\n",
       "<text text-anchor=\"start\" x=\"386\" y=\"-878\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256), (32, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"333,-849 333,-870 381,-870 381,-849 333,-849\"/>\n",
       "<text text-anchor=\"start\" x=\"338\" y=\"-857\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-849 381,-870 494,-870 494,-849 381,-849\"/>\n",
       "<text text-anchor=\"start\" x=\"408\" y=\"-857\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;9 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M569.51,-1005.82C555.82,-980.17 531.99,-937.92 518,-927 500,-912.94 478.34,-902.35 456.56,-894.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"457.48,-891.01 446.89,-891.03 455.18,-897.62 457.48,-891.01\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"494,-813 198,-813 198,-771 494,-771 494,-813\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"198,-771 198,-813 333,-813 333,-771 198,-771\"/>\n",
       "<text text-anchor=\"start\" x=\"203\" y=\"-795\" font-family=\"Linux libertine\" font-size=\"10.00\">TransformerEncoderLayer</text>\n",
       "<text text-anchor=\"start\" x=\"247\" y=\"-784\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"333,-792 333,-813 381,-813 381,-792 333,-792\"/>\n",
       "<text text-anchor=\"start\" x=\"343\" y=\"-800\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-792 381,-813 494,-813 494,-792 381,-792\"/>\n",
       "<text text-anchor=\"start\" x=\"386\" y=\"-800\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256), (32, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"333,-771 333,-792 381,-792 381,-771 333,-771\"/>\n",
       "<text text-anchor=\"start\" x=\"338\" y=\"-779\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-771 381,-792 494,-792 494,-771 381,-771\"/>\n",
       "<text text-anchor=\"start\" x=\"408\" y=\"-779\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;10 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>4&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M577.48,-1005.77C571.67,-968.36 553.72,-888.59 503,-849 485.21,-835.12 463.7,-824.5 442.38,-816.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"443.53,-813.14 432.94,-813.03 441.15,-819.72 443.53,-813.14\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"494,-735 198,-735 198,-693 494,-693 494,-735\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"198,-693 198,-735 333,-735 333,-693 198,-693\"/>\n",
       "<text text-anchor=\"start\" x=\"203\" y=\"-717\" font-family=\"Linux libertine\" font-size=\"10.00\">TransformerEncoderLayer</text>\n",
       "<text text-anchor=\"start\" x=\"247\" y=\"-706\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"333,-714 333,-735 381,-735 381,-714 333,-714\"/>\n",
       "<text text-anchor=\"start\" x=\"343\" y=\"-722\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-714 381,-735 494,-735 494,-714 381,-714\"/>\n",
       "<text text-anchor=\"start\" x=\"386\" y=\"-722\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256), (32, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"333,-693 333,-714 381,-714 381,-693 333,-693\"/>\n",
       "<text text-anchor=\"start\" x=\"338\" y=\"-701\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-693 381,-714 494,-714 494,-693 381,-693\"/>\n",
       "<text text-anchor=\"start\" x=\"408\" y=\"-701\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;11 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>4&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M582.64,-1005.97C584.69,-985.76 586.25,-953.78 580,-927 562.43,-851.7 560.75,-822.42 503,-771 487.49,-757.19 468.34,-746.72 448.81,-738.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"449.79,-735.44 439.2,-735.13 447.29,-741.97 449.79,-735.44\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"494,-657 198,-657 198,-615 494,-615 494,-657\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"198,-615 198,-657 333,-657 333,-615 198,-615\"/>\n",
       "<text text-anchor=\"start\" x=\"203\" y=\"-639\" font-family=\"Linux libertine\" font-size=\"10.00\">TransformerEncoderLayer</text>\n",
       "<text text-anchor=\"start\" x=\"247\" y=\"-628\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"333,-636 333,-657 381,-657 381,-636 333,-636\"/>\n",
       "<text text-anchor=\"start\" x=\"343\" y=\"-644\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-636 381,-657 494,-657 494,-636 381,-636\"/>\n",
       "<text text-anchor=\"start\" x=\"386\" y=\"-644\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256), (32, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"333,-615 333,-636 381,-636 381,-615 333,-615\"/>\n",
       "<text text-anchor=\"start\" x=\"338\" y=\"-623\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-615 381,-636 494,-636 494,-615 381,-615\"/>\n",
       "<text text-anchor=\"start\" x=\"408\" y=\"-623\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;12 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>4&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M597.25,-1005.98C619.67,-977.84 656,-924.05 656,-871 656,-871 656,-871 656,-791 656,-715.07 580.49,-676.58 504,-657.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"504.74,-653.64 494.19,-654.66 503.07,-660.44 504.74,-653.64\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"494,-579 198,-579 198,-537 494,-537 494,-579\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"198,-537 198,-579 333,-579 333,-537 198,-537\"/>\n",
       "<text text-anchor=\"start\" x=\"203\" y=\"-561\" font-family=\"Linux libertine\" font-size=\"10.00\">TransformerEncoderLayer</text>\n",
       "<text text-anchor=\"start\" x=\"247\" y=\"-550\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"333,-558 333,-579 381,-579 381,-558 333,-558\"/>\n",
       "<text text-anchor=\"start\" x=\"343\" y=\"-566\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-558 381,-579 494,-579 494,-558 381,-558\"/>\n",
       "<text text-anchor=\"start\" x=\"386\" y=\"-566\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256), (32, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"333,-537 333,-558 381,-558 381,-537 333,-537\"/>\n",
       "<text text-anchor=\"start\" x=\"338\" y=\"-545\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-537 381,-558 494,-558 494,-537 381,-537\"/>\n",
       "<text text-anchor=\"start\" x=\"408\" y=\"-545\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;13 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>4&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M627.83,-1005.83C631.13,-1003.46 634.24,-1000.85 637,-998 679.98,-953.5 694,-932.87 694,-871 694,-871 694,-871 694,-713 694,-623.28 596.07,-585.41 504.11,-569.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"504.52,-566.15 494.09,-567.99 503.39,-573.06 504.52,-566.15\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"162,-969 16,-969 16,-927 162,-927 162,-969\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"16,-927 16,-969 63,-969 63,-927 16,-927\"/>\n",
       "<text text-anchor=\"start\" x=\"31\" y=\"-951\" font-family=\"Linux libertine\" font-size=\"10.00\">triu</text>\n",
       "<text text-anchor=\"start\" x=\"21\" y=\"-940\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"63,-948 63,-969 111,-969 111,-948 63,-948\"/>\n",
       "<text text-anchor=\"start\" x=\"73\" y=\"-956\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"111,-948 111,-969 162,-969 162,-948 111,-948\"/>\n",
       "<text text-anchor=\"start\" x=\"116\" y=\"-956\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"63,-927 63,-948 111,-948 111,-927 63,-927\"/>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-935\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"111,-927 111,-948 162,-948 162,-927 111,-927\"/>\n",
       "<text text-anchor=\"start\" x=\"116\" y=\"-935\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 32) </text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M91.41,-926.63C92.34,-918.82 93.41,-909.73 94.43,-901.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.91,-901.5 95.61,-891.16 90.96,-900.68 97.91,-901.5\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"171,-813 25,-813 25,-771 171,-771 171,-813\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"25,-771 25,-813 72,-813 72,-771 25,-771\"/>\n",
       "<text text-anchor=\"start\" x=\"42\" y=\"-795\" font-family=\"Linux libertine\" font-size=\"10.00\">all</text>\n",
       "<text text-anchor=\"start\" x=\"30\" y=\"-784\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"72,-792 72,-813 120,-813 120,-792 72,-792\"/>\n",
       "<text text-anchor=\"start\" x=\"82\" y=\"-800\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"120,-792 120,-813 171,-813 171,-792 120,-792\"/>\n",
       "<text text-anchor=\"start\" x=\"125\" y=\"-800\" font-family=\"Linux libertine\" font-size=\"10.00\">(32, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"72,-771 72,-792 120,-792 120,-771 72,-771\"/>\n",
       "<text text-anchor=\"start\" x=\"77\" y=\"-779\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"120,-771 120,-792 171,-792 171,-771 120,-771\"/>\n",
       "<text text-anchor=\"start\" x=\"140\" y=\"-779\" font-family=\"Linux libertine\" font-size=\"10.00\">() </text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M98,-848.63C98,-840.82 98,-831.73 98,-823.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"101.5,-823.16 98,-813.16 94.5,-823.16 101.5,-823.16\"/>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M332.82,-926.63C334.69,-918.73 336.88,-909.53 338.92,-900.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"342.33,-901.7 341.23,-891.16 335.51,-900.08 342.33,-901.7\"/>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346,-848.63C346,-840.82 346,-831.73 346,-823.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.5,-823.16 346,-813.16 342.5,-823.16 349.5,-823.16\"/>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346,-770.63C346,-762.82 346,-753.73 346,-745.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.5,-745.16 346,-735.16 342.5,-745.16 349.5,-745.16\"/>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346,-692.63C346,-684.82 346,-675.73 346,-667.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.5,-667.16 346,-657.16 342.5,-667.16 349.5,-667.16\"/>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346,-614.63C346,-606.82 346,-597.73 346,-589.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.5,-589.16 346,-579.16 342.5,-589.16 349.5,-589.16\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"437,-501 255,-501 255,-459 437,-459 437,-501\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"255,-459 255,-501 320,-501 320,-459 255,-459\"/>\n",
       "<text text-anchor=\"start\" x=\"260\" y=\"-483\" font-family=\"Linux libertine\" font-size=\"10.00\">LayerNorm</text>\n",
       "<text text-anchor=\"start\" x=\"269\" y=\"-472\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"320,-480 320,-501 368,-501 368,-480 320,-480\"/>\n",
       "<text text-anchor=\"start\" x=\"330\" y=\"-488\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"368,-480 368,-501 437,-501 437,-480 368,-480\"/>\n",
       "<text text-anchor=\"start\" x=\"373\" y=\"-488\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"320,-459 320,-480 368,-480 368,-459 320,-459\"/>\n",
       "<text text-anchor=\"start\" x=\"325\" y=\"-467\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"368,-459 368,-480 437,-480 437,-459 368,-459\"/>\n",
       "<text text-anchor=\"start\" x=\"373\" y=\"-467\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346,-536.63C346,-528.82 346,-519.73 346,-511.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.5,-511.16 346,-501.16 342.5,-511.16 349.5,-511.16\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"428,-423 264,-423 264,-381 428,-381 428,-423\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"264,-381 264,-423 311,-423 311,-381 264,-381\"/>\n",
       "<text text-anchor=\"start\" x=\"273.5\" y=\"-405\" font-family=\"Linux libertine\" font-size=\"10.00\">mean</text>\n",
       "<text text-anchor=\"start\" x=\"269\" y=\"-394\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"311,-402 311,-423 359,-423 359,-402 311,-402\"/>\n",
       "<text text-anchor=\"start\" x=\"321\" y=\"-410\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"359,-402 359,-423 428,-423 428,-402 359,-402\"/>\n",
       "<text text-anchor=\"start\" x=\"364\" y=\"-410\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 32, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"311,-381 311,-402 359,-402 359,-381 311,-381\"/>\n",
       "<text text-anchor=\"start\" x=\"316\" y=\"-389\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"359,-381 359,-402 428,-402 428,-381 359,-381\"/>\n",
       "<text text-anchor=\"start\" x=\"373\" y=\"-389\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 256) </text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346,-458.63C346,-450.82 346,-441.73 346,-433.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.5,-433.16 346,-423.16 342.5,-433.16 349.5,-433.16\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"419,-344 273,-344 273,-302 419,-302 419,-344\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"273,-302 273,-344 320,-344 320,-302 273,-302\"/>\n",
       "<text text-anchor=\"start\" x=\"281\" y=\"-326\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"278\" y=\"-315\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"320,-323 320,-344 368,-344 368,-323 320,-323\"/>\n",
       "<text text-anchor=\"start\" x=\"330\" y=\"-331\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"368,-323 368,-344 419,-344 419,-323 368,-323\"/>\n",
       "<text text-anchor=\"start\" x=\"373\" y=\"-331\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"320,-302 320,-323 368,-323 368,-302 320,-302\"/>\n",
       "<text text-anchor=\"start\" x=\"325\" y=\"-310\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"368,-302 368,-323 419,-323 419,-302 368,-302\"/>\n",
       "<text text-anchor=\"start\" x=\"373\" y=\"-310\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 256) </text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346,-380.77C346,-372.63 346,-363.08 346,-354.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.5,-354.1 346,-344.1 342.5,-354.1 349.5,-354.1\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"419,-266 273,-266 273,-224 419,-224 419,-266\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"273,-224 273,-266 320,-266 320,-224 273,-224\"/>\n",
       "<text text-anchor=\"start\" x=\"283\" y=\"-248\" font-family=\"Linux libertine\" font-size=\"10.00\">GELU</text>\n",
       "<text text-anchor=\"start\" x=\"278\" y=\"-237\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"320,-245 320,-266 368,-266 368,-245 320,-245\"/>\n",
       "<text text-anchor=\"start\" x=\"330\" y=\"-253\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"368,-245 368,-266 419,-266 419,-245 368,-245\"/>\n",
       "<text text-anchor=\"start\" x=\"373\" y=\"-253\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"320,-224 320,-245 368,-245 368,-224 320,-224\"/>\n",
       "<text text-anchor=\"start\" x=\"325\" y=\"-232\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"368,-224 368,-245 419,-245 419,-224 368,-224\"/>\n",
       "<text text-anchor=\"start\" x=\"373\" y=\"-232\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 256) </text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346,-301.63C346,-293.82 346,-284.73 346,-276.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.5,-276.16 346,-266.16 342.5,-276.16 349.5,-276.16\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"420.5,-188 271.5,-188 271.5,-146 420.5,-146 420.5,-188\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"272,-146 272,-188 322,-188 322,-146 272,-146\"/>\n",
       "<text text-anchor=\"start\" x=\"277\" y=\"-170\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"278.5\" y=\"-159\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"322,-167 322,-188 370,-188 370,-167 322,-167\"/>\n",
       "<text text-anchor=\"start\" x=\"332\" y=\"-175\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"370,-167 370,-188 421,-188 421,-167 370,-167\"/>\n",
       "<text text-anchor=\"start\" x=\"375\" y=\"-175\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"322,-146 322,-167 370,-167 370,-146 322,-146\"/>\n",
       "<text text-anchor=\"start\" x=\"327\" y=\"-154\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"370,-146 370,-167 421,-167 421,-146 370,-146\"/>\n",
       "<text text-anchor=\"start\" x=\"375\" y=\"-154\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 256) </text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346,-223.63C346,-215.82 346,-206.73 346,-198.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.5,-198.16 346,-188.16 342.5,-198.16 349.5,-198.16\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>19</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"419,-110 273,-110 273,-68 419,-68 419,-110\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"273,-68 273,-110 320,-110 320,-68 273,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"281\" y=\"-92\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"278\" y=\"-81\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"320,-89 320,-110 368,-110 368,-89 320,-89\"/>\n",
       "<text text-anchor=\"start\" x=\"330\" y=\"-97\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"368,-89 368,-110 419,-110 419,-89 368,-89\"/>\n",
       "<text text-anchor=\"start\" x=\"373\" y=\"-97\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"320,-68 320,-89 368,-89 368,-68 320,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"325\" y=\"-76\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"368,-68 368,-89 419,-89 419,-68 368,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"379\" y=\"-76\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 2) </text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;19 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>18&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346,-145.63C346,-137.82 346,-128.73 346,-120.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.5,-120.16 346,-110.16 342.5,-120.16 349.5,-120.16\"/>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>20</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"402.5,-32 289.5,-32 289.5,0 402.5,0 402.5,-32\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"290,0 290,-32 367,-32 367,0 290,0\"/>\n",
       "<text text-anchor=\"start\" x=\"295\" y=\"-19\" font-family=\"Linux libertine\" font-size=\"10.00\">output&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"310\" y=\"-8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"367,0 367,-32 403,-32 403,0 367,0\"/>\n",
       "<text text-anchor=\"start\" x=\"372\" y=\"-13.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 2)</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;20 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>19&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346,-67.84C346,-59.89 346,-50.66 346,-42.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.5,-42.24 346,-32.24 342.5,-42.24 349.5,-42.24\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x79eed608f0e0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate simple diagram for the classifier model\n",
    "print(\"Generating classifier model diagram...\")\n",
    "\n",
    "# Create dummy input\n",
    "# Using a small batch size for the diagram\n",
    "dummy_batch_size = 2\n",
    "dummy_seq_len = 32\n",
    "dummy_input_ids = torch.randint(0, tokenizer_vocab, (dummy_batch_size, dummy_seq_len), dtype=torch.long).to(device)\n",
    "\n",
    "classifier_graph = draw_graph(\n",
    "    classifier_model,\n",
    "    input_data=dummy_input_ids,\n",
    "    device=device,\n",
    "    expand_nested=True,\n",
    "    depth=2, # Keep depth low for simplicity\n",
    "    save_graph=False\n",
    ")\n",
    "\n",
    "classifier_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a947240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " TagalogLMClassifier - Full Model Summary\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      " Language Model Backbone Summary\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape        Output Shape       Param #            Trainable\n",
       "====================================================================================================================================\n",
       "MiniTransformerLanguageModel (MiniTransformerLanguageModel)  [32, 256]          [32, 256, 32000]   131,072            False\n",
       "├─Embedding (token_embedding)                                [32, 256]          [32, 256, 256]     (8,192,000)        False\n",
       "├─Dropout (dropout)                                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "├─TransformerEncoder (transformer)                           [32, 256, 256]     [32, 256, 256]     --                 False\n",
       "│    └─ModuleList (layers)                                   --                 --                 --                 False\n",
       "│    │    └─TransformerEncoderLayer (0)                      [32, 256, 256]     [32, 256, 256]     --                 False\n",
       "│    │    │    └─LayerNorm (norm1)                           [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "│    │    │    └─MultiheadAttention (self_attn)              [32, 256, 256]     [32, 256, 256]     (263,168)          False\n",
       "│    │    │    └─Dropout (dropout1)                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "│    │    │    └─LayerNorm (norm2)                           [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "│    │    │    └─Linear (linear1)                            [32, 256, 256]     [32, 256, 1024]    (263,168)          False\n",
       "│    │    │    └─Dropout (dropout)                           [32, 256, 1024]    [32, 256, 1024]    --                 --\n",
       "│    │    │    └─Linear (linear2)                            [32, 256, 1024]    [32, 256, 256]     (262,400)          False\n",
       "│    │    │    └─Dropout (dropout2)                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "│    │    └─TransformerEncoderLayer (1)                      [32, 256, 256]     [32, 256, 256]     --                 False\n",
       "│    │    │    └─LayerNorm (norm1)                           [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "│    │    │    └─MultiheadAttention (self_attn)              [32, 256, 256]     [32, 256, 256]     (263,168)          False\n",
       "│    │    │    └─Dropout (dropout1)                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "│    │    │    └─LayerNorm (norm2)                           [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "│    │    │    └─Linear (linear1)                            [32, 256, 256]     [32, 256, 1024]    (263,168)          False\n",
       "│    │    │    └─Dropout (dropout)                           [32, 256, 1024]    [32, 256, 1024]    --                 --\n",
       "│    │    │    └─Linear (linear2)                            [32, 256, 1024]    [32, 256, 256]     (262,400)          False\n",
       "│    │    │    └─Dropout (dropout2)                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "│    │    └─TransformerEncoderLayer (2)                      [32, 256, 256]     [32, 256, 256]     --                 False\n",
       "│    │    │    └─LayerNorm (norm1)                           [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "│    │    │    └─MultiheadAttention (self_attn)              [32, 256, 256]     [32, 256, 256]     (263,168)          False\n",
       "│    │    │    └─Dropout (dropout1)                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "│    │    │    └─LayerNorm (norm2)                           [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "│    │    │    └─Linear (linear1)                            [32, 256, 256]     [32, 256, 1024]    (263,168)          False\n",
       "│    │    │    └─Dropout (dropout)                           [32, 256, 1024]    [32, 256, 1024]    --                 --\n",
       "│    │    │    └─Linear (linear2)                            [32, 256, 1024]    [32, 256, 256]     (262,400)          False\n",
       "│    │    │    └─Dropout (dropout2)                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "│    │    └─TransformerEncoderLayer (3)                      [32, 256, 256]     [32, 256, 256]     --                 False\n",
       "│    │    │    └─LayerNorm (norm1)                           [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "│    │    │    └─MultiheadAttention (self_attn)              [32, 256, 256]     [32, 256, 256]     (263,168)          False\n",
       "│    │    │    └─Dropout (dropout1)                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "│    │    │    └─LayerNorm (norm2)                           [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "│    │    │    └─Linear (linear1)                            [32, 256, 256]     [32, 256, 1024]    (263,168)          False\n",
       "│    │    │    └─Dropout (dropout)                           [32, 256, 1024]    [32, 256, 1024]    --                 --\n",
       "│    │    │    └─Linear (linear2)                            [32, 256, 1024]    [32, 256, 256]     (262,400)          False\n",
       "│    │    │    └─Dropout (dropout2)                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "│    │    └─TransformerEncoderLayer (4)                      [32, 256, 256]     [32, 256, 256]     --                 False\n",
       "│    │    │    └─LayerNorm (norm1)                           [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "│    │    │    └─MultiheadAttention (self_attn)              [32, 256, 256]     [32, 256, 256]     (263,168)          False\n",
       "│    │    │    └─Dropout (dropout1)                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "│    │    │    └─LayerNorm (norm2)                           [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "│    │    │    └─Linear (linear1)                            [32, 256, 256]     [32, 256, 1024]    (263,168)          False\n",
       "│    │    │    └─Dropout (dropout)                           [32, 256, 1024]    [32, 256, 1024]    --                 --\n",
       "│    │    │    └─Linear (linear2)                            [32, 256, 1024]    [32, 256, 256]     (262,400)          False\n",
       "│    │    │    └─Dropout (dropout2)                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "│    │    └─TransformerEncoderLayer (5)                      [32, 256, 256]     [32, 256, 256]     --                 False\n",
       "│    │    │    └─LayerNorm (norm1)                           [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "│    │    │    └─MultiheadAttention (self_attn)              [32, 256, 256]     [32, 256, 256]     (263,168)          False\n",
       "│    │    │    └─Dropout (dropout1)                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "│    │    │    └─LayerNorm (norm2)                           [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "│    │    │    └─Linear (linear1)                            [32, 256, 256]     [32, 256, 1024]    (263,168)          False\n",
       "│    │    │    └─Dropout (dropout)                           [32, 256, 1024]    [32, 256, 1024]    --                 --\n",
       "│    │    │    └─Linear (linear2)                            [32, 256, 1024]    [32, 256, 256]     (262,400)          False\n",
       "│    │    │    └─Dropout (dropout2)                          [32, 256, 256]     [32, 256, 256]     --                 --\n",
       "├─LayerNorm (layer_norm)                                     [32, 256, 256]     [32, 256, 256]     (512)              False\n",
       "├─Linear (lm_head)                                           [32, 256, 256]     [32, 256, 32000]   (8,224,000)        False\n",
       "====================================================================================================================================\n",
       "Total params: 21,286,144\n",
       "Trainable params: 0\n",
       "Non-trainable params: 21,286,144\n",
       "Total mult-adds (Units.MEGABYTES): 626.43\n",
       "====================================================================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 2835.35\n",
       "Params size (MB): 78.30\n",
       "Estimated Total Size (MB): 2913.72\n",
       "===================================================================================================================================="
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Architecture Summary using torchinfo\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except ImportError:\n",
    "    !pip install torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Summary for the full classifier model\n",
    "print(\"=\" * 70)\n",
    "print(\" TagalogLMClassifier - Full Model Summary\")\n",
    "print(\"=\" * 70)\n",
    "summary(\n",
    "    classifier_model,\n",
    "    input_size=(CLASSIFICATION_BATCH_SIZE, CLASSIFICATION_MAX_LENGTH),\n",
    "    dtypes=[torch.long],\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=18,\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=4,\n",
    ")\n",
    "\n",
    "# Summary for just the language model backbone\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" Language Model Backbone Summary\")\n",
    "print(\"=\" * 70)\n",
    "summary(\n",
    "    classifier_model.language_model,\n",
    "    input_size=(CLASSIFICATION_BATCH_SIZE, CLASSIFICATION_MAX_LENGTH),\n",
    "    dtypes=[torch.long],\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=18,\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b6e45",
   "metadata": {},
   "source": [
    "Training of the classifier + LM backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35978d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "def collate_batch(tokenizer, max_length: int, batch: List[Tuple[str, int]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    texts, labels = zip(*batch)\n",
    "    encoding = tokenizer(\n",
    "        list(texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    label_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    return encoding[\"input_ids\"], encoding[\"attention_mask\"], label_tensor\n",
    "\n",
    "def compute_metrics(logits: torch.Tensor, labels: torch.Tensor) -> Dict[str, float]:\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "\n",
    "    tp = ((preds == 1) & (labels == 1)).sum().item()\n",
    "    fp = ((preds == 1) & (labels == 0)).sum().item()\n",
    "    fn = ((preds == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": correct / total if total > 0 else 0.0,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "def aggregate_metrics(metric_list: List[Dict[str, float]]) -> Dict[str, float]:\n",
    "    if not metric_list:\n",
    "        return {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    keys = metric_list[0].keys()\n",
    "    results = {}\n",
    "    for key in keys:\n",
    "        values = [metrics[key] for metrics in metric_list]\n",
    "        results[key] = float(np.mean(values))\n",
    "        results[f\"{key}_std\"] = float(np.std(values))\n",
    "    return results\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: TagalogLMClassifier,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    metrics_accumulator: List[Dict[str, float]] = []\n",
    "\n",
    "    for input_ids, attention_mask, targets in loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        metrics_accumulator.append(compute_metrics(logits.cpu(), targets.cpu()))\n",
    "\n",
    "    avg_loss = total_loss / max(len(loader), 1)\n",
    "    avg_metrics = aggregate_metrics(metrics_accumulator)\n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "def train_classifier_one_epoch(\n",
    "    model: TagalogLMClassifier,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler: Optional[GradScaler],\n",
    "    device: torch.device,\n",
    "    grad_clip: float,\n",
    "    use_autocast: bool,\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    metrics_accumulator: List[Dict[str, float]] = []\n",
    "\n",
    "    for input_ids, attention_mask, targets in loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=use_autocast):\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "        if scaler is not None and use_autocast:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        total_loss += loss_val\n",
    "        metrics_accumulator.append(compute_metrics(logits.detach().cpu(), targets.detach().cpu()))\n",
    "\n",
    "    avg_loss = total_loss / max(len(loader), 1)\n",
    "    avg_metrics = aggregate_metrics(metrics_accumulator)\n",
    "    return avg_loss, avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5cfa363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17172/2128070876.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=True)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW([p for p in classifier_model.parameters() if p.requires_grad], lr=CLASSIFIER_LEARNING_RATE, weight_decay=CLASSIFIER_WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, CLASSIFIER_EPOCHS))\n",
    "scaler = GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66f9b260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting multi-seed training with seeds: [42, 43, 44, 45, 46]\n",
      "\n",
      "==================== Run with Seed 42 (1/5) ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17172/153772849.py:38: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb60382367de4679b7c6258632b36856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 42:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17172/516048449.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 42 Finished | Best F1: 0.7794\n",
      "\n",
      "==================== Run with Seed 43 (2/5) ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5092c2fd20b349e999e8e971dd538271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 43:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 43 Finished | Best F1: 0.7751\n",
      "\n",
      "==================== Run with Seed 44 (3/5) ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8d2aef3f404bdface4b7216aae8976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 44:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 44 Finished | Best F1: 0.7749\n",
      "\n",
      "==================== Run with Seed 45 (4/5) ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef46a2ede5ac440eb325d2e07ade7051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 45:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 45 Finished | Best F1: 0.7711\n",
      "\n",
      "==================== Run with Seed 46 (5/5) ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ce92b0240240f68cea025c5a33c34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 46:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 46 Finished | Best F1: 0.7774\n",
      "\n",
      "==================================================\n",
      "Multi-run Results (5 runs):\n",
      "Accuracy: 0.7959 ± 0.0043\n",
      "Precision: 0.7765 ± 0.0119\n",
      "Recall: 0.7864 ± 0.0118\n",
      "F1: 0.7756 ± 0.0028\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "SEEDS = [42, 43, 44, 45, 46]\n",
    "PATIENCE_LIMIT = 10\n",
    "all_run_metrics = []\n",
    "\n",
    "print(f\"Starting multi-seed training with seeds: {SEEDS}\")\n",
    "\n",
    "# We'll keep the history of the last run for visualization purposes\n",
    "history: List[Dict[str, Dict[str, float]]] = []\n",
    "\n",
    "for seed_idx, seed in enumerate(SEEDS):\n",
    "    print(f\"\\n{'='*20} Run with Seed {seed} ({seed_idx+1}/{len(SEEDS)}) {'='*20}\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "    # Re-initialize model\n",
    "    # Deepcopy backbone to ensure fresh start for each seed\n",
    "    current_backbone = copy.deepcopy(model)\n",
    "    \n",
    "    classifier_config = TagalogLMClassifierConfig(\n",
    "        num_labels=num_labels, \n",
    "        classifier_dropout=CLASSIFIER_DROPOUT,\n",
    "        pooling=POOLING_STRATEGY,\n",
    "        fine_tune_base=FINE_TUNE_BASE,\n",
    "    )\n",
    "    classifier_model = TagalogLMClassifier(current_backbone, classifier_config).to(device)\n",
    "    \n",
    "    # Re-initialize Optimizer and Scheduler\n",
    "    optimizer = AdamW([p for p in classifier_model.parameters() if p.requires_grad], lr=CLASSIFIER_LEARNING_RATE, weight_decay=CLASSIFIER_WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, CLASSIFIER_EPOCHS))\n",
    "    scaler = GradScaler(enabled=True)\n",
    "    \n",
    "    # Training Loop\n",
    "    best_val_f1 = 0.0\n",
    "    best_metrics_for_seed = {}\n",
    "    patience_counter = 0\n",
    "    model_path = f\"{OUTPUT_DIR}/tagalog_classifier_seed_{seed}.pt\"\n",
    "    \n",
    "    # Reset history for this seed\n",
    "    history = []\n",
    "    \n",
    "    # Use tqdm.notebook for proper Jupyter display\n",
    "    epoch_pbar = tqdm(range(1, CLASSIFIER_EPOCHS + 1), desc=f\"Seed {seed}\", leave=True)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        train_loss, train_metrics = train_classifier_one_epoch(\n",
    "            classifier_model,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            scaler,\n",
    "            device,\n",
    "            CLASSIFIER_GRADIENT_CLIPPING,\n",
    "            True,\n",
    "        )\n",
    "        val_loss, val_metrics = evaluate(classifier_model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_metrics\": train_metrics,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_metrics\": val_metrics,\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Update progress bar with metrics\n",
    "        epoch_pbar.set_postfix({\n",
    "            \"train_loss\": f\"{train_loss:.4f}\",\n",
    "            \"val_f1\": f\"{val_metrics.get('f1', 0.0):.4f}\",\n",
    "            \"best_f1\": f\"{best_val_f1:.4f}\",\n",
    "            \"patience\": f\"{patience_counter}/{PATIENCE_LIMIT}\"\n",
    "        })\n",
    "\n",
    "        improved = val_metrics.get(\"f1\", 0.0) > best_val_f1\n",
    "        if improved:\n",
    "            best_val_f1 = val_metrics.get(\"f1\", 0.0)\n",
    "            best_metrics_for_seed = val_metrics\n",
    "            patience_counter = 0\n",
    "            torch.save(classifier_model.state_dict(), model_path)\n",
    "    \n",
    "    print(f\"Seed {seed} Finished | Best F1: {best_val_f1:.4f}\")\n",
    "    if best_metrics_for_seed:\n",
    "        all_run_metrics.append(best_metrics_for_seed)\n",
    "\n",
    "# Aggregation and Reporting\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Multi-run Results ({len(SEEDS)} runs):\")\n",
    "metrics_keys = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "for key in metrics_keys:\n",
    "    values = [m.get(key, 0.0) for m in all_run_metrics]\n",
    "    if values:\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{key.capitalize()}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key.capitalize()}: N/A\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73627ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history with error bars\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    epochs = [h[\"epoch\"] for h in history]\n",
    "    train_loss = [h[\"train_loss\"] for h in history]\n",
    "    val_loss = [h[\"val_loss\"] for h in history]\n",
    "    \n",
    "    val_f1 = [h[\"val_metrics\"][\"f1\"] for h in history]\n",
    "    val_f1_std = [h[\"val_metrics\"].get(\"f1_std\", 0.0) for h in history]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_loss, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    \n",
    "    # Plot F1 with Error Bars\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.errorbar(epochs, val_f1, yerr=val_f1_std, label=\"Val F1\", fmt='-o', capsize=5)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Validation F1 Score (with Batch Variance)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if history:\n",
    "    plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai351",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
