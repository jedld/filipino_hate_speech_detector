{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6732162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jedld/miniconda3/envs/ai351/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import inspect\n",
    "import json\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig  # type: ignore\n",
    "except ImportError:  # pragma: no cover - optional optimization\n",
    "    BitsAndBytesConfig = None\n",
    "\n",
    "try:\n",
    "    from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training  # type: ignore[import]\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:  # pragma: no cover - dependency guard\n",
    "    LoraConfig = TaskType = None  # type: ignore[assignment]\n",
    "    get_peft_model = prepare_model_for_kbit_training = None  # type: ignore[assignment]\n",
    "    PEFT_AVAILABLE = False\n",
    "\n",
    "import tqdm.auto as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e0a5b",
   "metadata": {},
   "source": [
    "Experiment # 4\n",
    "======================\n",
    "\n",
    "XLM-RoBERTa (Multilingual) backbone + classifier head\n",
    "\n",
    "This experiment uses `xlm-roberta-base` (or similar multilingual models) instead of Llama 3. XLM-RoBERTa is an encoder-based model optimized for multilingual representation, making it a strong candidate for Tagalog and mixed-language datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd74ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "TRAIN_BF16 = True\n",
    "TRAIN_FP16 = True\n",
    "TRAIN_HEAD_ONLY = False # Changed to False to enable LoRA fine-tuning\n",
    "\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 64 # Usually 2x Rank\n",
    "LORA_DROPOUT = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d0bb7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fbb86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TokenizedDataset:\n",
    "    train: Dataset\n",
    "    validation: Dataset\n",
    "    test: Dataset\n",
    "    label2id: Dict[str, int]\n",
    "    id2label: Dict[int, str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f562f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred) -> Dict[str, float]:\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    labels = labels.astype(int)\n",
    "    accuracy = (preds == labels).mean().item()\n",
    "\n",
    "    classes = np.unique(labels)\n",
    "    precisions: List[float] = []\n",
    "    recalls: List[float] = []\n",
    "    f1_scores: List[float] = []\n",
    "\n",
    "    for cls in classes:\n",
    "        tp = np.logical_and(preds == cls, labels == cls).sum()\n",
    "        fp = np.logical_and(preds == cls, labels != cls).sum()\n",
    "        fn = np.logical_and(preds != cls, labels == cls).sum()\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    macro_precision = float(np.mean(precisions))\n",
    "    macro_recall = float(np.mean(recalls))\n",
    "    macro_f1 = float(np.mean(f1_scores))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": macro_precision,\n",
    "        \"recall\": macro_recall,\n",
    "        \"f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "def convert_labels(df: pd.DataFrame, label2id: Dict[str, int]) -> pd.DataFrame:\n",
    "    mapped = df.copy()\n",
    "    mapped[\"label\"] = mapped[\"label\"].astype(str).map(label2id)\n",
    "    if mapped[\"label\"].isnull().any():\n",
    "        missing = df.loc[mapped[\"label\"].isnull(), \"label\"].unique()\n",
    "        raise ValueError(f\"Encountered labels not in training set: {missing}\")\n",
    "    mapped[\"label\"] = mapped[\"label\"].astype(int)\n",
    "    return mapped\n",
    "\n",
    "def prepare_label_mappings(train_df: pd.DataFrame) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    unique_labels = sorted({str(label) for label in train_df[\"label\"].tolist()})\n",
    "    if len(unique_labels) < 2:\n",
    "        raise ValueError(\"Classification requires at least two unique labels.\")\n",
    "    label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "    return label2id, id2label\n",
    "\n",
    "def load_dataframe(path: Path, text_column: str, label_column: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if text_column not in df.columns or label_column not in df.columns:\n",
    "        raise ValueError(f\"File {path} must contain '{text_column}' and '{label_column}' columns.\")\n",
    "    return df[[text_column, label_column]].rename(columns={text_column: \"text\", label_column: \"label\"})\n",
    "\n",
    "def log_trainable_parameters(model: torch.nn.Module) -> None:\n",
    "    total_params = sum(param.numel() for param in model.parameters())\n",
    "    trainable_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "    ratio = (trainable_params / total_params * 100.0) if total_params > 0 else 0.0\n",
    "    print(\n",
    "        f\"Trainable params: {trainable_params:,} | Total params: {total_params:,} | Trainable%: {ratio:.4f}\"\n",
    "    )\n",
    "\n",
    "def tokenize_dataframe(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    tokenizer,\n",
    "    max_length: int,\n",
    ") -> TokenizedDataset:\n",
    "    train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "    val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "    test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "\n",
    "    def tokenize_batch(batch: Dict[str, List[str]]) -> Dict[str, List[int]]:\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    train_dataset = train_dataset.map(tokenize_batch, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_batch, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_batch, batched=True)\n",
    "\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    return TokenizedDataset(train=train_dataset, validation=val_dataset, test=test_dataset, label2id={}, id2label={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fbc2f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    base_model: str,\n",
    "    num_labels: int,\n",
    "    device: str,\n",
    "    bf16 : bool,\n",
    "    fp16 : bool,\n",
    "    train_head_only: bool,\n",
    "    lora_r: int,\n",
    "    lora_alpha: int,\n",
    "    lora_dropout: float,\n",
    "    lora_target_modules: Optional[List[str]],\n",
    ") -> torch.nn.Module:\n",
    "    # Use bfloat16 or float16 to fit in 24GB VRAM (Llama-3-8B is ~16GB in half precision)\n",
    "    if bf16:\n",
    "        torch_dtype = torch.bfloat16\n",
    "    elif fp16:\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        torch_dtype = torch.float32\n",
    "\n",
    "    def quant_config(is_4bit: bool, is_8bit: bool):\n",
    "        if not (is_4bit or is_8bit):\n",
    "            return None\n",
    "        if BitsAndBytesConfig is None:\n",
    "            raise RuntimeError(\"bitsandbytes is required for quantized loading but is not available.\")\n",
    "        if is_4bit and is_8bit:\n",
    "            raise ValueError(\"Only one of 4-bit or 8-bit quantization can be enabled at a time.\")\n",
    "        if is_4bit:\n",
    "            return BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "            )\n",
    "        return BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
    "\n",
    "    # For smaller models like XLM-R (270M), we might not need quantization, but keeping logic generic\n",
    "    quantization_config = quant_config(False, False)\n",
    "\n",
    "    device_map = None\n",
    "    if device == \"auto\":\n",
    "        if quantization_config is not None:\n",
    "            device_map = \"auto\"\n",
    "        elif not train_head_only:\n",
    "            device_map = \"auto\"\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model,\n",
    "        num_labels=num_labels,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=device_map,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    model.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "    if train_head_only:\n",
    "        for name, param in model.named_parameters():\n",
    "            trainable = name.startswith(\"score\") or name.startswith(\"classifier\")\n",
    "            param.requires_grad = trainable\n",
    "        # Handle different classifier head names (RoBERTa uses 'classifier', Llama uses 'score')\n",
    "        if hasattr(model, \"score\") and hasattr(model.score, \"weight\"):\n",
    "            torch.nn.init.normal_(model.score.weight, mean=0.0, std=model.config.initializer_range)\n",
    "            if getattr(model.score, \"bias\", None) is not None:\n",
    "                torch.nn.init.zeros_(model.score.bias)\n",
    "        elif hasattr(model, \"classifier\"):\n",
    "             # RoBERTa classifier is a module, usually initialized automatically, but we can ensure grad\n",
    "             pass\n",
    "\n",
    "        if device != \"auto\":\n",
    "            target_device = torch.device(device)\n",
    "            model.to(target_device)\n",
    "        return model\n",
    "\n",
    "    if not PEFT_AVAILABLE:\n",
    "        raise ImportError(\n",
    "            \"LoRA fine-tuning requires the 'peft' package. Install it via 'pip install peft'.\"\n",
    "        )\n",
    "\n",
    "    if lora_target_modules is None:\n",
    "        # Auto-detect target modules based on model type\n",
    "        if \"llama\" in base_model.lower():\n",
    "            target_modules = [\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "            ]\n",
    "        elif \"roberta\" in base_model.lower() or \"bert\" in base_model.lower():\n",
    "            target_modules = [\"query\", \"value\"] # Standard for RoBERTa/BERT\n",
    "        else:\n",
    "            # Fallback or let PEFT decide (passing None to LoraConfig might work for some, but explicit is safer)\n",
    "            target_modules = [\"q_proj\", \"v_proj\"] \n",
    "    else:\n",
    "        target_modules = lora_target_modules\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Ensure model is on the correct device when using LoRA\n",
    "    if device != \"auto\":\n",
    "        model.to(device)\n",
    "        \n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dea20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv =Path(\"data/combined/processed/train.csv\")\n",
    "val_csv =Path(\"data/combined/processed/validation.csv\")\n",
    "test_csv=Path(\"data/combined/processed/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df000671",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataframe(train_csv, \"text\", \"label\")\n",
    "val_df = load_dataframe(val_csv,  \"text\", \"label\")\n",
    "test_df = load_dataframe(test_csv,  \"text\", \"label\")\n",
    "\n",
    "label2id, id2label = prepare_label_mappings(train_df)\n",
    "train_df = convert_labels(train_df, label2id)\n",
    "val_df = convert_labels(val_df, label2id)\n",
    "test_df = convert_labels(test_df, label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "267c29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using XLM-RoBERTa Large as requested\n",
    "# Other options: \n",
    "# - \"xlm-roberta-base\" (Faster, less memory)\n",
    "# - \"jcblaise/roberta-tagalog-base\" (Tagalog-specific)\n",
    "base_model = \"xlm-roberta-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82381ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0879bab79bc8433eb03fa940ed7849b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18168 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227fadbfcb8449a79d3b6cdd0621e001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0359c41df324d7681d5be7ff86d0938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\" # RoBERTa usually handles padding correctly, but right is standard\n",
    "\n",
    "tokenized = tokenize_dataframe(train_df, val_df, test_df, tokenizer, MAX_LENGTH)\n",
    "tokenized = TokenizedDataset(\n",
    "    train=tokenized.train,\n",
    "    validation=tokenized.validation,\n",
    "    test=tokenized.test,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e41925",
   "metadata": {},
   "source": [
    "Setup model builder, pretrained XLM Roberta backbone + classifier head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc061e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,129,538 || all params: 568,022,020 || trainable%: 1.4312\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "        base_model=base_model,\n",
    "        num_labels=len(label2id),\n",
    "        device=device,\n",
    "        bf16=TRAIN_BF16,\n",
    "        fp16=TRAIN_FP16,\n",
    "        train_head_only=TRAIN_HEAD_ONLY,\n",
    "        lora_r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        # Target all linear layers in RoBERTa for maximum adaptation\n",
    "        lora_target_modules=[\"query\", \"key\", \"value\", \"dense\"] \n",
    "    )\n",
    "\n",
    "model.config.label2id = label2id\n",
    "model.config.id2label = id2label\n",
    "if hasattr(model.config, \"use_cache\"):\n",
    "    model.config.use_cache = False\n",
    "\n",
    "if TRAIN_HEAD_ONLY:\n",
    "    log_trainable_parameters(model)\n",
    "\n",
    "# DataCollatorWithPadding uses dynamic padding (padding=\"longest\") which is memory efficient\n",
    "# It pads each batch to the longest sequence in that batch, rather than the model's max length\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e418ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model summary for xlm-roberta-large...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                                          Input Shape          Output Shape         Param #              Trainable\n",
       "================================================================================================================================================================\n",
       "PeftModelForSequenceClassification (PeftModelForSequenceClassification)          --                   [1, 2]               --                   Partial\n",
       "├─LoraModel (base_model)                                                         --                   [1, 2]               --                   Partial\n",
       "│    └─XLMRobertaForSequenceClassification (model)                               --                   --                   --                   Partial\n",
       "│    │    └─XLMRobertaModel (roberta)                                            [1, 512]             [1, 512, 1024]       565,918,720          Partial\n",
       "│    │    └─ModulesToSaveWrapper (classifier)                                    [1, 512, 1024]       [1, 2]               2,103,300            Partial\n",
       "================================================================================================================================================================\n",
       "Total params: 568,022,020\n",
       "Trainable params: 8,129,538\n",
       "Non-trainable params: 559,892,482\n",
       "Total mult-adds (Units.MEGABYTES): 566.97\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1477.45\n",
       "Params size (MB): 1148.10\n",
       "Estimated Total Size (MB): 2625.55\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Architecture Summary using torchinfo\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except ImportError:\n",
    "    print(\"Installing torchinfo...\")\n",
    "    !pip install torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "print(f\"Generating model summary for {base_model}...\")\n",
    "\n",
    "# Create dummy inputs on the correct device to handle the forward pass correctly\n",
    "dummy_batch_size = 1\n",
    "dummy_inputs = {\n",
    "    \"input_ids\": torch.randint(0, tokenizer.vocab_size, (dummy_batch_size, MAX_LENGTH), dtype=torch.long).to(device),\n",
    "    \"attention_mask\": torch.ones((dummy_batch_size, MAX_LENGTH), dtype=torch.long).to(device)\n",
    "}\n",
    "\n",
    "# Display concise summary\n",
    "summary(\n",
    "    model,\n",
    "    input_data=dummy_inputs,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=3, # Depth 3 gives a good balance of detail for Transformer models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d5ddd",
   "metadata": {},
   "source": [
    "Setup training\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a99aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16 \n",
    "EVAL_BATCH_SIZE = 8\n",
    "OUTPUT_DIR = Path(\"models/xlm_roberta_large_classifier\")\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 \n",
    "EPOCHS = 25 # Increased epochs for LoRA convergence\n",
    "LEARNING_RATE = 2e-5 # Higher LR is standard for LoRA (vs 1e-5 or 2e-5 for full finetuning)\n",
    "LR_SCHEDULER_TYPE = \"cosine\" # Cosine schedule is often better for LoRA\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.06\n",
    "EVAL_STEPS = 100\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 100\n",
    "SAVE_TOTAL_LIMIT = 3\n",
    "BF16 = True\n",
    "FP16 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c9d18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73c98c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs: Dict[str, Any] = {}\n",
    "signature = inspect.signature(TrainingArguments.__init__)\n",
    "param_names = set(signature.parameters.keys())\n",
    "\n",
    "def set_kw(name: str, value: Any) -> None:\n",
    "    if name in param_names:\n",
    "        kwargs[name] = value\n",
    "\n",
    "def set_kw_alias(value: Any, *aliases: str) -> None:\n",
    "    for alias in aliases:\n",
    "        if alias in param_names:\n",
    "            kwargs[alias] = value\n",
    "            return\n",
    "\n",
    "set_kw(\"output_dir\", str(OUTPUT_DIR))\n",
    "set_kw(\"per_device_train_batch_size\", BATCH_SIZE)\n",
    "set_kw(\"per_device_eval_batch_size\", EVAL_BATCH_SIZE)\n",
    "set_kw(\"gradient_accumulation_steps\", GRADIENT_ACCUMULATION_STEPS)\n",
    "set_kw(\"num_train_epochs\", EPOCHS)\n",
    "set_kw(\"learning_rate\", LEARNING_RATE)\n",
    "set_kw(\"weight_decay\", WEIGHT_DECAY)\n",
    "set_kw(\"warmup_ratio\", WARMUP_RATIO)\n",
    "set_kw(\"lr_scheduler_type\", LR_SCHEDULER_TYPE) # Use the defined scheduler type\n",
    "set_kw(\"logging_steps\", LOGGING_STEPS)\n",
    "set_kw_alias(\"steps\", \"evaluation_strategy\", \"eval_strategy\")\n",
    "set_kw(\"eval_steps\", EVAL_STEPS)\n",
    "set_kw_alias(\"steps\", \"save_strategy\", \"checkpointing_strategy\")\n",
    "set_kw(\"save_steps\", SAVE_STEPS)\n",
    "set_kw(\"save_total_limit\", SAVE_TOTAL_LIMIT)\n",
    "set_kw(\"load_best_model_at_end\", True)\n",
    "set_kw(\"metric_for_best_model\", \"f1\")\n",
    "set_kw(\"greater_is_better\", True)\n",
    "set_kw(\"bf16\", BF16)\n",
    "set_kw(\"fp16\", FP16)\n",
    "set_kw(\"push_to_hub\", False)\n",
    "set_kw(\"hub_model_id\", None)\n",
    "set_kw(\"hub_token\", None)\n",
    "set_kw(\"remove_unused_columns\", False)\n",
    "set_kw(\"seed\", SEED)\n",
    "\n",
    "set_kw(\"report_to\", [\"tensorboard\"])\n",
    "set_kw(\"disable_tqdm\", False) # Ensure progress bar is shown\n",
    "\n",
    "training_args = TrainingArguments(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e8a9887",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir = OUTPUT_DIR / \"checkpoints\"\n",
    "checkpoints_dir.mkdir(parents=True, exist_ok=True)\n",
    "args = {\n",
    "    \"batch_size\": BATCH_SIZE\n",
    "}\n",
    "\n",
    "trainer_kwargs = {\n",
    "        \"model\": model,\n",
    "        \"args\": training_args,\n",
    "        \"train_dataset\": tokenized.train,\n",
    "        \"eval_dataset\": tokenized.validation,\n",
    "        \"data_collator\": data_collator,\n",
    "        \"compute_metrics\": compute_metrics,\n",
    "    }\n",
    "trainer_signature = inspect.signature(Trainer.__init__)\n",
    "if \"tokenizer\" in trainer_signature.parameters:\n",
    "    trainer_kwargs[\"tokenizer\"] = tokenizer\n",
    "if \"processing_class\" in trainer_signature.parameters:\n",
    "    trainer_kwargs[\"processing_class\"] = tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af0c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment with N=5 runs...\n",
      "\n",
      "========================================\n",
      "Run 1/5 (Seed: 42)\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,129,538 || all params: 568,022,020 || trainable%: 1.4312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4064241/2168342842.py:44: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7100' max='7100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7100/7100 1:01:40, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.727700</td>\n",
       "      <td>0.705095</td>\n",
       "      <td>0.476002</td>\n",
       "      <td>0.607285</td>\n",
       "      <td>0.510918</td>\n",
       "      <td>0.352137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.675400</td>\n",
       "      <td>0.657135</td>\n",
       "      <td>0.614707</td>\n",
       "      <td>0.617683</td>\n",
       "      <td>0.601315</td>\n",
       "      <td>0.593904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.649500</td>\n",
       "      <td>0.632120</td>\n",
       "      <td>0.640687</td>\n",
       "      <td>0.641251</td>\n",
       "      <td>0.641989</td>\n",
       "      <td>0.640374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.633800</td>\n",
       "      <td>0.606676</td>\n",
       "      <td>0.664025</td>\n",
       "      <td>0.676276</td>\n",
       "      <td>0.671542</td>\n",
       "      <td>0.663084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.567300</td>\n",
       "      <td>0.522812</td>\n",
       "      <td>0.738882</td>\n",
       "      <td>0.740547</td>\n",
       "      <td>0.741531</td>\n",
       "      <td>0.738792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>0.481516</td>\n",
       "      <td>0.768384</td>\n",
       "      <td>0.767609</td>\n",
       "      <td>0.768883</td>\n",
       "      <td>0.767809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.487200</td>\n",
       "      <td>0.473736</td>\n",
       "      <td>0.769265</td>\n",
       "      <td>0.768866</td>\n",
       "      <td>0.770289</td>\n",
       "      <td>0.768843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.471400</td>\n",
       "      <td>0.459477</td>\n",
       "      <td>0.779833</td>\n",
       "      <td>0.779016</td>\n",
       "      <td>0.777529</td>\n",
       "      <td>0.778094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>0.456780</td>\n",
       "      <td>0.789960</td>\n",
       "      <td>0.789055</td>\n",
       "      <td>0.788070</td>\n",
       "      <td>0.788480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.446561</td>\n",
       "      <td>0.794804</td>\n",
       "      <td>0.793894</td>\n",
       "      <td>0.793037</td>\n",
       "      <td>0.793403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>0.441251</td>\n",
       "      <td>0.800088</td>\n",
       "      <td>0.800237</td>\n",
       "      <td>0.797048</td>\n",
       "      <td>0.798071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.416200</td>\n",
       "      <td>0.440296</td>\n",
       "      <td>0.798327</td>\n",
       "      <td>0.798066</td>\n",
       "      <td>0.795668</td>\n",
       "      <td>0.796503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.429206</td>\n",
       "      <td>0.802290</td>\n",
       "      <td>0.801739</td>\n",
       "      <td>0.800075</td>\n",
       "      <td>0.800713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.436019</td>\n",
       "      <td>0.810216</td>\n",
       "      <td>0.809176</td>\n",
       "      <td>0.809216</td>\n",
       "      <td>0.809196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.441617</td>\n",
       "      <td>0.808454</td>\n",
       "      <td>0.807456</td>\n",
       "      <td>0.807250</td>\n",
       "      <td>0.807349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.430594</td>\n",
       "      <td>0.808895</td>\n",
       "      <td>0.808658</td>\n",
       "      <td>0.806423</td>\n",
       "      <td>0.807231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>0.415219</td>\n",
       "      <td>0.810216</td>\n",
       "      <td>0.809273</td>\n",
       "      <td>0.808891</td>\n",
       "      <td>0.809069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.393800</td>\n",
       "      <td>0.416186</td>\n",
       "      <td>0.811977</td>\n",
       "      <td>0.813144</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.811871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.391300</td>\n",
       "      <td>0.416734</td>\n",
       "      <td>0.813738</td>\n",
       "      <td>0.812694</td>\n",
       "      <td>0.813670</td>\n",
       "      <td>0.813048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>0.422922</td>\n",
       "      <td>0.818582</td>\n",
       "      <td>0.818532</td>\n",
       "      <td>0.816098</td>\n",
       "      <td>0.816972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.357000</td>\n",
       "      <td>0.412719</td>\n",
       "      <td>0.816821</td>\n",
       "      <td>0.816174</td>\n",
       "      <td>0.815043</td>\n",
       "      <td>0.815516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.410824</td>\n",
       "      <td>0.815500</td>\n",
       "      <td>0.817344</td>\n",
       "      <td>0.811534</td>\n",
       "      <td>0.813087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>0.814179</td>\n",
       "      <td>0.817253</td>\n",
       "      <td>0.809522</td>\n",
       "      <td>0.811331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.432796</td>\n",
       "      <td>0.812858</td>\n",
       "      <td>0.815571</td>\n",
       "      <td>0.808357</td>\n",
       "      <td>0.810093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>0.422224</td>\n",
       "      <td>0.821664</td>\n",
       "      <td>0.822123</td>\n",
       "      <td>0.818774</td>\n",
       "      <td>0.819881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.816821</td>\n",
       "      <td>0.819441</td>\n",
       "      <td>0.820383</td>\n",
       "      <td>0.816795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.307300</td>\n",
       "      <td>0.491292</td>\n",
       "      <td>0.809775</td>\n",
       "      <td>0.816525</td>\n",
       "      <td>0.815122</td>\n",
       "      <td>0.809744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.326400</td>\n",
       "      <td>0.416442</td>\n",
       "      <td>0.822545</td>\n",
       "      <td>0.821658</td>\n",
       "      <td>0.821352</td>\n",
       "      <td>0.821497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.304200</td>\n",
       "      <td>0.432522</td>\n",
       "      <td>0.825187</td>\n",
       "      <td>0.825623</td>\n",
       "      <td>0.822381</td>\n",
       "      <td>0.823470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.313600</td>\n",
       "      <td>0.431476</td>\n",
       "      <td>0.826948</td>\n",
       "      <td>0.825919</td>\n",
       "      <td>0.826822</td>\n",
       "      <td>0.826269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>0.453457</td>\n",
       "      <td>0.828710</td>\n",
       "      <td>0.828507</td>\n",
       "      <td>0.826574</td>\n",
       "      <td>0.827316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.432388</td>\n",
       "      <td>0.827829</td>\n",
       "      <td>0.826809</td>\n",
       "      <td>0.827772</td>\n",
       "      <td>0.827172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.284600</td>\n",
       "      <td>0.437735</td>\n",
       "      <td>0.827389</td>\n",
       "      <td>0.826440</td>\n",
       "      <td>0.827687</td>\n",
       "      <td>0.826830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>0.442227</td>\n",
       "      <td>0.827389</td>\n",
       "      <td>0.826355</td>\n",
       "      <td>0.827167</td>\n",
       "      <td>0.826682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.289100</td>\n",
       "      <td>0.438809</td>\n",
       "      <td>0.829590</td>\n",
       "      <td>0.828748</td>\n",
       "      <td>0.830194</td>\n",
       "      <td>0.829113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.456263</td>\n",
       "      <td>0.830471</td>\n",
       "      <td>0.829452</td>\n",
       "      <td>0.830038</td>\n",
       "      <td>0.829708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>0.471697</td>\n",
       "      <td>0.825187</td>\n",
       "      <td>0.826458</td>\n",
       "      <td>0.827981</td>\n",
       "      <td>0.825095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.449983</td>\n",
       "      <td>0.829590</td>\n",
       "      <td>0.828574</td>\n",
       "      <td>0.829087</td>\n",
       "      <td>0.828803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>0.443704</td>\n",
       "      <td>0.829590</td>\n",
       "      <td>0.829334</td>\n",
       "      <td>0.831106</td>\n",
       "      <td>0.829310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.281300</td>\n",
       "      <td>0.426854</td>\n",
       "      <td>0.834875</td>\n",
       "      <td>0.833866</td>\n",
       "      <td>0.834530</td>\n",
       "      <td>0.834151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.459254</td>\n",
       "      <td>0.829590</td>\n",
       "      <td>0.828807</td>\n",
       "      <td>0.830324</td>\n",
       "      <td>0.829145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.427826</td>\n",
       "      <td>0.833994</td>\n",
       "      <td>0.833242</td>\n",
       "      <td>0.832733</td>\n",
       "      <td>0.832968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.254300</td>\n",
       "      <td>0.444453</td>\n",
       "      <td>0.833113</td>\n",
       "      <td>0.832104</td>\n",
       "      <td>0.832694</td>\n",
       "      <td>0.832362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.256800</td>\n",
       "      <td>0.469432</td>\n",
       "      <td>0.830471</td>\n",
       "      <td>0.829960</td>\n",
       "      <td>0.831666</td>\n",
       "      <td>0.830128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.440456</td>\n",
       "      <td>0.838397</td>\n",
       "      <td>0.837487</td>\n",
       "      <td>0.837616</td>\n",
       "      <td>0.837550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.232600</td>\n",
       "      <td>0.456782</td>\n",
       "      <td>0.834875</td>\n",
       "      <td>0.834038</td>\n",
       "      <td>0.833814</td>\n",
       "      <td>0.833922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.234700</td>\n",
       "      <td>0.462899</td>\n",
       "      <td>0.835755</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.834956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.452124</td>\n",
       "      <td>0.830031</td>\n",
       "      <td>0.830131</td>\n",
       "      <td>0.827609</td>\n",
       "      <td>0.828522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>0.455460</td>\n",
       "      <td>0.837957</td>\n",
       "      <td>0.837454</td>\n",
       "      <td>0.836360</td>\n",
       "      <td>0.836827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.461955</td>\n",
       "      <td>0.837957</td>\n",
       "      <td>0.837829</td>\n",
       "      <td>0.835904</td>\n",
       "      <td>0.836652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.239200</td>\n",
       "      <td>0.461928</td>\n",
       "      <td>0.836636</td>\n",
       "      <td>0.835647</td>\n",
       "      <td>0.836171</td>\n",
       "      <td>0.835881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>0.466880</td>\n",
       "      <td>0.837076</td>\n",
       "      <td>0.836150</td>\n",
       "      <td>0.836321</td>\n",
       "      <td>0.836233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.467545</td>\n",
       "      <td>0.836636</td>\n",
       "      <td>0.835644</td>\n",
       "      <td>0.836692</td>\n",
       "      <td>0.836030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.239300</td>\n",
       "      <td>0.455312</td>\n",
       "      <td>0.836636</td>\n",
       "      <td>0.836524</td>\n",
       "      <td>0.834543</td>\n",
       "      <td>0.835307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.231500</td>\n",
       "      <td>0.467336</td>\n",
       "      <td>0.836636</td>\n",
       "      <td>0.835657</td>\n",
       "      <td>0.836106</td>\n",
       "      <td>0.835861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>0.469916</td>\n",
       "      <td>0.835755</td>\n",
       "      <td>0.835126</td>\n",
       "      <td>0.834309</td>\n",
       "      <td>0.834670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.221700</td>\n",
       "      <td>0.470696</td>\n",
       "      <td>0.836636</td>\n",
       "      <td>0.835698</td>\n",
       "      <td>0.835910</td>\n",
       "      <td>0.835800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.217300</td>\n",
       "      <td>0.481082</td>\n",
       "      <td>0.837517</td>\n",
       "      <td>0.836560</td>\n",
       "      <td>0.837773</td>\n",
       "      <td>0.836966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.479815</td>\n",
       "      <td>0.834875</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>0.835051</td>\n",
       "      <td>0.834298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.221500</td>\n",
       "      <td>0.477556</td>\n",
       "      <td>0.834434</td>\n",
       "      <td>0.833434</td>\n",
       "      <td>0.834446</td>\n",
       "      <td>0.833812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.224200</td>\n",
       "      <td>0.479331</td>\n",
       "      <td>0.833554</td>\n",
       "      <td>0.832579</td>\n",
       "      <td>0.832909</td>\n",
       "      <td>0.832734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>0.476483</td>\n",
       "      <td>0.835315</td>\n",
       "      <td>0.834311</td>\n",
       "      <td>0.834940</td>\n",
       "      <td>0.834584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.217800</td>\n",
       "      <td>0.478168</td>\n",
       "      <td>0.834434</td>\n",
       "      <td>0.833495</td>\n",
       "      <td>0.833664</td>\n",
       "      <td>0.833577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.205500</td>\n",
       "      <td>0.483325</td>\n",
       "      <td>0.835315</td>\n",
       "      <td>0.834304</td>\n",
       "      <td>0.835136</td>\n",
       "      <td>0.834641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.482728</td>\n",
       "      <td>0.834875</td>\n",
       "      <td>0.833871</td>\n",
       "      <td>0.834465</td>\n",
       "      <td>0.834131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>0.481721</td>\n",
       "      <td>0.835315</td>\n",
       "      <td>0.834306</td>\n",
       "      <td>0.835005</td>\n",
       "      <td>0.834603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.217600</td>\n",
       "      <td>0.479956</td>\n",
       "      <td>0.833994</td>\n",
       "      <td>0.833004</td>\n",
       "      <td>0.833449</td>\n",
       "      <td>0.833207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>0.480671</td>\n",
       "      <td>0.835755</td>\n",
       "      <td>0.834747</td>\n",
       "      <td>0.835481</td>\n",
       "      <td>0.835055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.479153</td>\n",
       "      <td>0.834875</td>\n",
       "      <td>0.833871</td>\n",
       "      <td>0.834465</td>\n",
       "      <td>0.834131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.202700</td>\n",
       "      <td>0.479095</td>\n",
       "      <td>0.835315</td>\n",
       "      <td>0.834311</td>\n",
       "      <td>0.834940</td>\n",
       "      <td>0.834584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.479470</td>\n",
       "      <td>0.834434</td>\n",
       "      <td>0.833433</td>\n",
       "      <td>0.833990</td>\n",
       "      <td>0.833679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2839' max='2271' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2271/2271 01:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Run 2/5 (Seed: 43)\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,129,538 || all params: 568,022,020 || trainable%: 1.4312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4064241/2168342842.py:44: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2974' max='7100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2974/7100 27:11 < 37:44, 1.82 it/s, Epoch 10.47/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.701200</td>\n",
       "      <td>0.693137</td>\n",
       "      <td>0.536768</td>\n",
       "      <td>0.268384</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.349284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.683900</td>\n",
       "      <td>0.659368</td>\n",
       "      <td>0.611185</td>\n",
       "      <td>0.608087</td>\n",
       "      <td>0.606108</td>\n",
       "      <td>0.606101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.662100</td>\n",
       "      <td>0.642520</td>\n",
       "      <td>0.622193</td>\n",
       "      <td>0.624957</td>\n",
       "      <td>0.625152</td>\n",
       "      <td>0.622180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.625400</td>\n",
       "      <td>0.601718</td>\n",
       "      <td>0.660062</td>\n",
       "      <td>0.662482</td>\n",
       "      <td>0.662902</td>\n",
       "      <td>0.660027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.527800</td>\n",
       "      <td>0.513759</td>\n",
       "      <td>0.745927</td>\n",
       "      <td>0.747233</td>\n",
       "      <td>0.740671</td>\n",
       "      <td>0.741832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.505200</td>\n",
       "      <td>0.516793</td>\n",
       "      <td>0.749009</td>\n",
       "      <td>0.757584</td>\n",
       "      <td>0.740482</td>\n",
       "      <td>0.741503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.488400</td>\n",
       "      <td>0.484561</td>\n",
       "      <td>0.760458</td>\n",
       "      <td>0.770747</td>\n",
       "      <td>0.751733</td>\n",
       "      <td>0.752985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.480800</td>\n",
       "      <td>0.458791</td>\n",
       "      <td>0.778071</td>\n",
       "      <td>0.777265</td>\n",
       "      <td>0.778558</td>\n",
       "      <td>0.777501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.452400</td>\n",
       "      <td>0.439920</td>\n",
       "      <td>0.785997</td>\n",
       "      <td>0.785247</td>\n",
       "      <td>0.783727</td>\n",
       "      <td>0.784308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.462600</td>\n",
       "      <td>0.448168</td>\n",
       "      <td>0.783796</td>\n",
       "      <td>0.793393</td>\n",
       "      <td>0.775946</td>\n",
       "      <td>0.777835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.456000</td>\n",
       "      <td>0.424846</td>\n",
       "      <td>0.798327</td>\n",
       "      <td>0.797437</td>\n",
       "      <td>0.798728</td>\n",
       "      <td>0.797753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.414300</td>\n",
       "      <td>0.426517</td>\n",
       "      <td>0.802730</td>\n",
       "      <td>0.801626</td>\n",
       "      <td>0.801853</td>\n",
       "      <td>0.801734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.422300</td>\n",
       "      <td>0.438880</td>\n",
       "      <td>0.791281</td>\n",
       "      <td>0.796248</td>\n",
       "      <td>0.785263</td>\n",
       "      <td>0.787134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.421200</td>\n",
       "      <td>0.433230</td>\n",
       "      <td>0.798327</td>\n",
       "      <td>0.803128</td>\n",
       "      <td>0.792542</td>\n",
       "      <td>0.794472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.399900</td>\n",
       "      <td>0.425584</td>\n",
       "      <td>0.803170</td>\n",
       "      <td>0.802366</td>\n",
       "      <td>0.803761</td>\n",
       "      <td>0.802656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.421376</td>\n",
       "      <td>0.808895</td>\n",
       "      <td>0.808215</td>\n",
       "      <td>0.807009</td>\n",
       "      <td>0.807504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.422702</td>\n",
       "      <td>0.809335</td>\n",
       "      <td>0.809525</td>\n",
       "      <td>0.806443</td>\n",
       "      <td>0.807462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.419687</td>\n",
       "      <td>0.806253</td>\n",
       "      <td>0.805157</td>\n",
       "      <td>0.805916</td>\n",
       "      <td>0.805460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.431481</td>\n",
       "      <td>0.805812</td>\n",
       "      <td>0.805047</td>\n",
       "      <td>0.804008</td>\n",
       "      <td>0.804444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>0.416281</td>\n",
       "      <td>0.811537</td>\n",
       "      <td>0.813908</td>\n",
       "      <td>0.807191</td>\n",
       "      <td>0.808855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.358500</td>\n",
       "      <td>0.423342</td>\n",
       "      <td>0.812417</td>\n",
       "      <td>0.811332</td>\n",
       "      <td>0.811984</td>\n",
       "      <td>0.811606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.412326</td>\n",
       "      <td>0.812417</td>\n",
       "      <td>0.813822</td>\n",
       "      <td>0.808663</td>\n",
       "      <td>0.810098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>0.440237</td>\n",
       "      <td>0.818142</td>\n",
       "      <td>0.817252</td>\n",
       "      <td>0.816860</td>\n",
       "      <td>0.817043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>0.417499</td>\n",
       "      <td>0.820343</td>\n",
       "      <td>0.819687</td>\n",
       "      <td>0.818650</td>\n",
       "      <td>0.819090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.414375</td>\n",
       "      <td>0.823426</td>\n",
       "      <td>0.825023</td>\n",
       "      <td>0.819764</td>\n",
       "      <td>0.821260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.447962</td>\n",
       "      <td>0.817701</td>\n",
       "      <td>0.817718</td>\n",
       "      <td>0.819445</td>\n",
       "      <td>0.817458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>0.418815</td>\n",
       "      <td>0.821664</td>\n",
       "      <td>0.822456</td>\n",
       "      <td>0.818514</td>\n",
       "      <td>0.819751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>0.432967</td>\n",
       "      <td>0.820784</td>\n",
       "      <td>0.820795</td>\n",
       "      <td>0.818279</td>\n",
       "      <td>0.819178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.308700</td>\n",
       "      <td>0.457521</td>\n",
       "      <td>0.824747</td>\n",
       "      <td>0.825115</td>\n",
       "      <td>0.826854</td>\n",
       "      <td>0.824566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_RUNS = 5\n",
    "seeds = [42 + i for i in range(N_RUNS)]\n",
    "all_metrics = {\n",
    "    \"Train\": {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []},\n",
    "    \"Validation\": {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []},\n",
    "    \"Test\": {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []},\n",
    "}\n",
    "\n",
    "print(f\"Starting experiment with N={N_RUNS} runs...\")\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(f\"Run {i+1}/{N_RUNS} (Seed: {seed})\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # 1. Re-build model to ensure fresh weights\n",
    "    # We use the global configuration variables\n",
    "    model = build_model(\n",
    "        base_model=base_model,\n",
    "        num_labels=len(label2id),\n",
    "        device=device,\n",
    "        bf16=TRAIN_BF16,\n",
    "        fp16=TRAIN_FP16,\n",
    "        train_head_only=TRAIN_HEAD_ONLY,\n",
    "        lora_r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        lora_target_modules=[\"query\", \"key\", \"value\", \"dense\"]\n",
    "    )\n",
    "    model.config.label2id = label2id\n",
    "    model.config.id2label = id2label\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "        \n",
    "    # 2. Update Training Arguments with new seed\n",
    "    # We update the global kwargs dictionary and recreate TrainingArguments\n",
    "    set_kw(\"seed\", seed)\n",
    "    set_kw(\"output_dir\", str(OUTPUT_DIR / f\"run_{seed}\")) \n",
    "    training_args = TrainingArguments(**kwargs)\n",
    "    \n",
    "    # 3. Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized.train,\n",
    "        eval_dataset=tokenized.validation,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # 4. Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # 5. Evaluate\n",
    "    for split_name, dataset in [(\"Train\", tokenized.train), (\"Validation\", tokenized.validation), (\"Test\", tokenized.test)]:\n",
    "        metrics = trainer.evaluate(eval_dataset=dataset)\n",
    "        # Extract metrics (remove 'eval_' prefix)\n",
    "        all_metrics[split_name][\"accuracy\"].append(metrics.get(\"eval_accuracy\", 0))\n",
    "        all_metrics[split_name][\"precision\"].append(metrics.get(\"eval_precision\", 0))\n",
    "        all_metrics[split_name][\"recall\"].append(metrics.get(\"eval_recall\", 0))\n",
    "        all_metrics[split_name][\"f1\"].append(metrics.get(\"eval_f1\", 0))\n",
    "\n",
    "# Calculate and Print Statistics\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"RESULTS (N={N_RUNS})\")\n",
    "print(f\"{'SPLIT':<15} {'ACCURACY':<25} {'PRECISION':<25} {'RECALL':<25} {'F1':<25}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for split_name in [\"Train\", \"Validation\", \"Test\"]:\n",
    "    row_str = f\"{split_name:<15} \"\n",
    "    for metric in [\"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
    "        values = all_metrics[split_name][metric]\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        # Format: Mean ± Std\n",
    "        row_str += f\"{mean:.4f} ± {std:.4f}   \".ljust(25)\n",
    "    print(row_str)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb55b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Training Visualization (Last Run)\n",
    "# ==========================================\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "except ImportError:\n",
    "    print(\"Installing matplotlib and seaborn...\")\n",
    "    !pip install matplotlib seaborn\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Set style for better aesthetics\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Retrieve logs from the trainer (This will be from the LAST run in the loop)\n",
    "history = trainer.state.log_history\n",
    "df = pd.DataFrame(history)\n",
    "\n",
    "# Separate training and evaluation logs\n",
    "# Training logs usually have 'loss' but no 'eval_loss'\n",
    "# Eval logs have 'eval_loss'\n",
    "train_df = df[df['loss'].notna()].dropna(subset=['loss'])\n",
    "eval_df = df[df['eval_loss'].notna()].dropna(subset=['eval_loss'])\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Loss Curves\n",
    "sns.lineplot(ax=axes[0], data=train_df, x='step', y='loss', label='Training Loss', alpha=0.7)\n",
    "if not eval_df.empty:\n",
    "    sns.lineplot(ax=axes[0], data=eval_df, x='step', y='eval_loss', label='Validation Loss', marker='o', linestyle='--')\n",
    "\n",
    "axes[0].set_title('Training and Validation Loss (Last Run)')\n",
    "axes[0].set_xlabel('Steps')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Metrics\n",
    "if not eval_df.empty:\n",
    "    # Select metrics to plot\n",
    "    metrics = ['eval_accuracy', 'eval_f1', 'eval_precision', 'eval_recall']\n",
    "    available_metrics = [m for m in metrics if m in eval_df.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        # Melt dataframe for easier plotting with seaborn\n",
    "        eval_melted = eval_df.melt(id_vars=['step'], value_vars=available_metrics, var_name='Metric', value_name='Score')\n",
    "        # Clean up metric names for legend (remove 'eval_' prefix)\n",
    "        eval_melted['Metric'] = eval_melted['Metric'].str.replace('eval_', '').str.capitalize()\n",
    "        \n",
    "        sns.lineplot(ax=axes[1], data=eval_melted, x='step', y='Score', hue='Metric', marker='o')\n",
    "        axes[1].set_title('Validation Metrics (Last Run)')\n",
    "        axes[1].set_xlabel('Steps')\n",
    "        axes[1].set_ylabel('Score')\n",
    "        axes[1].set_ylim(0, 1.05) # Metrics are usually between 0 and 1\n",
    "        axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'No standard metrics found in logs', ha='center')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No validation data available', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai351",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
